{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate GluNet on OhioT1DM\n",
    "\n",
    "GluNet was mainly reported as a personalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the Glucose, Meal, and Insulin data\n",
    "\n",
    "For cgm data, do general seperation based on time interval = 1hour:\n",
    "\n",
    "smaller than 1 hour, do spline interpolation (later)\n",
    "\n",
    "larger than 1 hour, seperate it as another dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../OhioT1DM/2018/train/570-ws-training.xml\"\n",
    "tree = ET.parse(filepath)\n",
    "root = tree.getroot()\n",
    "\n",
    "for child in root:\n",
    "    print(child.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 5\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set the \n",
    "def read_ohio(filepath, category, round):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # interval_timedelta = datetime.timedelta(minutes=interval_timedelta)\n",
    "\n",
    "    res = []\n",
    "    for item in root.findall(category):\n",
    "        entry0 = item[0].attrib\n",
    "        if round == True:\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts'])\n",
    "            entry0['ts'] = adjusted_ts\n",
    "        ts = entry0['ts']\n",
    "        entry0['ts'] = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        res.append([entry0])\n",
    "        for i in range(1, len(item)):\n",
    "            # last_entry = item[i - 1].attrib\n",
    "            entry = item[i].attrib\n",
    "            # t1 = datetime.datetime.strptime(entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # t0 = datetime.datetime.strptime(last_entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # delt = t1 - t0\n",
    "            # if category == \"glucose_level\":\n",
    "            #     if delt <= interval_timedelta:\n",
    "            #         res[-1].append([entry])\n",
    "            #     else:\n",
    "            #         res.append([entry])\n",
    "            # else:\n",
    "            ts = entry['ts']\n",
    "            if round == True:\n",
    "                adjusted_ts = round_up_to_nearest_five_minutes(ts)\n",
    "                entry['ts'] = adjusted_ts\n",
    "            entry['ts'] = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "            res.append([entry])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../OhioT1DM/2018/train/570-ws-training.xml\"\n",
    "glucose = read_ohio(filepath, \"glucose_level\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold filter\n",
    "\n",
    "The paper didn't report how did they do the threshold filtering, and didn't provide the filter value. So in this case, to avoid bias, we should? not conduct any threshold filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline interpolation and extrapolation\n",
    ": Spline interpolation or extrapolation technique are used when the missing CGM data are less than one\n",
    "hour (12 samples). Spline interpolation is implemented because\n",
    "it is more accurate, and simpler than some other methods [24].\n",
    "If the missing CMG data are longer than 1 hour, we consider it\n",
    "as a separate dataset in training. In the inference, extrapolation\n",
    "is adopted because future samples cannot be used in forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array([1,2,3,7,8,9])\n",
    "y = np.sin(x)\n",
    "cs = CubicSpline(x, y)\n",
    "xs = np.arange(-0.5, 9.6, 0.1)\n",
    "fig, ax = plt.subplots(figsize=(6.5, 4))\n",
    "ax.plot(x, y, 'o', label='data')\n",
    "ax.plot(xs, np.sin(xs), label='true')\n",
    "ax.plot(xs, cs(xs), label=\"S\")\n",
    "ax.plot(xs, cs(xs, 1), label=\"S'\")\n",
    "ax.plot(xs, cs(xs, 2), label=\"S''\")\n",
    "ax.plot(xs, cs(xs, 3), label=\"S'''\")\n",
    "ax.set_xlim(-0.5, 9.5)\n",
    "ax.legend(loc='lower left', ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame setup\n",
    "# data = {\n",
    "#     'timestamp': pd.to_datetime([\n",
    "#         '2021-12-07 01:17:00', '2021-12-07 01:22:00', '2021-12-07 01:27:00', '2021-12-07 01:32:00', '2021-12-07 01:37:00',\n",
    "#         '2021-12-07 04:00:00',  # Large gap example\n",
    "#         '2021-12-07 06:00:00',  # Another large gap example\n",
    "#         '2022-01-17 23:36:00', '2022-01-17 23:41:00', '2022-01-17 23:46:00', '2022-01-17 23:51:00', '2022-01-17 23:56:00'\n",
    "#     ]),\n",
    "#     'glucose_value': [101, 98, 104, 112, 120, 130, 135, 161, 164, 168, 172, 176]\n",
    "# }\n",
    "# glucose_df\n",
    "\n",
    "def segement_data_as_1hour(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=1)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# Example: print each segment\n",
    "segments = segement_data_as_1hour(glucose_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = \"segment_3\"\n",
    "\n",
    "# for ts in range(len(segments[sequence]['timestamp'])-1):\n",
    "#     if segments[sequence]['timestamp'][ts+1] - segments[sequence]['timestamp'][ts] > timedelta(minutes = 5):\n",
    "#         print(\"before: \", segments[sequence]['timestamp'][ts])\n",
    "#         print(\"after: \", segments[sequence]['timestamp'][ts+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments['segment_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_time = min(segments[sequence]['timestamp'])\n",
    "\n",
    "# # Convert datetime objects to the number of seconds since the reference time\n",
    "# datetime_seconds_since_start = [((dt - reference_time).total_seconds())/60 for dt in datetime_list] # Make it into minute\n",
    "# original_timestamp_in_segement = [((dt - reference_time).total_seconds())/60 for dt in segments[sequence]['timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.interpolate import CubicSpline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = original_timestamp_in_segement\n",
    "# y = np.array(segments['segment_3']['glucose_value'])\n",
    "# cs = CubicSpline(x, y)\n",
    "# xs = datetime_seconds_since_start\n",
    "\n",
    "# interpolated_xs = cs(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate datetime index for each entry in the numpy array\n",
    "# time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
    "\n",
    "# # Create DataFrame from the time index and glucose values\n",
    "# df_interpolated = pd.DataFrame({'timestamp': time_index_interpolated, 'glucose_value': interpolated_xs})\n",
    "\n",
    "# # Print the DataFrame to verify\n",
    "# print(df_interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_and_spline_interpolate(segments):\n",
    "    for sequence in segments:\n",
    "        # sequence = \"segment_3\"\n",
    "        detected_missing = 0\n",
    "        for ts in range(len( segments[sequence]['timestamp'])-1):\n",
    "            if segments[sequence]['timestamp'][ts+1] - segments[sequence]['timestamp'][ts] > timedelta(minutes = 6):\n",
    "                print(sequence)\n",
    "                print(\"before: \", segments[sequence]['timestamp'][ts])\n",
    "                print(\"after: \", segments[sequence]['timestamp'][ts+1])\n",
    "                detected_missing = 1\n",
    "            \n",
    "        if detected_missing == 1:\n",
    "            datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
    "            reference_time = min(segments[sequence]['timestamp'])\n",
    "\n",
    "            # Convert datetime objects to the number of seconds since the reference time\n",
    "            datetime_seconds_since_start = [((dt - reference_time).total_seconds())/60 for dt in datetime_list] # Make it into minute\n",
    "            original_timestamp_in_segement = [((dt - reference_time).total_seconds())/60 for dt in segments[sequence]['timestamp']]\n",
    "\n",
    "            x = original_timestamp_in_segement\n",
    "            y = np.array(segments[sequence]['glucose_value'])\n",
    "            cs = CubicSpline(x, y)\n",
    "            xs = datetime_seconds_since_start\n",
    "\n",
    "            interpolated_xs = cs(xs)\n",
    "            time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
    "\n",
    "            # Create DataFrame from the time index and glucose values\n",
    "            df_interpolated = pd.DataFrame({'timestamp': time_index_interpolated, 'glucose_value': interpolated_xs})\n",
    "            segments[sequence] = df_interpolated\n",
    "\n",
    "    return segments\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_segements['segment_2'][\"timestamp\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align other factors with the glucose information\n",
    "\n",
    "## Include meal info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = read_ohio(filepath, \"meal\", False)\n",
    "\n",
    "flattened_meal_data = [item[0] for item in meal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "meal_df = pd.DataFrame(flattened_meal_data)\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "meal_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carbs'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carbs'] == closest_meal['carbs']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carbs'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carbs'] = closest_meal['carbs']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments\n",
    "\n",
    "        # # Iterate through each timestamp in the segment\n",
    "        # for i, row in segment_df.iterrows():\n",
    "        #     # Find the closest meal timestamp and its carb information\n",
    "        #     unassigned_meals = meal_df[meal_df['assigned'] == False]\n",
    "        #     if not unassigned_meals.empty:\n",
    "        #         unassigned_meals['time_difference'] = abs(unassigned_meals['ts'] - row['timestamp'])\n",
    "        #         closest_meal = unassigned_meals.loc[unassigned_meals['time_difference'].idxmin()]\n",
    "\n",
    "        #         if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "        #             segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
    "        #             meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Update the segments with meal data\n",
    "meal_updated_segments = update_segments_with_meals(interpolated_segements, meal_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include the basal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal = read_ohio(filepath, \"basal\", False)\n",
    "\n",
    "flattened_basal_data = [item[0] for item in basal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "basal_df = pd.DataFrame(flattened_basal_data)\n",
    "\n",
    "basal_df['assigned'] = False\n",
    "basal_df['end_ts'] = basal_df['ts'].shift(-1)\n",
    "basal_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Update the segments with meal data\n",
    "basal_updated_segments = update_segments_with_basal(meal_updated_segments, basal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal_updated_segments['segment_1'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def read_ohio_bolus_tempbasal(filepath, category, round):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # interval_timedelta = datetime.timedelta(minutes=interval_timedelta)\n",
    "\n",
    "    res = []\n",
    "    for item in root.findall(category):\n",
    "        entry0 = item[0].attrib\n",
    "        if round == True:\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts_begin'])\n",
    "            entry0['ts_begin'] = adjusted_ts\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts_end'])\n",
    "            entry0['ts_end'] = adjusted_ts\n",
    "        \n",
    "        entry0['ts_begin'] = datetime.strptime(entry0['ts_begin'], \"%d-%m-%Y %H:%M:%S\")\n",
    "        entry0['ts_end'] = datetime.strptime(entry0['ts_end'], \"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "        res.append([entry0])\n",
    "        for i in range(1, len(item)):\n",
    "            # last_entry = item[i - 1].attrib\n",
    "            entry = item[i].attrib\n",
    "            # t1 = datetime.datetime.strptime(entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # t0 = datetime.datetime.strptime(last_entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # delt = t1 - t0\n",
    "            # if category == \"glucose_level\":\n",
    "            #     if delt <= interval_timedelta:\n",
    "            #         res[-1].append([entry])\n",
    "            #     else:\n",
    "            #         res.append([entry])\n",
    "            # else:\n",
    "            ts_begin = entry['ts_begin']\n",
    "            ts_end = entry['ts_end']\n",
    "            if round == True:\n",
    "                adjusted_ts_begin = round_up_to_nearest_five_minutes(ts_begin)\n",
    "                entry['ts_end'] = adjusted_ts_begin\n",
    "                adjusted_ts_end = round_up_to_nearest_five_minutes(ts_end)\n",
    "                entry['ts_end'] = adjusted_ts_end\n",
    "            entry['ts_begin'] = datetime.strptime(entry['ts_begin'], \"%d-%m-%Y %H:%M:%S\")\n",
    "            entry['ts_end'] = datetime.strptime(entry['ts_end'], \"%d-%m-%Y %H:%M:%S\")\n",
    "            if category == \"bolus\":\n",
    "                if entry['ts_begin'] != entry['ts_end']:\n",
    "                    print(\"Unequal: begin: \" + str(entry['ts_begin']) + \"end: \" + str(entry['ts_end']))\n",
    "            res.append([entry])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Bolus into the dataframe\n",
    "bolus = read_ohio_bolus_tempbasal(filepath, \"bolus\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_bolus_data = [item[0] for item in bolus]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "bolus_df = pd.DataFrame(flattened_bolus_data)\n",
    "\n",
    "bolus_df['assigned'] = False\n",
    "bolus_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts_begin'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['dose']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts_begin'] < closest_bolus['ts_begin'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments\n",
    "# def update_segments_with_bolus(segments, bolus_df):\n",
    "#     for segment_name, segment_df in segments.items():\n",
    "#         # Initialize the 'bolus' column to zeros\n",
    "#         segment_df['bolus_dose'] = 0\n",
    "\n",
    "#         # Iterate through each timestamp in the segment\n",
    "#         for i, row in segment_df.iterrows():\n",
    "#             # Find the closest meal timestamp and its carb information\n",
    "#             unassigned_bolus = bolus_df[bolus_df['assigned'] == False]\n",
    "#             if not unassigned_bolus.empty:\n",
    "#                 unassigned_bolus['time_difference'] = abs(unassigned_bolus['ts_begin'] - row['timestamp'])\n",
    "#                 closest_bolus = unassigned_bolus.loc[unassigned_bolus['time_difference'].idxmin()]\n",
    "\n",
    "#                 if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "#                     segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
    "#                     bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "\n",
    "\n",
    "#     return segments\n",
    "\n",
    "# Update the segments with meal data\n",
    "bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, bolus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolus_updated_segments['segment_1'][100:150]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include temp_basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempbasal = read_ohio_bolus_tempbasal(filepath, \"temp_basal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_tempbasal_data = [item[0] for item in tempbasal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "tempbasal_df = pd.DataFrame(flattened_tempbasal_data)\n",
    "\n",
    "tempbasal_df['assigned'] = False\n",
    "tempbasal_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_tempbasal(segments, tempbasal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        # segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, tempbasal_row in tempbasal_df.iterrows():\n",
    "                if tempbasal_row['ts_begin'] <= row['timestamp'] < tempbasal_row['ts_end']:\n",
    "                    segment_df.at[i, 'basal_rate'] = tempbasal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Update the segments with meal data\n",
    "final_updated_segments = update_segments_with_tempbasal(bolus_updated_segments, tempbasal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_updated_segments['segment_1'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_updated_segments['segment_1']['bolus_dose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct X and y, training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(segments):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        max_index = len(segment_df) - 22  # Subtracting 22 because we need to predict index + 21 and need index + 15 to exist\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index + 1):\n",
    "            # Extracting features from index i to i+15\n",
    "            features = segment_df.loc[i:i+15, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values.flatten()\n",
    "            # Extracting label for index i+21\n",
    "            # Do the label transform\n",
    "            label = segment_df.loc[i+21, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            \n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    labels_list = label_delta_transform(labels_list)    \n",
    "    print(\"after label transform. the len of label list \"+str(len(labels_list)))    \n",
    "    return features_list, labels_list\n",
    "    \n",
    "    # # Convert lists to PyTorch tensors\n",
    "    # features_tensor = torch.tensor(features_list, dtype=torch.float32)\n",
    "    # labels_tensor = torch.tensor(labels_list, dtype=torch.float32).unsqueeze(1)  # Making labels tensor 2D\n",
    "    \n",
    "    # return TensorDataset(features_tensor, labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list, labels_list = prepare_dataset(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels_list, bins= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper, they use -12.7 and 12.8 as the range to assign 1 to 256 to each label. I'm curious is the range fixed or can be adjusted based on the actual range of delta_G.\n",
    "\n",
    "To avoid obscure issue when recover the label, we use 10 and 90 percentile as the range instead of -12.7 and 12.8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dilate CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "Still in old pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to PyTorch tensors\n",
    "features_tensor = torch.tensor(features_list, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_list, dtype=torch.float32).unsqueeze(1)  # Making labels tensor 2D\n",
    "\n",
    "feature_label_tensor = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(feature_label_tensor, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example of using DataLoader in a training loop\n",
    "for features, labels in train_loader:\n",
    "    print(\"Features batch shape:\", features.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    # Example: print(features, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from wavenet_model import *\n",
    "# from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 1.X. \n",
    "\n",
    "This version is too old and hard to understand and implement. So currently deprecated this part in this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'features_list' and 'labels_list' are already defined and are numpy arrays\n",
    "features_tensor = tf.convert_to_tensor(features_list, dtype=tf.float32)\n",
    "labels_tensor = tf.convert_to_tensor(labels_list, dtype=tf.float32)\n",
    "labels_tensor = tf.expand_dims(labels_tensor, axis=1)  # Making labels tensor 2D\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and shuffle buffer size\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000  # Adjust this number based on the size of your dataset\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow iterator\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Example of using the iterator in a training loop\n",
    "features, labels = next(iterator)\n",
    "print(\"Features batch shape:\", features.shape)\n",
    "print(\"Label batch shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly use the dataset in a loop\n",
    "for features, labels in dataset.take(1):  # The 'take(1)' method fetches one batch\n",
    "    print(\"Features batch shape:\", features.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    # Example: print(features.numpy(), labels.numpy())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model, loss, and optimizer (assuming a simple model for demonstration)\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(10, activation='relu', input_shape=(features.shape[-1],)),\n",
    "#     tf.keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# # Training step\n",
    "# for features, labels in dataset:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(features, training=True)\n",
    "#         loss = loss_object(labels, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     print(\"Current loss:\", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "from wavenet import WaveNetModel, optimizer_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "# DATA_DIRECTORY = './VCTK-Corpus'\n",
    "LOGDIR_ROOT = './logdir'\n",
    "CHECKPOINT_EVERY = 50\n",
    "NUM_STEPS = int(1e5)\n",
    "LEARNING_RATE = 1e-3\n",
    "WAVENET_PARAMS = './wavenet_params.json'\n",
    "STARTED_DATESTRING = \"{0:%Y-%m-%dT%H-%M-%S}\".format(datetime.now())\n",
    "SAMPLE_SIZE = 100000\n",
    "L2_REGULARIZATION_STRENGTH = 0\n",
    "SILENCE_THRESHOLD = 0.3\n",
    "EPSILON = 0.001\n",
    "MOMENTUM = 0.9\n",
    "MAX_TO_KEEP = 5\n",
    "METADATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    class Args:\n",
    "        batch_size = 1\n",
    "        store_metadata = False\n",
    "        logdir = None  # Directly specify if needed or use logdir_root\n",
    "        logdir_root = './logdir'\n",
    "        restore_from = None\n",
    "        checkpoint_every = 50\n",
    "        num_steps = int(1e5)\n",
    "        learning_rate = 1e-3\n",
    "        wavenet_params = './wavenet_params.json'\n",
    "        started_datestring = \"{0:%Y-%m-%dT%H-%M-%S}\".format(datetime.now())\n",
    "        sample_size = 100000\n",
    "        l2_regularization_strength = 0\n",
    "        silence_threshold = 0.3\n",
    "        optimizer = 'adam'\n",
    "        momentum = 0.9\n",
    "        histograms = False\n",
    "        gc_channels = None\n",
    "        max_checkpoints = 5\n",
    "\n",
    "    return Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_arguments():\n",
    "#     def _str_to_bool(s):\n",
    "#         \"\"\"Convert string to bool (in argparse context).\"\"\"\n",
    "#         if s.lower() not in ['true', 'false']:\n",
    "#             raise ValueError('Argument needs to be a '\n",
    "#                              'boolean, got {}'.format(s))\n",
    "#         return {'true': True, 'false': False}[s.lower()]\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='WaveNet example network')\n",
    "#     parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,\n",
    "#                         help='How many wav files to process at once. Default: ' + str(BATCH_SIZE) + '.')\n",
    "#     # parser.add_argument('--data_dir', type=str, default=DATA_DIRECTORY,\n",
    "#     #                     help='The directory containing the VCTK corpus.')\n",
    "#     parser.add_argument('--store_metadata', type=bool, default=METADATA,\n",
    "#                         help='Whether to store advanced debugging information '\n",
    "#                         '(execution time, memory consumption) for use with '\n",
    "#                         'TensorBoard. Default: ' + str(METADATA) + '.')\n",
    "#     parser.add_argument('--logdir', type=str, default=None,\n",
    "#                         help='Directory in which to store the logging '\n",
    "#                         'information for TensorBoard. '\n",
    "#                         'If the model already exists, it will restore '\n",
    "#                         'the state and will continue training. '\n",
    "#                         'Cannot use with --logdir_root and --restore_from.')\n",
    "#     parser.add_argument('--logdir_root', type=str, default=None,\n",
    "#                         help='Root directory to place the logging '\n",
    "#                         'output and generated model. These are stored '\n",
    "#                         'under the dated subdirectory of --logdir_root. '\n",
    "#                         'Cannot use with --logdir.')\n",
    "#     parser.add_argument('--restore_from', type=str, default=None,\n",
    "#                         help='Directory in which to restore the model from. '\n",
    "#                         'This creates the new model under the dated directory '\n",
    "#                         'in --logdir_root. '\n",
    "#                         'Cannot use with --logdir.')\n",
    "#     parser.add_argument('--checkpoint_every', type=int,\n",
    "#                         default=CHECKPOINT_EVERY,\n",
    "#                         help='How many steps to save each checkpoint after. Default: ' + str(CHECKPOINT_EVERY) + '.')\n",
    "#     parser.add_argument('--num_steps', type=int, default=NUM_STEPS,\n",
    "#                         help='Number of training steps. Default: ' + str(NUM_STEPS) + '.')\n",
    "#     parser.add_argument('--learning_rate', type=float, default=LEARNING_RATE,\n",
    "#                         help='Learning rate for training. Default: ' + str(LEARNING_RATE) + '.')\n",
    "#     parser.add_argument('--wavenet_params', type=str, default=WAVENET_PARAMS,\n",
    "#                         help='JSON file with the network parameters. Default: ' + WAVENET_PARAMS + '.')\n",
    "#     parser.add_argument('--sample_size', type=int, default=SAMPLE_SIZE,\n",
    "#                         help='Concatenate and cut audio samples to this many '\n",
    "#                         'samples. Default: ' + str(SAMPLE_SIZE) + '.')\n",
    "#     parser.add_argument('--l2_regularization_strength', type=float,\n",
    "#                         default=L2_REGULARIZATION_STRENGTH,\n",
    "#                         help='Coefficient in the L2 regularization. '\n",
    "#                         'Default: False')\n",
    "#     parser.add_argument('--silence_threshold', type=float,\n",
    "#                         default=SILENCE_THRESHOLD,\n",
    "#                         help='Volume threshold below which to trim the start '\n",
    "#                         'and the end from the training set samples. Default: ' + str(SILENCE_THRESHOLD) + '.')\n",
    "#     parser.add_argument('--optimizer', type=str, default='adam',\n",
    "#                         choices=optimizer_factory.keys(),\n",
    "#                         help='Select the optimizer specified by this option. Default: adam.')\n",
    "#     parser.add_argument('--momentum', type=float,\n",
    "#                         default=MOMENTUM, help='Specify the momentum to be '\n",
    "#                         'used by sgd or rmsprop optimizer. Ignored by the '\n",
    "#                         'adam optimizer. Default: ' + str(MOMENTUM) + '.')\n",
    "#     parser.add_argument('--histograms', type=_str_to_bool, default=False,\n",
    "#                         help='Whether to store histogram summaries. Default: False')\n",
    "#     parser.add_argument('--gc_channels', type=int, default=None,\n",
    "#                         help='Number of global condition channels. Default: None. Expecting: Int')\n",
    "#     parser.add_argument('--max_checkpoints', type=int, default=MAX_TO_KEEP,\n",
    "#                         help='Maximum amount of checkpoints that will be kept alive. Default: '\n",
    "#                              + str(MAX_TO_KEEP) + '.')\n",
    "#     return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')\n",
    "\n",
    "\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_default_logdir(logdir_root):\n",
    "    logdir = os.path.join(logdir_root, 'train', STARTED_DATESTRING)\n",
    "    return logdir\n",
    "\n",
    "\n",
    "def validate_directories(args):\n",
    "    \"\"\"Validate and arrange directory related arguments.\"\"\"\n",
    "\n",
    "    # Validation\n",
    "    if args.logdir and args.logdir_root:\n",
    "        raise ValueError(\"--logdir and --logdir_root cannot be \"\n",
    "                         \"specified at the same time.\")\n",
    "\n",
    "    if args.logdir and args.restore_from:\n",
    "        raise ValueError(\n",
    "            \"--logdir and --restore_from cannot be specified at the same \"\n",
    "            \"time. This is to keep your previous model from unexpected \"\n",
    "            \"overwrites.\\n\"\n",
    "            \"Use --logdir_root to specify the root of the directory which \"\n",
    "            \"will be automatically created with current date and time, or use \"\n",
    "            \"only --logdir to just continue the training from the last \"\n",
    "            \"checkpoint.\")\n",
    "\n",
    "    # Arrangement\n",
    "    logdir_root = args.logdir_root\n",
    "    if logdir_root is None:\n",
    "        logdir_root = LOGDIR_ROOT\n",
    "\n",
    "    logdir = args.logdir\n",
    "    if logdir is None:\n",
    "        logdir = get_default_logdir(logdir_root)\n",
    "        print('Using default logdir: {}'.format(logdir))\n",
    "\n",
    "    restore_from = args.restore_from\n",
    "    if restore_from is None:\n",
    "        # args.logdir and args.restore_from are exclusive,\n",
    "        # so it is guaranteed the logdir here is newly created.\n",
    "        restore_from = logdir\n",
    "\n",
    "    return {\n",
    "        'logdir': logdir,\n",
    "        'logdir_root': args.logdir_root,\n",
    "        'restore_from': restore_from\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_arguments()\n",
    "\n",
    "try:\n",
    "    directories = validate_directories(args)\n",
    "except ValueError as e:\n",
    "    print(\"Some arguments are wrong:\")\n",
    "    print(str(e))\n",
    "    \n",
    "\n",
    "logdir = directories['logdir']\n",
    "restore_from = directories['restore_from']\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "with open(args.wavenet_params, 'r') as f:\n",
    "    wavenet_params = json.load(f)\n",
    "\n",
    "# Create coordinator.\n",
    "coord = tf.train.Coordinator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network.\n",
    "net = WaveNetModel(\n",
    "    batch_size=args.batch_size,\n",
    "    dilations=wavenet_params[\"dilations\"],\n",
    "    filter_width=wavenet_params[\"filter_width\"],\n",
    "    residual_channels=wavenet_params[\"residual_channels\"],\n",
    "    dilation_channels=wavenet_params[\"dilation_channels\"],\n",
    "    skip_channels=wavenet_params[\"skip_channels\"],\n",
    "    quantization_channels=wavenet_params[\"quantization_channels\"],\n",
    "    use_biases=wavenet_params[\"use_biases\"],\n",
    "    scalar_input=wavenet_params[\"scalar_input\"],\n",
    "    initial_filter_width=wavenet_params[\"initial_filter_width\"],\n",
    "    histograms=args.histograms,\n",
    "    global_condition_channels=args.gc_channels,\n",
    "    global_condition_cardinality=None)\n",
    "\n",
    "if args.l2_regularization_strength == 0:\n",
    "    args.l2_regularization_strength = None\n",
    "loss = net.loss(input_batch=dataset,\n",
    "                global_condition_batch=None,\n",
    "                l2_regularization_strength=args.l2_regularization_strength)\n",
    "optimizer = optimizer_factory[args.optimizer](\n",
    "                learning_rate=args.learning_rate,\n",
    "                momentum=args.momentum)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_variable(name, shape):\n",
    "    \"\"\"Create a trainable variable.\"\"\"\n",
    "    return tf.Variable(initial_value=tf.random.normal(shape), name=name, trainable=True)\n",
    "\n",
    "def create_bias_variable(name, shape):\n",
    "    \"\"\"Create a bias variable initialized to zero.\"\"\"\n",
    "    return tf.Variable(initial_value=tf.zeros(shape), name=name, trainable=True)\n",
    "\n",
    "class WaveNetModel(tf.keras.Model):\n",
    "    def __init__(self, dilations, filter_width, residual_channels, dilation_channels, skip_channels, \n",
    "                 quantization_channels, use_biases=False, global_condition_channels=None, \n",
    "                 global_condition_cardinality=None, scalar_input=False, initial_filter_width=32, **kwargs):\n",
    "        super(WaveNetModel, self).__init__(**kwargs)\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.residual_channels = residual_channels\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.global_condition_channels = global_condition_channels\n",
    "        self.global_condition_cardinality = global_condition_cardinality\n",
    "\n",
    "        # Build model layers\n",
    "        self.build_layers()\n",
    "\n",
    "    def build_layers(self):\n",
    "        if self.global_condition_cardinality is not None:\n",
    "            self.gc_embedding = tf.keras.layers.Embedding(self.global_condition_cardinality,\n",
    "                                                          self.global_condition_channels,\n",
    "                                                          name='gc_embedding')\n",
    "\n",
    "        self.causal_layer = tf.keras.layers.Conv1D(filters=self.residual_channels,\n",
    "                                                   kernel_size=self.initial_filter_width if self.scalar_input else self.filter_width,\n",
    "                                                   padding='causal',\n",
    "                                                   use_bias=self.use_biases,\n",
    "                                                   name='causal_layer')\n",
    "\n",
    "        self.dilated_layers = []\n",
    "        for i, dilation in enumerate(self.dilations):\n",
    "            self.dilated_layers.append(\n",
    "                tf.keras.layers.Conv1D(filters=self.dilation_channels,\n",
    "                                       kernel_size=self.filter_width,\n",
    "                                       dilation_rate=dilation,\n",
    "                                       padding='causal',\n",
    "                                       activation='tanh',\n",
    "                                       use_bias=self.use_biases,\n",
    "                                       name=f'dilated_conv_{i}')\n",
    "            )\n",
    "\n",
    "        self.post_process1 = tf.keras.layers.Conv1D(filters=self.skip_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    activation='relu',\n",
    "                                                    use_bias=self.use_biases,\n",
    "                                                    name='postprocess1')\n",
    "        self.post_process2 = tf.keras.layers.Conv1D(filters=self.quantization_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    activation='relu',\n",
    "                                                    use_bias=self.use_biases,\n",
    "                                                    name='postprocess2')\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.causal_layer(inputs)\n",
    "        for layer in self.dilated_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.post_process1(x)\n",
    "        x = self.post_process2(x)\n",
    "        return x\n",
    "\n",
    "# Example instantiation and use\n",
    "model = WaveNetModel(dilations=[1, 2, 4], filter_width=2, residual_channels=32, dilation_channels=64,\n",
    "                     skip_channels=128, quantization_channels=256, use_biases=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
