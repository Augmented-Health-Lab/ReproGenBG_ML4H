{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate GluNet on T1DEXI\n",
    "\n",
    "GluNet was mainly reported as a personalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = 6\n",
    "history_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 5\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def preprocess_t1dexi_cgm(path, round):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject.groupby('LBCAT')\n",
    "    # Create a dictionary to store the split DataFrames\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "    new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "\n",
    "    new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    if round == True:\n",
    "        rounded_timestamp = []\n",
    "        for ts in new_df_cgm[\"LBDTC\"]:\n",
    "            rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "        new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "        formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "    else:\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "    \n",
    "    return formatted_data\n",
    "    \n",
    "    # # Assuming self.interval_timedelta is set, for example:\n",
    "    # interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # # Create a list to store the results\n",
    "    # res = []\n",
    "\n",
    "    # # Initialize the first group\n",
    "    # if not subject.empty:\n",
    "    #     current_group = [subject.iloc[0]['LBORRES']]\n",
    "    #     last_time = subject.iloc[0]['LBDTC']\n",
    "\n",
    "    # # Iterate over rows in DataFrame starting from the second row\n",
    "    # for index, row in subject.iloc[1:].iterrows():\n",
    "    #     current_time = row['LBDTC']\n",
    "    #     if (current_time - last_time) <= interval_timedelta:\n",
    "    #         # If the time difference is within the limit, add to the current group\n",
    "    #         current_group.append(row['LBORRES'])\n",
    "    #     else:\n",
    "    #         # Otherwise, start a new group\n",
    "    #         res.append(current_group)\n",
    "    #         current_group = [row['LBORRES']]\n",
    "    #     last_time = current_time\n",
    "\n",
    "    # # Add the last group if it's not empty\n",
    "    # if current_group:\n",
    "    #     res.append(current_group)\n",
    "    \n",
    "    # # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "\n",
    "\n",
    "    # return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_1hour(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=1)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_and_spline_interpolate(segments):\n",
    "    for sequence in segments:\n",
    "        # sequence = \"segment_3\"\n",
    "        detected_missing = 0\n",
    "        for ts in range(len( segments[sequence]['timestamp'])-1):\n",
    "            if segments[sequence]['timestamp'][ts+1] - segments[sequence]['timestamp'][ts] > timedelta(minutes = 6):\n",
    "                print(sequence)\n",
    "                print(\"before: \", segments[sequence]['timestamp'][ts])\n",
    "                print(\"after: \", segments[sequence]['timestamp'][ts+1])\n",
    "                detected_missing = 1\n",
    "            \n",
    "        if detected_missing == 1:\n",
    "            datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
    "            reference_time = min(segments[sequence]['timestamp'])\n",
    "\n",
    "            # Convert datetime objects to the number of seconds since the reference time\n",
    "            datetime_seconds_since_start = [((dt - reference_time).total_seconds())/60 for dt in datetime_list] # Make it into minute\n",
    "            original_timestamp_in_segement = [((dt - reference_time).total_seconds())/60 for dt in segments[sequence]['timestamp']]\n",
    "\n",
    "            x = original_timestamp_in_segement\n",
    "            y = np.array(segments[sequence]['glucose_value'])\n",
    "            cs = CubicSpline(x, y)\n",
    "            xs = datetime_seconds_since_start\n",
    "\n",
    "            interpolated_xs = cs(xs)\n",
    "            time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
    "\n",
    "            # Create DataFrame from the time index and glucose values\n",
    "            df_interpolated = pd.DataFrame({'timestamp': time_index_interpolated, 'glucose_value': interpolated_xs})\n",
    "            segments[sequence] = df_interpolated\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carbs'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carbs'] == closest_meal['carbs']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carbs'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carbs'] = closest_meal['carbs']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def preprocess_t1dexi_bolus_tempbasal(filepath, round):\n",
    "    subject_facm = pd.read_csv(filepath)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_bolus = split_dfs[\"BOLUS\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_bolus.reset_index(drop=True, inplace=True)\n",
    "    new_df_bolus = new_df_bolus.rename(columns={'FAORRES': 'dose', 'FADTC': 'ts_begin'})\n",
    "    new_df_bolus['assigned'] = False\n",
    "    # new_df_bolus['end_ts'] = new_df_bolus['ts_begin'].shift(-1)\n",
    "    return new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts_begin'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['dose']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts_begin'] < closest_bolus['ts_begin'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "# def prepare_dataset(segments, ph):\n",
    "#     '''\n",
    "#     ph = 6, 30 minutes ahead\n",
    "#     ph = 12, 60 minutes ahead\n",
    "#     '''\n",
    "#     features_list = []\n",
    "#     labels_list = []\n",
    "#     raw_glu_list = []\n",
    "    \n",
    "#     # Iterate over each segment\n",
    "#     for segment_name, segment_df in segments.items():\n",
    "#         # Ensure all columns are of numeric type\n",
    "#         segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "#         segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "#         segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "#         # Fill NaNs that might have been introduced by conversion errors\n",
    "#         segment_df.fillna(0, inplace=True)\n",
    "\n",
    "#         # Maximum index for creating a complete feature set\n",
    "#         max_index = len(segment_df) - (15+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + 15 to exist\n",
    "        \n",
    "#         # Iterate through the data to create feature-label pairs\n",
    "#         for i in range(max_index + 1):\n",
    "#             # Extracting features from index i to i+15\n",
    "#             features = segment_df.loc[i:i+15, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values#.flatten()\n",
    "#             # Extracting label for index i+21\n",
    "#             # Do the label transform\n",
    "#             label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "#             raw_glu_list.append(segment_df.loc[i+15+ph, 'glucose_value'])\n",
    "#             features_list.append(features)\n",
    "#             labels_list.append(label)\n",
    "            \n",
    "#     print(\"len of features_list \" + str(len(features_list)))\n",
    "#     print(\"len of labels_list \" + str(len(labels_list)))\n",
    "#     new_labels_list = label_delta_transform(labels_list)    \n",
    "#     print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "#     return features_list, labels_list, new_labels_list, raw_glu_list\n",
    "\n",
    "def prepare_dataset(segments, ph):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dilate CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation):\n",
    "        super(WaveNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=1+dilation - 2^(dilation-1))\n",
    "        self.conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=dilation)\n",
    "        self.res_conv = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape of x: \", x.shape)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        # print(\"shape of first out: \", out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        # print(\"shape of second out: \", out.shape)\n",
    "        res = self.res_conv(x)\n",
    "        # print(\"shape of res: \", res.shape)\n",
    "        return out + res\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, dilations):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv1d(in_channels, 32, kernel_size=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([WaveNetBlock(32, dilation) for dilation in dilations])\n",
    "        self.final_conv1 = nn.Conv1d(32, 128, kernel_size=2, padding=0)\n",
    "        self.final_conv2 = nn.Conv1d(128, 256, kernel_size=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        for block in self.blocks:\n",
    "            # print(\"enter the block loop\")\n",
    "            x = block(x)\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = F.relu(self.final_conv2(x))\n",
    "        x = x[:, :, -1]  # Get the last time step\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_channels = 4  # Number of features\n",
    "output_channels = 1  # Predicting a single value (glucose level)\n",
    "num_blocks = 4  # Number of WaveNet blocks\n",
    "dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "print(model)\n",
    "\n",
    "# Example of how to define the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']\n",
    "\n",
    "#  '1287.csv','1112.csv' no basal  '85.csv', '911.csv',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = pd.read_csv(f\"../LB_split/854.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose = preprocess_t1dexi_cgm(f\"../LB_split/854.csv\", False)\n",
    "glucose[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline interpolation and extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: print each segment\n",
    "segments = segement_data_as_1hour(glucose_df)\n",
    "interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align other factors with the glucose information\n",
    "\n",
    "## Include meal info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = pd.read_csv(\"../ML_split/854.csv\")\n",
    "selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "# Extract unique dates\n",
    "unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "# Convert to list\n",
    "meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "cleaned_segments = {}\n",
    "\n",
    "# Iterate through each segment and filter by unique dates\n",
    "for segment_name, df in interpolated_segements.items():\n",
    "    # Convert timestamp column to datetime and then extract the date part\n",
    "    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    \n",
    "    # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "    filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "    \n",
    "    # Drop the 'date' column as it's no longer needed\n",
    "    filtered_df = filtered_df.drop(columns=['date'])\n",
    "    \n",
    "    # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "    cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "# Update the segments with meal data\n",
    "meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_facm = pd.read_csv(f\"../FACM_split/854.csv\")\n",
    "# Group by 'Category' column\n",
    "grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "split_dfs = {category: group for category, group in grouped}\n",
    "# Step 1: Extract the desired columns\n",
    "new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "new_df_basal.reset_index(drop=True, inplace=True)\n",
    "new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "new_df_basal['assigned'] = False\n",
    "new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "new_df_basal[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the segments with meal data\n",
    "basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/854.csv\", False)\n",
    "bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to deal with large meal missingness\n",
    "\n",
    "1. Use 0 to impute \n",
    "2. Only use the days with meal record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the method 2\n",
    "# Therefore we have the meal_avaiable_dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct X and y, training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Build training and validation loader\n",
    "# features_array = np.array(features_list)\n",
    "# labels_array = np.array(raw_glu_list) # Maybe need to replace this\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(features_array, labels_array, test_size=0.2, shuffle= False)\n",
    "\n",
    "# # Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolus_updated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "# Assuming features_list and raw_glu_list are already defined\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs.permute(0, 2, 1))\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on test set: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement on the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_list = []\n",
    "for ffile in overlap:\n",
    "\n",
    "    subject = pd.read_csv(f\"../LB_split/{ffile}\")\n",
    "    glucose = preprocess_t1dexi_cgm(f\"../LB_split/{ffile}\", False)\n",
    "    glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "    # Create the multi-channel database\n",
    "    g_data = []\n",
    "    for timestamp in glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': glucose_dict[timestamp],\n",
    "            # 'meal_type': None,\n",
    "            # 'meal_carbs': 0\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "    # Convert glucose values to numeric type for analysis\n",
    "    glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "    segments = segement_data_as_1hour(glucose_df)\n",
    "    interpolated_segements = detect_missing_and_spline_interpolate(segments)\n",
    "    meal = pd.read_csv(f\"../ML_split/{ffile}\")\n",
    "    selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "    meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "    meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    meal_df['assigned'] = False\n",
    "\n",
    "    # Extract unique dates\n",
    "    unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "    # Convert to list\n",
    "    meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "    cleaned_segments = {}\n",
    "\n",
    "    # Iterate through each segment and filter by unique dates\n",
    "    for segment_name, df in interpolated_segements.items():\n",
    "        # Convert timestamp column to datetime and then extract the date part\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        \n",
    "        # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "        filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "        \n",
    "        # Drop the 'date' column as it's no longer needed\n",
    "        filtered_df = filtered_df.drop(columns=['date'])\n",
    "        \n",
    "        # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "        cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "    # Update the segments with meal data\n",
    "    meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)\n",
    "\n",
    "    subject_facm = pd.read_csv(f\"../FACM_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_basal.reset_index(drop=True, inplace=True)\n",
    "    new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "    new_df_basal['assigned'] = False\n",
    "    new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "    \n",
    "    basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n",
    "\n",
    "    new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/{ffile}\", False)\n",
    "    bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)\n",
    "\n",
    "    features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "    # Assuming features_list and raw_glu_list are already defined\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(raw_glu_list)\n",
    "\n",
    "    # Step 1: Split into 80% train+val and 20% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Convert the splits to torch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "\n",
    "    # Example of how to define the loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
    "\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on test set: {rmse}')\n",
    "    test_rmse_list.append(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
