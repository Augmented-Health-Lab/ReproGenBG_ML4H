{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate GluNet on T1DEXI\n",
    "\n",
    "GluNet was mainly reported as a personalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = 6\n",
    "history_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 5\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def preprocess_t1dexi_cgm(path, round):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject.groupby('LBCAT')\n",
    "    # Create a dictionary to store the split DataFrames\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "    new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "\n",
    "    new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    if round == True:\n",
    "        rounded_timestamp = []\n",
    "        for ts in new_df_cgm[\"LBDTC\"]:\n",
    "            rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "        new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "        formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "    else:\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "    \n",
    "    return formatted_data\n",
    "    \n",
    "    # # Assuming self.interval_timedelta is set, for example:\n",
    "    # interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # # Create a list to store the results\n",
    "    # res = []\n",
    "\n",
    "    # # Initialize the first group\n",
    "    # if not subject.empty:\n",
    "    #     current_group = [subject.iloc[0]['LBORRES']]\n",
    "    #     last_time = subject.iloc[0]['LBDTC']\n",
    "\n",
    "    # # Iterate over rows in DataFrame starting from the second row\n",
    "    # for index, row in subject.iloc[1:].iterrows():\n",
    "    #     current_time = row['LBDTC']\n",
    "    #     if (current_time - last_time) <= interval_timedelta:\n",
    "    #         # If the time difference is within the limit, add to the current group\n",
    "    #         current_group.append(row['LBORRES'])\n",
    "    #     else:\n",
    "    #         # Otherwise, start a new group\n",
    "    #         res.append(current_group)\n",
    "    #         current_group = [row['LBORRES']]\n",
    "    #     last_time = current_time\n",
    "\n",
    "    # # Add the last group if it's not empty\n",
    "    # if current_group:\n",
    "    #     res.append(current_group)\n",
    "    \n",
    "    # # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "\n",
    "\n",
    "    # return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_1hour(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=1)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_and_spline_interpolate(segments):\n",
    "    for sequence in segments:\n",
    "        # sequence = \"segment_3\"\n",
    "        detected_missing = 0\n",
    "        for ts in range(len( segments[sequence]['timestamp'])-1):\n",
    "            if segments[sequence]['timestamp'][ts+1] - segments[sequence]['timestamp'][ts] > timedelta(minutes = 6):\n",
    "                print(sequence)\n",
    "                print(\"before: \", segments[sequence]['timestamp'][ts])\n",
    "                print(\"after: \", segments[sequence]['timestamp'][ts+1])\n",
    "                detected_missing = 1\n",
    "            \n",
    "        if detected_missing == 1:\n",
    "            datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
    "            reference_time = min(segments[sequence]['timestamp'])\n",
    "\n",
    "            # Convert datetime objects to the number of seconds since the reference time\n",
    "            datetime_seconds_since_start = [((dt - reference_time).total_seconds())/60 for dt in datetime_list] # Make it into minute\n",
    "            original_timestamp_in_segement = [((dt - reference_time).total_seconds())/60 for dt in segments[sequence]['timestamp']]\n",
    "\n",
    "            x = original_timestamp_in_segement\n",
    "            y = np.array(segments[sequence]['glucose_value'])\n",
    "            cs = CubicSpline(x, y)\n",
    "            xs = datetime_seconds_since_start\n",
    "\n",
    "            interpolated_xs = cs(xs)\n",
    "            time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
    "\n",
    "            # Create DataFrame from the time index and glucose values\n",
    "            df_interpolated = pd.DataFrame({'timestamp': time_index_interpolated, 'glucose_value': interpolated_xs})\n",
    "            segments[sequence] = df_interpolated\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carbs'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carbs'] == closest_meal['carbs']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carbs'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carbs'] = closest_meal['carbs']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def preprocess_t1dexi_bolus_tempbasal(filepath, round):\n",
    "    subject_facm = pd.read_csv(filepath)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_bolus = split_dfs[\"BOLUS\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_bolus.reset_index(drop=True, inplace=True)\n",
    "    new_df_bolus = new_df_bolus.rename(columns={'FAORRES': 'dose', 'FADTC': 'ts_begin'})\n",
    "    new_df_bolus['assigned'] = False\n",
    "    # new_df_bolus['end_ts'] = new_df_bolus['ts_begin'].shift(-1)\n",
    "    return new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts_begin'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['dose']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts_begin'] < closest_bolus['ts_begin'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "# def prepare_dataset(segments, ph):\n",
    "#     '''\n",
    "#     ph = 6, 30 minutes ahead\n",
    "#     ph = 12, 60 minutes ahead\n",
    "#     '''\n",
    "#     features_list = []\n",
    "#     labels_list = []\n",
    "#     raw_glu_list = []\n",
    "    \n",
    "#     # Iterate over each segment\n",
    "#     for segment_name, segment_df in segments.items():\n",
    "#         # Ensure all columns are of numeric type\n",
    "#         segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "#         segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "#         segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "#         # Fill NaNs that might have been introduced by conversion errors\n",
    "#         segment_df.fillna(0, inplace=True)\n",
    "\n",
    "#         # Maximum index for creating a complete feature set\n",
    "#         max_index = len(segment_df) - (15+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + 15 to exist\n",
    "        \n",
    "#         # Iterate through the data to create feature-label pairs\n",
    "#         for i in range(max_index + 1):\n",
    "#             # Extracting features from index i to i+15\n",
    "#             features = segment_df.loc[i:i+15, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values#.flatten()\n",
    "#             # Extracting label for index i+21\n",
    "#             # Do the label transform\n",
    "#             label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "#             raw_glu_list.append(segment_df.loc[i+15+ph, 'glucose_value'])\n",
    "#             features_list.append(features)\n",
    "#             labels_list.append(label)\n",
    "            \n",
    "#     print(\"len of features_list \" + str(len(features_list)))\n",
    "#     print(\"len of labels_list \" + str(len(labels_list)))\n",
    "#     new_labels_list = label_delta_transform(labels_list)    \n",
    "#     print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "#     return features_list, labels_list, new_labels_list, raw_glu_list\n",
    "\n",
    "def prepare_dataset(segments, ph):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaveNet(\n",
      "  (initial_conv): Conv1d(4, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (2): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(4,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(8,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (final_conv1): Conv1d(32, 128, kernel_size=(2,), stride=(1,))\n",
      "  (final_conv2): Conv1d(128, 256, kernel_size=(2,), stride=(1,))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the dilate CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation):\n",
    "        super(WaveNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=1+dilation - 2^(dilation-1))\n",
    "        self.conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=dilation)\n",
    "        self.res_conv = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape of x: \", x.shape)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        # print(\"shape of first out: \", out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        # print(\"shape of second out: \", out.shape)\n",
    "        res = self.res_conv(x)\n",
    "        # print(\"shape of res: \", res.shape)\n",
    "        return out + res\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, dilations):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv1d(in_channels, 32, kernel_size=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([WaveNetBlock(32, dilation) for dilation in dilations])\n",
    "        self.final_conv1 = nn.Conv1d(32, 128, kernel_size=2, padding=0)\n",
    "        self.final_conv2 = nn.Conv1d(128, 256, kernel_size=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        for block in self.blocks:\n",
    "            # print(\"enter the block loop\")\n",
    "            x = block(x)\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = F.relu(self.final_conv2(x))\n",
    "        x = x[:, :, -1]  # Get the last time step\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_channels = 4  # Number of features\n",
    "output_channels = 1  # Predicting a single value (glucose level)\n",
    "num_blocks = 4  # Number of WaveNet blocks\n",
    "dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "print(model)\n",
    "\n",
    "# Example of how to define the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']\n",
    "\n",
    "#  '1287.csv','1112.csv' no basal  '85.csv', '911.csv',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = pd.read_csv(f\"../LB_split/854.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'ts': datetime.datetime(2020, 9, 29, 0, 4, 38), 'value': 84.0}],\n",
       " [{'ts': datetime.datetime(2020, 9, 29, 0, 9, 38), 'value': 83.0}],\n",
       " [{'ts': datetime.datetime(2020, 9, 29, 0, 14, 38), 'value': 83.0}],\n",
       " [{'ts': datetime.datetime(2020, 9, 29, 0, 19, 38), 'value': 83.0}],\n",
       " [{'ts': datetime.datetime(2020, 9, 29, 0, 24, 38), 'value': 83.0}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose = preprocess_t1dexi_cgm(f\"../LB_split/854.csv\", False)\n",
    "glucose[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2% lower threshold: 69.0\n",
      "98% upper threshold: 226.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>glucose_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-29 00:04:38</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-29 00:09:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-29 00:14:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-29 00:19:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-29 00:24:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>2020-10-26 23:38:59</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>2020-10-26 23:43:59</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>2020-10-26 23:48:59</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>2020-10-26 23:53:59</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>2020-10-26 23:58:59</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7869 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  glucose_value\n",
       "0    2020-09-29 00:04:38           84.0\n",
       "1    2020-09-29 00:09:38           83.0\n",
       "2    2020-09-29 00:14:38           83.0\n",
       "3    2020-09-29 00:19:38           83.0\n",
       "4    2020-09-29 00:24:38           83.0\n",
       "...                  ...            ...\n",
       "7864 2020-10-26 23:38:59          211.0\n",
       "7865 2020-10-26 23:43:59          206.0\n",
       "7866 2020-10-26 23:48:59          201.0\n",
       "7867 2020-10-26 23:53:59          197.0\n",
       "7868 2020-10-26 23:58:59          194.0\n",
       "\n",
       "[7869 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline interpolation and extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_4\n",
      "before:  2020-10-18 11:20:12\n",
      "after:  2020-10-18 11:33:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    }
   ],
   "source": [
    "# Example: print each segment\n",
    "segments = segement_data_as_1hour(glucose_df)\n",
    "interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align other factors with the glucose information\n",
    "\n",
    "## Include meal info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = pd.read_csv(\"../ML_split/854.csv\")\n",
    "selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "# Extract unique dates\n",
    "unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "# Convert to list\n",
    "meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "cleaned_segments = {}\n",
    "\n",
    "# Iterate through each segment and filter by unique dates\n",
    "for segment_name, df in interpolated_segements.items():\n",
    "    # Convert timestamp column to datetime and then extract the date part\n",
    "    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    \n",
    "    # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "    filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "    \n",
    "    # Drop the 'date' column as it's no longer needed\n",
    "    filtered_df = filtered_df.drop(columns=['date'])\n",
    "    \n",
    "    # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "    cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "# Update the segments with meal data\n",
    "meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\726347256.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>ts</th>\n",
       "      <th>assigned</th>\n",
       "      <th>end_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2020-09-29 00:01:12</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 00:46:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101</td>\n",
       "      <td>2020-09-29 00:46:03</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 00:51:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2020-09-29 00:51:02</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:10:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121</td>\n",
       "      <td>2020-09-29 01:10:58</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:15:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178</td>\n",
       "      <td>2020-09-29 01:15:58</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:20:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.300</td>\n",
       "      <td>2020-09-29 01:20:57</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:25:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.367</td>\n",
       "      <td>2020-09-29 01:25:56</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:30:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.351</td>\n",
       "      <td>2020-09-29 01:30:56</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:35:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.375</td>\n",
       "      <td>2020-09-29 01:35:55</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:40:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.409</td>\n",
       "      <td>2020-09-29 01:40:54</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:45:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value                  ts  assigned              end_ts\n",
       "0  0.000 2020-09-29 00:01:12     False 2020-09-29 00:46:03\n",
       "1  0.101 2020-09-29 00:46:03     False 2020-09-29 00:51:02\n",
       "2  0.000 2020-09-29 00:51:02     False 2020-09-29 01:10:58\n",
       "3  0.121 2020-09-29 01:10:58     False 2020-09-29 01:15:58\n",
       "4  0.178 2020-09-29 01:15:58     False 2020-09-29 01:20:57\n",
       "5  0.300 2020-09-29 01:20:57     False 2020-09-29 01:25:56\n",
       "6  0.367 2020-09-29 01:25:56     False 2020-09-29 01:30:56\n",
       "7  0.351 2020-09-29 01:30:56     False 2020-09-29 01:35:55\n",
       "8  0.375 2020-09-29 01:35:55     False 2020-09-29 01:40:54\n",
       "9  0.409 2020-09-29 01:40:54     False 2020-09-29 01:45:54"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_facm = pd.read_csv(f\"../FACM_split/854.csv\")\n",
    "# Group by 'Category' column\n",
    "grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "split_dfs = {category: group for category, group in grouped}\n",
    "# Step 1: Extract the desired columns\n",
    "new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "new_df_basal.reset_index(drop=True, inplace=True)\n",
    "new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "new_df_basal['assigned'] = False\n",
    "new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "new_df_basal[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the segments with meal data\n",
    "basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.517' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_4228\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.787' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    }
   ],
   "source": [
    "new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/854.csv\", False)\n",
    "bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to deal with large meal missingness\n",
    "\n",
    "1. Use 0 to impute \n",
    "2. Only use the days with meal record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the method 2\n",
    "# Therefore we have the meal_avaiable_dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct X and y, training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Build training and validation loader\n",
    "# features_array = np.array(features_list)\n",
    "# labels_array = np.array(raw_glu_list) # Maybe need to replace this\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(features_array, labels_array, test_size=0.2, shuffle= False)\n",
    "\n",
    "# # Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_1': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_2':               timestamp  glucose_value  carbs  basal_rate  bolus_dose\n",
       " 142 2020-09-30 00:04:40          186.0      0       0.425       0.000\n",
       " 143 2020-09-30 00:09:39          184.0      0       0.425       0.000\n",
       " 144 2020-09-30 00:14:39          187.0      0       0.425       0.000\n",
       " 145 2020-09-30 00:19:39          189.0      0       0.573       0.517\n",
       " 146 2020-09-30 00:24:39          190.0      0       0.425       0.000\n",
       " ..                  ...            ...    ...         ...         ...\n",
       " 425 2020-09-30 23:39:41          172.0      0       0.727       0.000\n",
       " 426 2020-09-30 23:44:42          168.0      0       0.661       0.000\n",
       " 427 2020-09-30 23:49:42          161.0      0       0.625       0.000\n",
       " 428 2020-09-30 23:54:41          164.0      0       0.625       0.000\n",
       " 429 2020-09-30 23:59:41          171.0      0       0.625       0.000\n",
       " \n",
       " [288 rows x 5 columns],\n",
       " 'segment_3':               timestamp  glucose_value  carbs basal_rate  bolus_dose\n",
       " 383 2020-10-11 00:04:59           63.0      0        0.0         0.0\n",
       " 384 2020-10-11 00:09:59           63.0      0        0.0         0.0\n",
       " 385 2020-10-11 00:14:59           64.0      0        0.0         0.0\n",
       " 386 2020-10-11 00:19:59           65.0      0        0.0         0.0\n",
       " 387 2020-10-11 00:24:59           65.0      0        0.0         0.0\n",
       " ..                  ...            ...    ...        ...         ...\n",
       " 665 2020-10-11 23:35:00          103.0      0       0.32         0.0\n",
       " 666 2020-10-11 23:40:01           95.0      0      0.117         0.0\n",
       " 667 2020-10-11 23:45:01           92.0      0        0.0         0.0\n",
       " 668 2020-10-11 23:50:01           92.0      0        0.0         0.0\n",
       " 669 2020-10-11 23:55:01           92.0      0        0.0         0.0\n",
       " \n",
       " [287 rows x 5 columns],\n",
       " 'segment_4': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_5': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bolus_updated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  0\n",
      "len of segment_df is  288\n",
      "len of segment_df is  287\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of features_list 533\n"
     ]
    }
   ],
   "source": [
    "features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "# Assuming features_list and raw_glu_list are already defined\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1370.9013061523438\n",
      "Epoch 2, Validation Loss: 741.941650390625\n",
      "Epoch 3, Validation Loss: 1056.4132080078125\n",
      "Epoch 4, Validation Loss: 1784.5481567382812\n",
      "Epoch 5, Validation Loss: 1608.6460571289062\n",
      "Epoch 6, Validation Loss: 1420.0771179199219\n",
      "Epoch 7, Validation Loss: 1432.3649597167969\n",
      "Epoch 8, Validation Loss: 1519.9700317382812\n",
      "Epoch 9, Validation Loss: 1585.4831237792969\n",
      "Epoch 10, Validation Loss: 1607.5440673828125\n",
      "Epoch 11, Validation Loss: 1609.9667358398438\n",
      "Epoch 12, Validation Loss: 1605.6324462890625\n",
      "Epoch 13, Validation Loss: 1619.8182373046875\n",
      "Epoch 14, Validation Loss: 1658.970703125\n",
      "Epoch 15, Validation Loss: 1668.3689575195312\n",
      "Epoch 16, Validation Loss: 1641.9288330078125\n",
      "Epoch 17, Validation Loss: 1614.5851440429688\n",
      "Epoch 18, Validation Loss: 1578.1190795898438\n",
      "Epoch 19, Validation Loss: 1594.5660400390625\n",
      "Epoch 20, Validation Loss: 1632.71044921875\n",
      "Epoch 21, Validation Loss: 1619.6407470703125\n",
      "Epoch 22, Validation Loss: 1606.58544921875\n",
      "Epoch 23, Validation Loss: 1572.4669799804688\n",
      "Epoch 24, Validation Loss: 1534.2493591308594\n",
      "Epoch 25, Validation Loss: 1512.1528625488281\n",
      "Epoch 26, Validation Loss: 1503.336181640625\n",
      "Epoch 27, Validation Loss: 1516.697265625\n",
      "Epoch 28, Validation Loss: 1514.1624755859375\n",
      "Epoch 29, Validation Loss: 1490.19775390625\n",
      "Epoch 30, Validation Loss: 1469.1863708496094\n",
      "Epoch 31, Validation Loss: 1458.172607421875\n",
      "Epoch 32, Validation Loss: 1451.109619140625\n",
      "Epoch 33, Validation Loss: 1465.2622375488281\n",
      "Epoch 34, Validation Loss: 1456.46142578125\n",
      "Epoch 35, Validation Loss: 1445.3154602050781\n",
      "Epoch 36, Validation Loss: 1439.6007690429688\n",
      "Epoch 37, Validation Loss: 1431.0315551757812\n",
      "Epoch 38, Validation Loss: 1421.9909362792969\n",
      "Epoch 39, Validation Loss: 1444.765380859375\n",
      "Epoch 40, Validation Loss: 1442.2821655273438\n",
      "Epoch 41, Validation Loss: 1463.6566772460938\n",
      "Epoch 42, Validation Loss: 1466.8180236816406\n",
      "Epoch 43, Validation Loss: 1440.4456787109375\n",
      "Epoch 44, Validation Loss: 1427.6900634765625\n",
      "Epoch 45, Validation Loss: 1456.489013671875\n",
      "Epoch 46, Validation Loss: 1455.902099609375\n",
      "Epoch 47, Validation Loss: 1416.6024169921875\n",
      "Epoch 48, Validation Loss: 1382.8258666992188\n",
      "Epoch 49, Validation Loss: 1365.7761535644531\n",
      "Epoch 50, Validation Loss: 1445.2756958007812\n",
      "Epoch 51, Validation Loss: 1455.0374145507812\n",
      "Epoch 52, Validation Loss: 1406.0942687988281\n",
      "Epoch 53, Validation Loss: 1365.7386779785156\n",
      "Epoch 54, Validation Loss: 1345.3187561035156\n",
      "Epoch 55, Validation Loss: 1336.377685546875\n",
      "Epoch 56, Validation Loss: 1330.7420654296875\n",
      "Epoch 57, Validation Loss: 1333.673095703125\n",
      "Epoch 58, Validation Loss: 1326.2960205078125\n",
      "Epoch 59, Validation Loss: 1324.7887573242188\n",
      "Epoch 60, Validation Loss: 1317.335693359375\n",
      "Epoch 61, Validation Loss: 1324.8773193359375\n",
      "Epoch 62, Validation Loss: 1352.2403564453125\n",
      "Epoch 63, Validation Loss: 1355.0465393066406\n",
      "Epoch 64, Validation Loss: 1323.2342529296875\n",
      "Epoch 65, Validation Loss: 1296.7956848144531\n",
      "Epoch 66, Validation Loss: 1281.2994689941406\n",
      "Epoch 67, Validation Loss: 1282.0007934570312\n",
      "Epoch 68, Validation Loss: 1277.010009765625\n",
      "Epoch 69, Validation Loss: 1278.18994140625\n",
      "Epoch 70, Validation Loss: 1274.3932495117188\n",
      "Epoch 71, Validation Loss: 1273.0375061035156\n",
      "Epoch 72, Validation Loss: 1266.6839294433594\n",
      "Epoch 73, Validation Loss: 1261.1784057617188\n",
      "Epoch 74, Validation Loss: 1295.2804565429688\n",
      "Epoch 75, Validation Loss: 1294.1747131347656\n",
      "Epoch 76, Validation Loss: 1265.6957702636719\n",
      "Epoch 77, Validation Loss: 1243.5809936523438\n",
      "Epoch 78, Validation Loss: 1294.95068359375\n",
      "Epoch 79, Validation Loss: 1290.4658508300781\n",
      "Epoch 80, Validation Loss: 1255.4126281738281\n",
      "Epoch 81, Validation Loss: 1282.2964782714844\n",
      "Epoch 82, Validation Loss: 1272.5546264648438\n",
      "Epoch 83, Validation Loss: 1237.8833618164062\n",
      "Epoch 84, Validation Loss: 1224.0194091796875\n",
      "Epoch 85, Validation Loss: 1213.4537963867188\n",
      "Epoch 86, Validation Loss: 1278.8621215820312\n",
      "Epoch 87, Validation Loss: 1246.4852600097656\n",
      "Epoch 88, Validation Loss: 1221.0681762695312\n",
      "Epoch 89, Validation Loss: 1205.6964721679688\n",
      "Epoch 90, Validation Loss: 1201.4437866210938\n",
      "Epoch 91, Validation Loss: 1194.0654296875\n",
      "Epoch 92, Validation Loss: 1184.652099609375\n",
      "Epoch 93, Validation Loss: 1184.8525695800781\n",
      "Epoch 94, Validation Loss: 1229.965087890625\n",
      "Epoch 95, Validation Loss: 1288.3887939453125\n",
      "Epoch 96, Validation Loss: 1408.351318359375\n",
      "Epoch 97, Validation Loss: 1430.9756469726562\n",
      "Epoch 98, Validation Loss: 1346.7608642578125\n",
      "Epoch 99, Validation Loss: 1197.8804321289062\n",
      "Epoch 100, Validation Loss: 1110.4359130859375\n",
      "Epoch 101, Validation Loss: 1096.1906433105469\n",
      "Epoch 102, Validation Loss: 1106.1673278808594\n",
      "Epoch 103, Validation Loss: 1125.3789367675781\n",
      "Epoch 104, Validation Loss: 1185.5199279785156\n",
      "Epoch 105, Validation Loss: 1193.7153625488281\n",
      "Epoch 106, Validation Loss: 1146.0498046875\n",
      "Epoch 107, Validation Loss: 1143.821533203125\n",
      "Epoch 108, Validation Loss: 1152.1864929199219\n",
      "Epoch 109, Validation Loss: 1151.2133483886719\n",
      "Epoch 110, Validation Loss: 1140.1997375488281\n",
      "Epoch 111, Validation Loss: 1103.007080078125\n",
      "Epoch 112, Validation Loss: 1107.3689880371094\n",
      "Epoch 113, Validation Loss: 1096.6394653320312\n",
      "Epoch 114, Validation Loss: 1092.0814514160156\n",
      "Epoch 115, Validation Loss: 1099.462158203125\n",
      "Epoch 116, Validation Loss: 1106.0405883789062\n",
      "Epoch 117, Validation Loss: 1071.0853881835938\n",
      "Epoch 118, Validation Loss: 1110.8912048339844\n",
      "Epoch 119, Validation Loss: 1086.5684509277344\n",
      "Epoch 120, Validation Loss: 1087.6103515625\n",
      "Epoch 121, Validation Loss: 1066.7686157226562\n",
      "Epoch 122, Validation Loss: 1126.2749633789062\n",
      "Epoch 123, Validation Loss: 1074.0785522460938\n",
      "Epoch 124, Validation Loss: 1068.7117919921875\n",
      "Epoch 125, Validation Loss: 1051.2947387695312\n",
      "Epoch 126, Validation Loss: 1015.7575378417969\n",
      "Epoch 127, Validation Loss: 1076.370849609375\n",
      "Epoch 128, Validation Loss: 1073.6962890625\n",
      "Epoch 129, Validation Loss: 996.0666351318359\n",
      "Epoch 130, Validation Loss: 990.1185760498047\n",
      "Epoch 131, Validation Loss: 994.1903991699219\n",
      "Epoch 132, Validation Loss: 985.614013671875\n",
      "Epoch 133, Validation Loss: 1010.5424194335938\n",
      "Epoch 134, Validation Loss: 992.5524291992188\n",
      "Epoch 135, Validation Loss: 1384.6460571289062\n",
      "Epoch 136, Validation Loss: 1379.1474609375\n",
      "Epoch 137, Validation Loss: 1164.6735229492188\n",
      "Epoch 138, Validation Loss: 916.9417724609375\n",
      "Epoch 139, Validation Loss: 872.5304260253906\n",
      "Epoch 140, Validation Loss: 911.4351348876953\n",
      "Epoch 141, Validation Loss: 946.3697204589844\n",
      "Epoch 142, Validation Loss: 936.3069915771484\n",
      "Epoch 143, Validation Loss: 919.3220520019531\n",
      "Epoch 144, Validation Loss: 905.8721618652344\n",
      "Epoch 145, Validation Loss: 907.2293243408203\n",
      "Epoch 146, Validation Loss: 909.5052490234375\n",
      "Epoch 147, Validation Loss: 933.024169921875\n",
      "Epoch 148, Validation Loss: 942.4592895507812\n",
      "Epoch 149, Validation Loss: 913.0076904296875\n",
      "Epoch 150, Validation Loss: 883.8354034423828\n",
      "Epoch 151, Validation Loss: 889.1722564697266\n",
      "Epoch 152, Validation Loss: 898.4103698730469\n",
      "Epoch 153, Validation Loss: 900.9380645751953\n",
      "Epoch 154, Validation Loss: 902.2566375732422\n",
      "Epoch 155, Validation Loss: 892.8737487792969\n",
      "Epoch 156, Validation Loss: 892.3432922363281\n",
      "Epoch 157, Validation Loss: 899.6611938476562\n",
      "Epoch 158, Validation Loss: 893.3985290527344\n",
      "Epoch 159, Validation Loss: 901.7293701171875\n",
      "Epoch 160, Validation Loss: 888.815673828125\n",
      "Epoch 161, Validation Loss: 856.0892791748047\n",
      "Epoch 162, Validation Loss: 941.5866088867188\n",
      "Epoch 163, Validation Loss: 895.3288421630859\n",
      "Epoch 164, Validation Loss: 1067.0421752929688\n",
      "Epoch 165, Validation Loss: 999.8515625\n",
      "Epoch 166, Validation Loss: 859.0624542236328\n",
      "Epoch 167, Validation Loss: 818.7936553955078\n",
      "Epoch 168, Validation Loss: 840.0120086669922\n",
      "Epoch 169, Validation Loss: 863.5558166503906\n",
      "Epoch 170, Validation Loss: 875.6747131347656\n",
      "Epoch 171, Validation Loss: 858.6918792724609\n",
      "Epoch 172, Validation Loss: 846.8939971923828\n",
      "Epoch 173, Validation Loss: 876.4634246826172\n",
      "Epoch 174, Validation Loss: 941.2881927490234\n",
      "Epoch 175, Validation Loss: 858.4537048339844\n",
      "Epoch 176, Validation Loss: 1242.2508544921875\n",
      "Epoch 177, Validation Loss: 1197.1483764648438\n",
      "Epoch 178, Validation Loss: 904.3724670410156\n",
      "Epoch 179, Validation Loss: 758.0115356445312\n",
      "Epoch 180, Validation Loss: 789.02392578125\n",
      "Epoch 181, Validation Loss: 892.2021636962891\n",
      "Epoch 182, Validation Loss: 905.5680694580078\n",
      "Epoch 183, Validation Loss: 823.7498626708984\n",
      "Epoch 184, Validation Loss: 789.6470947265625\n",
      "Epoch 185, Validation Loss: 815.5807495117188\n",
      "Epoch 186, Validation Loss: 853.8203735351562\n",
      "Epoch 187, Validation Loss: 817.9974060058594\n",
      "Epoch 188, Validation Loss: 805.5142669677734\n",
      "Epoch 189, Validation Loss: 796.1720581054688\n",
      "Epoch 190, Validation Loss: 781.4026184082031\n",
      "Epoch 191, Validation Loss: 823.8578186035156\n",
      "Epoch 192, Validation Loss: 769.5225524902344\n",
      "Epoch 193, Validation Loss: 832.7120056152344\n",
      "Epoch 194, Validation Loss: 915.400390625\n",
      "Epoch 195, Validation Loss: 823.9102630615234\n",
      "Epoch 196, Validation Loss: 951.68896484375\n",
      "Epoch 197, Validation Loss: 885.6103515625\n",
      "Epoch 198, Validation Loss: 734.9740447998047\n",
      "Epoch 199, Validation Loss: 771.2772674560547\n",
      "Epoch 200, Validation Loss: 786.9012451171875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 20.826229095458984\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs.permute(0, 2, 1))\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on test set: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement on the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['854.csv',\n",
       " '979.csv',\n",
       " '816.csv',\n",
       " '953.csv',\n",
       " '981.csv',\n",
       " '1617.csv',\n",
       " '1343.csv',\n",
       " '987.csv',\n",
       " '255.csv',\n",
       " '85.csv',\n",
       " '907.csv',\n",
       " '856.csv',\n",
       " '354.csv',\n",
       " '894.csv',\n",
       " '911.csv',\n",
       " '862.csv',\n",
       " '900.csv',\n",
       " '695.csv']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1112.csv'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.97289"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(test_rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_list.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20.959793,\n",
       " 16.30154,\n",
       " 21.350086,\n",
       " 26.749659,\n",
       " 38.991474,\n",
       " 26.847044,\n",
       " 22.590736,\n",
       " 29.177532,\n",
       " 16.917473,\n",
       " nan,\n",
       " 40.724693,\n",
       " 21.829384,\n",
       " 26.778894,\n",
       " 22.288874,\n",
       " nan,\n",
       " 17.317373,\n",
       " 26.705725]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.911290625"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(test_rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse_list = [20.959793,\n",
    " 16.30154,\n",
    " 21.350086,\n",
    " 26.749659,\n",
    " 38.991474,\n",
    " 26.847044,\n",
    " 22.590736,\n",
    " 29.177532,\n",
    " 16.917473,\n",
    " np.nan,\n",
    " 40.724693,\n",
    " 21.829384,\n",
    " 26.778894,\n",
    " 22.288874,\n",
    " np.nan,\n",
    " 39.05037,\n",
    " 17.317373,\n",
    " 26.705725]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_4\n",
      "before:  2020-10-18 11:20:12\n",
      "after:  2020-10-18 11:33:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.517' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.787' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  0\n",
      "len of segment_df is  288\n",
      "len of segment_df is  287\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of features_list 533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 7563.031982421875\n",
      "Epoch 2, Validation Loss: 750.0057067871094\n",
      "Epoch 3, Validation Loss: 2077.92724609375\n",
      "Epoch 4, Validation Loss: 795.2610015869141\n",
      "Epoch 5, Validation Loss: 1001.4903259277344\n",
      "Epoch 6, Validation Loss: 1275.7015380859375\n",
      "Epoch 7, Validation Loss: 1150.9737548828125\n",
      "Epoch 8, Validation Loss: 1122.0437316894531\n",
      "Epoch 9, Validation Loss: 1183.2345275878906\n",
      "Epoch 10, Validation Loss: 1228.5830688476562\n",
      "Epoch 11, Validation Loss: 1247.4743041992188\n",
      "Epoch 12, Validation Loss: 1260.5186462402344\n",
      "Epoch 13, Validation Loss: 1280.817626953125\n",
      "Epoch 14, Validation Loss: 1287.8399658203125\n",
      "Epoch 15, Validation Loss: 1296.237548828125\n",
      "Epoch 16, Validation Loss: 1317.33251953125\n",
      "Epoch 17, Validation Loss: 1341.5916748046875\n",
      "Epoch 18, Validation Loss: 1353.3803100585938\n",
      "Epoch 19, Validation Loss: 1336.3400268554688\n",
      "Epoch 20, Validation Loss: 1325.2136840820312\n",
      "Epoch 21, Validation Loss: 1319.5448608398438\n",
      "Epoch 22, Validation Loss: 1320.15869140625\n",
      "Epoch 23, Validation Loss: 1321.1200561523438\n",
      "Epoch 24, Validation Loss: 1328.0998229980469\n",
      "Epoch 25, Validation Loss: 1325.9173889160156\n",
      "Epoch 26, Validation Loss: 1326.5280151367188\n",
      "Epoch 27, Validation Loss: 1324.5747375488281\n",
      "Epoch 28, Validation Loss: 1374.6725463867188\n",
      "Epoch 29, Validation Loss: 1375.0205688476562\n",
      "Epoch 30, Validation Loss: 1333.873046875\n",
      "Epoch 31, Validation Loss: 1303.5109252929688\n",
      "Epoch 32, Validation Loss: 1295.5041198730469\n",
      "Epoch 33, Validation Loss: 1294.2829284667969\n",
      "Epoch 34, Validation Loss: 1299.9725646972656\n",
      "Epoch 35, Validation Loss: 1297.3290710449219\n",
      "Epoch 36, Validation Loss: 1295.1480407714844\n",
      "Epoch 37, Validation Loss: 1292.7293090820312\n",
      "Epoch 38, Validation Loss: 1303.4089660644531\n",
      "Epoch 39, Validation Loss: 1304.0083923339844\n",
      "Epoch 40, Validation Loss: 1303.3068237304688\n",
      "Epoch 41, Validation Loss: 1296.5427856445312\n",
      "Epoch 42, Validation Loss: 1282.1932983398438\n",
      "Epoch 43, Validation Loss: 1270.2342529296875\n",
      "Epoch 44, Validation Loss: 1274.0130310058594\n",
      "Epoch 45, Validation Loss: 1261.9816589355469\n",
      "Epoch 46, Validation Loss: 1255.6234130859375\n",
      "Epoch 47, Validation Loss: 1263.4346618652344\n",
      "Epoch 48, Validation Loss: 1259.0744018554688\n",
      "Epoch 49, Validation Loss: 1243.3270874023438\n",
      "Epoch 50, Validation Loss: 1304.9229431152344\n",
      "Epoch 51, Validation Loss: 1298.1564636230469\n",
      "Epoch 52, Validation Loss: 1270.3827209472656\n",
      "Epoch 53, Validation Loss: 1261.24560546875\n",
      "Epoch 54, Validation Loss: 1237.5037536621094\n",
      "Epoch 55, Validation Loss: 1224.3246765136719\n",
      "Epoch 56, Validation Loss: 1232.8533935546875\n",
      "Epoch 57, Validation Loss: 1231.4166870117188\n",
      "Epoch 58, Validation Loss: 1226.4336547851562\n",
      "Epoch 59, Validation Loss: 1305.101318359375\n",
      "Epoch 60, Validation Loss: 1272.250732421875\n",
      "Epoch 61, Validation Loss: 1222.7823181152344\n",
      "Epoch 62, Validation Loss: 1180.916259765625\n",
      "Epoch 63, Validation Loss: 1201.4219360351562\n",
      "Epoch 64, Validation Loss: 1232.5361328125\n",
      "Epoch 65, Validation Loss: 1195.7272033691406\n",
      "Epoch 66, Validation Loss: 1183.6688537597656\n",
      "Epoch 67, Validation Loss: 1165.7579345703125\n",
      "Epoch 68, Validation Loss: 1159.8601989746094\n",
      "Epoch 69, Validation Loss: 1165.7477416992188\n",
      "Epoch 70, Validation Loss: 1145.6655578613281\n",
      "Epoch 71, Validation Loss: 1147.4780883789062\n",
      "Epoch 72, Validation Loss: 1177.2094421386719\n",
      "Epoch 73, Validation Loss: 1255.36572265625\n",
      "Epoch 74, Validation Loss: 1200.2723388671875\n",
      "Epoch 75, Validation Loss: 1174.5292053222656\n",
      "Epoch 76, Validation Loss: 1116.2583312988281\n",
      "Epoch 77, Validation Loss: 1100.2041015625\n",
      "Epoch 78, Validation Loss: 1095.1121520996094\n",
      "Epoch 79, Validation Loss: 1105.4379272460938\n",
      "Epoch 80, Validation Loss: 1094.7078247070312\n",
      "Epoch 81, Validation Loss: 1104.1796264648438\n",
      "Epoch 82, Validation Loss: 1090.3134765625\n",
      "Epoch 83, Validation Loss: 1089.6759948730469\n",
      "Epoch 84, Validation Loss: 1065.7919311523438\n",
      "Epoch 85, Validation Loss: 1064.9578857421875\n",
      "Epoch 86, Validation Loss: 1086.0817260742188\n",
      "Epoch 87, Validation Loss: 1076.407470703125\n",
      "Epoch 88, Validation Loss: 1056.9043579101562\n",
      "Epoch 89, Validation Loss: 1071.5937805175781\n",
      "Epoch 90, Validation Loss: 1074.8763732910156\n",
      "Epoch 91, Validation Loss: 1055.5808715820312\n",
      "Epoch 92, Validation Loss: 1037.2393188476562\n",
      "Epoch 93, Validation Loss: 1082.7898254394531\n",
      "Epoch 94, Validation Loss: 1102.0807189941406\n",
      "Epoch 95, Validation Loss: 1105.1980895996094\n",
      "Epoch 96, Validation Loss: 1065.6086120605469\n",
      "Epoch 97, Validation Loss: 1126.7874145507812\n",
      "Epoch 98, Validation Loss: 1346.0635986328125\n",
      "Epoch 99, Validation Loss: 1324.2300415039062\n",
      "Epoch 100, Validation Loss: 1176.9943542480469\n",
      "Epoch 101, Validation Loss: 1056.90380859375\n",
      "Epoch 102, Validation Loss: 1024.8185119628906\n",
      "Epoch 103, Validation Loss: 1092.1939086914062\n",
      "Epoch 104, Validation Loss: 1201.5845336914062\n",
      "Epoch 105, Validation Loss: 1280.5248413085938\n",
      "Epoch 106, Validation Loss: 1126.3507690429688\n",
      "Epoch 107, Validation Loss: 1025.8650817871094\n",
      "Epoch 108, Validation Loss: 989.6089477539062\n",
      "Epoch 109, Validation Loss: 976.1093444824219\n",
      "Epoch 110, Validation Loss: 1011.6112060546875\n",
      "Epoch 111, Validation Loss: 1025.7052001953125\n",
      "Epoch 112, Validation Loss: 987.6879577636719\n",
      "Epoch 113, Validation Loss: 957.1917724609375\n",
      "Epoch 114, Validation Loss: 994.1737976074219\n",
      "Epoch 115, Validation Loss: 1039.5040588378906\n",
      "Epoch 116, Validation Loss: 1030.353271484375\n",
      "Epoch 117, Validation Loss: 975.447998046875\n",
      "Epoch 118, Validation Loss: 970.1871948242188\n",
      "Epoch 119, Validation Loss: 965.3055725097656\n",
      "Epoch 120, Validation Loss: 950.6757507324219\n",
      "Epoch 121, Validation Loss: 947.2828216552734\n",
      "Epoch 122, Validation Loss: 939.7352447509766\n",
      "Epoch 123, Validation Loss: 935.5376434326172\n",
      "Epoch 124, Validation Loss: 936.8895416259766\n",
      "Epoch 125, Validation Loss: 973.5020141601562\n",
      "Epoch 126, Validation Loss: 1011.7399597167969\n",
      "Epoch 127, Validation Loss: 988.5238952636719\n",
      "Epoch 128, Validation Loss: 933.3732604980469\n",
      "Epoch 129, Validation Loss: 923.2038116455078\n",
      "Epoch 130, Validation Loss: 931.7030487060547\n",
      "Epoch 131, Validation Loss: 933.4592590332031\n",
      "Epoch 132, Validation Loss: 929.949951171875\n",
      "Epoch 133, Validation Loss: 934.9820098876953\n",
      "Epoch 134, Validation Loss: 920.2759246826172\n",
      "Epoch 135, Validation Loss: 946.6094055175781\n",
      "Epoch 136, Validation Loss: 936.8015899658203\n",
      "Epoch 137, Validation Loss: 926.9726257324219\n",
      "Epoch 138, Validation Loss: 917.7127532958984\n",
      "Epoch 139, Validation Loss: 932.6046295166016\n",
      "Epoch 140, Validation Loss: 921.7952880859375\n",
      "Epoch 141, Validation Loss: 906.7685241699219\n",
      "Epoch 142, Validation Loss: 910.3585815429688\n",
      "Epoch 143, Validation Loss: 905.8783264160156\n",
      "Epoch 144, Validation Loss: 921.1460571289062\n",
      "Epoch 145, Validation Loss: 901.478759765625\n",
      "Epoch 146, Validation Loss: 901.2196807861328\n",
      "Epoch 147, Validation Loss: 888.8003234863281\n",
      "Epoch 148, Validation Loss: 933.7666625976562\n",
      "Epoch 149, Validation Loss: 887.4717102050781\n",
      "Epoch 150, Validation Loss: 890.5844116210938\n",
      "Epoch 151, Validation Loss: 890.9494171142578\n",
      "Epoch 152, Validation Loss: 884.3927001953125\n",
      "Epoch 153, Validation Loss: 886.1701812744141\n",
      "Epoch 154, Validation Loss: 893.8516693115234\n",
      "Epoch 155, Validation Loss: 868.484619140625\n",
      "Epoch 156, Validation Loss: 873.385009765625\n",
      "Epoch 157, Validation Loss: 865.5477294921875\n",
      "Epoch 158, Validation Loss: 874.910888671875\n",
      "Epoch 159, Validation Loss: 852.4106750488281\n",
      "Epoch 160, Validation Loss: 848.5071411132812\n",
      "Epoch 161, Validation Loss: 853.6889343261719\n",
      "Epoch 162, Validation Loss: 868.4904327392578\n",
      "Epoch 163, Validation Loss: 835.4474487304688\n",
      "Epoch 164, Validation Loss: 847.3708038330078\n",
      "Epoch 165, Validation Loss: 824.27099609375\n",
      "Epoch 166, Validation Loss: 840.1850891113281\n",
      "Epoch 167, Validation Loss: 822.2580871582031\n",
      "Epoch 168, Validation Loss: 829.9612121582031\n",
      "Epoch 169, Validation Loss: 819.5069122314453\n",
      "Epoch 170, Validation Loss: 826.3582153320312\n",
      "Epoch 171, Validation Loss: 815.5844573974609\n",
      "Epoch 172, Validation Loss: 823.9526977539062\n",
      "Epoch 173, Validation Loss: 807.2925415039062\n",
      "Epoch 174, Validation Loss: 809.3193359375\n",
      "Epoch 175, Validation Loss: 798.2781372070312\n",
      "Epoch 176, Validation Loss: 841.4618225097656\n",
      "Epoch 177, Validation Loss: 809.0265045166016\n",
      "Epoch 178, Validation Loss: 813.9908142089844\n",
      "Epoch 179, Validation Loss: 813.2341461181641\n",
      "Epoch 180, Validation Loss: 797.7678527832031\n",
      "Epoch 181, Validation Loss: 788.5368957519531\n",
      "Epoch 182, Validation Loss: 799.818603515625\n",
      "Epoch 183, Validation Loss: 788.4825744628906\n",
      "Epoch 184, Validation Loss: 790.7042236328125\n",
      "Epoch 185, Validation Loss: 777.7810821533203\n",
      "Epoch 186, Validation Loss: 788.3287811279297\n",
      "Epoch 187, Validation Loss: 774.6659545898438\n",
      "Epoch 188, Validation Loss: 780.5783843994141\n",
      "Epoch 189, Validation Loss: 782.2426452636719\n",
      "Epoch 190, Validation Loss: 771.142333984375\n",
      "Epoch 191, Validation Loss: 777.5737609863281\n",
      "Epoch 192, Validation Loss: 762.3240966796875\n",
      "Epoch 193, Validation Loss: 765.8829345703125\n",
      "Epoch 194, Validation Loss: 764.3705749511719\n",
      "Epoch 195, Validation Loss: 765.9065399169922\n",
      "Epoch 196, Validation Loss: 759.1493377685547\n",
      "Epoch 197, Validation Loss: 757.7518157958984\n",
      "Epoch 198, Validation Loss: 771.6341094970703\n",
      "Epoch 199, Validation Loss: 771.8946228027344\n",
      "Epoch 200, Validation Loss: 766.8962707519531\n",
      "RMSE on test set: 21.516889572143555\n",
      "segment_1\n",
      "before:  2021-03-16 15:47:27\n",
      "after:  2021-03-16 15:57:28\n",
      "segment_1\n",
      "before:  2021-03-18 17:42:35\n",
      "after:  2021-03-18 18:22:34\n",
      "segment_1\n",
      "before:  2021-03-18 20:32:36\n",
      "after:  2021-03-18 20:42:35\n",
      "segment_3\n",
      "before:  2021-03-29 06:33:08\n",
      "after:  2021-03-29 06:53:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2192\n",
      "len of segment_df is  1236\n",
      "len of segment_df is  1550\n",
      "len of segment_df is  691\n",
      "len of segment_df is  0\n",
      "len of features_list 5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 448.06903892093237\n",
      "Epoch 2, Validation Loss: 439.23451624976263\n",
      "Epoch 3, Validation Loss: 439.61533610026044\n",
      "Epoch 4, Validation Loss: 429.5147025850084\n",
      "Epoch 5, Validation Loss: 435.7624968422784\n",
      "Epoch 6, Validation Loss: 438.7056664360894\n",
      "Epoch 7, Validation Loss: 443.46521271599664\n",
      "Epoch 8, Validation Loss: 446.95055770874023\n",
      "Epoch 9, Validation Loss: 432.7098831600613\n",
      "Epoch 10, Validation Loss: 432.2045841217041\n",
      "Epoch 11, Validation Loss: 437.33047019110785\n",
      "Epoch 12, Validation Loss: 438.51423835754395\n",
      "Epoch 13, Validation Loss: 444.24784639146594\n",
      "Epoch 14, Validation Loss: 449.52316665649414\n",
      "Epoch 15, Validation Loss: 456.88501739501953\n",
      "Epoch 16, Validation Loss: 463.13510089450415\n",
      "Epoch 17, Validation Loss: 470.3709242078993\n",
      "Epoch 18, Validation Loss: 476.29133902655707\n",
      "Epoch 19, Validation Loss: 483.97328906589087\n",
      "Epoch 20, Validation Loss: 488.96157921685113\n",
      "Epoch 21, Validation Loss: 493.307623969184\n",
      "Epoch 22, Validation Loss: 497.6912909613715\n",
      "Epoch 23, Validation Loss: 500.2181044684516\n",
      "Epoch 24, Validation Loss: 499.79600948757593\n",
      "Epoch 25, Validation Loss: 498.62657165527344\n",
      "Epoch 26, Validation Loss: 500.02027384440106\n",
      "Epoch 27, Validation Loss: 497.1799952189128\n",
      "Epoch 28, Validation Loss: 494.2364379035102\n",
      "Epoch 29, Validation Loss: 475.0042338901096\n",
      "Epoch 30, Validation Loss: 480.4433763292101\n",
      "Epoch 31, Validation Loss: 458.8873591952854\n",
      "Epoch 32, Validation Loss: 446.59455066257055\n",
      "Epoch 33, Validation Loss: 431.78415319654675\n",
      "Epoch 34, Validation Loss: 440.4715328216553\n",
      "Epoch 35, Validation Loss: 408.57876099480524\n",
      "Epoch 36, Validation Loss: 399.03950055440265\n",
      "Epoch 37, Validation Loss: 399.28887388441296\n",
      "Epoch 38, Validation Loss: 388.8798893822564\n",
      "Epoch 39, Validation Loss: 378.7437909444173\n",
      "Epoch 40, Validation Loss: 374.10201358795166\n",
      "Epoch 41, Validation Loss: 373.11685201856824\n",
      "Epoch 42, Validation Loss: 372.9579674402873\n",
      "Epoch 43, Validation Loss: 369.8659494188097\n",
      "Epoch 44, Validation Loss: 366.5622442033556\n",
      "Epoch 45, Validation Loss: 367.8064073986477\n",
      "Epoch 46, Validation Loss: 364.5037064022488\n",
      "Epoch 47, Validation Loss: 363.07834900750055\n",
      "Epoch 48, Validation Loss: 363.22278033362494\n",
      "Epoch 49, Validation Loss: 362.9095862706502\n",
      "Epoch 50, Validation Loss: 363.8623292711046\n",
      "Epoch 51, Validation Loss: 364.49869463178845\n",
      "Epoch 52, Validation Loss: 357.7091762754652\n",
      "Epoch 53, Validation Loss: 358.9934154086643\n",
      "Epoch 54, Validation Loss: 358.3429416020711\n",
      "Epoch 55, Validation Loss: 358.1276443269518\n",
      "Epoch 56, Validation Loss: 356.3366689682007\n",
      "Epoch 57, Validation Loss: 357.9512652291192\n",
      "Epoch 58, Validation Loss: 355.3984611299303\n",
      "Epoch 59, Validation Loss: 355.76161766052246\n",
      "Epoch 60, Validation Loss: 357.1710671318902\n",
      "Epoch 61, Validation Loss: 356.09550295935736\n",
      "Epoch 62, Validation Loss: 358.4034138785468\n",
      "Epoch 63, Validation Loss: 356.51568571726483\n",
      "Epoch 64, Validation Loss: 358.86646959516736\n",
      "Epoch 65, Validation Loss: 356.90875795152454\n",
      "Epoch 66, Validation Loss: 357.802606370714\n",
      "Epoch 67, Validation Loss: 356.0757369995117\n",
      "Epoch 68, Validation Loss: 355.36933612823486\n",
      "Epoch 69, Validation Loss: 356.7214781443278\n",
      "Epoch 70, Validation Loss: 356.59695042504205\n",
      "Epoch 71, Validation Loss: 353.02891699473065\n",
      "Epoch 72, Validation Loss: 356.7039970821804\n",
      "Epoch 73, Validation Loss: 353.44621711307104\n",
      "Epoch 74, Validation Loss: 355.1555274327596\n",
      "Epoch 75, Validation Loss: 355.06142054663763\n",
      "Epoch 76, Validation Loss: 355.36235968271893\n",
      "Epoch 77, Validation Loss: 355.88517072465686\n",
      "Epoch 78, Validation Loss: 354.30763212839764\n",
      "Epoch 79, Validation Loss: 354.47832033369275\n",
      "Epoch 80, Validation Loss: 358.11111715104846\n",
      "Epoch 81, Validation Loss: 358.1002305348714\n",
      "Epoch 82, Validation Loss: 353.5291222466363\n",
      "Epoch 83, Validation Loss: 361.3913350635105\n",
      "Epoch 84, Validation Loss: 357.8297872543335\n",
      "Epoch 85, Validation Loss: 358.960098584493\n",
      "Epoch 86, Validation Loss: 358.93421914842395\n",
      "Epoch 87, Validation Loss: 357.6959811316596\n",
      "Epoch 88, Validation Loss: 359.81229813893634\n",
      "Epoch 89, Validation Loss: 363.13468742370605\n",
      "Epoch 90, Validation Loss: 355.8397945827908\n",
      "Epoch 91, Validation Loss: 357.02645662095813\n",
      "Epoch 92, Validation Loss: 358.37135961320666\n",
      "Epoch 93, Validation Loss: 357.0945807562934\n",
      "Epoch 94, Validation Loss: 361.4349778493245\n",
      "Epoch 95, Validation Loss: 358.54710536532934\n",
      "Epoch 96, Validation Loss: 358.92997445000543\n",
      "Epoch 97, Validation Loss: 358.7083519829644\n",
      "Epoch 98, Validation Loss: 356.9449888865153\n",
      "Epoch 99, Validation Loss: 363.5038768980238\n",
      "Epoch 100, Validation Loss: 366.8341114256117\n",
      "Epoch 101, Validation Loss: 363.3183309766981\n",
      "Epoch 102, Validation Loss: 362.44387361738416\n",
      "Epoch 103, Validation Loss: 364.5573845969306\n",
      "Epoch 104, Validation Loss: 369.1267660988702\n",
      "Epoch 105, Validation Loss: 359.6831039852566\n",
      "Epoch 106, Validation Loss: 359.68921120961505\n",
      "Epoch 107, Validation Loss: 367.1037999259101\n",
      "Epoch 108, Validation Loss: 360.55345408121747\n",
      "Epoch 109, Validation Loss: 365.6901723013984\n",
      "Epoch 110, Validation Loss: 366.1844126383464\n",
      "Epoch 111, Validation Loss: 373.1097272237142\n",
      "Epoch 112, Validation Loss: 378.49910110897486\n",
      "Epoch 113, Validation Loss: 355.8483943939209\n",
      "Epoch 114, Validation Loss: 360.8781255086263\n",
      "Epoch 115, Validation Loss: 355.9603116777208\n",
      "Epoch 116, Validation Loss: 364.07434357537164\n",
      "Epoch 117, Validation Loss: 363.88440958658856\n",
      "Epoch 118, Validation Loss: 364.391693327162\n",
      "Epoch 119, Validation Loss: 365.00101968977185\n",
      "Epoch 120, Validation Loss: 367.22982184092206\n",
      "Epoch 121, Validation Loss: 368.9937090343899\n",
      "Epoch 122, Validation Loss: 364.9625637266371\n",
      "Epoch 123, Validation Loss: 372.66370497809515\n",
      "Epoch 124, Validation Loss: 366.7944754494561\n",
      "Epoch 125, Validation Loss: 365.4388654496935\n",
      "Epoch 126, Validation Loss: 368.854311519199\n",
      "Epoch 127, Validation Loss: 365.5360674328274\n",
      "Epoch 128, Validation Loss: 366.42783800760907\n",
      "Epoch 129, Validation Loss: 363.3910557428996\n",
      "Epoch 130, Validation Loss: 357.9692560831706\n",
      "Epoch 131, Validation Loss: 363.98100015852185\n",
      "Epoch 132, Validation Loss: 366.303505897522\n",
      "Epoch 133, Validation Loss: 358.29652902815076\n",
      "Epoch 134, Validation Loss: 364.6281920539008\n",
      "Epoch 135, Validation Loss: 365.5799543592665\n",
      "Epoch 136, Validation Loss: 371.4803580178155\n",
      "Epoch 137, Validation Loss: 366.1000629001194\n",
      "Epoch 138, Validation Loss: 368.12878693474664\n",
      "Epoch 139, Validation Loss: 363.9514807595147\n",
      "Epoch 140, Validation Loss: 366.78674570719403\n",
      "Epoch 141, Validation Loss: 366.1653940412733\n",
      "Epoch 142, Validation Loss: 364.38382095760767\n",
      "Epoch 143, Validation Loss: 370.0710282855564\n",
      "Epoch 144, Validation Loss: 376.04033120473224\n",
      "Epoch 145, Validation Loss: 370.76275581783716\n",
      "Epoch 146, Validation Loss: 373.05879963768854\n",
      "Epoch 147, Validation Loss: 367.087519009908\n",
      "Epoch 148, Validation Loss: 370.51254166497125\n",
      "Epoch 149, Validation Loss: 362.41043800777857\n",
      "Epoch 150, Validation Loss: 362.15016725328235\n",
      "Epoch 151, Validation Loss: 355.84837500254315\n",
      "Epoch 152, Validation Loss: 354.1017681757609\n",
      "Epoch 153, Validation Loss: 354.0185891257392\n",
      "Epoch 154, Validation Loss: 350.9789417054918\n",
      "Epoch 155, Validation Loss: 355.3554683261448\n",
      "Epoch 156, Validation Loss: 352.70927704705133\n",
      "Epoch 157, Validation Loss: 357.50254917144775\n",
      "Epoch 158, Validation Loss: 367.0435052447849\n",
      "Epoch 159, Validation Loss: 354.96388710869684\n",
      "Epoch 160, Validation Loss: 360.30441167619495\n",
      "Epoch 161, Validation Loss: 356.2517079247369\n",
      "Epoch 162, Validation Loss: 361.2156991958618\n",
      "Epoch 163, Validation Loss: 360.0990565617879\n",
      "Epoch 164, Validation Loss: 362.72738711039227\n",
      "Epoch 165, Validation Loss: 362.83889865875244\n",
      "Epoch 166, Validation Loss: 364.5737660725911\n",
      "Epoch 167, Validation Loss: 366.99421956804065\n",
      "Epoch 168, Validation Loss: 364.294990963406\n",
      "Epoch 169, Validation Loss: 361.1873417960273\n",
      "Epoch 170, Validation Loss: 373.26474062601727\n",
      "Epoch 171, Validation Loss: 364.9602165222168\n",
      "Epoch 172, Validation Loss: 363.6752559873793\n",
      "Epoch 173, Validation Loss: 365.51429155137805\n",
      "Epoch 174, Validation Loss: 361.4559901555379\n",
      "Epoch 175, Validation Loss: 360.51723681555853\n",
      "Epoch 176, Validation Loss: 359.4545313517253\n",
      "Epoch 177, Validation Loss: 371.1643026140001\n",
      "Epoch 178, Validation Loss: 371.30236021677655\n",
      "Epoch 179, Validation Loss: 376.22444894578723\n",
      "Epoch 180, Validation Loss: 376.3681458367242\n",
      "Epoch 181, Validation Loss: 364.77128961351184\n",
      "Epoch 182, Validation Loss: 378.56886683570013\n",
      "Epoch 183, Validation Loss: 381.37870608435736\n",
      "Epoch 184, Validation Loss: 382.2666947046916\n",
      "Epoch 185, Validation Loss: 375.74136871761743\n",
      "Epoch 186, Validation Loss: 382.51252312130396\n",
      "Epoch 187, Validation Loss: 372.4120694266425\n",
      "Epoch 188, Validation Loss: 390.36302873823377\n",
      "Epoch 189, Validation Loss: 398.9178824954563\n",
      "Epoch 190, Validation Loss: 388.48817592196997\n",
      "Epoch 191, Validation Loss: 398.9952046076457\n",
      "Epoch 192, Validation Loss: 385.9106442133586\n",
      "Epoch 193, Validation Loss: 372.31392150455054\n",
      "Epoch 194, Validation Loss: 384.9829756418864\n",
      "Epoch 195, Validation Loss: 384.5452988942464\n",
      "Epoch 196, Validation Loss: 382.98305288950604\n",
      "Epoch 197, Validation Loss: 385.6539651023017\n",
      "Epoch 198, Validation Loss: 381.228389316135\n",
      "Epoch 199, Validation Loss: 376.16128646002875\n",
      "Epoch 200, Validation Loss: 389.2291759914822\n",
      "RMSE on test set: 18.029905319213867\n",
      "segment_1\n",
      "before:  2020-09-25 00:29:25\n",
      "after:  2020-09-25 01:24:25\n",
      "segment_1\n",
      "before:  2020-09-27 13:44:34\n",
      "after:  2020-09-27 14:39:34\n",
      "segment_1\n",
      "before:  2020-09-27 14:54:33\n",
      "after:  2020-09-27 15:29:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-09-30 14:59:42\n",
      "after:  2020-09-30 15:15:04\n",
      "segment_2\n",
      "before:  2020-10-05 13:55:00\n",
      "after:  2020-10-05 14:35:00\n",
      "segment_2\n",
      "before:  2020-10-07 10:25:05\n",
      "after:  2020-10-07 11:00:05\n",
      "segment_3\n",
      "before:  2020-10-07 13:45:07\n",
      "after:  2020-10-07 14:15:06\n",
      "segment_3\n",
      "before:  2020-10-08 01:40:07\n",
      "after:  2020-10-08 02:25:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2533\n",
      "len of segment_df is  2196\n",
      "len of segment_df is  643\n",
      "len of segment_df is  2287\n",
      "len of features_list 7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1241.571738878886\n",
      "Epoch 2, Validation Loss: 1191.3774701754253\n",
      "Epoch 3, Validation Loss: 1151.8623541196187\n",
      "Epoch 4, Validation Loss: 1108.2750202814739\n",
      "Epoch 5, Validation Loss: 1084.254919052124\n",
      "Epoch 6, Validation Loss: 1065.799903869629\n",
      "Epoch 7, Validation Loss: 1052.7871402104695\n",
      "Epoch 8, Validation Loss: 1040.145338376363\n",
      "Epoch 9, Validation Loss: 1028.365951538086\n",
      "Epoch 10, Validation Loss: 1020.2697877883911\n",
      "Epoch 11, Validation Loss: 1018.6973841985067\n",
      "Epoch 12, Validation Loss: 1019.7020696004232\n",
      "Epoch 13, Validation Loss: 1020.3151035308838\n",
      "Epoch 14, Validation Loss: 1022.8724959691366\n",
      "Epoch 15, Validation Loss: 1027.7880407969158\n",
      "Epoch 16, Validation Loss: 1039.681100845337\n",
      "Epoch 17, Validation Loss: 1049.5748297373455\n",
      "Epoch 18, Validation Loss: 1037.125968615214\n",
      "Epoch 19, Validation Loss: 1055.369010925293\n",
      "Epoch 20, Validation Loss: 1063.641526222229\n",
      "Epoch 21, Validation Loss: 1117.073247909546\n",
      "Epoch 22, Validation Loss: 1108.196623166402\n",
      "Epoch 23, Validation Loss: 1139.8074690500896\n",
      "Epoch 24, Validation Loss: 1149.2938906351726\n",
      "Epoch 25, Validation Loss: 1170.3567991256714\n",
      "Epoch 26, Validation Loss: 1215.9415044784546\n",
      "Epoch 27, Validation Loss: 1228.8466806411743\n",
      "Epoch 28, Validation Loss: 1230.600512822469\n",
      "Epoch 29, Validation Loss: 1235.979000409444\n",
      "Epoch 30, Validation Loss: 1026.3196703592937\n",
      "Epoch 31, Validation Loss: 1211.7798309326172\n",
      "Epoch 32, Validation Loss: 1181.230152130127\n",
      "Epoch 33, Validation Loss: 1152.6393362681072\n",
      "Epoch 34, Validation Loss: 1183.2045958836873\n",
      "Epoch 35, Validation Loss: 1100.3686593373616\n",
      "Epoch 36, Validation Loss: 1101.4954560597737\n",
      "Epoch 37, Validation Loss: 1201.42303498586\n",
      "Epoch 38, Validation Loss: 1072.2732442220051\n",
      "Epoch 39, Validation Loss: 1049.730720837911\n",
      "Epoch 40, Validation Loss: 1041.7622702916462\n",
      "Epoch 41, Validation Loss: 1029.9807141621907\n",
      "Epoch 42, Validation Loss: 1046.0382102330525\n",
      "Epoch 43, Validation Loss: 1032.1261784235637\n",
      "Epoch 44, Validation Loss: 1025.813533147176\n",
      "Epoch 45, Validation Loss: 1068.437069574992\n",
      "Epoch 46, Validation Loss: 1145.5206289291382\n",
      "Epoch 47, Validation Loss: 1118.3125073115032\n",
      "Epoch 48, Validation Loss: 1010.7697658538818\n",
      "Epoch 49, Validation Loss: 1017.5286795298258\n",
      "Epoch 50, Validation Loss: 1010.9006414413452\n",
      "Epoch 51, Validation Loss: 1120.2266461054485\n",
      "Epoch 52, Validation Loss: 1125.0047245025635\n",
      "Epoch 53, Validation Loss: 1114.4965922037761\n",
      "Epoch 54, Validation Loss: 1089.0841204325359\n",
      "Epoch 55, Validation Loss: 1065.165184020996\n",
      "Epoch 56, Validation Loss: 1045.5106468200684\n",
      "Epoch 57, Validation Loss: 1149.634256362915\n",
      "Epoch 58, Validation Loss: 999.3737560907999\n",
      "Epoch 59, Validation Loss: 1040.6066897710164\n",
      "Epoch 60, Validation Loss: 1014.4241479237875\n",
      "Epoch 61, Validation Loss: 1032.4287395477295\n",
      "Epoch 62, Validation Loss: 1001.120558420817\n",
      "Epoch 63, Validation Loss: 1025.3458887736003\n",
      "Epoch 64, Validation Loss: 1018.2759784062704\n",
      "Epoch 65, Validation Loss: 1066.4948240915935\n",
      "Epoch 66, Validation Loss: 1077.4761905670166\n",
      "Epoch 67, Validation Loss: 1007.7194404602051\n",
      "Epoch 68, Validation Loss: 1043.1098839441936\n",
      "Epoch 69, Validation Loss: 1011.9322096506754\n",
      "Epoch 70, Validation Loss: 1010.9643863042196\n",
      "Epoch 71, Validation Loss: 1010.9555610020956\n",
      "Epoch 72, Validation Loss: 1008.6237554550171\n",
      "Epoch 73, Validation Loss: 1006.4914595286051\n",
      "Epoch 74, Validation Loss: 1009.1390953063965\n",
      "Epoch 75, Validation Loss: 1017.3525075912476\n",
      "Epoch 76, Validation Loss: 1008.1661828358968\n",
      "Epoch 77, Validation Loss: 1014.3755232493082\n",
      "Epoch 78, Validation Loss: 1022.488613764445\n",
      "Epoch 79, Validation Loss: 1093.5608196258545\n",
      "Epoch 80, Validation Loss: 1013.7784430185953\n",
      "Epoch 81, Validation Loss: 1016.284039815267\n",
      "Epoch 82, Validation Loss: 1018.4905865987142\n",
      "Epoch 83, Validation Loss: 1016.4997959136963\n",
      "Epoch 84, Validation Loss: 1032.159746170044\n",
      "Epoch 85, Validation Loss: 1005.6582412719727\n",
      "Epoch 86, Validation Loss: 1009.0536994934082\n",
      "Epoch 87, Validation Loss: 1030.2809762954712\n",
      "Epoch 88, Validation Loss: 1035.9574807484944\n",
      "Epoch 89, Validation Loss: 1026.98712793986\n",
      "Epoch 90, Validation Loss: 1056.5518423716228\n",
      "Epoch 91, Validation Loss: 1016.7319428126017\n",
      "Epoch 92, Validation Loss: 1020.2402737935384\n",
      "Epoch 93, Validation Loss: 1026.4386310577393\n",
      "Epoch 94, Validation Loss: 1019.7012736002604\n",
      "Epoch 95, Validation Loss: 1024.732966740926\n",
      "Epoch 96, Validation Loss: 1033.3611443837483\n",
      "Epoch 97, Validation Loss: 1021.1505908966064\n",
      "Epoch 98, Validation Loss: 1025.2897300720215\n",
      "Epoch 99, Validation Loss: 1031.5183760325115\n",
      "Epoch 100, Validation Loss: 1037.149564107259\n",
      "Epoch 101, Validation Loss: 1032.2824535369873\n",
      "Epoch 102, Validation Loss: 1044.267665863037\n",
      "Epoch 103, Validation Loss: 1043.9032878875732\n",
      "Epoch 104, Validation Loss: 1040.2990805308025\n",
      "Epoch 105, Validation Loss: 1033.372797648112\n",
      "Epoch 106, Validation Loss: 1038.3011128107707\n",
      "Epoch 107, Validation Loss: 1041.1174631118774\n",
      "Epoch 108, Validation Loss: 1027.4582405090332\n",
      "Epoch 109, Validation Loss: 1054.4805380503337\n",
      "Epoch 110, Validation Loss: 1036.7454636891682\n",
      "Epoch 111, Validation Loss: 1007.8324642181396\n",
      "Epoch 112, Validation Loss: 1036.8873017628987\n",
      "Epoch 113, Validation Loss: 1037.8167826334636\n",
      "Epoch 114, Validation Loss: 1045.3197002410889\n",
      "Epoch 115, Validation Loss: 1053.294843673706\n",
      "Epoch 116, Validation Loss: 1061.0869000752766\n",
      "Epoch 117, Validation Loss: 1075.0326639811199\n",
      "Epoch 118, Validation Loss: 1046.749927520752\n",
      "Epoch 119, Validation Loss: 1053.8141899108887\n",
      "Epoch 120, Validation Loss: 1042.5705973307292\n",
      "Epoch 121, Validation Loss: 1037.7930583953857\n",
      "Epoch 122, Validation Loss: 1037.695614496867\n",
      "Epoch 123, Validation Loss: 1024.4942623774211\n",
      "Epoch 124, Validation Loss: 1048.0280431111653\n",
      "Epoch 125, Validation Loss: 1047.1766157150269\n",
      "Epoch 126, Validation Loss: 1086.104105313619\n",
      "Epoch 127, Validation Loss: 1036.0020745595295\n",
      "Epoch 128, Validation Loss: 1038.6715984344482\n",
      "Epoch 129, Validation Loss: 1051.7455717722576\n",
      "Epoch 130, Validation Loss: 1132.3579368591309\n",
      "Epoch 131, Validation Loss: 1024.0049743652344\n",
      "Epoch 132, Validation Loss: 1044.512097676595\n",
      "Epoch 133, Validation Loss: 1073.5640347798665\n",
      "Epoch 134, Validation Loss: 1057.7643925348918\n",
      "Epoch 135, Validation Loss: 1050.8594687779744\n",
      "Epoch 136, Validation Loss: 1052.548620859782\n",
      "Epoch 137, Validation Loss: 1066.6060574849446\n",
      "Epoch 138, Validation Loss: 1066.7536118825276\n",
      "Epoch 139, Validation Loss: 1055.921257019043\n",
      "Epoch 140, Validation Loss: 1075.7589778900146\n",
      "Epoch 141, Validation Loss: 1065.593755086263\n",
      "Epoch 142, Validation Loss: 1068.864714940389\n",
      "Epoch 143, Validation Loss: 1070.6705280939739\n",
      "Epoch 144, Validation Loss: 1059.2069307963054\n",
      "Epoch 145, Validation Loss: 1109.2048075993855\n",
      "Epoch 146, Validation Loss: 1080.8746627171834\n",
      "Epoch 147, Validation Loss: 1116.8647352854412\n",
      "Epoch 148, Validation Loss: 1142.5419855117798\n",
      "Epoch 149, Validation Loss: 1110.369031270345\n",
      "Epoch 150, Validation Loss: 1101.8862930933635\n",
      "Epoch 151, Validation Loss: 1095.5394287109375\n",
      "Epoch 152, Validation Loss: 1102.6548169453938\n",
      "Epoch 153, Validation Loss: 1070.1545804341633\n",
      "Epoch 154, Validation Loss: 1054.662057240804\n",
      "Epoch 155, Validation Loss: 1101.5354957580566\n",
      "Epoch 156, Validation Loss: 1081.2915013631184\n",
      "Epoch 157, Validation Loss: 1107.3590609232585\n",
      "Epoch 158, Validation Loss: 1079.5071093241374\n",
      "Epoch 159, Validation Loss: 1080.3766282399495\n",
      "Epoch 160, Validation Loss: 1081.9168300628662\n",
      "Epoch 161, Validation Loss: 1101.9654865264893\n",
      "Epoch 162, Validation Loss: 1105.608413060506\n",
      "Epoch 163, Validation Loss: 1116.6249128977458\n",
      "Epoch 164, Validation Loss: 1143.6835641860962\n",
      "Epoch 165, Validation Loss: 1094.5945777893066\n",
      "Epoch 166, Validation Loss: 1122.0872745513916\n",
      "Epoch 167, Validation Loss: 1119.9301560719807\n",
      "Epoch 168, Validation Loss: 1066.572808901469\n",
      "Epoch 169, Validation Loss: 1081.6791311899822\n",
      "Epoch 170, Validation Loss: 1082.7460664113362\n",
      "Epoch 171, Validation Loss: 1048.5441818237305\n",
      "Epoch 172, Validation Loss: 1082.9705057144165\n",
      "Epoch 173, Validation Loss: 1091.9127683639526\n",
      "Epoch 174, Validation Loss: 1079.5120703379314\n",
      "Epoch 175, Validation Loss: 1118.0613250732422\n",
      "Epoch 176, Validation Loss: 1092.9190934499104\n",
      "Epoch 177, Validation Loss: 1071.160527865092\n",
      "Epoch 178, Validation Loss: 1058.7081079483032\n",
      "Epoch 179, Validation Loss: 1058.1433747609456\n",
      "Epoch 180, Validation Loss: 1065.3883905410767\n",
      "Epoch 181, Validation Loss: 1055.0437971750896\n",
      "Epoch 182, Validation Loss: 1063.3997170130413\n",
      "Epoch 183, Validation Loss: 1094.2003625233967\n",
      "Epoch 184, Validation Loss: 1053.0534499486287\n",
      "Epoch 185, Validation Loss: 1067.6275084813435\n",
      "Epoch 186, Validation Loss: 1081.632474263509\n",
      "Epoch 187, Validation Loss: 1080.073631922404\n",
      "Epoch 188, Validation Loss: 1051.227825164795\n",
      "Epoch 189, Validation Loss: 1055.2692810694377\n",
      "Epoch 190, Validation Loss: 1045.9043887456257\n",
      "Epoch 191, Validation Loss: 1053.7634992599487\n",
      "Epoch 192, Validation Loss: 1073.3382844924927\n",
      "Epoch 193, Validation Loss: 1049.898947397868\n",
      "Epoch 194, Validation Loss: 1090.201639175415\n",
      "Epoch 195, Validation Loss: 1052.6997826894124\n",
      "Epoch 196, Validation Loss: 1044.0358778635662\n",
      "Epoch 197, Validation Loss: 1040.5257390340169\n",
      "Epoch 198, Validation Loss: 1037.464587211609\n",
      "Epoch 199, Validation Loss: 1030.013784090678\n",
      "Epoch 200, Validation Loss: 1056.3675212860107\n",
      "RMSE on test set: 19.94300079345703\n",
      "segment_1\n",
      "before:  2020-11-10 04:42:56\n",
      "after:  2020-11-10 05:12:57\n",
      "segment_1\n",
      "before:  2020-11-10 13:07:56\n",
      "after:  2020-11-10 13:42:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-11-10 20:37:59\n",
      "after:  2020-11-10 21:32:56\n",
      "segment_2\n",
      "before:  2020-11-10 22:17:56\n",
      "after:  2020-11-10 23:12:56\n",
      "segment_4\n",
      "before:  2020-11-11 05:22:56\n",
      "after:  2020-11-11 06:12:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.08' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.23' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  864\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of segment_df is  576\n",
      "len of segment_df is  600\n",
      "len of segment_df is  252\n",
      "len of segment_df is  0\n",
      "len of features_list 2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1103.2405569893974\n",
      "Epoch 2, Validation Loss: 1101.0787876674108\n",
      "Epoch 3, Validation Loss: 1091.1430969238281\n",
      "Epoch 4, Validation Loss: 1081.943320138114\n",
      "Epoch 5, Validation Loss: 1074.3517194475446\n",
      "Epoch 6, Validation Loss: 1067.2527662004743\n",
      "Epoch 7, Validation Loss: 1060.0807538713727\n",
      "Epoch 8, Validation Loss: 1053.0581883021764\n",
      "Epoch 9, Validation Loss: 1045.84858921596\n",
      "Epoch 10, Validation Loss: 1039.248792375837\n",
      "Epoch 11, Validation Loss: 1032.05717250279\n",
      "Epoch 12, Validation Loss: 1025.712892804827\n",
      "Epoch 13, Validation Loss: 1019.787111554827\n",
      "Epoch 14, Validation Loss: 1012.7672446114676\n",
      "Epoch 15, Validation Loss: 1005.9293954031808\n",
      "Epoch 16, Validation Loss: 999.4317256382534\n",
      "Epoch 17, Validation Loss: 992.4348776681082\n",
      "Epoch 18, Validation Loss: 986.6677856445312\n",
      "Epoch 19, Validation Loss: 981.2235238211496\n",
      "Epoch 20, Validation Loss: 975.81154523577\n",
      "Epoch 21, Validation Loss: 969.6256801060268\n",
      "Epoch 22, Validation Loss: 964.1282697405134\n",
      "Epoch 23, Validation Loss: 958.2673775809152\n",
      "Epoch 24, Validation Loss: 954.194562639509\n",
      "Epoch 25, Validation Loss: 947.6367885044643\n",
      "Epoch 26, Validation Loss: 942.8584267752511\n",
      "Epoch 27, Validation Loss: 940.7118726457868\n",
      "Epoch 28, Validation Loss: 935.4848872593471\n",
      "Epoch 29, Validation Loss: 931.4328896658761\n",
      "Epoch 30, Validation Loss: 927.8932778494699\n",
      "Epoch 31, Validation Loss: 922.1213727678571\n",
      "Epoch 32, Validation Loss: 920.4436623709543\n",
      "Epoch 33, Validation Loss: 916.259521484375\n",
      "Epoch 34, Validation Loss: 910.8280857631138\n",
      "Epoch 35, Validation Loss: 909.6465759277344\n",
      "Epoch 36, Validation Loss: 907.0132250104632\n",
      "Epoch 37, Validation Loss: 905.0530526297433\n",
      "Epoch 38, Validation Loss: 899.9017050606864\n",
      "Epoch 39, Validation Loss: 898.3186122349331\n",
      "Epoch 40, Validation Loss: 894.092023577009\n",
      "Epoch 41, Validation Loss: 892.3308563232422\n",
      "Epoch 42, Validation Loss: 889.1172594342913\n",
      "Epoch 43, Validation Loss: 887.8805694580078\n",
      "Epoch 44, Validation Loss: 884.5947374616351\n",
      "Epoch 45, Validation Loss: 883.1383013044085\n",
      "Epoch 46, Validation Loss: 880.8759068080357\n",
      "Epoch 47, Validation Loss: 878.5184631347656\n",
      "Epoch 48, Validation Loss: 877.0003029959543\n",
      "Epoch 49, Validation Loss: 873.8282972063337\n",
      "Epoch 50, Validation Loss: 875.4517931256976\n",
      "Epoch 51, Validation Loss: 872.7314213344029\n",
      "Epoch 52, Validation Loss: 870.8545989990234\n",
      "Epoch 53, Validation Loss: 868.2132284981864\n",
      "Epoch 54, Validation Loss: 866.039302280971\n",
      "Epoch 55, Validation Loss: 863.8469085693359\n",
      "Epoch 56, Validation Loss: 863.4606257847378\n",
      "Epoch 57, Validation Loss: 861.6824210030692\n",
      "Epoch 58, Validation Loss: 859.1833103724888\n",
      "Epoch 59, Validation Loss: 857.8944266183036\n",
      "Epoch 60, Validation Loss: 856.2600969587054\n",
      "Epoch 61, Validation Loss: 854.4214477539062\n",
      "Epoch 62, Validation Loss: 854.805906023298\n",
      "Epoch 63, Validation Loss: 853.3697095598493\n",
      "Epoch 64, Validation Loss: 852.427982875279\n",
      "Epoch 65, Validation Loss: 852.8899688720703\n",
      "Epoch 66, Validation Loss: 851.4117104666574\n",
      "Epoch 67, Validation Loss: 850.6442347935268\n",
      "Epoch 68, Validation Loss: 849.4919847760882\n",
      "Epoch 69, Validation Loss: 847.97414289202\n",
      "Epoch 70, Validation Loss: 848.1660810198102\n",
      "Epoch 71, Validation Loss: 847.4483882359096\n",
      "Epoch 72, Validation Loss: 846.9728393554688\n",
      "Epoch 73, Validation Loss: 847.2471378871372\n",
      "Epoch 74, Validation Loss: 848.0379856654575\n",
      "Epoch 75, Validation Loss: 845.9513680594308\n",
      "Epoch 76, Validation Loss: 846.4442923409598\n",
      "Epoch 77, Validation Loss: 846.4627881731305\n",
      "Epoch 78, Validation Loss: 847.5432390485491\n",
      "Epoch 79, Validation Loss: 844.5893075125558\n",
      "Epoch 80, Validation Loss: 843.2843388148716\n",
      "Epoch 81, Validation Loss: 843.5365447998047\n",
      "Epoch 82, Validation Loss: 842.3675646100726\n",
      "Epoch 83, Validation Loss: 842.1514369419643\n",
      "Epoch 84, Validation Loss: 841.2520424979074\n",
      "Epoch 85, Validation Loss: 841.3896244594029\n",
      "Epoch 86, Validation Loss: 840.1187678745815\n",
      "Epoch 87, Validation Loss: 840.2957087925503\n",
      "Epoch 88, Validation Loss: 841.1190294538226\n",
      "Epoch 89, Validation Loss: 841.0271301269531\n",
      "Epoch 90, Validation Loss: 842.2252894810268\n",
      "Epoch 91, Validation Loss: 841.5992104666574\n",
      "Epoch 92, Validation Loss: 839.5420401436942\n",
      "Epoch 93, Validation Loss: 840.1499633789062\n",
      "Epoch 94, Validation Loss: 839.706050327846\n",
      "Epoch 95, Validation Loss: 841.351787022182\n",
      "Epoch 96, Validation Loss: 841.4474814278739\n",
      "Epoch 97, Validation Loss: 844.7087816510882\n",
      "Epoch 98, Validation Loss: 840.20017351423\n",
      "Epoch 99, Validation Loss: 843.3229697091239\n",
      "Epoch 100, Validation Loss: 842.6237051827567\n",
      "Epoch 101, Validation Loss: 840.2767116001675\n",
      "Epoch 102, Validation Loss: 844.5838993617466\n",
      "Epoch 103, Validation Loss: 847.1602935791016\n",
      "Epoch 104, Validation Loss: 840.387224469866\n",
      "Epoch 105, Validation Loss: 843.9850398472378\n",
      "Epoch 106, Validation Loss: 844.9753025599888\n",
      "Epoch 107, Validation Loss: 848.6436200823102\n",
      "Epoch 108, Validation Loss: 848.2233646937779\n",
      "Epoch 109, Validation Loss: 846.8809749058315\n",
      "Epoch 110, Validation Loss: 848.3090035574777\n",
      "Epoch 111, Validation Loss: 850.3830936976841\n",
      "Epoch 112, Validation Loss: 847.663312639509\n",
      "Epoch 113, Validation Loss: 850.899396623884\n",
      "Epoch 114, Validation Loss: 851.4892251150949\n",
      "Epoch 115, Validation Loss: 850.8401598249163\n",
      "Epoch 116, Validation Loss: 851.1874760219029\n",
      "Epoch 117, Validation Loss: 845.1440080915179\n",
      "Epoch 118, Validation Loss: 845.9896414620536\n",
      "Epoch 119, Validation Loss: 846.6109706333706\n",
      "Epoch 120, Validation Loss: 847.7514997209821\n",
      "Epoch 121, Validation Loss: 848.9715554373605\n",
      "Epoch 122, Validation Loss: 847.5975646972656\n",
      "Epoch 123, Validation Loss: 851.6892569405692\n",
      "Epoch 124, Validation Loss: 849.0610264369419\n",
      "Epoch 125, Validation Loss: 843.8858925955636\n",
      "Epoch 126, Validation Loss: 847.8306644984654\n",
      "Epoch 127, Validation Loss: 849.3013065883091\n",
      "Epoch 128, Validation Loss: 850.4486563546317\n",
      "Epoch 129, Validation Loss: 848.7825099400112\n",
      "Epoch 130, Validation Loss: 848.2886744907925\n",
      "Epoch 131, Validation Loss: 850.2852151053293\n",
      "Epoch 132, Validation Loss: 850.875974382673\n",
      "Epoch 133, Validation Loss: 853.4416787283761\n",
      "Epoch 134, Validation Loss: 850.2656729561942\n",
      "Epoch 135, Validation Loss: 854.9719783238003\n",
      "Epoch 136, Validation Loss: 848.6602739606585\n",
      "Epoch 137, Validation Loss: 847.489015851702\n",
      "Epoch 138, Validation Loss: 850.5869424002511\n",
      "Epoch 139, Validation Loss: 853.9033835274832\n",
      "Epoch 140, Validation Loss: 853.4818660191128\n",
      "Epoch 141, Validation Loss: 850.8488464355469\n",
      "Epoch 142, Validation Loss: 852.7613154820034\n",
      "Epoch 143, Validation Loss: 854.1732046944754\n",
      "Epoch 144, Validation Loss: 852.5035487583706\n",
      "Epoch 145, Validation Loss: 854.9975149972098\n",
      "Epoch 146, Validation Loss: 857.6163177490234\n",
      "Epoch 147, Validation Loss: 857.0010768345425\n",
      "Epoch 148, Validation Loss: 850.6227199009487\n",
      "Epoch 149, Validation Loss: 849.711920601981\n",
      "Epoch 150, Validation Loss: 850.9286978585379\n",
      "Epoch 151, Validation Loss: 857.7274758475168\n",
      "Epoch 152, Validation Loss: 857.1127777099609\n",
      "Epoch 153, Validation Loss: 855.3769640241351\n",
      "Epoch 154, Validation Loss: 856.7074890136719\n",
      "Epoch 155, Validation Loss: 852.9098379952567\n",
      "Epoch 156, Validation Loss: 856.0633370535714\n",
      "Epoch 157, Validation Loss: 856.985846383231\n",
      "Epoch 158, Validation Loss: 861.3548932756696\n",
      "Epoch 159, Validation Loss: 859.9258771623884\n",
      "Epoch 160, Validation Loss: 858.6711665562221\n",
      "Epoch 161, Validation Loss: 847.8882620675223\n",
      "Epoch 162, Validation Loss: 860.7273450578962\n",
      "Epoch 163, Validation Loss: 856.3958827427456\n",
      "Epoch 164, Validation Loss: 857.5496673583984\n",
      "Epoch 165, Validation Loss: 854.5972922188895\n",
      "Epoch 166, Validation Loss: 849.8299822126116\n",
      "Epoch 167, Validation Loss: 855.9900360107422\n",
      "Epoch 168, Validation Loss: 856.4217485700335\n",
      "Epoch 169, Validation Loss: 846.3517172677176\n",
      "Epoch 170, Validation Loss: 843.5696585518973\n",
      "Epoch 171, Validation Loss: 843.6228397914341\n",
      "Epoch 172, Validation Loss: 841.6535273960659\n",
      "Epoch 173, Validation Loss: 846.6504331316266\n",
      "Epoch 174, Validation Loss: 854.1025150844029\n",
      "Epoch 175, Validation Loss: 852.250978742327\n",
      "Epoch 176, Validation Loss: 857.0334788731167\n",
      "Epoch 177, Validation Loss: 859.7713448660714\n",
      "Epoch 178, Validation Loss: 866.6289073399136\n",
      "Epoch 179, Validation Loss: 862.7113407679966\n",
      "Epoch 180, Validation Loss: 857.04392351423\n",
      "Epoch 181, Validation Loss: 858.9051382882254\n",
      "Epoch 182, Validation Loss: 850.248066493443\n",
      "Epoch 183, Validation Loss: 844.5635190691266\n",
      "Epoch 184, Validation Loss: 860.5826176234654\n",
      "Epoch 185, Validation Loss: 862.1358773367746\n",
      "Epoch 186, Validation Loss: 867.6646597726004\n",
      "Epoch 187, Validation Loss: 867.6385519845145\n",
      "Epoch 188, Validation Loss: 874.6650717599051\n",
      "Epoch 189, Validation Loss: 877.8837062290737\n",
      "Epoch 190, Validation Loss: 869.9884120396206\n",
      "Epoch 191, Validation Loss: 875.2267139979771\n",
      "Epoch 192, Validation Loss: 884.554192679269\n",
      "Epoch 193, Validation Loss: 897.9835510253906\n",
      "Epoch 194, Validation Loss: 884.8669782366071\n",
      "Epoch 195, Validation Loss: 883.5770329066685\n",
      "Epoch 196, Validation Loss: 865.3676256452288\n",
      "Epoch 197, Validation Loss: 867.023435320173\n",
      "Epoch 198, Validation Loss: 861.5594111851284\n",
      "Epoch 199, Validation Loss: 865.9392416817801\n",
      "Epoch 200, Validation Loss: 867.5539681570871\n",
      "RMSE on test set: 30.38313102722168\n",
      "segment_2\n",
      "before:  2020-05-22 01:48:05\n",
      "after:  2020-05-22 01:58:05\n",
      "segment_2\n",
      "before:  2020-05-22 01:58:05\n",
      "after:  2020-05-22 02:08:05\n",
      "segment_2\n",
      "before:  2020-05-22 02:08:05\n",
      "after:  2020-05-22 02:18:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2257\n",
      "len of segment_df is  18\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of features_list 2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 979.4480498177664\n",
      "Epoch 2, Validation Loss: 1034.529087611607\n",
      "Epoch 3, Validation Loss: 1268.6502881731305\n",
      "Epoch 4, Validation Loss: 1412.0214298793248\n",
      "Epoch 5, Validation Loss: 1397.5888366699219\n",
      "Epoch 6, Validation Loss: 1376.7184382847377\n",
      "Epoch 7, Validation Loss: 1367.8838370186943\n",
      "Epoch 8, Validation Loss: 1362.6793605259486\n",
      "Epoch 9, Validation Loss: 1322.3353990827288\n",
      "Epoch 10, Validation Loss: 1334.4305419921875\n",
      "Epoch 11, Validation Loss: 1309.7139674595423\n",
      "Epoch 12, Validation Loss: 1304.2722734723773\n",
      "Epoch 13, Validation Loss: 1300.2095533098493\n",
      "Epoch 14, Validation Loss: 1282.9427664620537\n",
      "Epoch 15, Validation Loss: 1537.8667733328682\n",
      "Epoch 16, Validation Loss: 1292.2635345458984\n",
      "Epoch 17, Validation Loss: 1251.1514369419642\n",
      "Epoch 18, Validation Loss: 1243.5386505126953\n",
      "Epoch 19, Validation Loss: 1226.112265450614\n",
      "Epoch 20, Validation Loss: 1229.4998016357422\n",
      "Epoch 21, Validation Loss: 1180.1865016392298\n",
      "Epoch 22, Validation Loss: 1168.029268537249\n",
      "Epoch 23, Validation Loss: 1201.3552812848773\n",
      "Epoch 24, Validation Loss: 1248.321768624442\n",
      "Epoch 25, Validation Loss: 1140.4452274867467\n",
      "Epoch 26, Validation Loss: 1103.190630231585\n",
      "Epoch 27, Validation Loss: 1082.2469155447823\n",
      "Epoch 28, Validation Loss: 1047.7629165649414\n",
      "Epoch 29, Validation Loss: 1022.6115831647601\n",
      "Epoch 30, Validation Loss: 1004.2249058314732\n",
      "Epoch 31, Validation Loss: 986.5386679513114\n",
      "Epoch 32, Validation Loss: 969.0115944998605\n",
      "Epoch 33, Validation Loss: 947.8041719709124\n",
      "Epoch 34, Validation Loss: 940.0354919433594\n",
      "Epoch 35, Validation Loss: 932.2583062308175\n",
      "Epoch 36, Validation Loss: 922.5069220406668\n",
      "Epoch 37, Validation Loss: 912.4302869524274\n",
      "Epoch 38, Validation Loss: 903.0334734235491\n",
      "Epoch 39, Validation Loss: 913.7776805332729\n",
      "Epoch 40, Validation Loss: 896.418720790318\n",
      "Epoch 41, Validation Loss: 885.6344288417271\n",
      "Epoch 42, Validation Loss: 880.7386371067593\n",
      "Epoch 43, Validation Loss: 872.1187062944684\n",
      "Epoch 44, Validation Loss: 871.3698403494699\n",
      "Epoch 45, Validation Loss: 866.0772835867746\n",
      "Epoch 46, Validation Loss: 858.8567733764648\n",
      "Epoch 47, Validation Loss: 865.969104221889\n",
      "Epoch 48, Validation Loss: 851.2284682137625\n",
      "Epoch 49, Validation Loss: 860.4191932678223\n",
      "Epoch 50, Validation Loss: 870.375681740897\n",
      "Epoch 51, Validation Loss: 846.2177292960031\n",
      "Epoch 52, Validation Loss: 851.2529623849051\n",
      "Epoch 53, Validation Loss: 837.1397247314453\n",
      "Epoch 54, Validation Loss: 851.7038759504046\n",
      "Epoch 55, Validation Loss: 839.5191901070731\n",
      "Epoch 56, Validation Loss: 928.0685697283063\n",
      "Epoch 57, Validation Loss: 841.260434286935\n",
      "Epoch 58, Validation Loss: 876.1985244750977\n",
      "Epoch 59, Validation Loss: 860.8013098580496\n",
      "Epoch 60, Validation Loss: 838.7005914960589\n",
      "Epoch 61, Validation Loss: 910.4479413713727\n",
      "Epoch 62, Validation Loss: 941.1258231571743\n",
      "Epoch 63, Validation Loss: 979.063117163522\n",
      "Epoch 64, Validation Loss: 994.4563947405134\n",
      "Epoch 65, Validation Loss: 949.9154447828021\n",
      "Epoch 66, Validation Loss: 1001.6095273154123\n",
      "Epoch 67, Validation Loss: 1000.2428016662598\n",
      "Epoch 68, Validation Loss: 1053.261504309518\n",
      "Epoch 69, Validation Loss: 903.6171027592251\n",
      "Epoch 70, Validation Loss: 1052.4418651035853\n",
      "Epoch 71, Validation Loss: 1022.5952023097446\n",
      "Epoch 72, Validation Loss: 1016.1463454110282\n",
      "Epoch 73, Validation Loss: 1021.7588947841099\n",
      "Epoch 74, Validation Loss: 1005.0682653699603\n",
      "Epoch 75, Validation Loss: 1029.222912652152\n",
      "Epoch 76, Validation Loss: 1056.284769330706\n",
      "Epoch 77, Validation Loss: 1051.8936323438372\n",
      "Epoch 78, Validation Loss: 1029.5844680241175\n",
      "Epoch 79, Validation Loss: 1039.1923179626465\n",
      "Epoch 80, Validation Loss: 1063.044082369123\n",
      "Epoch 81, Validation Loss: 1049.1199373517718\n",
      "Epoch 82, Validation Loss: 1016.5335243770054\n",
      "Epoch 83, Validation Loss: 1065.9881311144147\n",
      "Epoch 84, Validation Loss: 1061.6270735604423\n",
      "Epoch 85, Validation Loss: 1066.657328741891\n",
      "Epoch 86, Validation Loss: 1062.9396528516497\n",
      "Epoch 87, Validation Loss: 1072.465708868844\n",
      "Epoch 88, Validation Loss: 1094.907498223441\n",
      "Epoch 89, Validation Loss: 1052.3938694000244\n",
      "Epoch 90, Validation Loss: 1026.9042173113141\n",
      "Epoch 91, Validation Loss: 1069.2330875396729\n",
      "Epoch 92, Validation Loss: 1072.903216770717\n",
      "Epoch 93, Validation Loss: 1050.569248199463\n",
      "Epoch 94, Validation Loss: 1049.727437700544\n",
      "Epoch 95, Validation Loss: 1066.7036040169853\n",
      "Epoch 96, Validation Loss: 1081.5435622079033\n",
      "Epoch 97, Validation Loss: 1030.4253583635602\n",
      "Epoch 98, Validation Loss: 1029.7482332502093\n",
      "Epoch 99, Validation Loss: 1022.9702598026821\n",
      "Epoch 100, Validation Loss: 1018.5627689361572\n",
      "Epoch 101, Validation Loss: 954.5161604200091\n",
      "Epoch 102, Validation Loss: 1107.6200899396624\n",
      "Epoch 103, Validation Loss: 960.1694548470633\n",
      "Epoch 104, Validation Loss: 1013.9323703220913\n",
      "Epoch 105, Validation Loss: 1016.7585302080427\n",
      "Epoch 106, Validation Loss: 1040.6229117257255\n",
      "Epoch 107, Validation Loss: 1074.7907842908587\n",
      "Epoch 108, Validation Loss: 1039.8559161594935\n",
      "Epoch 109, Validation Loss: 1060.425248827253\n",
      "Epoch 110, Validation Loss: 1077.1732343946185\n",
      "Epoch 111, Validation Loss: 1036.707420349121\n",
      "Epoch 112, Validation Loss: 1068.9296449933734\n",
      "Epoch 113, Validation Loss: 1045.402244567871\n",
      "Epoch 114, Validation Loss: 1103.0625588553291\n",
      "Epoch 115, Validation Loss: 1100.7185925074987\n",
      "Epoch 116, Validation Loss: 1164.1190201895577\n",
      "Epoch 117, Validation Loss: 995.7103936331613\n",
      "Epoch 118, Validation Loss: 1081.1907373155866\n",
      "Epoch 119, Validation Loss: 1115.8963091714043\n",
      "Epoch 120, Validation Loss: 1101.0491529192243\n",
      "Epoch 121, Validation Loss: 1141.5187317984444\n",
      "Epoch 122, Validation Loss: 1165.8190318516322\n",
      "Epoch 123, Validation Loss: 1203.6327056884766\n",
      "Epoch 124, Validation Loss: 1199.6443361554827\n",
      "Epoch 125, Validation Loss: 1201.0688198634557\n",
      "Epoch 126, Validation Loss: 1073.7566969735283\n",
      "Epoch 127, Validation Loss: 1012.214478628976\n",
      "Epoch 128, Validation Loss: 1140.5980769566127\n",
      "Epoch 129, Validation Loss: 1204.9491958618164\n",
      "Epoch 130, Validation Loss: 1214.4700513567243\n",
      "Epoch 131, Validation Loss: 1242.2547732761927\n",
      "Epoch 132, Validation Loss: 1358.9717712402344\n",
      "Epoch 133, Validation Loss: 1326.0740879603795\n",
      "Epoch 134, Validation Loss: 1269.3097654070173\n",
      "Epoch 135, Validation Loss: 1434.450593130929\n",
      "Epoch 136, Validation Loss: 1370.1182359967913\n",
      "Epoch 137, Validation Loss: 1429.2560511997767\n",
      "Epoch 138, Validation Loss: 1427.4531664167132\n",
      "Epoch 139, Validation Loss: 1345.3087812151227\n",
      "Epoch 140, Validation Loss: 1321.723626273019\n",
      "Epoch 141, Validation Loss: 1276.9508939470563\n",
      "Epoch 142, Validation Loss: 1157.8925007411412\n",
      "Epoch 143, Validation Loss: 1050.1888531276159\n",
      "Epoch 144, Validation Loss: 1131.1660161699567\n",
      "Epoch 145, Validation Loss: 1047.1521584647041\n",
      "Epoch 146, Validation Loss: 1034.4021110534668\n",
      "Epoch 147, Validation Loss: 1000.6924381256104\n",
      "Epoch 148, Validation Loss: 1075.1948138645716\n",
      "Epoch 149, Validation Loss: 999.9413206917899\n",
      "Epoch 150, Validation Loss: 1064.6422369820732\n",
      "Epoch 151, Validation Loss: 1102.5065819876534\n",
      "Epoch 152, Validation Loss: 1037.7555994306292\n",
      "Epoch 153, Validation Loss: 1131.606775556292\n",
      "Epoch 154, Validation Loss: 1198.9851728166852\n",
      "Epoch 155, Validation Loss: 1211.806856427874\n",
      "Epoch 156, Validation Loss: 1167.5091116768974\n",
      "Epoch 157, Validation Loss: 1147.266832079206\n",
      "Epoch 158, Validation Loss: 1407.7965981619698\n",
      "Epoch 159, Validation Loss: 1296.3341718401227\n",
      "Epoch 160, Validation Loss: 1276.9327011108398\n",
      "Epoch 161, Validation Loss: 1331.5419082641602\n",
      "Epoch 162, Validation Loss: 1355.2762189592634\n",
      "Epoch 163, Validation Loss: 1264.399069104876\n",
      "Epoch 164, Validation Loss: 1344.8082645961217\n",
      "Epoch 165, Validation Loss: 1489.867704119001\n",
      "Epoch 166, Validation Loss: 1478.1441312517438\n",
      "Epoch 167, Validation Loss: 1535.1249335152763\n",
      "Epoch 168, Validation Loss: 1486.4790616716657\n",
      "Epoch 169, Validation Loss: 1106.6986171177455\n",
      "Epoch 170, Validation Loss: 1488.082590375628\n",
      "Epoch 171, Validation Loss: 1376.7040230887276\n",
      "Epoch 172, Validation Loss: 1328.3468693324498\n",
      "Epoch 173, Validation Loss: 1578.604637145996\n",
      "Epoch 174, Validation Loss: 1115.514826638358\n",
      "Epoch 175, Validation Loss: 1272.0904808044434\n",
      "Epoch 176, Validation Loss: 1152.1939501081195\n",
      "Epoch 177, Validation Loss: 1217.4800472259521\n",
      "Epoch 178, Validation Loss: 1295.5937380109515\n",
      "Epoch 179, Validation Loss: 1359.0724797930036\n",
      "Epoch 180, Validation Loss: 1378.2810069492884\n",
      "Epoch 181, Validation Loss: 1188.8677226475306\n",
      "Epoch 182, Validation Loss: 1129.281097957066\n",
      "Epoch 183, Validation Loss: 1258.3532398768834\n",
      "Epoch 184, Validation Loss: 1225.6377116612025\n",
      "Epoch 185, Validation Loss: 1289.4117540631976\n",
      "Epoch 186, Validation Loss: 1182.0549332754952\n",
      "Epoch 187, Validation Loss: 1061.7692593165807\n",
      "Epoch 188, Validation Loss: 1168.2831151144844\n",
      "Epoch 189, Validation Loss: 1171.6641169956752\n",
      "Epoch 190, Validation Loss: 1230.8087921142578\n",
      "Epoch 191, Validation Loss: 893.1886209760394\n",
      "Epoch 192, Validation Loss: 971.711674281529\n",
      "Epoch 193, Validation Loss: 1052.6121183122907\n",
      "Epoch 194, Validation Loss: 1305.606294631958\n",
      "Epoch 195, Validation Loss: 1019.9005546569824\n",
      "Epoch 196, Validation Loss: 1160.1190141950335\n",
      "Epoch 197, Validation Loss: 1433.8104553222656\n",
      "Epoch 198, Validation Loss: 1432.967408316476\n",
      "Epoch 199, Validation Loss: 1285.32201058524\n",
      "Epoch 200, Validation Loss: 1508.1635055541992\n",
      "RMSE on test set: 38.19290542602539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2132\n",
      "len of segment_df is  2517\n",
      "len of segment_df is  323\n",
      "len of segment_df is  2856\n",
      "len of segment_df is  143\n",
      "len of features_list 7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1514.7254758453369\n",
      "Epoch 2, Validation Loss: 1493.4703018951416\n",
      "Epoch 3, Validation Loss: 1490.368906402588\n",
      "Epoch 4, Validation Loss: 1464.7524592590332\n",
      "Epoch 5, Validation Loss: 1449.4807788848877\n",
      "Epoch 6, Validation Loss: 1438.2822220611572\n",
      "Epoch 7, Validation Loss: 1425.0823397827148\n",
      "Epoch 8, Validation Loss: 1424.4110047149659\n",
      "Epoch 9, Validation Loss: 1405.9335554504394\n",
      "Epoch 10, Validation Loss: 1424.5816656494142\n",
      "Epoch 11, Validation Loss: 1397.1515202331543\n",
      "Epoch 12, Validation Loss: 1403.5083520507812\n",
      "Epoch 13, Validation Loss: 1368.6836346435548\n",
      "Epoch 14, Validation Loss: 1363.9708392333985\n",
      "Epoch 15, Validation Loss: 1387.185528564453\n",
      "Epoch 16, Validation Loss: 1346.0534930419922\n",
      "Epoch 17, Validation Loss: 1333.861323852539\n",
      "Epoch 18, Validation Loss: 1331.6733715820312\n",
      "Epoch 19, Validation Loss: 1322.7782507324218\n",
      "Epoch 20, Validation Loss: 1319.0180178833007\n",
      "Epoch 21, Validation Loss: 1316.5361529541015\n",
      "Epoch 22, Validation Loss: 1313.6026422119141\n",
      "Epoch 23, Validation Loss: 1307.9760382080078\n",
      "Epoch 24, Validation Loss: 1305.3535272216798\n",
      "Epoch 25, Validation Loss: 1302.5677911376954\n",
      "Epoch 26, Validation Loss: 1301.1449594116211\n",
      "Epoch 27, Validation Loss: 1299.6858367919922\n",
      "Epoch 28, Validation Loss: 1298.2615478515625\n",
      "Epoch 29, Validation Loss: 1296.8660559082032\n",
      "Epoch 30, Validation Loss: 1295.5333630371094\n",
      "Epoch 31, Validation Loss: 1295.888569946289\n",
      "Epoch 32, Validation Loss: 1295.1425161743164\n",
      "Epoch 33, Validation Loss: 1294.3662005615233\n",
      "Epoch 34, Validation Loss: 1294.5810229492188\n",
      "Epoch 35, Validation Loss: 1293.5903631591798\n",
      "Epoch 36, Validation Loss: 1293.306187133789\n",
      "Epoch 37, Validation Loss: 1293.6540316772462\n",
      "Epoch 38, Validation Loss: 1293.0077062988282\n",
      "Epoch 39, Validation Loss: 1292.778247680664\n",
      "Epoch 40, Validation Loss: 1296.472763671875\n",
      "Epoch 41, Validation Loss: 1293.3790002441406\n",
      "Epoch 42, Validation Loss: 1293.327728881836\n",
      "Epoch 43, Validation Loss: 1294.3595849609376\n",
      "Epoch 44, Validation Loss: 1294.41875\n",
      "Epoch 45, Validation Loss: 1291.1551489257813\n",
      "Epoch 46, Validation Loss: 1294.420451965332\n",
      "Epoch 47, Validation Loss: 1293.2282421875\n",
      "Epoch 48, Validation Loss: 1295.871131286621\n",
      "Epoch 49, Validation Loss: 1292.6906826782226\n",
      "Epoch 50, Validation Loss: 1293.9990307617188\n",
      "Epoch 51, Validation Loss: 1295.4646618652343\n",
      "Epoch 52, Validation Loss: 1298.8597631835937\n",
      "Epoch 53, Validation Loss: 1297.6860650634765\n",
      "Epoch 54, Validation Loss: 1299.537108154297\n",
      "Epoch 55, Validation Loss: 1293.613856201172\n",
      "Epoch 56, Validation Loss: 1297.515319213867\n",
      "Epoch 57, Validation Loss: 1295.557085571289\n",
      "Epoch 58, Validation Loss: 1300.4588748168944\n",
      "Epoch 59, Validation Loss: 1303.4398162841796\n",
      "Epoch 60, Validation Loss: 1302.0613049316407\n",
      "Epoch 61, Validation Loss: 1307.3955535888672\n",
      "Epoch 62, Validation Loss: 1302.0824560546876\n",
      "Epoch 63, Validation Loss: 1303.0461755371093\n",
      "Epoch 64, Validation Loss: 1306.489306640625\n",
      "Epoch 65, Validation Loss: 1303.593243408203\n",
      "Epoch 66, Validation Loss: 1303.7384320068359\n",
      "Epoch 67, Validation Loss: 1310.100340576172\n",
      "Epoch 68, Validation Loss: 1306.2491900634766\n",
      "Epoch 69, Validation Loss: 1305.6332495117188\n",
      "Epoch 70, Validation Loss: 1308.692573852539\n",
      "Epoch 71, Validation Loss: 1303.8619671630859\n",
      "Epoch 72, Validation Loss: 1293.1538861083984\n",
      "Epoch 73, Validation Loss: 1307.3962600708007\n",
      "Epoch 74, Validation Loss: 1305.430223388672\n",
      "Epoch 75, Validation Loss: 1308.1450186157226\n",
      "Epoch 76, Validation Loss: 1308.6436773681642\n",
      "Epoch 77, Validation Loss: 1313.633621826172\n",
      "Epoch 78, Validation Loss: 1308.8546032714844\n",
      "Epoch 79, Validation Loss: 1304.0055548095704\n",
      "Epoch 80, Validation Loss: 1313.329537963867\n",
      "Epoch 81, Validation Loss: 1315.6969464111328\n",
      "Epoch 82, Validation Loss: 1307.950595703125\n",
      "Epoch 83, Validation Loss: 1300.8505303955078\n",
      "Epoch 84, Validation Loss: 1309.236538696289\n",
      "Epoch 85, Validation Loss: 1317.7807540893555\n",
      "Epoch 86, Validation Loss: 1308.5513830566406\n",
      "Epoch 87, Validation Loss: 1317.8964764404298\n",
      "Epoch 88, Validation Loss: 1318.050023803711\n",
      "Epoch 89, Validation Loss: 1330.4871575927734\n",
      "Epoch 90, Validation Loss: 1325.9382720947265\n",
      "Epoch 91, Validation Loss: 1305.488516845703\n",
      "Epoch 92, Validation Loss: 1300.2200946044923\n",
      "Epoch 93, Validation Loss: 1309.4675274658202\n",
      "Epoch 94, Validation Loss: 1315.3552185058593\n",
      "Epoch 95, Validation Loss: 1326.2602001953126\n",
      "Epoch 96, Validation Loss: 1321.8293670654298\n",
      "Epoch 97, Validation Loss: 1321.2852642822265\n",
      "Epoch 98, Validation Loss: 1321.463441772461\n",
      "Epoch 99, Validation Loss: 1309.9233984375\n",
      "Epoch 100, Validation Loss: 1317.8359454345702\n",
      "Epoch 101, Validation Loss: 1338.703999938965\n",
      "Epoch 102, Validation Loss: 1318.404161682129\n",
      "Epoch 103, Validation Loss: 1317.9794311523438\n",
      "Epoch 104, Validation Loss: 1326.5550842285156\n",
      "Epoch 105, Validation Loss: 1327.576869506836\n",
      "Epoch 106, Validation Loss: 1324.134019165039\n",
      "Epoch 107, Validation Loss: 1327.2910037231445\n",
      "Epoch 108, Validation Loss: 1325.554887084961\n",
      "Epoch 109, Validation Loss: 1332.7154913330078\n",
      "Epoch 110, Validation Loss: 1339.8446881103516\n",
      "Epoch 111, Validation Loss: 1339.5335134887696\n",
      "Epoch 112, Validation Loss: 1339.884027709961\n",
      "Epoch 113, Validation Loss: 1321.933154296875\n",
      "Epoch 114, Validation Loss: 1328.3344079589845\n",
      "Epoch 115, Validation Loss: 1333.0369299316405\n",
      "Epoch 116, Validation Loss: 1334.4359576416016\n",
      "Epoch 117, Validation Loss: 1340.2751287841797\n",
      "Epoch 118, Validation Loss: 1320.6000378417968\n",
      "Epoch 119, Validation Loss: 1331.4341833496094\n",
      "Epoch 120, Validation Loss: 1329.8621704101563\n",
      "Epoch 121, Validation Loss: 1330.9976922607423\n",
      "Epoch 122, Validation Loss: 1302.4220782470702\n",
      "Epoch 123, Validation Loss: 1320.5421759033204\n",
      "Epoch 124, Validation Loss: 1328.8913787841798\n",
      "Epoch 125, Validation Loss: 1318.455743408203\n",
      "Epoch 126, Validation Loss: 1329.2391674804687\n",
      "Epoch 127, Validation Loss: 1331.2579125976563\n",
      "Epoch 128, Validation Loss: 1330.0890924072266\n",
      "Epoch 129, Validation Loss: 1343.8099847412109\n",
      "Epoch 130, Validation Loss: 1327.642342529297\n",
      "Epoch 131, Validation Loss: 1330.3098364257812\n",
      "Epoch 132, Validation Loss: 1321.9404327392579\n",
      "Epoch 133, Validation Loss: 1339.733911743164\n",
      "Epoch 134, Validation Loss: 1341.201226196289\n",
      "Epoch 135, Validation Loss: 1339.0370294189454\n",
      "Epoch 136, Validation Loss: 1344.0974267578124\n",
      "Epoch 137, Validation Loss: 1333.3309429931642\n",
      "Epoch 138, Validation Loss: 1342.7422869873046\n",
      "Epoch 139, Validation Loss: 1332.2041625976562\n",
      "Epoch 140, Validation Loss: 1335.9902893066405\n",
      "Epoch 141, Validation Loss: 1332.5451373291016\n",
      "Epoch 142, Validation Loss: 1358.7321069335937\n",
      "Epoch 143, Validation Loss: 1332.354790649414\n",
      "Epoch 144, Validation Loss: 1335.8462451171874\n",
      "Epoch 145, Validation Loss: 1356.394545288086\n",
      "Epoch 146, Validation Loss: 1320.0080603027343\n",
      "Epoch 147, Validation Loss: 1329.9139904785156\n",
      "Epoch 148, Validation Loss: 1363.558900756836\n",
      "Epoch 149, Validation Loss: 1291.689423828125\n",
      "Epoch 150, Validation Loss: 1322.4601312255859\n",
      "Epoch 151, Validation Loss: 1312.916029663086\n",
      "Epoch 152, Validation Loss: 1316.3372528076172\n",
      "Epoch 153, Validation Loss: 1342.8986352539061\n",
      "Epoch 154, Validation Loss: 1330.8921734619141\n",
      "Epoch 155, Validation Loss: 1331.6147247314452\n",
      "Epoch 156, Validation Loss: 1363.6722216796875\n",
      "Epoch 157, Validation Loss: 1345.357603149414\n",
      "Epoch 158, Validation Loss: 1356.4658410644531\n",
      "Epoch 159, Validation Loss: 1354.9146966552735\n",
      "Epoch 160, Validation Loss: 1381.8572631835937\n",
      "Epoch 161, Validation Loss: 1342.0797839355469\n",
      "Epoch 162, Validation Loss: 1376.2924395751952\n",
      "Epoch 163, Validation Loss: 1363.2953802490235\n",
      "Epoch 164, Validation Loss: 1360.488338623047\n",
      "Epoch 165, Validation Loss: 1387.4861657714844\n",
      "Epoch 166, Validation Loss: 1372.610729675293\n",
      "Epoch 167, Validation Loss: 1369.9621826171874\n",
      "Epoch 168, Validation Loss: 1375.135047607422\n",
      "Epoch 169, Validation Loss: 1346.9400311279296\n",
      "Epoch 170, Validation Loss: 1359.0024383544921\n",
      "Epoch 171, Validation Loss: 1362.6078930664062\n",
      "Epoch 172, Validation Loss: 1375.4386401367187\n",
      "Epoch 173, Validation Loss: 1405.1015588378907\n",
      "Epoch 174, Validation Loss: 1399.7754852294922\n",
      "Epoch 175, Validation Loss: 1400.9547436523437\n",
      "Epoch 176, Validation Loss: 1416.1407995605468\n",
      "Epoch 177, Validation Loss: 1373.382837524414\n",
      "Epoch 178, Validation Loss: 1438.1015661621093\n",
      "Epoch 179, Validation Loss: 1380.1926043701171\n",
      "Epoch 180, Validation Loss: 1401.7977453613282\n",
      "Epoch 181, Validation Loss: 1388.7407220458983\n",
      "Epoch 182, Validation Loss: 1485.5362512207032\n",
      "Epoch 183, Validation Loss: 1358.4916662597657\n",
      "Epoch 184, Validation Loss: 1303.481948852539\n",
      "Epoch 185, Validation Loss: 1293.1950952148438\n",
      "Epoch 186, Validation Loss: 1325.3526745605468\n",
      "Epoch 187, Validation Loss: 1368.523618774414\n",
      "Epoch 188, Validation Loss: 1391.6128198242188\n",
      "Epoch 189, Validation Loss: 1377.8482012939453\n",
      "Epoch 190, Validation Loss: 1451.3444464111328\n",
      "Epoch 191, Validation Loss: 1388.4104986572265\n",
      "Epoch 192, Validation Loss: 1449.917788696289\n",
      "Epoch 193, Validation Loss: 1415.7806396484375\n",
      "Epoch 194, Validation Loss: 1471.008387451172\n",
      "Epoch 195, Validation Loss: 1388.4189855957031\n",
      "Epoch 196, Validation Loss: 1396.7709729003907\n",
      "Epoch 197, Validation Loss: 1373.7189086914063\n",
      "Epoch 198, Validation Loss: 1384.0205737304686\n",
      "Epoch 199, Validation Loss: 1404.0163671875\n",
      "Epoch 200, Validation Loss: 1395.6986486816406\n",
      "RMSE on test set: 25.19378089904785\n",
      "segment_1\n",
      "before:  2020-07-23 18:09:32\n",
      "after:  2020-07-23 18:29:32\n",
      "segment_1\n",
      "before:  2020-07-23 22:14:32\n",
      "after:  2020-07-23 22:49:32\n",
      "segment_1\n",
      "before:  2020-07-23 22:49:32\n",
      "after:  2020-07-23 23:04:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-08-02 14:15:01\n",
      "after:  2020-08-02 14:59:51\n",
      "segment_3\n",
      "before:  2020-08-07 11:54:59\n",
      "after:  2020-08-07 12:49:59\n",
      "segment_3\n",
      "before:  2020-08-07 17:09:59\n",
      "after:  2020-08-07 17:45:00\n",
      "segment_3\n",
      "before:  2020-08-07 18:44:59\n",
      "after:  2020-08-07 19:09:59\n",
      "segment_3\n",
      "before:  2020-08-07 19:44:59\n",
      "after:  2020-08-07 20:00:00\n",
      "segment_3\n",
      "before:  2020-08-09 10:20:02\n",
      "after:  2020-08-09 10:40:02\n",
      "segment_3\n",
      "before:  2020-08-09 10:45:03\n",
      "after:  2020-08-09 11:00:03\n",
      "segment_3\n",
      "before:  2020-08-09 11:15:03\n",
      "after:  2020-08-09 11:45:03\n",
      "segment_3\n",
      "before:  2020-08-09 12:00:03\n",
      "after:  2020-08-09 12:15:03\n",
      "segment_4\n",
      "before:  2020-08-09 17:00:03\n",
      "after:  2020-08-09 17:30:03\n",
      "segment_4\n",
      "before:  2020-08-09 18:35:03\n",
      "after:  2020-08-09 19:15:03\n",
      "segment_4\n",
      "before:  2020-08-10 08:00:04\n",
      "after:  2020-08-10 08:20:04\n",
      "segment_4\n",
      "before:  2020-08-10 08:20:04\n",
      "after:  2020-08-10 09:10:05\n",
      "segment_6\n",
      "before:  2020-08-10 14:30:07\n",
      "after:  2020-08-10 15:10:09\n",
      "segment_6\n",
      "before:  2020-08-10 15:50:05\n",
      "after:  2020-08-10 16:40:05\n",
      "segment_6\n",
      "before:  2020-08-10 17:15:05\n",
      "after:  2020-08-10 18:05:06\n",
      "segment_6\n",
      "before:  2020-08-10 18:05:06\n",
      "after:  2020-08-10 18:25:05\n",
      "segment_6\n",
      "before:  2020-08-10 20:05:05\n",
      "after:  2020-08-10 20:50:05\n",
      "segment_6\n",
      "before:  2020-08-10 20:55:06\n",
      "after:  2020-08-10 21:55:06\n",
      "segment_6\n",
      "before:  2020-08-11 08:55:06\n",
      "after:  2020-08-11 09:10:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2417\n",
      "len of segment_df is  1992\n",
      "len of segment_df is  1676\n",
      "len of segment_df is  248\n",
      "len of segment_df is  4\n",
      "len of segment_df is  238\n",
      "len of features_list 6466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 449.1341709182376\n",
      "Epoch 2, Validation Loss: 444.3567414056687\n",
      "Epoch 3, Validation Loss: 440.9508245104835\n",
      "Epoch 4, Validation Loss: 437.33232271103634\n",
      "Epoch 5, Validation Loss: 433.61249814714705\n",
      "Epoch 6, Validation Loss: 428.5571093786331\n",
      "Epoch 7, Validation Loss: 424.1324440184094\n",
      "Epoch 8, Validation Loss: 419.5976009141831\n",
      "Epoch 9, Validation Loss: 414.9339729263669\n",
      "Epoch 10, Validation Loss: 410.06409679140364\n",
      "Epoch 11, Validation Loss: 405.098108291626\n",
      "Epoch 12, Validation Loss: 400.41883886428104\n",
      "Epoch 13, Validation Loss: 397.21344264348346\n",
      "Epoch 14, Validation Loss: 394.78571962174914\n",
      "Epoch 15, Validation Loss: 393.57019111088346\n",
      "Epoch 16, Validation Loss: 393.70132441747757\n",
      "Epoch 17, Validation Loss: 393.72873424348376\n",
      "Epoch 18, Validation Loss: 397.7926519484747\n",
      "Epoch 19, Validation Loss: 397.70475932529996\n",
      "Epoch 20, Validation Loss: 401.8703391665504\n",
      "Epoch 21, Validation Loss: 399.80815542311893\n",
      "Epoch 22, Validation Loss: 405.3311596825009\n",
      "Epoch 23, Validation Loss: 405.7743392217727\n",
      "Epoch 24, Validation Loss: 405.0970566159203\n",
      "Epoch 25, Validation Loss: 403.9615552993048\n",
      "Epoch 26, Validation Loss: 402.68071256365096\n",
      "Epoch 27, Validation Loss: 403.80322247459776\n",
      "Epoch 28, Validation Loss: 400.6513328552246\n",
      "Epoch 29, Validation Loss: 402.52789942423504\n",
      "Epoch 30, Validation Loss: 402.83587192353747\n",
      "Epoch 31, Validation Loss: 403.41996964954194\n",
      "Epoch 32, Validation Loss: 403.13973853701634\n",
      "Epoch 33, Validation Loss: 403.6184407188779\n",
      "Epoch 34, Validation Loss: 401.43363988967167\n",
      "Epoch 35, Validation Loss: 399.73507290794737\n",
      "Epoch 36, Validation Loss: 396.4153760274251\n",
      "Epoch 37, Validation Loss: 393.6667905535017\n",
      "Epoch 38, Validation Loss: 390.75973092942013\n",
      "Epoch 39, Validation Loss: 387.98219753447034\n",
      "Epoch 40, Validation Loss: 385.25004105340867\n",
      "Epoch 41, Validation Loss: 382.0284133184524\n",
      "Epoch 42, Validation Loss: 379.4980181739444\n",
      "Epoch 43, Validation Loss: 378.69160079956055\n",
      "Epoch 44, Validation Loss: 374.4536871228899\n",
      "Epoch 45, Validation Loss: 375.2451723189581\n",
      "Epoch 46, Validation Loss: 372.66296659197127\n",
      "Epoch 47, Validation Loss: 370.33959543137325\n",
      "Epoch 48, Validation Loss: 368.00998369852704\n",
      "Epoch 49, Validation Loss: 368.74011076064335\n",
      "Epoch 50, Validation Loss: 372.33303778512135\n",
      "Epoch 51, Validation Loss: 366.1233076368059\n",
      "Epoch 52, Validation Loss: 366.0569059281122\n",
      "Epoch 53, Validation Loss: 369.6349284762428\n",
      "Epoch 54, Validation Loss: 370.0774275461833\n",
      "Epoch 55, Validation Loss: 364.47453008379256\n",
      "Epoch 56, Validation Loss: 364.26871653965543\n",
      "Epoch 57, Validation Loss: 365.5787973858061\n",
      "Epoch 58, Validation Loss: 363.4129008338565\n",
      "Epoch 59, Validation Loss: 363.74001866295225\n",
      "Epoch 60, Validation Loss: 365.0871953510103\n",
      "Epoch 61, Validation Loss: 364.47475515093123\n",
      "Epoch 62, Validation Loss: 367.05533845084057\n",
      "Epoch 63, Validation Loss: 364.9761404309954\n",
      "Epoch 64, Validation Loss: 368.8176391238258\n",
      "Epoch 65, Validation Loss: 365.1034670330229\n",
      "Epoch 66, Validation Loss: 368.03926050095333\n",
      "Epoch 67, Validation Loss: 368.24792108081635\n",
      "Epoch 68, Validation Loss: 371.84295826866514\n",
      "Epoch 69, Validation Loss: 368.88652311052596\n",
      "Epoch 70, Validation Loss: 368.5114910489037\n",
      "Epoch 71, Validation Loss: 368.021083740961\n",
      "Epoch 72, Validation Loss: 370.80428995404924\n",
      "Epoch 73, Validation Loss: 365.3145522163028\n",
      "Epoch 74, Validation Loss: 366.17869685945055\n",
      "Epoch 75, Validation Loss: 367.46870431445893\n",
      "Epoch 76, Validation Loss: 369.14687474568683\n",
      "Epoch 77, Validation Loss: 367.8878196534656\n",
      "Epoch 78, Validation Loss: 367.2232398078555\n",
      "Epoch 79, Validation Loss: 364.9410940806071\n",
      "Epoch 80, Validation Loss: 368.78779801868257\n",
      "Epoch 81, Validation Loss: 366.90031533014206\n",
      "Epoch 82, Validation Loss: 368.0749940418062\n",
      "Epoch 83, Validation Loss: 367.06232506888256\n",
      "Epoch 84, Validation Loss: 367.26128696259997\n",
      "Epoch 85, Validation Loss: 370.71761549086796\n",
      "Epoch 86, Validation Loss: 374.2740592956543\n",
      "Epoch 87, Validation Loss: 370.89530063810804\n",
      "Epoch 88, Validation Loss: 371.7409933181036\n",
      "Epoch 89, Validation Loss: 373.2483340672084\n",
      "Epoch 90, Validation Loss: 373.6933738163539\n",
      "Epoch 91, Validation Loss: 380.313388915289\n",
      "Epoch 92, Validation Loss: 382.9551645914714\n",
      "Epoch 93, Validation Loss: 392.1386781420027\n",
      "Epoch 94, Validation Loss: 394.95745731535413\n",
      "Epoch 95, Validation Loss: 391.62399464561827\n",
      "Epoch 96, Validation Loss: 391.7685050056094\n",
      "Epoch 97, Validation Loss: 393.29463613600956\n",
      "Epoch 98, Validation Loss: 403.25433694748654\n",
      "Epoch 99, Validation Loss: 411.87006696065265\n",
      "Epoch 100, Validation Loss: 401.5322180248442\n",
      "Epoch 101, Validation Loss: 430.4235850742885\n",
      "Epoch 102, Validation Loss: 439.2860339936756\n",
      "Epoch 103, Validation Loss: 425.25315511794315\n",
      "Epoch 104, Validation Loss: 450.5748783293225\n",
      "Epoch 105, Validation Loss: 471.05113365536647\n",
      "Epoch 106, Validation Loss: 425.48920295352025\n",
      "Epoch 107, Validation Loss: 476.4677716209775\n",
      "Epoch 108, Validation Loss: 492.14031873430525\n",
      "Epoch 109, Validation Loss: 490.9877620878674\n",
      "Epoch 110, Validation Loss: 484.77721405029297\n",
      "Epoch 111, Validation Loss: 515.8819587344215\n",
      "Epoch 112, Validation Loss: 458.6783314659482\n",
      "Epoch 113, Validation Loss: 495.55513581775483\n",
      "Epoch 114, Validation Loss: 548.3771231515067\n",
      "Epoch 115, Validation Loss: 493.79779343377976\n",
      "Epoch 116, Validation Loss: 585.814949398949\n",
      "Epoch 117, Validation Loss: 564.4987876528785\n",
      "Epoch 118, Validation Loss: 590.466058640253\n",
      "Epoch 119, Validation Loss: 588.9417942592075\n",
      "Epoch 120, Validation Loss: 601.4336424328033\n",
      "Epoch 121, Validation Loss: 600.3409162248884\n",
      "Epoch 122, Validation Loss: 595.0346098400298\n",
      "Epoch 123, Validation Loss: 626.8001156761533\n",
      "Epoch 124, Validation Loss: 603.0639328729538\n",
      "Epoch 125, Validation Loss: 615.4524441673642\n",
      "Epoch 126, Validation Loss: 609.7440141950335\n",
      "Epoch 127, Validation Loss: 614.0015643891834\n",
      "Epoch 128, Validation Loss: 623.9887019566128\n",
      "Epoch 129, Validation Loss: 583.6486046200707\n",
      "Epoch 130, Validation Loss: 614.5979243687221\n",
      "Epoch 131, Validation Loss: 607.1217120942615\n",
      "Epoch 132, Validation Loss: 591.588875180199\n",
      "Epoch 133, Validation Loss: 602.6877688453311\n",
      "Epoch 134, Validation Loss: 612.3234143938337\n",
      "Epoch 135, Validation Loss: 594.3592754545666\n",
      "Epoch 136, Validation Loss: 606.7340879894439\n",
      "Epoch 137, Validation Loss: 593.246574038551\n",
      "Epoch 138, Validation Loss: 586.1720363071987\n",
      "Epoch 139, Validation Loss: 577.3857182094029\n",
      "Epoch 140, Validation Loss: 578.2811896914527\n",
      "Epoch 141, Validation Loss: 603.9500674293155\n",
      "Epoch 142, Validation Loss: 580.0931781587146\n",
      "Epoch 143, Validation Loss: 574.062986101423\n",
      "Epoch 144, Validation Loss: 584.1254018147787\n",
      "Epoch 145, Validation Loss: 580.8986845470611\n",
      "Epoch 146, Validation Loss: 566.9986826578776\n",
      "Epoch 147, Validation Loss: 564.6772039504278\n",
      "Epoch 148, Validation Loss: 570.7859664190383\n",
      "Epoch 149, Validation Loss: 500.8014112200056\n",
      "Epoch 150, Validation Loss: 568.9706529889788\n",
      "Epoch 151, Validation Loss: 557.1374827793667\n",
      "Epoch 152, Validation Loss: 564.3253384544736\n",
      "Epoch 153, Validation Loss: 597.5073016938709\n",
      "Epoch 154, Validation Loss: 559.8291146414621\n",
      "Epoch 155, Validation Loss: 591.9887506394159\n",
      "Epoch 156, Validation Loss: 552.1610012962705\n",
      "Epoch 157, Validation Loss: 571.2941741943359\n",
      "Epoch 158, Validation Loss: 545.102555047898\n",
      "Epoch 159, Validation Loss: 570.8901083809989\n",
      "Epoch 160, Validation Loss: 565.8451886858259\n",
      "Epoch 161, Validation Loss: 551.5240681966146\n",
      "Epoch 162, Validation Loss: 561.2765517461868\n",
      "Epoch 163, Validation Loss: 553.2955972580683\n",
      "Epoch 164, Validation Loss: 561.9683205740793\n",
      "Epoch 165, Validation Loss: 518.3279902140299\n",
      "Epoch 166, Validation Loss: 549.6448498680478\n",
      "Epoch 167, Validation Loss: 566.1412556966146\n",
      "Epoch 168, Validation Loss: 446.69851303100586\n",
      "Epoch 169, Validation Loss: 563.0842786516462\n",
      "Epoch 170, Validation Loss: 568.3440755208334\n",
      "Epoch 171, Validation Loss: 411.60132780529204\n",
      "Epoch 172, Validation Loss: 544.489277430943\n",
      "Epoch 173, Validation Loss: 553.2041982014974\n",
      "Epoch 174, Validation Loss: 561.5409745715913\n",
      "Epoch 175, Validation Loss: 538.6590877714611\n",
      "Epoch 176, Validation Loss: 524.6106018793015\n",
      "Epoch 177, Validation Loss: 543.6781830560593\n",
      "Epoch 178, Validation Loss: 522.707394917806\n",
      "Epoch 179, Validation Loss: 574.4618464878628\n",
      "Epoch 180, Validation Loss: 528.2520439511254\n",
      "Epoch 181, Validation Loss: 551.6003559657505\n",
      "Epoch 182, Validation Loss: 515.6482020786831\n",
      "Epoch 183, Validation Loss: 463.94498152959915\n",
      "Epoch 184, Validation Loss: 526.2886850266229\n",
      "Epoch 185, Validation Loss: 502.92708950950987\n",
      "Epoch 186, Validation Loss: 545.3660910470145\n",
      "Epoch 187, Validation Loss: 543.6322947910854\n",
      "Epoch 188, Validation Loss: 602.7388756161645\n",
      "Epoch 189, Validation Loss: 544.9490225655692\n",
      "Epoch 190, Validation Loss: 546.0182585943313\n",
      "Epoch 191, Validation Loss: 440.54816127958753\n",
      "Epoch 192, Validation Loss: 507.27969251360213\n",
      "Epoch 193, Validation Loss: 542.1779283796038\n",
      "Epoch 194, Validation Loss: 483.7655189150856\n",
      "Epoch 195, Validation Loss: 507.32480984642393\n",
      "Epoch 196, Validation Loss: 493.8646527244931\n",
      "Epoch 197, Validation Loss: 582.7418736049107\n",
      "Epoch 198, Validation Loss: 515.690683274042\n",
      "Epoch 199, Validation Loss: 520.7737248738607\n",
      "Epoch 200, Validation Loss: 465.91632407052174\n",
      "RMSE on test set: 22.343402862548828\n",
      "segment_1\n",
      "before:  2020-11-03 05:43:01\n",
      "after:  2020-11-03 06:28:02\n",
      "segment_1\n",
      "before:  2020-11-08 02:23:19\n",
      "after:  2020-11-08 03:18:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-11-16 06:18:10\n",
      "after:  2020-11-16 06:28:10\n",
      "segment_3\n",
      "before:  2020-11-19 05:33:18\n",
      "after:  2020-11-19 06:08:18\n",
      "segment_3\n",
      "before:  2020-11-19 07:23:17\n",
      "after:  2020-11-19 08:18:18\n",
      "segment_3\n",
      "before:  2020-11-19 20:43:19\n",
      "after:  2020-11-19 21:33:19\n",
      "segment_4\n",
      "before:  2020-11-20 00:33:19\n",
      "after:  2020-11-20 01:23:19\n",
      "segment_4\n",
      "before:  2020-11-20 01:23:19\n",
      "after:  2020-11-20 01:58:19\n",
      "segment_4\n",
      "before:  2020-11-20 02:03:19\n",
      "after:  2020-11-20 02:33:19\n",
      "segment_4\n",
      "before:  2020-11-20 02:53:20\n",
      "after:  2020-11-20 03:53:20\n",
      "segment_4\n",
      "before:  2020-11-20 05:58:20\n",
      "after:  2020-11-20 06:38:21\n",
      "segment_4\n",
      "before:  2020-11-20 06:53:20\n",
      "after:  2020-11-20 07:23:20\n",
      "segment_4\n",
      "before:  2020-11-20 07:38:20\n",
      "after:  2020-11-20 07:53:20\n",
      "segment_4\n",
      "before:  2020-11-20 08:08:20\n",
      "after:  2020-11-20 08:58:21\n",
      "segment_4\n",
      "before:  2020-11-20 09:03:21\n",
      "after:  2020-11-20 09:23:21\n",
      "segment_4\n",
      "before:  2020-11-20 09:38:21\n",
      "after:  2020-11-20 10:28:22\n",
      "segment_4\n",
      "before:  2020-11-20 11:13:21\n",
      "after:  2020-11-20 12:03:21\n",
      "segment_4\n",
      "before:  2020-11-20 14:03:21\n",
      "after:  2020-11-20 14:38:21\n",
      "segment_4\n",
      "before:  2020-11-20 17:23:23\n",
      "after:  2020-11-20 17:38:22\n",
      "segment_4\n",
      "before:  2020-11-20 17:38:22\n",
      "after:  2020-11-20 17:53:22\n",
      "segment_5\n",
      "before:  2020-11-20 22:53:22\n",
      "after:  2020-11-20 23:33:21\n",
      "segment_5\n",
      "before:  2020-11-20 23:48:22\n",
      "after:  2020-11-21 00:38:23\n",
      "segment_5\n",
      "before:  2020-11-21 00:48:22\n",
      "after:  2020-11-21 01:43:23\n",
      "segment_5\n",
      "before:  2020-11-21 01:43:23\n",
      "after:  2020-11-21 02:38:23\n",
      "segment_6\n",
      "before:  2020-11-21 05:13:22\n",
      "after:  2020-11-21 06:08:24\n",
      "segment_6\n",
      "before:  2020-11-21 06:33:23\n",
      "after:  2020-11-21 07:33:23\n",
      "segment_6\n",
      "before:  2020-11-21 07:33:23\n",
      "after:  2020-11-21 08:08:24\n",
      "segment_6\n",
      "before:  2020-11-21 08:08:24\n",
      "after:  2020-11-21 08:33:23\n",
      "segment_6\n",
      "before:  2020-11-21 10:53:23\n",
      "after:  2020-11-21 11:13:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2289\n",
      "len of segment_df is  1526\n",
      "len of segment_df is  787\n",
      "len of segment_df is  230\n",
      "len of segment_df is  60\n",
      "len of segment_df is  98\n",
      "len of segment_df is  9\n",
      "len of segment_df is  11\n",
      "len of segment_df is  1\n",
      "len of segment_df is  537\n",
      "len of segment_df is  1733\n",
      "len of features_list 7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 2158.0242651234503\n",
      "Epoch 2, Validation Loss: 2204.9597808174467\n",
      "Epoch 3, Validation Loss: 2129.0173729606295\n",
      "Epoch 4, Validation Loss: 2084.8075612938924\n",
      "Epoch 5, Validation Loss: 2052.860887610394\n",
      "Epoch 6, Validation Loss: 2050.461206850798\n",
      "Epoch 7, Validation Loss: 2016.4995415729024\n",
      "Epoch 8, Validation Loss: 1997.1532723799996\n",
      "Epoch 9, Validation Loss: 1976.3480015630307\n",
      "Epoch 10, Validation Loss: 1946.9402056154997\n",
      "Epoch 11, Validation Loss: 1924.606611666472\n",
      "Epoch 12, Validation Loss: 1938.5873711627462\n",
      "Epoch 13, Validation Loss: 1894.3080447653065\n",
      "Epoch 14, Validation Loss: 1890.7767350570016\n",
      "Epoch 15, Validation Loss: 1881.639074906059\n",
      "Epoch 16, Validation Loss: 1874.630820232889\n",
      "Epoch 17, Validation Loss: 1879.1941568125849\n",
      "Epoch 18, Validation Loss: 1889.8276579483695\n",
      "Epoch 19, Validation Loss: 1885.4909422501273\n",
      "Epoch 20, Validation Loss: 1888.5870265131412\n",
      "Epoch 21, Validation Loss: 1892.6196481455927\n",
      "Epoch 22, Validation Loss: 1898.4180835226307\n",
      "Epoch 23, Validation Loss: 1857.9684242580247\n",
      "Epoch 24, Validation Loss: 1912.2540913457456\n",
      "Epoch 25, Validation Loss: 1844.4894929968793\n",
      "Epoch 26, Validation Loss: 1924.7191029424253\n",
      "Epoch 27, Validation Loss: 1915.0651059358017\n",
      "Epoch 28, Validation Loss: 1872.7579677415931\n",
      "Epoch 29, Validation Loss: 1926.3365047288978\n",
      "Epoch 30, Validation Loss: 1929.999395619268\n",
      "Epoch 31, Validation Loss: 1925.727791827658\n",
      "Epoch 32, Validation Loss: 1940.1871749214504\n",
      "Epoch 33, Validation Loss: 1932.2585382876189\n",
      "Epoch 34, Validation Loss: 1929.6263374660325\n",
      "Epoch 35, Validation Loss: 1934.3385434358017\n",
      "Epoch 36, Validation Loss: 1955.9614602793818\n",
      "Epoch 37, Validation Loss: 1959.883759871773\n",
      "Epoch 38, Validation Loss: 1948.5342029073963\n",
      "Epoch 39, Validation Loss: 1958.8505149509597\n",
      "Epoch 40, Validation Loss: 1930.3996887207031\n",
      "Epoch 41, Validation Loss: 1937.1613285230553\n",
      "Epoch 42, Validation Loss: 1945.8402112877886\n",
      "Epoch 43, Validation Loss: 1944.3555549953294\n",
      "Epoch 44, Validation Loss: 1961.920376321544\n",
      "Epoch 45, Validation Loss: 1978.5183457084324\n",
      "Epoch 46, Validation Loss: 1956.6387621008832\n",
      "Epoch 47, Validation Loss: 1960.6922992208729\n",
      "Epoch 48, Validation Loss: 1958.8880568794582\n",
      "Epoch 49, Validation Loss: 1955.8370606795602\n",
      "Epoch 50, Validation Loss: 1953.1146923562756\n",
      "Epoch 51, Validation Loss: 1974.5738485585089\n",
      "Epoch 52, Validation Loss: 1952.0312453560207\n",
      "Epoch 53, Validation Loss: 1938.7535247802734\n",
      "Epoch 54, Validation Loss: 1960.2952005137568\n",
      "Epoch 55, Validation Loss: 1999.5875297214675\n",
      "Epoch 56, Validation Loss: 1991.333084769871\n",
      "Epoch 57, Validation Loss: 1996.9435650369396\n",
      "Epoch 58, Validation Loss: 1978.6754442297895\n",
      "Epoch 59, Validation Loss: 1983.2036637015965\n",
      "Epoch 60, Validation Loss: 1979.761594025985\n",
      "Epoch 61, Validation Loss: 1987.8554010805876\n",
      "Epoch 62, Validation Loss: 1984.274917602539\n",
      "Epoch 63, Validation Loss: 1959.6420772386634\n",
      "Epoch 64, Validation Loss: 1979.3766665251358\n",
      "Epoch 65, Validation Loss: 1989.5934335459833\n",
      "Epoch 66, Validation Loss: 2003.44738504161\n",
      "Epoch 67, Validation Loss: 1995.0988510795262\n",
      "Epoch 68, Validation Loss: 1982.5273901897929\n",
      "Epoch 69, Validation Loss: 2005.5668454377549\n",
      "Epoch 70, Validation Loss: 1983.7447264298148\n",
      "Epoch 71, Validation Loss: 1985.1800789211106\n",
      "Epoch 72, Validation Loss: 1978.7475838039231\n",
      "Epoch 73, Validation Loss: 1992.8340669714887\n",
      "Epoch 74, Validation Loss: 1965.2008069909136\n",
      "Epoch 75, Validation Loss: 1986.4053278384001\n",
      "Epoch 76, Validation Loss: 1939.5125400709069\n",
      "Epoch 77, Validation Loss: 1967.1961298403533\n",
      "Epoch 78, Validation Loss: 1963.8719037926714\n",
      "Epoch 79, Validation Loss: 1924.4199052893598\n",
      "Epoch 80, Validation Loss: 1931.1293872335682\n",
      "Epoch 81, Validation Loss: 1982.5773328698199\n",
      "Epoch 82, Validation Loss: 1903.006625631581\n",
      "Epoch 83, Validation Loss: 1913.4991766888163\n",
      "Epoch 84, Validation Loss: 1890.9859446649966\n",
      "Epoch 85, Validation Loss: 1916.7063658341117\n",
      "Epoch 86, Validation Loss: 1869.1320820684018\n",
      "Epoch 87, Validation Loss: 1925.7316907799761\n",
      "Epoch 88, Validation Loss: 1858.4036049220872\n",
      "Epoch 89, Validation Loss: 1897.1124360457711\n",
      "Epoch 90, Validation Loss: 1849.620286029318\n",
      "Epoch 91, Validation Loss: 1862.023101143215\n",
      "Epoch 92, Validation Loss: 1850.0946555759597\n",
      "Epoch 93, Validation Loss: 1850.5524789561396\n",
      "Epoch 94, Validation Loss: 1862.4682268889053\n",
      "Epoch 95, Validation Loss: 1848.9987245642621\n",
      "Epoch 96, Validation Loss: 1843.148442475692\n",
      "Epoch 97, Validation Loss: 1920.0908713962722\n",
      "Epoch 98, Validation Loss: 1908.4658140099566\n",
      "Epoch 99, Validation Loss: 1965.9985733032227\n",
      "Epoch 100, Validation Loss: 1925.7293114040208\n",
      "Epoch 101, Validation Loss: 1925.641401871391\n",
      "Epoch 102, Validation Loss: 1950.3039374973464\n",
      "Epoch 103, Validation Loss: 1909.4673634404721\n",
      "Epoch 104, Validation Loss: 1890.1728339817214\n",
      "Epoch 105, Validation Loss: 1862.9302842513375\n",
      "Epoch 106, Validation Loss: 1888.4352576214335\n",
      "Epoch 107, Validation Loss: 1856.505471934443\n",
      "Epoch 108, Validation Loss: 1842.1562642636507\n",
      "Epoch 109, Validation Loss: 1829.448894003163\n",
      "Epoch 110, Validation Loss: 1858.5136808312457\n",
      "Epoch 111, Validation Loss: 1830.1706625896952\n",
      "Epoch 112, Validation Loss: 1862.8212665060291\n",
      "Epoch 113, Validation Loss: 1868.114881100862\n",
      "Epoch 114, Validation Loss: 1829.4955825805664\n",
      "Epoch 115, Validation Loss: 1927.6101681253185\n",
      "Epoch 116, Validation Loss: 1888.8977720841117\n",
      "Epoch 117, Validation Loss: 1887.654374661653\n",
      "Epoch 118, Validation Loss: 1890.2425298276155\n",
      "Epoch 119, Validation Loss: 1912.7027017344599\n",
      "Epoch 120, Validation Loss: 1913.7429159413214\n",
      "Epoch 121, Validation Loss: 1864.2953252377717\n",
      "Epoch 122, Validation Loss: 1892.283624731976\n",
      "Epoch 123, Validation Loss: 1906.6632438327956\n",
      "Epoch 124, Validation Loss: 1893.2024665500808\n",
      "Epoch 125, Validation Loss: 1894.7259126746137\n",
      "Epoch 126, Validation Loss: 1930.5163614024286\n",
      "Epoch 127, Validation Loss: 1910.7581747303839\n",
      "Epoch 128, Validation Loss: 1906.708289768385\n",
      "Epoch 129, Validation Loss: 1907.2183479640794\n",
      "Epoch 130, Validation Loss: 1874.4783384903617\n",
      "Epoch 131, Validation Loss: 1931.8407171498175\n",
      "Epoch 132, Validation Loss: 1990.7963140736456\n",
      "Epoch 133, Validation Loss: 1959.4382835056472\n",
      "Epoch 134, Validation Loss: 1942.9373092651367\n",
      "Epoch 135, Validation Loss: 1912.7756410681684\n",
      "Epoch 136, Validation Loss: 1959.5061254086702\n",
      "Epoch 137, Validation Loss: 1910.2345756862474\n",
      "Epoch 138, Validation Loss: 1924.4016136501145\n",
      "Epoch 139, Validation Loss: 1945.4840671705163\n",
      "Epoch 140, Validation Loss: 1980.911815809167\n",
      "Epoch 141, Validation Loss: 1952.1657510840375\n",
      "Epoch 142, Validation Loss: 1962.7627649721892\n",
      "Epoch 143, Validation Loss: 1933.1611903646717\n",
      "Epoch 144, Validation Loss: 1920.6549097144086\n",
      "Epoch 145, Validation Loss: 1881.3266442340353\n",
      "Epoch 146, Validation Loss: 1908.9382242949112\n",
      "Epoch 147, Validation Loss: 1921.2993404554284\n",
      "Epoch 148, Validation Loss: 1900.5971420951512\n",
      "Epoch 149, Validation Loss: 1949.769090237825\n",
      "Epoch 150, Validation Loss: 2012.3331842837126\n",
      "Epoch 151, Validation Loss: 1892.6353962110436\n",
      "Epoch 152, Validation Loss: 1894.1955641041632\n",
      "Epoch 153, Validation Loss: 1945.3764250382133\n",
      "Epoch 154, Validation Loss: 1894.3997908882473\n",
      "Epoch 155, Validation Loss: 1928.6960822395656\n",
      "Epoch 156, Validation Loss: 1917.3817798780358\n",
      "Epoch 157, Validation Loss: 1929.060795162035\n",
      "Epoch 158, Validation Loss: 2004.2116768878439\n",
      "Epoch 159, Validation Loss: 2013.4928948775582\n",
      "Epoch 160, Validation Loss: 1983.1016520624576\n",
      "Epoch 161, Validation Loss: 2052.82476790055\n",
      "Epoch 162, Validation Loss: 2064.739263119905\n",
      "Epoch 163, Validation Loss: 2062.1034665315046\n",
      "Epoch 164, Validation Loss: 2177.046542623769\n",
      "Epoch 165, Validation Loss: 2054.686978713326\n",
      "Epoch 166, Validation Loss: 2070.571333512016\n",
      "Epoch 167, Validation Loss: 1905.4676082445228\n",
      "Epoch 168, Validation Loss: 1901.2592252648394\n",
      "Epoch 169, Validation Loss: 1956.7418960903\n",
      "Epoch 170, Validation Loss: 2013.3426208496094\n",
      "Epoch 171, Validation Loss: 2069.3854307091756\n",
      "Epoch 172, Validation Loss: 2030.7963527181873\n",
      "Epoch 173, Validation Loss: 2125.0276194033418\n",
      "Epoch 174, Validation Loss: 1982.456447103749\n",
      "Epoch 175, Validation Loss: 2007.1097355718198\n",
      "Epoch 176, Validation Loss: 1997.490841575291\n",
      "Epoch 177, Validation Loss: 2038.7492412069569\n",
      "Epoch 178, Validation Loss: 2014.9221679024074\n",
      "Epoch 179, Validation Loss: 2066.3431127797003\n",
      "Epoch 180, Validation Loss: 2034.3011428169582\n",
      "Epoch 181, Validation Loss: 2080.1615978738537\n",
      "Epoch 182, Validation Loss: 2198.542560743249\n",
      "Epoch 183, Validation Loss: 2184.933375151261\n",
      "Epoch 184, Validation Loss: 2059.6470522673235\n",
      "Epoch 185, Validation Loss: 2092.8614909959874\n",
      "Epoch 186, Validation Loss: 2106.5616624251657\n",
      "Epoch 187, Validation Loss: 2068.2037273904552\n",
      "Epoch 188, Validation Loss: 2113.332569288171\n",
      "Epoch 189, Validation Loss: 2157.3629100633702\n",
      "Epoch 190, Validation Loss: 2127.8864029594088\n",
      "Epoch 191, Validation Loss: 2182.43270940366\n",
      "Epoch 192, Validation Loss: 2146.15519780698\n",
      "Epoch 193, Validation Loss: 2108.1650838437286\n",
      "Epoch 194, Validation Loss: 2086.2223998360014\n",
      "Epoch 195, Validation Loss: 2125.222940859587\n",
      "Epoch 196, Validation Loss: 2153.018547721531\n",
      "Epoch 197, Validation Loss: 2184.4949098669963\n",
      "Epoch 198, Validation Loss: 2007.8705787658691\n",
      "Epoch 199, Validation Loss: 2074.7401958963146\n",
      "Epoch 200, Validation Loss: 2112.442818019701\n",
      "RMSE on test set: 30.739221572875977\n",
      "segment_5\n",
      "before:  2020-04-15 16:22:38\n",
      "after:  2020-04-15 16:52:39\n",
      "segment_8\n",
      "before:  2020-04-28 04:13:17\n",
      "after:  2020-04-28 04:53:17\n",
      "segment_8\n",
      "before:  2020-04-28 07:43:17\n",
      "after:  2020-04-28 08:18:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  229\n",
      "len of segment_df is  1915\n",
      "len of segment_df is  882\n",
      "len of segment_df is  718\n",
      "len of segment_df is  810\n",
      "len of segment_df is  685\n",
      "len of segment_df is  1043\n",
      "len of segment_df is  662\n",
      "len of features_list 6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 479.9233826723966\n",
      "Epoch 2, Validation Loss: 474.2158409465443\n",
      "Epoch 3, Validation Loss: 470.24659330194646\n",
      "Epoch 4, Validation Loss: 465.4047073017467\n",
      "Epoch 5, Validation Loss: 460.3548152230003\n",
      "Epoch 6, Validation Loss: 454.8322771245783\n",
      "Epoch 7, Validation Loss: 449.26843816583806\n",
      "Epoch 8, Validation Loss: 443.44021623784846\n",
      "Epoch 9, Validation Loss: 437.57764573530716\n",
      "Epoch 10, Validation Loss: 431.7727456526323\n",
      "Epoch 11, Validation Loss: 426.894344329834\n",
      "Epoch 12, Validation Loss: 420.0771512118253\n",
      "Epoch 13, Validation Loss: 415.3513200933283\n",
      "Epoch 14, Validation Loss: 411.41037542169744\n",
      "Epoch 15, Validation Loss: 408.356708873402\n",
      "Epoch 16, Validation Loss: 408.8267565640536\n",
      "Epoch 17, Validation Loss: 410.27595415982336\n",
      "Epoch 18, Validation Loss: 418.6321126764471\n",
      "Epoch 19, Validation Loss: 426.3969969315962\n",
      "Epoch 20, Validation Loss: 446.86666731400925\n",
      "Epoch 21, Validation Loss: 478.98229078813034\n",
      "Epoch 22, Validation Loss: 506.43601989746094\n",
      "Epoch 23, Validation Loss: 525.6878128051758\n",
      "Epoch 24, Validation Loss: 529.6913202459162\n",
      "Epoch 25, Validation Loss: 519.3845589377663\n",
      "Epoch 26, Validation Loss: 506.6488494873047\n",
      "Epoch 27, Validation Loss: 493.46969812566584\n",
      "Epoch 28, Validation Loss: 485.7807124744762\n",
      "Epoch 29, Validation Loss: 481.9081302989613\n",
      "Epoch 30, Validation Loss: 476.2382195212624\n",
      "Epoch 31, Validation Loss: 511.26970950039953\n",
      "Epoch 32, Validation Loss: 475.3450296575373\n",
      "Epoch 33, Validation Loss: 469.25977741588247\n",
      "Epoch 34, Validation Loss: 467.2818631258878\n",
      "Epoch 35, Validation Loss: 460.4189251986417\n",
      "Epoch 36, Validation Loss: 458.5778142755682\n",
      "Epoch 37, Validation Loss: 456.06547477028585\n",
      "Epoch 38, Validation Loss: 453.3775003606623\n",
      "Epoch 39, Validation Loss: 453.18475966020065\n",
      "Epoch 40, Validation Loss: 452.04381075772375\n",
      "Epoch 41, Validation Loss: 447.61678938432175\n",
      "Epoch 42, Validation Loss: 448.22748426957565\n",
      "Epoch 43, Validation Loss: 443.52789757468486\n",
      "Epoch 44, Validation Loss: 444.6027180064808\n",
      "Epoch 45, Validation Loss: 440.94973997636276\n",
      "Epoch 46, Validation Loss: 440.09294960715556\n",
      "Epoch 47, Validation Loss: 439.504751032049\n",
      "Epoch 48, Validation Loss: 439.0466336337003\n",
      "Epoch 49, Validation Loss: 435.98934797807175\n",
      "Epoch 50, Validation Loss: 436.7574646689675\n",
      "Epoch 51, Validation Loss: 433.49694616144353\n",
      "Epoch 52, Validation Loss: 445.9940546209162\n",
      "Epoch 53, Validation Loss: 431.55671622536397\n",
      "Epoch 54, Validation Loss: 430.34741939197886\n",
      "Epoch 55, Validation Loss: 431.0401583584872\n",
      "Epoch 56, Validation Loss: 430.2573946172541\n",
      "Epoch 57, Validation Loss: 431.50155362215907\n",
      "Epoch 58, Validation Loss: 428.02838758988815\n",
      "Epoch 59, Validation Loss: 431.0776089754972\n",
      "Epoch 60, Validation Loss: 429.8805826360529\n",
      "Epoch 61, Validation Loss: 434.37610279430044\n",
      "Epoch 62, Validation Loss: 426.3319660533558\n",
      "Epoch 63, Validation Loss: 428.1450458873402\n",
      "Epoch 64, Validation Loss: 426.6381981589577\n",
      "Epoch 65, Validation Loss: 428.5969640558416\n",
      "Epoch 66, Validation Loss: 429.2668571472168\n",
      "Epoch 67, Validation Loss: 434.46319302645594\n",
      "Epoch 68, Validation Loss: 427.86672938953745\n",
      "Epoch 69, Validation Loss: 428.883056640625\n",
      "Epoch 70, Validation Loss: 429.7249707308683\n",
      "Epoch 71, Validation Loss: 430.4084174416282\n",
      "Epoch 72, Validation Loss: 430.64676319469106\n",
      "Epoch 73, Validation Loss: 432.88707906549627\n",
      "Epoch 74, Validation Loss: 430.61142314564097\n",
      "Epoch 75, Validation Loss: 429.1467697837136\n",
      "Epoch 76, Validation Loss: 434.31908139315516\n",
      "Epoch 77, Validation Loss: 430.70645419034093\n",
      "Epoch 78, Validation Loss: 428.0414255315607\n",
      "Epoch 79, Validation Loss: 428.3697461214933\n",
      "Epoch 80, Validation Loss: 427.48439025878906\n",
      "Epoch 81, Validation Loss: 427.96229900013316\n",
      "Epoch 82, Validation Loss: 433.1281578757546\n",
      "Epoch 83, Validation Loss: 438.48978285356003\n",
      "Epoch 84, Validation Loss: 439.31957452947444\n",
      "Epoch 85, Validation Loss: 443.1104216142134\n",
      "Epoch 86, Validation Loss: 445.9003205732866\n",
      "Epoch 87, Validation Loss: 448.36547782204366\n",
      "Epoch 88, Validation Loss: 449.2846880826083\n",
      "Epoch 89, Validation Loss: 459.39999944513494\n",
      "Epoch 90, Validation Loss: 453.731053092263\n",
      "Epoch 91, Validation Loss: 425.8522359674627\n",
      "Epoch 92, Validation Loss: 425.032245982777\n",
      "Epoch 93, Validation Loss: 426.9659000743519\n",
      "Epoch 94, Validation Loss: 428.9695642644709\n",
      "Epoch 95, Validation Loss: 430.9729766845703\n",
      "Epoch 96, Validation Loss: 432.47359952059657\n",
      "Epoch 97, Validation Loss: 425.2948206121271\n",
      "Epoch 98, Validation Loss: 432.9921112060547\n",
      "Epoch 99, Validation Loss: 431.775708285245\n",
      "Epoch 100, Validation Loss: 430.1992714621804\n",
      "Epoch 101, Validation Loss: 436.09690510142934\n",
      "Epoch 102, Validation Loss: 431.1740348122337\n",
      "Epoch 103, Validation Loss: 433.91954387318003\n",
      "Epoch 104, Validation Loss: 426.6532911820845\n",
      "Epoch 105, Validation Loss: 426.57779971036047\n",
      "Epoch 106, Validation Loss: 428.0645883733576\n",
      "Epoch 107, Validation Loss: 431.0575478293679\n",
      "Epoch 108, Validation Loss: 423.5329506613991\n",
      "Epoch 109, Validation Loss: 440.6780450994318\n",
      "Epoch 110, Validation Loss: 432.58634532581675\n",
      "Epoch 111, Validation Loss: 434.26241441206497\n",
      "Epoch 112, Validation Loss: 448.01594612815165\n",
      "Epoch 113, Validation Loss: 441.1597831032493\n",
      "Epoch 114, Validation Loss: 443.1919777610085\n",
      "Epoch 115, Validation Loss: 440.36595153808594\n",
      "Epoch 116, Validation Loss: 453.2072788585316\n",
      "Epoch 117, Validation Loss: 446.4309387207031\n",
      "Epoch 118, Validation Loss: 449.37828826904297\n",
      "Epoch 119, Validation Loss: 453.4251695112749\n",
      "Epoch 120, Validation Loss: 451.3946678855202\n",
      "Epoch 121, Validation Loss: 451.02686934037644\n",
      "Epoch 122, Validation Loss: 464.67719130082565\n",
      "Epoch 123, Validation Loss: 433.7785498879173\n",
      "Epoch 124, Validation Loss: 443.53392167524856\n",
      "Epoch 125, Validation Loss: 447.810903375799\n",
      "Epoch 126, Validation Loss: 446.7022885409269\n",
      "Epoch 127, Validation Loss: 446.8640670776367\n",
      "Epoch 128, Validation Loss: 449.58033960515803\n",
      "Epoch 129, Validation Loss: 446.1161519830877\n",
      "Epoch 130, Validation Loss: 450.2558101307262\n",
      "Epoch 131, Validation Loss: 457.5498275756836\n",
      "Epoch 132, Validation Loss: 458.89709888805044\n",
      "Epoch 133, Validation Loss: 456.3145377419212\n",
      "Epoch 134, Validation Loss: 459.94420970569956\n",
      "Epoch 135, Validation Loss: 457.29873934659093\n",
      "Epoch 136, Validation Loss: 464.70109141956675\n",
      "Epoch 137, Validation Loss: 462.7834146673029\n",
      "Epoch 138, Validation Loss: 450.9309879649769\n",
      "Epoch 139, Validation Loss: 453.1243508078835\n",
      "Epoch 140, Validation Loss: 459.865497935902\n",
      "Epoch 141, Validation Loss: 463.76345131613994\n",
      "Epoch 142, Validation Loss: 463.94547549161047\n",
      "Epoch 143, Validation Loss: 448.56494556773794\n",
      "Epoch 144, Validation Loss: 445.9653854370117\n",
      "Epoch 145, Validation Loss: 437.7759829434481\n",
      "Epoch 146, Validation Loss: 444.6389583240856\n",
      "Epoch 147, Validation Loss: 443.6021298495206\n",
      "Epoch 148, Validation Loss: 432.4727152044123\n",
      "Epoch 149, Validation Loss: 434.70034859397197\n",
      "Epoch 150, Validation Loss: 440.95433391224253\n",
      "Epoch 151, Validation Loss: 432.41048570112747\n",
      "Epoch 152, Validation Loss: 440.3791940862482\n",
      "Epoch 153, Validation Loss: 433.08354048295456\n",
      "Epoch 154, Validation Loss: 467.96706321022725\n",
      "Epoch 155, Validation Loss: 430.73294206099075\n",
      "Epoch 156, Validation Loss: 430.21979383988815\n",
      "Epoch 157, Validation Loss: 428.26663693514735\n",
      "Epoch 158, Validation Loss: 433.54347229003906\n",
      "Epoch 159, Validation Loss: 430.2112336592241\n",
      "Epoch 160, Validation Loss: 411.22307656028056\n",
      "Epoch 161, Validation Loss: 408.46710690585047\n",
      "Epoch 162, Validation Loss: 425.03182220458984\n",
      "Epoch 163, Validation Loss: 431.55615789240056\n",
      "Epoch 164, Validation Loss: 432.7434095902876\n",
      "Epoch 165, Validation Loss: 427.8661325628107\n",
      "Epoch 166, Validation Loss: 424.5167867487127\n",
      "Epoch 167, Validation Loss: 437.91523534601384\n",
      "Epoch 168, Validation Loss: 448.8442140059038\n",
      "Epoch 169, Validation Loss: 435.76264745538884\n",
      "Epoch 170, Validation Loss: 455.49198428067297\n",
      "Epoch 171, Validation Loss: 465.1752083518288\n",
      "Epoch 172, Validation Loss: 455.3308757435192\n",
      "Epoch 173, Validation Loss: 467.14861575039953\n",
      "Epoch 174, Validation Loss: 452.35607147216797\n",
      "Epoch 175, Validation Loss: 460.4907164140181\n",
      "Epoch 176, Validation Loss: 478.0809582796964\n",
      "Epoch 177, Validation Loss: 486.2219307639382\n",
      "Epoch 178, Validation Loss: 482.5591652610085\n",
      "Epoch 179, Validation Loss: 477.3797246759588\n",
      "Epoch 180, Validation Loss: 471.6885556307706\n",
      "Epoch 181, Validation Loss: 481.04278078946203\n",
      "Epoch 182, Validation Loss: 442.9630196311257\n",
      "Epoch 183, Validation Loss: 479.4182551990856\n",
      "Epoch 184, Validation Loss: 485.2590283480558\n",
      "Epoch 185, Validation Loss: 473.3788035999645\n",
      "Epoch 186, Validation Loss: 467.6166374900124\n",
      "Epoch 187, Validation Loss: 474.3501205444336\n",
      "Epoch 188, Validation Loss: 459.62301358309657\n",
      "Epoch 189, Validation Loss: 489.18803267045456\n",
      "Epoch 190, Validation Loss: 492.9169269908558\n",
      "Epoch 191, Validation Loss: 472.7080473466353\n",
      "Epoch 192, Validation Loss: 478.25803444602275\n",
      "Epoch 193, Validation Loss: 472.3381611217152\n",
      "Epoch 194, Validation Loss: 459.1657388860529\n",
      "Epoch 195, Validation Loss: 469.5269726839933\n",
      "Epoch 196, Validation Loss: 449.27240614457565\n",
      "Epoch 197, Validation Loss: 467.77690055153585\n",
      "Epoch 198, Validation Loss: 444.9844727949663\n",
      "Epoch 199, Validation Loss: 449.47298431396484\n",
      "Epoch 200, Validation Loss: 459.31195068359375\n",
      "RMSE on test set: 20.28285026550293\n",
      "segment_4\n",
      "before:  2021-01-05 02:29:57\n",
      "after:  2021-01-05 02:44:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\438950037.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.3' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.8' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.8' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.9' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.6' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  2820\n",
      "len of segment_df is  36\n",
      "len of segment_df is  864\n",
      "len of segment_df is  288\n",
      "len of features_list 3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1948.1435353205754\n",
      "Epoch 2, Validation Loss: 1860.2651252012986\n",
      "Epoch 3, Validation Loss: 1846.1281257042517\n",
      "Epoch 4, Validation Loss: 1828.4197362753061\n",
      "Epoch 5, Validation Loss: 1818.7817425361047\n",
      "Epoch 6, Validation Loss: 1816.5123957120454\n",
      "Epoch 7, Validation Loss: 1806.7311644920935\n",
      "Epoch 8, Validation Loss: 1798.0435895186204\n",
      "Epoch 9, Validation Loss: 1791.0802315932053\n",
      "Epoch 10, Validation Loss: 1779.4357760502742\n",
      "Epoch 11, Validation Loss: 1782.3049854865442\n",
      "Epoch 12, Validation Loss: 1780.8157669947698\n",
      "Epoch 13, Validation Loss: 1765.1550589341384\n",
      "Epoch 14, Validation Loss: 1738.8790941972\n",
      "Epoch 15, Validation Loss: 1728.6963178194487\n",
      "Epoch 16, Validation Loss: 1703.4682455796462\n",
      "Epoch 17, Validation Loss: 1686.4395815775945\n",
      "Epoch 18, Validation Loss: 1678.2163458970876\n",
      "Epoch 19, Validation Loss: 1664.1930510447576\n",
      "Epoch 20, Validation Loss: 1654.5145725837122\n",
      "Epoch 21, Validation Loss: 1647.0109458336462\n",
      "Epoch 22, Validation Loss: 1642.5888704153208\n",
      "Epoch 23, Validation Loss: 1637.7305418161245\n",
      "Epoch 24, Validation Loss: 1637.526208437406\n",
      "Epoch 25, Validation Loss: 1634.840460557204\n",
      "Epoch 26, Validation Loss: 1634.5479360727163\n",
      "Epoch 27, Validation Loss: 1636.4918424166167\n",
      "Epoch 28, Validation Loss: 1635.1081724900466\n",
      "Epoch 29, Validation Loss: 1637.1952585073618\n",
      "Epoch 30, Validation Loss: 1639.2773502056416\n",
      "Epoch 31, Validation Loss: 1641.148397592398\n",
      "Epoch 32, Validation Loss: 1645.9417912409856\n",
      "Epoch 33, Validation Loss: 1656.329818725586\n",
      "Epoch 34, Validation Loss: 1662.4480555607722\n",
      "Epoch 35, Validation Loss: 1674.7371274507964\n",
      "Epoch 36, Validation Loss: 1676.6483729435847\n",
      "Epoch 37, Validation Loss: 1688.1330190805288\n",
      "Epoch 38, Validation Loss: 1683.826164832482\n",
      "Epoch 39, Validation Loss: 1695.2589369553787\n",
      "Epoch 40, Validation Loss: 1697.2515070988582\n",
      "Epoch 41, Validation Loss: 1697.5182002140925\n",
      "Epoch 42, Validation Loss: 1693.5134723369893\n",
      "Epoch 43, Validation Loss: 1691.6169222318208\n",
      "Epoch 44, Validation Loss: 1691.0960294283354\n",
      "Epoch 45, Validation Loss: 1690.1303499661958\n",
      "Epoch 46, Validation Loss: 1688.0749652569111\n",
      "Epoch 47, Validation Loss: 1689.3984046349158\n",
      "Epoch 48, Validation Loss: 1687.5909588153545\n",
      "Epoch 49, Validation Loss: 1690.3555603027344\n",
      "Epoch 50, Validation Loss: 1687.5787212665264\n",
      "Epoch 51, Validation Loss: 1688.6124126727764\n",
      "Epoch 52, Validation Loss: 1687.3115492600662\n",
      "Epoch 53, Validation Loss: 1692.763671875\n",
      "Epoch 54, Validation Loss: 1698.1817157451924\n",
      "Epoch 55, Validation Loss: 1699.2311589167668\n",
      "Epoch 56, Validation Loss: 1702.62307035006\n",
      "Epoch 57, Validation Loss: 1721.8736971341646\n",
      "Epoch 58, Validation Loss: 1724.6720698429988\n",
      "Epoch 59, Validation Loss: 1720.2362107496995\n",
      "Epoch 60, Validation Loss: 1730.255849984976\n",
      "Epoch 61, Validation Loss: 1759.86279296875\n",
      "Epoch 62, Validation Loss: 1808.3850473257212\n",
      "Epoch 63, Validation Loss: 1785.0948439378005\n",
      "Epoch 64, Validation Loss: 1883.0181180513823\n",
      "Epoch 65, Validation Loss: 1844.5756131685698\n",
      "Epoch 66, Validation Loss: 1865.447519155649\n",
      "Epoch 67, Validation Loss: 1901.9940467247595\n",
      "Epoch 68, Validation Loss: 1965.619891826923\n",
      "Epoch 69, Validation Loss: 1786.627901517428\n",
      "Epoch 70, Validation Loss: 1950.2667846679688\n",
      "Epoch 71, Validation Loss: 2026.6839435283955\n",
      "Epoch 72, Validation Loss: 2082.529015174279\n",
      "Epoch 73, Validation Loss: 1990.1811265211838\n",
      "Epoch 74, Validation Loss: 1888.873319185697\n",
      "Epoch 75, Validation Loss: 2124.55715238131\n",
      "Epoch 76, Validation Loss: 1906.9204970139724\n",
      "Epoch 77, Validation Loss: 2006.6131544846755\n",
      "Epoch 78, Validation Loss: 2146.0728548490083\n",
      "Epoch 79, Validation Loss: 2168.0697232759917\n",
      "Epoch 80, Validation Loss: 1952.5126248873198\n",
      "Epoch 81, Validation Loss: 2109.3096900353066\n",
      "Epoch 82, Validation Loss: 2108.4882460374097\n",
      "Epoch 83, Validation Loss: 2084.326678936298\n",
      "Epoch 84, Validation Loss: 2206.2362060546875\n",
      "Epoch 85, Validation Loss: 2080.0046997070312\n",
      "Epoch 86, Validation Loss: 2103.472639817458\n",
      "Epoch 87, Validation Loss: 2067.8021028958833\n",
      "Epoch 88, Validation Loss: 2005.7657822829026\n",
      "Epoch 89, Validation Loss: 2002.2437626765325\n",
      "Epoch 90, Validation Loss: 2038.7987107496995\n",
      "Epoch 91, Validation Loss: 2061.688539945162\n",
      "Epoch 92, Validation Loss: 1876.4283400315505\n",
      "Epoch 93, Validation Loss: 1971.1570199819712\n",
      "Epoch 94, Validation Loss: 1963.2728459284856\n",
      "Epoch 95, Validation Loss: 1964.49019916241\n",
      "Epoch 96, Validation Loss: 2022.7766770582932\n",
      "Epoch 97, Validation Loss: 1872.4605360764724\n",
      "Epoch 98, Validation Loss: 2062.913888784555\n",
      "Epoch 99, Validation Loss: 2060.8531165489785\n",
      "Epoch 100, Validation Loss: 1954.5387761042668\n",
      "Epoch 101, Validation Loss: 1882.3236976036658\n",
      "Epoch 102, Validation Loss: 1943.1077998234675\n",
      "Epoch 103, Validation Loss: 1988.5950646033655\n",
      "Epoch 104, Validation Loss: 1958.7041837252104\n",
      "Epoch 105, Validation Loss: 1911.821772648738\n",
      "Epoch 106, Validation Loss: 2034.7836538461538\n",
      "Epoch 107, Validation Loss: 1975.2330838716948\n",
      "Epoch 108, Validation Loss: 1929.8274583082932\n",
      "Epoch 109, Validation Loss: 1933.3613140399639\n",
      "Epoch 110, Validation Loss: 1857.5123783991887\n",
      "Epoch 111, Validation Loss: 1875.361818753756\n",
      "Epoch 112, Validation Loss: 1859.2809377817007\n",
      "Epoch 113, Validation Loss: 1863.1393338716948\n",
      "Epoch 114, Validation Loss: 1895.677743765024\n",
      "Epoch 115, Validation Loss: 1864.3344421386719\n",
      "Epoch 116, Validation Loss: 1882.5161837064302\n",
      "Epoch 117, Validation Loss: 1756.4596088115986\n",
      "Epoch 118, Validation Loss: 1828.5277498685396\n",
      "Epoch 119, Validation Loss: 1870.7183391864482\n",
      "Epoch 120, Validation Loss: 1882.6675532414363\n",
      "Epoch 121, Validation Loss: 1808.5720637394832\n",
      "Epoch 122, Validation Loss: 1765.0682513897236\n",
      "Epoch 123, Validation Loss: 1929.5059368426982\n",
      "Epoch 124, Validation Loss: 1880.409675011268\n",
      "Epoch 125, Validation Loss: 1912.8645230806792\n",
      "Epoch 126, Validation Loss: 1883.8343693659856\n",
      "Epoch 127, Validation Loss: 1921.0085918719951\n",
      "Epoch 128, Validation Loss: 1908.9811166616587\n",
      "Epoch 129, Validation Loss: 1929.0164278470552\n",
      "Epoch 130, Validation Loss: 1907.7914405235877\n",
      "Epoch 131, Validation Loss: 1899.1178307166467\n",
      "Epoch 132, Validation Loss: 1905.2417766864482\n",
      "Epoch 133, Validation Loss: 1858.1419114332932\n",
      "Epoch 134, Validation Loss: 1916.4789593036357\n",
      "Epoch 135, Validation Loss: 1995.5009460449219\n",
      "Epoch 136, Validation Loss: 1976.9444462702825\n",
      "Epoch 137, Validation Loss: 1928.3729107196514\n",
      "Epoch 138, Validation Loss: 1937.395047701322\n",
      "Epoch 139, Validation Loss: 1932.6190279447114\n",
      "Epoch 140, Validation Loss: 1705.7959735576924\n",
      "Epoch 141, Validation Loss: 1841.0963604266826\n",
      "Epoch 142, Validation Loss: 1932.0547133225662\n",
      "Epoch 143, Validation Loss: 1956.8641451322114\n",
      "Epoch 144, Validation Loss: 1902.8147277832031\n",
      "Epoch 145, Validation Loss: 1871.6215256911057\n",
      "Epoch 146, Validation Loss: 1769.5155334472656\n",
      "Epoch 147, Validation Loss: 1852.9191401554988\n",
      "Epoch 148, Validation Loss: 1828.7139516977163\n",
      "Epoch 149, Validation Loss: 1906.610614483173\n",
      "Epoch 150, Validation Loss: 1831.8613023024338\n",
      "Epoch 151, Validation Loss: 1760.3177091158354\n",
      "Epoch 152, Validation Loss: 1878.517329289363\n",
      "Epoch 153, Validation Loss: 1820.9949904221755\n",
      "Epoch 154, Validation Loss: 1796.9584303635818\n",
      "Epoch 155, Validation Loss: 1799.9727548452524\n",
      "Epoch 156, Validation Loss: 1805.5784935584436\n",
      "Epoch 157, Validation Loss: 1778.2364736703726\n",
      "Epoch 158, Validation Loss: 1790.2747873159556\n",
      "Epoch 159, Validation Loss: 1802.537581223708\n",
      "Epoch 160, Validation Loss: 1775.6545551006611\n",
      "Epoch 161, Validation Loss: 1801.3328106219951\n",
      "Epoch 162, Validation Loss: 1706.8279606745793\n",
      "Epoch 163, Validation Loss: 1837.4398897611177\n",
      "Epoch 164, Validation Loss: 1724.597400371845\n",
      "Epoch 165, Validation Loss: 1757.563748873197\n",
      "Epoch 166, Validation Loss: 1763.7928631122295\n",
      "Epoch 167, Validation Loss: 1768.1258779672476\n",
      "Epoch 168, Validation Loss: 1756.3061828613281\n",
      "Epoch 169, Validation Loss: 1808.5699509840745\n",
      "Epoch 170, Validation Loss: 1770.4580735426682\n",
      "Epoch 171, Validation Loss: 1832.157930814303\n",
      "Epoch 172, Validation Loss: 1860.9358943058894\n",
      "Epoch 173, Validation Loss: 1903.2017329289363\n",
      "Epoch 174, Validation Loss: 1875.5223506047175\n",
      "Epoch 175, Validation Loss: 1816.1333547738882\n",
      "Epoch 176, Validation Loss: 1882.3927518404448\n",
      "Epoch 177, Validation Loss: 1886.03757418119\n",
      "Epoch 178, Validation Loss: 1865.7699655386118\n",
      "Epoch 179, Validation Loss: 1802.584247295673\n",
      "Epoch 180, Validation Loss: 1695.5302311823918\n",
      "Epoch 181, Validation Loss: 1761.6298217773438\n",
      "Epoch 182, Validation Loss: 1826.4786071777344\n",
      "Epoch 183, Validation Loss: 2023.0453444260818\n",
      "Epoch 184, Validation Loss: 1927.8773897611177\n",
      "Epoch 185, Validation Loss: 1886.766108586238\n",
      "Epoch 186, Validation Loss: 1926.10842191256\n",
      "Epoch 187, Validation Loss: 1861.662149282602\n",
      "Epoch 188, Validation Loss: 1877.9064190204326\n",
      "Epoch 189, Validation Loss: 1950.0180053710938\n",
      "Epoch 190, Validation Loss: 1931.4250230055588\n",
      "Epoch 191, Validation Loss: 1936.179694542518\n",
      "Epoch 192, Validation Loss: 1947.2663902869592\n",
      "Epoch 193, Validation Loss: 1937.4352323091948\n",
      "Epoch 194, Validation Loss: 1934.7588735727163\n",
      "Epoch 195, Validation Loss: 1918.8106313852163\n",
      "Epoch 196, Validation Loss: 1938.6978924091045\n",
      "Epoch 197, Validation Loss: 1884.3865778996394\n",
      "Epoch 198, Validation Loss: 1791.8518207256611\n",
      "Epoch 199, Validation Loss: 1818.804694542518\n",
      "Epoch 200, Validation Loss: 1793.646474984976\n",
      "RMSE on test set: 43.805782318115234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of segment_df is  288\n",
      "len of features_list 267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 12063.5107421875\n",
      "Epoch 2, Validation Loss: 9042.4013671875\n",
      "Epoch 3, Validation Loss: 1345.351806640625\n",
      "Epoch 4, Validation Loss: 1802.572021484375\n",
      "Epoch 5, Validation Loss: 3050.404296875\n",
      "Epoch 6, Validation Loss: 1377.1583251953125\n",
      "Epoch 7, Validation Loss: 1510.5181884765625\n",
      "Epoch 8, Validation Loss: 2042.0318603515625\n",
      "Epoch 9, Validation Loss: 1453.1737060546875\n",
      "Epoch 10, Validation Loss: 1568.605224609375\n",
      "Epoch 11, Validation Loss: 1728.247314453125\n",
      "Epoch 12, Validation Loss: 1506.193603515625\n",
      "Epoch 13, Validation Loss: 1612.5089111328125\n",
      "Epoch 14, Validation Loss: 1615.5927734375\n",
      "Epoch 15, Validation Loss: 1551.42333984375\n",
      "Epoch 16, Validation Loss: 1613.111083984375\n",
      "Epoch 17, Validation Loss: 1580.460693359375\n",
      "Epoch 18, Validation Loss: 1580.6375732421875\n",
      "Epoch 19, Validation Loss: 1596.7249755859375\n",
      "Epoch 20, Validation Loss: 1578.1524658203125\n",
      "Epoch 21, Validation Loss: 1589.5855712890625\n",
      "Epoch 22, Validation Loss: 1585.6944580078125\n",
      "Epoch 23, Validation Loss: 1583.4117431640625\n",
      "Epoch 24, Validation Loss: 1587.4765625\n",
      "Epoch 25, Validation Loss: 1583.3072509765625\n",
      "Epoch 26, Validation Loss: 1585.4541015625\n",
      "Epoch 27, Validation Loss: 1584.4842529296875\n",
      "Epoch 28, Validation Loss: 1583.902099609375\n",
      "Epoch 29, Validation Loss: 1584.523681640625\n",
      "Epoch 30, Validation Loss: 1583.4873046875\n",
      "Epoch 31, Validation Loss: 1583.837158203125\n",
      "Epoch 32, Validation Loss: 1583.363037109375\n",
      "Epoch 33, Validation Loss: 1583.1728515625\n",
      "Epoch 34, Validation Loss: 1583.0740966796875\n",
      "Epoch 35, Validation Loss: 1582.718505859375\n",
      "Epoch 36, Validation Loss: 1582.6351318359375\n",
      "Epoch 37, Validation Loss: 1582.3463134765625\n",
      "Epoch 38, Validation Loss: 1582.17919921875\n",
      "Epoch 39, Validation Loss: 1581.9857177734375\n",
      "Epoch 40, Validation Loss: 1581.7740478515625\n",
      "Epoch 41, Validation Loss: 1581.5953369140625\n",
      "Epoch 42, Validation Loss: 1581.3621826171875\n",
      "Epoch 43, Validation Loss: 1581.1724853515625\n",
      "Epoch 44, Validation Loss: 1580.979248046875\n",
      "Epoch 45, Validation Loss: 1580.800048828125\n",
      "Epoch 46, Validation Loss: 1580.5953369140625\n",
      "Epoch 47, Validation Loss: 1580.398193359375\n",
      "Epoch 48, Validation Loss: 1580.175537109375\n",
      "Epoch 49, Validation Loss: 1579.983154296875\n",
      "Epoch 50, Validation Loss: 1579.7779541015625\n",
      "Epoch 51, Validation Loss: 1579.5634765625\n",
      "Epoch 52, Validation Loss: 1579.3580322265625\n",
      "Epoch 53, Validation Loss: 1579.1600341796875\n",
      "Epoch 54, Validation Loss: 1578.946044921875\n",
      "Epoch 55, Validation Loss: 1578.744140625\n",
      "Epoch 56, Validation Loss: 1578.550537109375\n",
      "Epoch 57, Validation Loss: 1578.318359375\n",
      "Epoch 58, Validation Loss: 1578.1138916015625\n",
      "Epoch 59, Validation Loss: 1577.92041015625\n",
      "Epoch 60, Validation Loss: 1577.6865234375\n",
      "Epoch 61, Validation Loss: 1577.48095703125\n",
      "Epoch 62, Validation Loss: 1577.2825927734375\n",
      "Epoch 63, Validation Loss: 1577.043212890625\n",
      "Epoch 64, Validation Loss: 1576.842041015625\n",
      "Epoch 65, Validation Loss: 1576.6505126953125\n",
      "Epoch 66, Validation Loss: 1576.3876953125\n",
      "Epoch 67, Validation Loss: 1576.1944580078125\n",
      "Epoch 68, Validation Loss: 1576.00244140625\n",
      "Epoch 69, Validation Loss: 1575.7490234375\n",
      "Epoch 70, Validation Loss: 1575.52587890625\n",
      "Epoch 71, Validation Loss: 1575.321044921875\n",
      "Epoch 72, Validation Loss: 1575.043212890625\n",
      "Epoch 73, Validation Loss: 1574.8592529296875\n",
      "Epoch 74, Validation Loss: 1574.6522216796875\n",
      "Epoch 75, Validation Loss: 1574.3685302734375\n",
      "Epoch 76, Validation Loss: 1574.139404296875\n",
      "Epoch 77, Validation Loss: 1573.936767578125\n",
      "Epoch 78, Validation Loss: 1573.638427734375\n",
      "Epoch 79, Validation Loss: 1573.4224853515625\n",
      "Epoch 80, Validation Loss: 1573.2220458984375\n",
      "Epoch 81, Validation Loss: 1572.9056396484375\n",
      "Epoch 82, Validation Loss: 1572.70166015625\n",
      "Epoch 83, Validation Loss: 1572.4970703125\n",
      "Epoch 84, Validation Loss: 1572.16357421875\n",
      "Epoch 85, Validation Loss: 1571.954345703125\n",
      "Epoch 86, Validation Loss: 1571.7435302734375\n",
      "Epoch 87, Validation Loss: 1571.41015625\n",
      "Epoch 88, Validation Loss: 1571.19775390625\n",
      "Epoch 89, Validation Loss: 1570.979736328125\n",
      "Epoch 90, Validation Loss: 1570.63671875\n",
      "Epoch 91, Validation Loss: 1570.4111328125\n",
      "Epoch 92, Validation Loss: 1570.2119140625\n",
      "Epoch 93, Validation Loss: 1569.8446044921875\n",
      "Epoch 94, Validation Loss: 1569.62841796875\n",
      "Epoch 95, Validation Loss: 1569.430908203125\n",
      "Epoch 96, Validation Loss: 1569.048583984375\n",
      "Epoch 97, Validation Loss: 1568.8284912109375\n",
      "Epoch 98, Validation Loss: 1568.6214599609375\n",
      "Epoch 99, Validation Loss: 1568.3157958984375\n",
      "Epoch 100, Validation Loss: 1568.2706298828125\n",
      "Epoch 101, Validation Loss: 1567.830078125\n",
      "Epoch 102, Validation Loss: 1567.4217529296875\n",
      "Epoch 103, Validation Loss: 1567.3516845703125\n",
      "Epoch 104, Validation Loss: 1567.2779541015625\n",
      "Epoch 105, Validation Loss: 1566.830078125\n",
      "Epoch 106, Validation Loss: 1566.313232421875\n",
      "Epoch 107, Validation Loss: 1566.3172607421875\n",
      "Epoch 108, Validation Loss: 1566.1649169921875\n",
      "Epoch 109, Validation Loss: 1565.841064453125\n",
      "Epoch 110, Validation Loss: 1565.4466552734375\n",
      "Epoch 111, Validation Loss: 1565.06787109375\n",
      "Epoch 112, Validation Loss: 1565.0042724609375\n",
      "Epoch 113, Validation Loss: 1564.584228515625\n",
      "Epoch 114, Validation Loss: 1564.2783203125\n",
      "Epoch 115, Validation Loss: 1563.745361328125\n",
      "Epoch 116, Validation Loss: 1563.7767333984375\n",
      "Epoch 117, Validation Loss: 1563.3079833984375\n",
      "Epoch 118, Validation Loss: 1563.08544921875\n",
      "Epoch 119, Validation Loss: 1562.5140380859375\n",
      "Epoch 120, Validation Loss: 1562.552490234375\n",
      "Epoch 121, Validation Loss: 1562.05419921875\n",
      "Epoch 122, Validation Loss: 1561.9727783203125\n",
      "Epoch 123, Validation Loss: 1561.6470947265625\n",
      "Epoch 124, Validation Loss: 1561.2164306640625\n",
      "Epoch 125, Validation Loss: 1560.9324951171875\n",
      "Epoch 126, Validation Loss: 1560.742431640625\n",
      "Epoch 127, Validation Loss: 1560.2406005859375\n",
      "Epoch 128, Validation Loss: 1559.98046875\n",
      "Epoch 129, Validation Loss: 1559.8223876953125\n",
      "Epoch 130, Validation Loss: 1559.2562255859375\n",
      "Epoch 131, Validation Loss: 1558.969970703125\n",
      "Epoch 132, Validation Loss: 1558.71875\n",
      "Epoch 133, Validation Loss: 1558.1258544921875\n",
      "Epoch 134, Validation Loss: 1557.8990478515625\n",
      "Epoch 135, Validation Loss: 1557.662841796875\n",
      "Epoch 136, Validation Loss: 1557.1292724609375\n",
      "Epoch 137, Validation Loss: 1556.916015625\n",
      "Epoch 138, Validation Loss: 1556.643310546875\n",
      "Epoch 139, Validation Loss: 1556.2808837890625\n",
      "Epoch 140, Validation Loss: 1556.285888671875\n",
      "Epoch 141, Validation Loss: 1555.4013671875\n",
      "Epoch 142, Validation Loss: 1554.9232177734375\n",
      "Epoch 143, Validation Loss: 1555.0013427734375\n",
      "Epoch 144, Validation Loss: 1554.952880859375\n",
      "Epoch 145, Validation Loss: 1554.4056396484375\n",
      "Epoch 146, Validation Loss: 1553.3114013671875\n",
      "Epoch 147, Validation Loss: 1553.8076171875\n",
      "Epoch 148, Validation Loss: 1553.513427734375\n",
      "Epoch 149, Validation Loss: 1552.781494140625\n",
      "Epoch 150, Validation Loss: 1552.1826171875\n",
      "Epoch 151, Validation Loss: 1552.115234375\n",
      "Epoch 152, Validation Loss: 1552.3250732421875\n",
      "Epoch 153, Validation Loss: 1551.4593505859375\n",
      "Epoch 154, Validation Loss: 1550.7069091796875\n",
      "Epoch 155, Validation Loss: 1550.7244873046875\n",
      "Epoch 156, Validation Loss: 1550.771484375\n",
      "Epoch 157, Validation Loss: 1550.35595703125\n",
      "Epoch 158, Validation Loss: 1549.7645263671875\n",
      "Epoch 159, Validation Loss: 1549.1431884765625\n",
      "Epoch 160, Validation Loss: 1549.2564697265625\n",
      "Epoch 161, Validation Loss: 1548.633544921875\n",
      "Epoch 162, Validation Loss: 1548.3568115234375\n",
      "Epoch 163, Validation Loss: 1547.6529541015625\n",
      "Epoch 164, Validation Loss: 1547.7982177734375\n",
      "Epoch 165, Validation Loss: 1547.0576171875\n",
      "Epoch 166, Validation Loss: 1546.838134765625\n",
      "Epoch 167, Validation Loss: 1546.0478515625\n",
      "Epoch 168, Validation Loss: 1546.2681884765625\n",
      "Epoch 169, Validation Loss: 1545.49072265625\n",
      "Epoch 170, Validation Loss: 1545.5458984375\n",
      "Epoch 171, Validation Loss: 1545.2738037109375\n",
      "Epoch 172, Validation Loss: 1544.4871826171875\n",
      "Epoch 173, Validation Loss: 1544.03515625\n",
      "Epoch 174, Validation Loss: 1543.4610595703125\n",
      "Epoch 175, Validation Loss: 1543.3685302734375\n",
      "Epoch 176, Validation Loss: 1542.6898193359375\n",
      "Epoch 177, Validation Loss: 1542.773193359375\n",
      "Epoch 178, Validation Loss: 1542.1458740234375\n",
      "Epoch 179, Validation Loss: 1541.748779296875\n",
      "Epoch 180, Validation Loss: 1541.18017578125\n",
      "Epoch 181, Validation Loss: 1541.24609375\n",
      "Epoch 182, Validation Loss: 1540.3974609375\n",
      "Epoch 183, Validation Loss: 1540.18994140625\n",
      "Epoch 184, Validation Loss: 1539.822021484375\n",
      "Epoch 185, Validation Loss: 1539.2940673828125\n",
      "Epoch 186, Validation Loss: 1538.9073486328125\n",
      "Epoch 187, Validation Loss: 1538.6234130859375\n",
      "Epoch 188, Validation Loss: 1537.8909912109375\n",
      "Epoch 189, Validation Loss: 1537.627197265625\n",
      "Epoch 190, Validation Loss: 1537.3707275390625\n",
      "Epoch 191, Validation Loss: 1536.620849609375\n",
      "Epoch 192, Validation Loss: 1536.355224609375\n",
      "Epoch 193, Validation Loss: 1536.1475830078125\n",
      "Epoch 194, Validation Loss: 1535.498291015625\n",
      "Epoch 195, Validation Loss: 1535.838623046875\n",
      "Epoch 196, Validation Loss: 1534.7685546875\n",
      "Epoch 197, Validation Loss: 1533.76611328125\n",
      "Epoch 198, Validation Loss: 1533.958984375\n",
      "Epoch 199, Validation Loss: 1534.1673583984375\n",
      "Epoch 200, Validation Loss: 1533.2117919921875\n",
      "RMSE on test set: 21.37425422668457\n",
      "segment_1\n",
      "before:  2020-11-01 06:28:16\n",
      "after:  2020-11-01 07:28:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-11-06 03:43:14\n",
      "after:  2020-11-06 04:38:14\n",
      "segment_2\n",
      "before:  2020-11-06 21:38:14\n",
      "after:  2020-11-06 22:03:13\n",
      "segment_4\n",
      "before:  2020-11-16 10:23:10\n",
      "after:  2020-11-16 11:03:09\n",
      "segment_6\n",
      "before:  2020-11-18 19:43:08\n",
      "after:  2020-11-18 19:58:08\n",
      "segment_6\n",
      "before:  2020-11-18 19:58:08\n",
      "after:  2020-11-18 20:13:08\n",
      "segment_6\n",
      "before:  2020-11-25 11:08:05\n",
      "after:  2020-11-25 11:23:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  864\n",
      "len of segment_df is  1027\n",
      "len of segment_df is  1430\n",
      "len of segment_df is  996\n",
      "len of segment_df is  115\n",
      "len of segment_df is  2327\n",
      "len of features_list 6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1205.2298162551153\n",
      "Epoch 2, Validation Loss: 1308.8496718633742\n",
      "Epoch 3, Validation Loss: 1259.0700785319011\n",
      "Epoch 4, Validation Loss: 1261.7122744605654\n",
      "Epoch 5, Validation Loss: 1286.0391010102771\n",
      "Epoch 6, Validation Loss: 1292.8165123349145\n",
      "Epoch 7, Validation Loss: 1371.1293683733259\n",
      "Epoch 8, Validation Loss: 1435.7815573556084\n",
      "Epoch 9, Validation Loss: 1509.154764811198\n",
      "Epoch 10, Validation Loss: 1593.0958222888764\n",
      "Epoch 11, Validation Loss: 1684.2159024193174\n",
      "Epoch 12, Validation Loss: 1755.9718744187128\n",
      "Epoch 13, Validation Loss: 1821.5888344900948\n",
      "Epoch 14, Validation Loss: 1838.716056460426\n",
      "Epoch 15, Validation Loss: 1927.9559464227586\n",
      "Epoch 16, Validation Loss: 1859.2672758556548\n",
      "Epoch 17, Validation Loss: 1839.5372804914202\n",
      "Epoch 18, Validation Loss: 1850.3560485839844\n",
      "Epoch 19, Validation Loss: 1853.9694333757673\n",
      "Epoch 20, Validation Loss: 1850.0446882702056\n",
      "Epoch 21, Validation Loss: 1870.4976265316918\n",
      "Epoch 22, Validation Loss: 1865.8763961791992\n",
      "Epoch 23, Validation Loss: 1869.231472923642\n",
      "Epoch 24, Validation Loss: 1815.9421949840728\n",
      "Epoch 25, Validation Loss: 1835.3636156717937\n",
      "Epoch 26, Validation Loss: 1771.169792175293\n",
      "Epoch 27, Validation Loss: 1758.3300792149134\n",
      "Epoch 28, Validation Loss: 1738.1409207298643\n",
      "Epoch 29, Validation Loss: 1681.001969110398\n",
      "Epoch 30, Validation Loss: 1688.378839583624\n",
      "Epoch 31, Validation Loss: 1695.8846335638136\n",
      "Epoch 32, Validation Loss: 1630.5430101667132\n",
      "Epoch 33, Validation Loss: 1580.2222190130324\n",
      "Epoch 34, Validation Loss: 1550.7420645214263\n",
      "Epoch 35, Validation Loss: 1536.862625667027\n",
      "Epoch 36, Validation Loss: 1573.4961150033134\n",
      "Epoch 37, Validation Loss: 1566.2107829139345\n",
      "Epoch 38, Validation Loss: 1446.4541742234003\n",
      "Epoch 39, Validation Loss: 1428.7662382579986\n",
      "Epoch 40, Validation Loss: 1882.8779692876906\n",
      "Epoch 41, Validation Loss: 1415.4169949122838\n",
      "Epoch 42, Validation Loss: 1478.3837814331055\n",
      "Epoch 43, Validation Loss: 1333.1155330113002\n",
      "Epoch 44, Validation Loss: 1317.7121480305989\n",
      "Epoch 45, Validation Loss: 1512.6566903250557\n",
      "Epoch 46, Validation Loss: 1340.3243342808314\n",
      "Epoch 47, Validation Loss: 1299.882591610863\n",
      "Epoch 48, Validation Loss: 1273.8132327851795\n",
      "Epoch 49, Validation Loss: 1369.0347916739327\n",
      "Epoch 50, Validation Loss: 1291.6898200625465\n",
      "Epoch 51, Validation Loss: 1224.4273325602214\n",
      "Epoch 52, Validation Loss: 1219.4848058791388\n",
      "Epoch 53, Validation Loss: 1210.6639818464007\n",
      "Epoch 54, Validation Loss: 1200.9010089692615\n",
      "Epoch 55, Validation Loss: 1200.3832459222704\n",
      "Epoch 56, Validation Loss: 1235.743805294945\n",
      "Epoch 57, Validation Loss: 1160.2809887840635\n",
      "Epoch 58, Validation Loss: 1158.563956851051\n",
      "Epoch 59, Validation Loss: 1140.2244582403273\n",
      "Epoch 60, Validation Loss: 1178.3102358863466\n",
      "Epoch 61, Validation Loss: 1113.860335577102\n",
      "Epoch 62, Validation Loss: 1169.9514723278228\n",
      "Epoch 63, Validation Loss: 1137.8688895815894\n",
      "Epoch 64, Validation Loss: 1122.0095585414342\n",
      "Epoch 65, Validation Loss: 1098.9336039225261\n",
      "Epoch 66, Validation Loss: 1185.879133678618\n",
      "Epoch 67, Validation Loss: 1074.6663956415086\n",
      "Epoch 68, Validation Loss: 1098.2378881545294\n",
      "Epoch 69, Validation Loss: 1040.5803912934803\n",
      "Epoch 70, Validation Loss: 1062.7992030552455\n",
      "Epoch 71, Validation Loss: 1033.79992494129\n",
      "Epoch 72, Validation Loss: 1041.092542375837\n",
      "Epoch 73, Validation Loss: 1067.4606130690802\n",
      "Epoch 74, Validation Loss: 1016.0115865071615\n",
      "Epoch 75, Validation Loss: 1038.3019841512044\n",
      "Epoch 76, Validation Loss: 1090.8584474836077\n",
      "Epoch 77, Validation Loss: 1038.140515645345\n",
      "Epoch 78, Validation Loss: 1031.6661493210565\n",
      "Epoch 79, Validation Loss: 1070.0555906749908\n",
      "Epoch 80, Validation Loss: 995.7891456967309\n",
      "Epoch 81, Validation Loss: 1010.2246366228376\n",
      "Epoch 82, Validation Loss: 1021.0194498697916\n",
      "Epoch 83, Validation Loss: 1027.8588166009813\n",
      "Epoch 84, Validation Loss: 1038.8913210914247\n",
      "Epoch 85, Validation Loss: 1005.264649164109\n",
      "Epoch 86, Validation Loss: 1014.345460437593\n",
      "Epoch 87, Validation Loss: 1065.8011144002278\n",
      "Epoch 88, Validation Loss: 1025.0311003185454\n",
      "Epoch 89, Validation Loss: 997.1307416643415\n",
      "Epoch 90, Validation Loss: 985.2061509631928\n",
      "Epoch 91, Validation Loss: 987.093991960798\n",
      "Epoch 92, Validation Loss: 1018.4650904337565\n",
      "Epoch 93, Validation Loss: 1031.6869862874348\n",
      "Epoch 94, Validation Loss: 977.412121000744\n",
      "Epoch 95, Validation Loss: 997.4480089460101\n",
      "Epoch 96, Validation Loss: 982.9453379313151\n",
      "Epoch 97, Validation Loss: 1057.1002509707496\n",
      "Epoch 98, Validation Loss: 993.0324826921735\n",
      "Epoch 99, Validation Loss: 989.6888485863095\n",
      "Epoch 100, Validation Loss: 997.6546932402111\n",
      "Epoch 101, Validation Loss: 1459.9762028285436\n",
      "Epoch 102, Validation Loss: 1151.1423350742884\n",
      "Epoch 103, Validation Loss: 987.1636112758091\n",
      "Epoch 104, Validation Loss: 968.5371282668341\n",
      "Epoch 105, Validation Loss: 987.3454855056036\n",
      "Epoch 106, Validation Loss: 965.745353335426\n",
      "Epoch 107, Validation Loss: 967.2800936017718\n",
      "Epoch 108, Validation Loss: 957.8042526245117\n",
      "Epoch 109, Validation Loss: 966.4607562110538\n",
      "Epoch 110, Validation Loss: 956.9234437488374\n",
      "Epoch 111, Validation Loss: 965.3497525169736\n",
      "Epoch 112, Validation Loss: 963.5134168352399\n",
      "Epoch 113, Validation Loss: 977.6682942708334\n",
      "Epoch 114, Validation Loss: 969.0768697829474\n",
      "Epoch 115, Validation Loss: 962.8970896402994\n",
      "Epoch 116, Validation Loss: 965.2083841959635\n",
      "Epoch 117, Validation Loss: 965.9548783075242\n",
      "Epoch 118, Validation Loss: 960.4442323957171\n",
      "Epoch 119, Validation Loss: 957.7097934541248\n",
      "Epoch 120, Validation Loss: 963.1476563953217\n",
      "Epoch 121, Validation Loss: 965.0535125732422\n",
      "Epoch 122, Validation Loss: 953.5465320405506\n",
      "Epoch 123, Validation Loss: 960.8600136893136\n",
      "Epoch 124, Validation Loss: 986.4068131219773\n",
      "Epoch 125, Validation Loss: 976.7865397135416\n",
      "Epoch 126, Validation Loss: 967.8894566127232\n",
      "Epoch 127, Validation Loss: 993.9501611618768\n",
      "Epoch 128, Validation Loss: 997.3453223818824\n",
      "Epoch 129, Validation Loss: 985.3955790201823\n",
      "Epoch 130, Validation Loss: 982.5805693126861\n",
      "Epoch 131, Validation Loss: 976.7544839041574\n",
      "Epoch 132, Validation Loss: 971.1096707298642\n",
      "Epoch 133, Validation Loss: 969.5220990862165\n",
      "Epoch 134, Validation Loss: 965.9137246268136\n",
      "Epoch 135, Validation Loss: 977.1007675897507\n",
      "Epoch 136, Validation Loss: 948.8210652669271\n",
      "Epoch 137, Validation Loss: 961.7842327299572\n",
      "Epoch 138, Validation Loss: 951.9444187709263\n",
      "Epoch 139, Validation Loss: 948.8576231456939\n",
      "Epoch 140, Validation Loss: 958.9473680768695\n",
      "Epoch 141, Validation Loss: 1052.9172301519484\n",
      "Epoch 142, Validation Loss: 1018.5383787609283\n",
      "Epoch 143, Validation Loss: 971.5358428955078\n",
      "Epoch 144, Validation Loss: 950.2377130417597\n",
      "Epoch 145, Validation Loss: 972.9389292399088\n",
      "Epoch 146, Validation Loss: 956.423341296968\n",
      "Epoch 147, Validation Loss: 978.8689676920573\n",
      "Epoch 148, Validation Loss: 989.1645129975818\n",
      "Epoch 149, Validation Loss: 992.3590894426618\n",
      "Epoch 150, Validation Loss: 1027.4657389322917\n",
      "Epoch 151, Validation Loss: 1010.6786520821707\n",
      "Epoch 152, Validation Loss: 985.0308365594773\n",
      "Epoch 153, Validation Loss: 976.9739837646484\n",
      "Epoch 154, Validation Loss: 1001.8437645321801\n",
      "Epoch 155, Validation Loss: 1023.0041532970611\n",
      "Epoch 156, Validation Loss: 997.7058708554223\n",
      "Epoch 157, Validation Loss: 1016.0191795712426\n",
      "Epoch 158, Validation Loss: 1019.2273697626023\n",
      "Epoch 159, Validation Loss: 1053.019288562593\n",
      "Epoch 160, Validation Loss: 1153.9007575625465\n",
      "Epoch 161, Validation Loss: 990.5562497093564\n",
      "Epoch 162, Validation Loss: 1005.6056038992746\n",
      "Epoch 163, Validation Loss: 999.9179324195499\n",
      "Epoch 164, Validation Loss: 970.4422687348865\n",
      "Epoch 165, Validation Loss: 958.4996090843564\n",
      "Epoch 166, Validation Loss: 971.4048476446243\n",
      "Epoch 167, Validation Loss: 956.5449770972842\n",
      "Epoch 168, Validation Loss: 969.6361258370536\n",
      "Epoch 169, Validation Loss: 970.5412902832031\n",
      "Epoch 170, Validation Loss: 975.0272471110026\n",
      "Epoch 171, Validation Loss: 976.112810407366\n",
      "Epoch 172, Validation Loss: 987.1583281017486\n",
      "Epoch 173, Validation Loss: 969.9295799618676\n",
      "Epoch 174, Validation Loss: 981.5863487606957\n",
      "Epoch 175, Validation Loss: 978.6238752092634\n",
      "Epoch 176, Validation Loss: 979.2645161946615\n",
      "Epoch 177, Validation Loss: 972.2245948428199\n",
      "Epoch 178, Validation Loss: 1016.595225016276\n",
      "Epoch 179, Validation Loss: 998.162345522926\n",
      "Epoch 180, Validation Loss: 1229.483659290132\n",
      "Epoch 181, Validation Loss: 970.3698279971168\n",
      "Epoch 182, Validation Loss: 971.0805119105747\n",
      "Epoch 183, Validation Loss: 975.0931229364304\n",
      "Epoch 184, Validation Loss: 994.451894850958\n",
      "Epoch 185, Validation Loss: 1014.493181501116\n",
      "Epoch 186, Validation Loss: 981.5152071998233\n",
      "Epoch 187, Validation Loss: 1015.948009672619\n",
      "Epoch 188, Validation Loss: 997.744367327009\n",
      "Epoch 189, Validation Loss: 983.0296013241723\n",
      "Epoch 190, Validation Loss: 986.2141912551153\n",
      "Epoch 191, Validation Loss: 997.5564284551712\n",
      "Epoch 192, Validation Loss: 1003.107659476144\n",
      "Epoch 193, Validation Loss: 1023.5323500860305\n",
      "Epoch 194, Validation Loss: 975.0721406482514\n",
      "Epoch 195, Validation Loss: 977.8591948009673\n",
      "Epoch 196, Validation Loss: 996.3015696207682\n",
      "Epoch 197, Validation Loss: 981.5008494059244\n",
      "Epoch 198, Validation Loss: 992.8519112723214\n",
      "Epoch 199, Validation Loss: 1042.3465670631044\n",
      "Epoch 200, Validation Loss: 1060.6549312046595\n",
      "RMSE on test set: 26.374431610107422\n",
      "segment_1\n",
      "before:  2020-03-09 03:38:08\n",
      "after:  2020-03-09 03:48:08\n",
      "segment_1\n",
      "before:  2020-03-09 03:48:08\n",
      "after:  2020-03-09 04:03:09\n",
      "segment_1\n",
      "before:  2020-03-09 07:58:10\n",
      "after:  2020-03-09 08:13:10\n",
      "segment_1\n",
      "before:  2020-03-09 10:03:10\n",
      "after:  2020-03-09 10:13:10\n",
      "segment_1\n",
      "before:  2020-03-09 10:18:10\n",
      "after:  2020-03-09 10:28:10\n",
      "segment_1\n",
      "before:  2020-03-09 10:38:10\n",
      "after:  2020-03-09 10:58:10\n",
      "segment_1\n",
      "before:  2020-03-10 06:18:12\n",
      "after:  2020-03-10 06:28:13\n",
      "segment_1\n",
      "before:  2020-03-10 13:53:14\n",
      "after:  2020-03-10 14:43:13\n",
      "segment_1\n",
      "before:  2020-03-10 15:58:13\n",
      "after:  2020-03-10 16:08:13\n",
      "segment_1\n",
      "before:  2020-03-11 03:13:16\n",
      "after:  2020-03-11 03:33:16\n",
      "segment_1\n",
      "before:  2020-03-11 04:23:16\n",
      "after:  2020-03-11 04:38:16\n",
      "segment_1\n",
      "before:  2020-03-11 06:28:16\n",
      "after:  2020-03-11 06:43:16\n",
      "segment_1\n",
      "before:  2020-03-11 11:53:16\n",
      "after:  2020-03-11 12:08:16\n",
      "segment_1\n",
      "before:  2020-03-11 13:38:17\n",
      "after:  2020-03-11 13:48:17\n",
      "segment_2\n",
      "before:  2020-03-11 17:43:27\n",
      "after:  2020-03-11 17:53:18\n",
      "segment_2\n",
      "before:  2020-03-12 04:23:20\n",
      "after:  2020-03-12 04:33:20\n",
      "segment_2\n",
      "before:  2020-03-12 07:38:20\n",
      "after:  2020-03-12 07:58:21\n",
      "segment_2\n",
      "before:  2020-03-12 09:08:21\n",
      "after:  2020-03-12 09:23:20\n",
      "segment_2\n",
      "before:  2020-03-12 09:53:20\n",
      "after:  2020-03-12 10:03:20\n",
      "segment_2\n",
      "before:  2020-03-12 11:33:19\n",
      "after:  2020-03-12 11:48:20\n",
      "segment_2\n",
      "before:  2020-03-12 12:58:20\n",
      "after:  2020-03-12 13:08:20\n",
      "segment_2\n",
      "before:  2020-03-13 09:53:24\n",
      "after:  2020-03-13 10:03:24\n",
      "segment_2\n",
      "before:  2020-03-13 10:08:24\n",
      "after:  2020-03-13 10:18:24\n",
      "segment_2\n",
      "before:  2020-03-13 11:08:25\n",
      "after:  2020-03-13 11:43:24\n",
      "segment_2\n",
      "before:  2020-03-13 15:28:24\n",
      "after:  2020-03-13 15:38:24\n",
      "segment_2\n",
      "before:  2020-03-13 15:38:24\n",
      "after:  2020-03-13 15:48:23\n",
      "segment_2\n",
      "before:  2020-03-13 16:03:24\n",
      "after:  2020-03-13 16:18:24\n",
      "segment_3\n",
      "before:  2020-03-14 02:58:26\n",
      "after:  2020-03-14 03:08:27\n",
      "segment_3\n",
      "before:  2020-03-14 03:23:27\n",
      "after:  2020-03-14 03:58:26\n",
      "segment_3\n",
      "before:  2020-03-14 05:13:27\n",
      "after:  2020-03-14 05:28:26\n",
      "segment_3\n",
      "before:  2020-03-14 06:28:27\n",
      "after:  2020-03-14 06:38:27\n",
      "segment_3\n",
      "before:  2020-03-14 08:18:26\n",
      "after:  2020-03-14 08:33:27\n",
      "segment_3\n",
      "before:  2020-03-14 11:13:26\n",
      "after:  2020-03-14 11:23:27\n",
      "segment_3\n",
      "before:  2020-03-14 11:33:27\n",
      "after:  2020-03-14 11:48:27\n",
      "segment_3\n",
      "before:  2020-03-14 14:28:28\n",
      "after:  2020-03-14 14:38:28\n",
      "segment_3\n",
      "before:  2020-03-14 15:53:28\n",
      "after:  2020-03-14 16:03:28\n",
      "segment_3\n",
      "before:  2020-03-14 16:38:28\n",
      "after:  2020-03-14 16:53:28\n",
      "segment_3\n",
      "before:  2020-03-15 07:13:29\n",
      "after:  2020-03-15 07:23:29\n",
      "segment_3\n",
      "before:  2020-03-15 12:43:31\n",
      "after:  2020-03-15 12:53:30\n",
      "segment_3\n",
      "before:  2020-03-15 13:33:30\n",
      "after:  2020-03-15 13:43:31\n",
      "segment_3\n",
      "before:  2020-03-16 05:58:34\n",
      "after:  2020-03-16 06:23:34\n",
      "segment_3\n",
      "before:  2020-03-16 07:18:34\n",
      "after:  2020-03-16 07:28:33\n",
      "segment_3\n",
      "before:  2020-03-16 08:03:33\n",
      "after:  2020-03-16 08:13:32\n",
      "segment_3\n",
      "before:  2020-03-16 11:58:34\n",
      "after:  2020-03-16 12:08:33\n",
      "segment_3\n",
      "before:  2020-03-16 12:18:33\n",
      "after:  2020-03-16 12:28:33\n",
      "segment_3\n",
      "before:  2020-03-16 13:58:33\n",
      "after:  2020-03-16 14:13:34\n",
      "segment_3\n",
      "before:  2020-03-16 15:08:34\n",
      "after:  2020-03-16 15:23:34\n",
      "segment_3\n",
      "before:  2020-03-17 07:08:37\n",
      "after:  2020-03-17 07:23:37\n",
      "segment_3\n",
      "before:  2020-03-17 09:53:38\n",
      "after:  2020-03-17 10:08:37\n",
      "segment_3\n",
      "before:  2020-03-17 14:08:39\n",
      "after:  2020-03-17 14:28:38\n",
      "segment_3\n",
      "before:  2020-03-17 17:28:38\n",
      "after:  2020-03-17 17:43:38\n",
      "segment_3\n",
      "before:  2020-03-18 03:03:39\n",
      "after:  2020-03-18 03:28:39\n",
      "segment_3\n",
      "before:  2020-03-18 04:53:40\n",
      "after:  2020-03-18 05:08:39\n",
      "segment_3\n",
      "before:  2020-03-18 05:28:40\n",
      "after:  2020-03-18 05:38:40\n",
      "segment_3\n",
      "before:  2020-03-18 06:28:40\n",
      "after:  2020-03-18 06:38:40\n",
      "segment_3\n",
      "before:  2020-03-18 06:43:40\n",
      "after:  2020-03-18 06:58:40\n",
      "segment_3\n",
      "before:  2020-03-18 09:48:40\n",
      "after:  2020-03-18 09:58:40\n",
      "segment_3\n",
      "before:  2020-03-18 10:43:40\n",
      "after:  2020-03-18 10:53:40\n",
      "segment_3\n",
      "before:  2020-03-18 14:03:41\n",
      "after:  2020-03-18 14:13:41\n",
      "segment_3\n",
      "before:  2020-03-18 17:58:42\n",
      "after:  2020-03-18 18:13:42\n",
      "segment_3\n",
      "before:  2020-03-18 18:13:42\n",
      "after:  2020-03-18 18:23:42\n",
      "segment_3\n",
      "before:  2020-03-19 04:28:43\n",
      "after:  2020-03-19 04:43:43\n",
      "segment_3\n",
      "before:  2020-03-19 04:58:43\n",
      "after:  2020-03-19 05:08:43\n",
      "segment_3\n",
      "before:  2020-03-19 07:58:43\n",
      "after:  2020-03-19 08:13:43\n",
      "segment_3\n",
      "before:  2020-03-19 12:03:43\n",
      "after:  2020-03-19 12:23:44\n",
      "segment_3\n",
      "before:  2020-03-19 14:18:44\n",
      "after:  2020-03-19 14:28:44\n",
      "segment_3\n",
      "before:  2020-03-19 15:38:44\n",
      "after:  2020-03-19 15:53:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_4\n",
      "before:  2020-03-20 03:03:46\n",
      "after:  2020-03-20 03:23:45\n",
      "segment_4\n",
      "before:  2020-03-20 06:53:46\n",
      "after:  2020-03-20 07:33:46\n",
      "segment_4\n",
      "before:  2020-03-20 08:48:46\n",
      "after:  2020-03-20 08:58:46\n",
      "segment_4\n",
      "before:  2020-03-20 16:33:49\n",
      "after:  2020-03-20 16:48:48\n",
      "segment_4\n",
      "before:  2020-03-21 03:48:49\n",
      "after:  2020-03-21 03:58:49\n",
      "segment_4\n",
      "before:  2020-03-21 04:08:49\n",
      "after:  2020-03-21 04:18:49\n",
      "segment_4\n",
      "before:  2020-03-21 04:33:51\n",
      "after:  2020-03-21 04:43:51\n",
      "segment_4\n",
      "before:  2020-03-21 06:48:51\n",
      "after:  2020-03-21 06:58:51\n",
      "segment_4\n",
      "before:  2020-03-21 07:08:50\n",
      "after:  2020-03-21 07:18:50\n",
      "segment_4\n",
      "before:  2020-03-21 08:03:50\n",
      "after:  2020-03-21 08:13:49\n",
      "segment_4\n",
      "before:  2020-03-21 08:43:50\n",
      "after:  2020-03-21 08:58:50\n",
      "segment_4\n",
      "before:  2020-03-21 09:18:50\n",
      "after:  2020-03-21 09:28:50\n",
      "segment_4\n",
      "before:  2020-03-21 10:18:50\n",
      "after:  2020-03-21 10:28:50\n",
      "segment_4\n",
      "before:  2020-03-21 12:18:51\n",
      "after:  2020-03-21 12:28:51\n",
      "segment_5\n",
      "before:  2020-03-21 18:03:52\n",
      "after:  2020-03-21 18:13:52\n",
      "segment_6\n",
      "before:  2020-03-21 22:13:52\n",
      "after:  2020-03-21 22:23:53\n",
      "segment_6\n",
      "before:  2020-03-22 08:33:55\n",
      "after:  2020-03-22 08:43:55\n",
      "segment_6\n",
      "before:  2020-03-22 08:48:54\n",
      "after:  2020-03-22 09:03:55\n",
      "segment_6\n",
      "before:  2020-03-22 09:08:55\n",
      "after:  2020-03-22 09:18:55\n",
      "segment_6\n",
      "before:  2020-03-22 09:43:55\n",
      "after:  2020-03-22 09:58:54\n",
      "segment_6\n",
      "before:  2020-03-22 10:13:55\n",
      "after:  2020-03-22 10:28:55\n",
      "segment_6\n",
      "before:  2020-03-22 10:38:55\n",
      "after:  2020-03-22 10:48:54\n",
      "segment_6\n",
      "before:  2020-03-22 14:58:55\n",
      "after:  2020-03-22 15:08:56\n",
      "segment_6\n",
      "before:  2020-03-22 17:48:55\n",
      "after:  2020-03-22 17:58:55\n",
      "segment_6\n",
      "before:  2020-03-22 18:38:55\n",
      "after:  2020-03-22 18:48:56\n",
      "segment_6\n",
      "before:  2020-03-23 05:13:56\n",
      "after:  2020-03-23 05:43:57\n",
      "segment_6\n",
      "before:  2020-03-23 07:33:57\n",
      "after:  2020-03-23 07:43:58\n",
      "segment_6\n",
      "before:  2020-03-23 08:43:58\n",
      "after:  2020-03-23 08:58:58\n",
      "segment_6\n",
      "before:  2020-03-23 09:13:58\n",
      "after:  2020-03-23 09:23:58\n",
      "segment_6\n",
      "before:  2020-03-23 10:48:59\n",
      "after:  2020-03-23 11:03:58\n",
      "segment_6\n",
      "before:  2020-03-23 11:38:58\n",
      "after:  2020-03-23 11:48:59\n",
      "segment_6\n",
      "before:  2020-03-23 13:53:57\n",
      "after:  2020-03-23 14:03:58\n",
      "segment_6\n",
      "before:  2020-03-23 15:38:58\n",
      "after:  2020-03-23 15:48:58\n",
      "segment_6\n",
      "before:  2020-03-23 15:58:58\n",
      "after:  2020-03-23 16:13:58\n",
      "segment_6\n",
      "before:  2020-03-23 16:58:58\n",
      "after:  2020-03-23 17:08:59\n",
      "segment_6\n",
      "before:  2020-03-23 18:43:58\n",
      "after:  2020-03-23 18:53:59\n",
      "segment_6\n",
      "before:  2020-03-23 22:44:00\n",
      "after:  2020-03-23 22:53:59\n",
      "segment_6\n",
      "before:  2020-03-24 04:44:01\n",
      "after:  2020-03-24 04:54:01\n",
      "segment_6\n",
      "before:  2020-03-24 05:59:01\n",
      "after:  2020-03-24 06:09:02\n",
      "segment_6\n",
      "before:  2020-03-24 12:54:02\n",
      "after:  2020-03-24 13:29:02\n",
      "segment_6\n",
      "before:  2020-03-24 14:49:02\n",
      "after:  2020-03-24 15:04:02\n",
      "segment_6\n",
      "before:  2020-03-24 17:04:03\n",
      "after:  2020-03-24 17:14:01\n",
      "segment_6\n",
      "before:  2020-03-24 17:54:02\n",
      "after:  2020-03-24 18:04:02\n",
      "segment_6\n",
      "before:  2020-03-25 04:19:03\n",
      "after:  2020-03-25 04:29:03\n",
      "segment_6\n",
      "before:  2020-03-25 06:29:04\n",
      "after:  2020-03-25 06:39:05\n",
      "segment_6\n",
      "before:  2020-03-25 06:54:05\n",
      "after:  2020-03-25 07:04:04\n",
      "segment_6\n",
      "before:  2020-03-25 07:34:05\n",
      "after:  2020-03-25 07:44:04\n",
      "segment_6\n",
      "before:  2020-03-25 07:54:04\n",
      "after:  2020-03-25 08:09:04\n",
      "segment_6\n",
      "before:  2020-03-25 08:19:03\n",
      "after:  2020-03-25 08:29:04\n",
      "segment_6\n",
      "before:  2020-03-25 08:29:04\n",
      "after:  2020-03-25 08:44:03\n",
      "segment_6\n",
      "before:  2020-03-25 14:39:05\n",
      "after:  2020-03-25 14:49:05\n",
      "segment_6\n",
      "before:  2020-03-25 15:14:05\n",
      "after:  2020-03-25 15:29:05\n",
      "segment_6\n",
      "before:  2020-03-25 15:39:05\n",
      "after:  2020-03-25 15:49:05\n",
      "segment_6\n",
      "before:  2020-03-25 15:59:05\n",
      "after:  2020-03-25 16:09:05\n",
      "segment_6\n",
      "before:  2020-03-25 17:04:05\n",
      "after:  2020-03-25 17:14:06\n",
      "segment_6\n",
      "before:  2020-03-25 18:34:06\n",
      "after:  2020-03-25 18:44:06\n",
      "segment_6\n",
      "before:  2020-03-25 20:04:07\n",
      "after:  2020-03-25 20:14:06\n",
      "segment_6\n",
      "before:  2020-03-26 03:04:08\n",
      "after:  2020-03-26 03:14:08\n",
      "segment_6\n",
      "before:  2020-03-26 03:34:07\n",
      "after:  2020-03-26 03:59:07\n",
      "segment_6\n",
      "before:  2020-03-26 04:24:08\n",
      "after:  2020-03-26 04:34:08\n",
      "segment_6\n",
      "before:  2020-03-26 08:54:07\n",
      "after:  2020-03-26 09:04:08\n",
      "segment_6\n",
      "before:  2020-03-26 09:04:08\n",
      "after:  2020-03-26 09:14:07\n",
      "segment_6\n",
      "before:  2020-03-26 09:39:08\n",
      "after:  2020-03-26 09:54:08\n",
      "segment_6\n",
      "before:  2020-03-26 10:04:07\n",
      "after:  2020-03-26 10:19:08\n",
      "segment_6\n",
      "before:  2020-03-26 12:54:08\n",
      "after:  2020-03-26 13:04:08\n",
      "segment_6\n",
      "before:  2020-03-26 13:14:08\n",
      "after:  2020-03-26 13:39:08\n",
      "segment_6\n",
      "before:  2020-03-26 13:49:08\n",
      "after:  2020-03-26 13:59:08\n",
      "segment_6\n",
      "before:  2020-03-26 16:19:09\n",
      "after:  2020-03-26 16:29:09\n",
      "segment_6\n",
      "before:  2020-03-26 18:19:09\n",
      "after:  2020-03-26 18:34:09\n",
      "segment_6\n",
      "before:  2020-03-27 04:14:11\n",
      "after:  2020-03-27 04:24:11\n",
      "segment_6\n",
      "before:  2020-03-27 05:44:10\n",
      "after:  2020-03-27 05:54:10\n",
      "segment_6\n",
      "before:  2020-03-27 06:44:10\n",
      "after:  2020-03-27 06:54:10\n",
      "segment_6\n",
      "before:  2020-03-27 07:29:10\n",
      "after:  2020-03-27 07:39:11\n",
      "segment_6\n",
      "before:  2020-03-27 10:09:11\n",
      "after:  2020-03-27 10:24:10\n",
      "segment_6\n",
      "before:  2020-03-27 10:39:11\n",
      "after:  2020-03-27 10:49:11\n",
      "segment_6\n",
      "before:  2020-03-27 11:19:11\n",
      "after:  2020-03-27 11:44:12\n",
      "segment_6\n",
      "before:  2020-03-27 13:19:12\n",
      "after:  2020-03-27 13:29:12\n",
      "segment_6\n",
      "before:  2020-03-27 13:54:11\n",
      "after:  2020-03-27 14:19:11\n",
      "segment_6\n",
      "before:  2020-03-27 15:39:12\n",
      "after:  2020-03-27 15:49:12\n",
      "segment_6\n",
      "before:  2020-03-27 17:29:12\n",
      "after:  2020-03-27 17:39:12\n",
      "segment_6\n",
      "before:  2020-03-28 07:59:13\n",
      "after:  2020-03-28 08:09:14\n",
      "segment_6\n",
      "before:  2020-03-28 13:24:16\n",
      "after:  2020-03-28 13:39:16\n",
      "segment_6\n",
      "before:  2020-03-28 22:44:16\n",
      "after:  2020-03-28 22:54:16\n",
      "segment_6\n",
      "before:  2020-03-29 04:24:18\n",
      "after:  2020-03-29 04:34:17\n",
      "segment_6\n",
      "before:  2020-03-29 04:39:18\n",
      "after:  2020-03-29 04:54:17\n",
      "segment_6\n",
      "before:  2020-03-29 10:49:18\n",
      "after:  2020-03-29 10:59:19\n",
      "segment_6\n",
      "before:  2020-03-29 13:34:19\n",
      "after:  2020-03-29 13:44:20\n",
      "segment_6\n",
      "before:  2020-03-29 14:09:20\n",
      "after:  2020-03-29 14:19:18\n",
      "segment_6\n",
      "before:  2020-03-29 16:14:19\n",
      "after:  2020-03-29 16:24:18\n",
      "segment_6\n",
      "before:  2020-03-29 17:49:19\n",
      "after:  2020-03-29 17:59:19\n",
      "segment_6\n",
      "before:  2020-03-30 05:04:21\n",
      "after:  2020-03-30 05:34:21\n",
      "segment_6\n",
      "before:  2020-03-30 07:29:21\n",
      "after:  2020-03-30 07:39:21\n",
      "segment_6\n",
      "before:  2020-03-30 07:44:22\n",
      "after:  2020-03-30 07:54:22\n",
      "segment_6\n",
      "before:  2020-03-30 07:59:22\n",
      "after:  2020-03-30 08:44:22\n",
      "segment_6\n",
      "before:  2020-03-30 12:24:22\n",
      "after:  2020-03-30 12:34:22\n",
      "segment_6\n",
      "before:  2020-03-30 13:29:23\n",
      "after:  2020-03-30 13:39:23\n",
      "segment_6\n",
      "before:  2020-03-30 16:14:23\n",
      "after:  2020-03-30 16:24:23\n",
      "segment_6\n",
      "before:  2020-03-31 02:49:23\n",
      "after:  2020-03-31 02:59:24\n",
      "segment_6\n",
      "before:  2020-03-31 03:14:23\n",
      "after:  2020-03-31 03:24:24\n",
      "segment_6\n",
      "before:  2020-03-31 08:49:24\n",
      "after:  2020-03-31 08:59:24\n",
      "segment_7\n",
      "before:  2020-03-31 14:14:26\n",
      "after:  2020-03-31 14:29:26\n",
      "segment_7\n",
      "before:  2020-03-31 15:59:27\n",
      "after:  2020-03-31 16:29:27\n",
      "segment_8\n",
      "before:  2020-04-01 02:54:28\n",
      "after:  2020-04-01 03:14:28\n",
      "segment_8\n",
      "before:  2020-04-01 05:14:29\n",
      "after:  2020-04-01 05:29:29\n",
      "segment_8\n",
      "before:  2020-04-01 07:34:28\n",
      "after:  2020-04-01 07:44:28\n",
      "segment_8\n",
      "before:  2020-04-01 08:34:29\n",
      "after:  2020-04-01 08:44:29\n",
      "segment_8\n",
      "before:  2020-04-01 12:29:29\n",
      "after:  2020-04-01 12:54:30\n",
      "segment_8\n",
      "before:  2020-04-01 14:04:29\n",
      "after:  2020-04-01 14:14:30\n",
      "segment_8\n",
      "before:  2020-04-01 17:34:30\n",
      "after:  2020-04-01 18:04:30\n",
      "segment_8\n",
      "before:  2020-04-01 18:09:30\n",
      "after:  2020-04-01 18:24:31\n",
      "segment_8\n",
      "before:  2020-04-02 06:19:30\n",
      "after:  2020-04-02 06:34:31\n",
      "segment_8\n",
      "before:  2020-04-02 14:34:32\n",
      "after:  2020-04-02 14:59:32\n",
      "segment_8\n",
      "before:  2020-04-02 16:34:33\n",
      "after:  2020-04-02 17:04:32\n",
      "segment_8\n",
      "before:  2020-04-03 02:54:34\n",
      "after:  2020-04-03 03:09:34\n",
      "segment_8\n",
      "before:  2020-04-03 05:19:34\n",
      "after:  2020-04-03 05:34:34\n",
      "segment_8\n",
      "before:  2020-04-03 06:29:34\n",
      "after:  2020-04-03 06:39:35\n",
      "segment_8\n",
      "before:  2020-04-03 06:49:34\n",
      "after:  2020-04-03 06:59:34\n",
      "segment_8\n",
      "before:  2020-04-03 07:54:34\n",
      "after:  2020-04-03 08:04:34\n",
      "segment_8\n",
      "before:  2020-04-03 10:39:35\n",
      "after:  2020-04-03 10:49:35\n",
      "segment_8\n",
      "before:  2020-04-03 11:39:35\n",
      "after:  2020-04-03 11:54:35\n",
      "segment_8\n",
      "before:  2020-04-03 12:04:35\n",
      "after:  2020-04-03 12:19:36\n",
      "segment_8\n",
      "before:  2020-04-03 12:19:36\n",
      "after:  2020-04-03 12:39:36\n",
      "segment_8\n",
      "before:  2020-04-04 06:29:39\n",
      "after:  2020-04-04 06:39:38\n",
      "segment_8\n",
      "before:  2020-04-04 08:44:39\n",
      "after:  2020-04-04 08:54:39\n",
      "segment_8\n",
      "before:  2020-04-04 08:54:39\n",
      "after:  2020-04-04 09:04:39\n",
      "segment_8\n",
      "before:  2020-04-04 11:54:40\n",
      "after:  2020-04-04 12:14:40\n",
      "segment_8\n",
      "before:  2020-04-04 14:14:40\n",
      "after:  2020-04-04 14:29:40\n",
      "segment_8\n",
      "before:  2020-04-04 20:49:40\n",
      "after:  2020-04-04 20:59:40\n",
      "segment_8\n",
      "before:  2020-04-04 21:04:40\n",
      "after:  2020-04-04 21:14:41\n",
      "segment_8\n",
      "before:  2020-04-05 07:59:42\n",
      "after:  2020-04-05 08:09:41\n",
      "segment_8\n",
      "before:  2020-04-05 10:24:42\n",
      "after:  2020-04-05 10:39:42\n",
      "segment_8\n",
      "before:  2020-04-05 12:34:43\n",
      "after:  2020-04-05 12:44:42\n",
      "segment_8\n",
      "before:  2020-04-05 17:29:43\n",
      "after:  2020-04-05 17:39:43\n",
      "segment_8\n",
      "before:  2020-04-05 17:54:43\n",
      "after:  2020-04-05 18:04:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  747\n",
      "len of segment_df is  603\n",
      "len of segment_df is  1691\n",
      "len of segment_df is  502\n",
      "len of segment_df is  17\n",
      "len of segment_df is  2773\n",
      "len of segment_df is  70\n",
      "len of segment_df is  1477\n",
      "len of features_list 7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 505.29354934692384\n",
      "Epoch 2, Validation Loss: 519.4673739624023\n",
      "Epoch 3, Validation Loss: 510.7467582702637\n",
      "Epoch 4, Validation Loss: 500.13732406616214\n",
      "Epoch 5, Validation Loss: 488.1624168395996\n",
      "Epoch 6, Validation Loss: 479.53656494140625\n",
      "Epoch 7, Validation Loss: 465.49220809936526\n",
      "Epoch 8, Validation Loss: 451.1173889160156\n",
      "Epoch 9, Validation Loss: 439.3251089477539\n",
      "Epoch 10, Validation Loss: 426.82486099243164\n",
      "Epoch 11, Validation Loss: 417.2453295898438\n",
      "Epoch 12, Validation Loss: 411.4429167175293\n",
      "Epoch 13, Validation Loss: 407.75968048095706\n",
      "Epoch 14, Validation Loss: 406.46437240600585\n",
      "Epoch 15, Validation Loss: 406.97811080932615\n",
      "Epoch 16, Validation Loss: 408.7678033447266\n",
      "Epoch 17, Validation Loss: 411.0380961608887\n",
      "Epoch 18, Validation Loss: 414.06385055541995\n",
      "Epoch 19, Validation Loss: 416.27627578735354\n",
      "Epoch 20, Validation Loss: 419.38617736816406\n",
      "Epoch 21, Validation Loss: 422.2899914550781\n",
      "Epoch 22, Validation Loss: 422.40394149780275\n",
      "Epoch 23, Validation Loss: 425.5133087158203\n",
      "Epoch 24, Validation Loss: 426.31946533203126\n",
      "Epoch 25, Validation Loss: 425.21194610595705\n",
      "Epoch 26, Validation Loss: 428.9661097717285\n",
      "Epoch 27, Validation Loss: 426.87264526367187\n",
      "Epoch 28, Validation Loss: 430.9664533996582\n",
      "Epoch 29, Validation Loss: 425.8773695373535\n",
      "Epoch 30, Validation Loss: 427.7348315429688\n",
      "Epoch 31, Validation Loss: 428.03988418579104\n",
      "Epoch 32, Validation Loss: 429.9902757263184\n",
      "Epoch 33, Validation Loss: 429.780654296875\n",
      "Epoch 34, Validation Loss: 430.22284729003906\n",
      "Epoch 35, Validation Loss: 430.623876953125\n",
      "Epoch 36, Validation Loss: 430.7352752685547\n",
      "Epoch 37, Validation Loss: 431.59020843505857\n",
      "Epoch 38, Validation Loss: 427.0827886962891\n",
      "Epoch 39, Validation Loss: 430.00442459106443\n",
      "Epoch 40, Validation Loss: 431.0652996826172\n",
      "Epoch 41, Validation Loss: 431.79320251464844\n",
      "Epoch 42, Validation Loss: 428.550855255127\n",
      "Epoch 43, Validation Loss: 432.85019912719724\n",
      "Epoch 44, Validation Loss: 428.5071183776856\n",
      "Epoch 45, Validation Loss: 430.65043579101564\n",
      "Epoch 46, Validation Loss: 430.23574569702146\n",
      "Epoch 47, Validation Loss: 430.1880464172363\n",
      "Epoch 48, Validation Loss: 432.10387313842773\n",
      "Epoch 49, Validation Loss: 430.1643388366699\n",
      "Epoch 50, Validation Loss: 429.2968226623535\n",
      "Epoch 51, Validation Loss: 431.30263809204104\n",
      "Epoch 52, Validation Loss: 429.3497671508789\n",
      "Epoch 53, Validation Loss: 425.22985458374023\n",
      "Epoch 54, Validation Loss: 427.50749801635743\n",
      "Epoch 55, Validation Loss: 427.15543060302736\n",
      "Epoch 56, Validation Loss: 429.4631748962402\n",
      "Epoch 57, Validation Loss: 427.8041961669922\n",
      "Epoch 58, Validation Loss: 426.51123275756834\n",
      "Epoch 59, Validation Loss: 421.1922845458984\n",
      "Epoch 60, Validation Loss: 425.83962280273437\n",
      "Epoch 61, Validation Loss: 425.04421920776366\n",
      "Epoch 62, Validation Loss: 425.05417739868165\n",
      "Epoch 63, Validation Loss: 428.78568176269533\n",
      "Epoch 64, Validation Loss: 424.0433473205566\n",
      "Epoch 65, Validation Loss: 424.10317031860353\n",
      "Epoch 66, Validation Loss: 428.13752716064454\n",
      "Epoch 67, Validation Loss: 421.8850843811035\n",
      "Epoch 68, Validation Loss: 421.7895640563965\n",
      "Epoch 69, Validation Loss: 424.1310174560547\n",
      "Epoch 70, Validation Loss: 426.3664880371094\n",
      "Epoch 71, Validation Loss: 419.0927774047852\n",
      "Epoch 72, Validation Loss: 419.51998138427734\n",
      "Epoch 73, Validation Loss: 417.09891235351563\n",
      "Epoch 74, Validation Loss: 417.1806524658203\n",
      "Epoch 75, Validation Loss: 422.4721495056152\n",
      "Epoch 76, Validation Loss: 416.88780487060546\n",
      "Epoch 77, Validation Loss: 416.09827865600585\n",
      "Epoch 78, Validation Loss: 422.171335144043\n",
      "Epoch 79, Validation Loss: 415.31445465087893\n",
      "Epoch 80, Validation Loss: 420.4672737121582\n",
      "Epoch 81, Validation Loss: 418.49961547851564\n",
      "Epoch 82, Validation Loss: 410.91688385009763\n",
      "Epoch 83, Validation Loss: 407.3045495605469\n",
      "Epoch 84, Validation Loss: 408.5859353637695\n",
      "Epoch 85, Validation Loss: 414.14293212890624\n",
      "Epoch 86, Validation Loss: 408.5537576293945\n",
      "Epoch 87, Validation Loss: 408.8840621948242\n",
      "Epoch 88, Validation Loss: 408.6261994934082\n",
      "Epoch 89, Validation Loss: 418.98816650390626\n",
      "Epoch 90, Validation Loss: 421.90977218627927\n",
      "Epoch 91, Validation Loss: 413.1329570007324\n",
      "Epoch 92, Validation Loss: 410.8687577819824\n",
      "Epoch 93, Validation Loss: 412.0126985168457\n",
      "Epoch 94, Validation Loss: 403.0328842163086\n",
      "Epoch 95, Validation Loss: 403.88715118408203\n",
      "Epoch 96, Validation Loss: 406.67545761108397\n",
      "Epoch 97, Validation Loss: 407.40961624145507\n",
      "Epoch 98, Validation Loss: 402.36834106445315\n",
      "Epoch 99, Validation Loss: 403.1444448852539\n",
      "Epoch 100, Validation Loss: 407.15100296020506\n",
      "Epoch 101, Validation Loss: 405.3477131652832\n",
      "Epoch 102, Validation Loss: 401.2216311645508\n",
      "Epoch 103, Validation Loss: 400.7599238586426\n",
      "Epoch 104, Validation Loss: 402.71855560302737\n",
      "Epoch 105, Validation Loss: 415.312507019043\n",
      "Epoch 106, Validation Loss: 411.4380862426758\n",
      "Epoch 107, Validation Loss: 402.2360302734375\n",
      "Epoch 108, Validation Loss: 399.84459411621094\n",
      "Epoch 109, Validation Loss: 396.3201129150391\n",
      "Epoch 110, Validation Loss: 398.83992767333984\n",
      "Epoch 111, Validation Loss: 397.65525466918945\n",
      "Epoch 112, Validation Loss: 404.48028381347655\n",
      "Epoch 113, Validation Loss: 403.2820652770996\n",
      "Epoch 114, Validation Loss: 398.9305749511719\n",
      "Epoch 115, Validation Loss: 397.4727069091797\n",
      "Epoch 116, Validation Loss: 402.30723449707034\n",
      "Epoch 117, Validation Loss: 397.5130213928223\n",
      "Epoch 118, Validation Loss: 403.0896647644043\n",
      "Epoch 119, Validation Loss: 421.4723370361328\n",
      "Epoch 120, Validation Loss: 409.58632583618163\n",
      "Epoch 121, Validation Loss: 410.140793762207\n",
      "Epoch 122, Validation Loss: 406.54207168579103\n",
      "Epoch 123, Validation Loss: 405.9908337402344\n",
      "Epoch 124, Validation Loss: 413.25579788208006\n",
      "Epoch 125, Validation Loss: 410.01146865844726\n",
      "Epoch 126, Validation Loss: 425.54789367675784\n",
      "Epoch 127, Validation Loss: 411.1337173461914\n",
      "Epoch 128, Validation Loss: 400.83225708007814\n",
      "Epoch 129, Validation Loss: 411.9846635437012\n",
      "Epoch 130, Validation Loss: 405.34536712646485\n",
      "Epoch 131, Validation Loss: 404.4720927429199\n",
      "Epoch 132, Validation Loss: 399.6279609680176\n",
      "Epoch 133, Validation Loss: 401.21678253173826\n",
      "Epoch 134, Validation Loss: 398.53069580078125\n",
      "Epoch 135, Validation Loss: 397.9101403808594\n",
      "Epoch 136, Validation Loss: 397.20416885375977\n",
      "Epoch 137, Validation Loss: 398.15761108398436\n",
      "Epoch 138, Validation Loss: 397.0960455322266\n",
      "Epoch 139, Validation Loss: 396.4518664550781\n",
      "Epoch 140, Validation Loss: 400.5665455627441\n",
      "Epoch 141, Validation Loss: 395.75533172607425\n",
      "Epoch 142, Validation Loss: 403.78767196655275\n",
      "Epoch 143, Validation Loss: 396.8737303161621\n",
      "Epoch 144, Validation Loss: 396.80786361694334\n",
      "Epoch 145, Validation Loss: 399.8926133728027\n",
      "Epoch 146, Validation Loss: 401.24395431518553\n",
      "Epoch 147, Validation Loss: 408.8926516723633\n",
      "Epoch 148, Validation Loss: 402.4603091430664\n",
      "Epoch 149, Validation Loss: 409.58002456665037\n",
      "Epoch 150, Validation Loss: 407.87665725708007\n",
      "Epoch 151, Validation Loss: 401.68253143310545\n",
      "Epoch 152, Validation Loss: 399.9242948913574\n",
      "Epoch 153, Validation Loss: 402.2194721984863\n",
      "Epoch 154, Validation Loss: 402.89475341796873\n",
      "Epoch 155, Validation Loss: 402.9182150268555\n",
      "Epoch 156, Validation Loss: 401.4616815185547\n",
      "Epoch 157, Validation Loss: 393.5705271911621\n",
      "Epoch 158, Validation Loss: 393.98785919189453\n",
      "Epoch 159, Validation Loss: 393.2405706787109\n",
      "Epoch 160, Validation Loss: 393.45074951171875\n",
      "Epoch 161, Validation Loss: 390.46860137939456\n",
      "Epoch 162, Validation Loss: 395.1503253173828\n",
      "Epoch 163, Validation Loss: 392.85399398803713\n",
      "Epoch 164, Validation Loss: 396.328642578125\n",
      "Epoch 165, Validation Loss: 395.55837966918943\n",
      "Epoch 166, Validation Loss: 401.4525088500977\n",
      "Epoch 167, Validation Loss: 395.22893188476564\n",
      "Epoch 168, Validation Loss: 399.8721235656738\n",
      "Epoch 169, Validation Loss: 401.81314666748045\n",
      "Epoch 170, Validation Loss: 397.14181564331057\n",
      "Epoch 171, Validation Loss: 403.95920639038087\n",
      "Epoch 172, Validation Loss: 399.96595581054686\n",
      "Epoch 173, Validation Loss: 395.4701898193359\n",
      "Epoch 174, Validation Loss: 399.232624206543\n",
      "Epoch 175, Validation Loss: 397.8499601745605\n",
      "Epoch 176, Validation Loss: 398.84811569213866\n",
      "Epoch 177, Validation Loss: 399.4489013671875\n",
      "Epoch 178, Validation Loss: 398.70641494750976\n",
      "Epoch 179, Validation Loss: 400.8093099975586\n",
      "Epoch 180, Validation Loss: 399.2178239440918\n",
      "Epoch 181, Validation Loss: 399.3393101501465\n",
      "Epoch 182, Validation Loss: 400.5704476928711\n",
      "Epoch 183, Validation Loss: 400.8430793762207\n",
      "Epoch 184, Validation Loss: 402.18772583007814\n",
      "Epoch 185, Validation Loss: 404.54161911010743\n",
      "Epoch 186, Validation Loss: 406.5458485412598\n",
      "Epoch 187, Validation Loss: 405.3325321960449\n",
      "Epoch 188, Validation Loss: 410.8993510437012\n",
      "Epoch 189, Validation Loss: 412.89492279052735\n",
      "Epoch 190, Validation Loss: 417.9421844482422\n",
      "Epoch 191, Validation Loss: 415.5782536315918\n",
      "Epoch 192, Validation Loss: 425.56725997924804\n",
      "Epoch 193, Validation Loss: 416.31925872802736\n",
      "Epoch 194, Validation Loss: 413.1597305297852\n",
      "Epoch 195, Validation Loss: 418.8356620788574\n",
      "Epoch 196, Validation Loss: 411.5764663696289\n",
      "Epoch 197, Validation Loss: 412.18868698120116\n",
      "Epoch 198, Validation Loss: 409.9146151733398\n",
      "Epoch 199, Validation Loss: 413.7374752807617\n",
      "Epoch 200, Validation Loss: 410.39907791137693\n",
      "RMSE on test set: 20.304546356201172\n",
      "segment_4\n",
      "before:  2020-10-04 21:59:20\n",
      "after:  2020-10-04 22:49:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '5.4' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4.2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.1' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  1440\n",
      "len of segment_df is  1728\n",
      "len of segment_df is  1152\n",
      "len of segment_df is  0\n",
      "len of features_list 4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 3828.728690011161\n",
      "Epoch 2, Validation Loss: 3778.401026044573\n",
      "Epoch 3, Validation Loss: 3943.2431989397323\n",
      "Epoch 4, Validation Loss: 3983.91493334089\n",
      "Epoch 5, Validation Loss: 3935.8121381487167\n",
      "Epoch 6, Validation Loss: 4030.717542375837\n",
      "Epoch 7, Validation Loss: 3769.2246769496373\n",
      "Epoch 8, Validation Loss: 3616.4455860682897\n",
      "Epoch 9, Validation Loss: 3613.249210902623\n",
      "Epoch 10, Validation Loss: 3385.3441946847097\n",
      "Epoch 11, Validation Loss: 3304.404342651367\n",
      "Epoch 12, Validation Loss: 3251.70828465053\n",
      "Epoch 13, Validation Loss: 3221.8487821306503\n",
      "Epoch 14, Validation Loss: 3231.897907802037\n",
      "Epoch 15, Validation Loss: 3207.2267815726145\n",
      "Epoch 16, Validation Loss: 3190.160378592355\n",
      "Epoch 17, Validation Loss: 3183.8158732822963\n",
      "Epoch 18, Validation Loss: 3149.521170479911\n",
      "Epoch 19, Validation Loss: 3142.0797598702566\n",
      "Epoch 20, Validation Loss: 3089.0728585379466\n",
      "Epoch 21, Validation Loss: 3076.2620784214564\n",
      "Epoch 22, Validation Loss: 3033.830914088658\n",
      "Epoch 23, Validation Loss: 3021.573875427246\n",
      "Epoch 24, Validation Loss: 3010.4601909092494\n",
      "Epoch 25, Validation Loss: 3006.2796478271484\n",
      "Epoch 26, Validation Loss: 2988.817413330078\n",
      "Epoch 27, Validation Loss: 2972.4591151646205\n",
      "Epoch 28, Validation Loss: 2960.169913155692\n",
      "Epoch 29, Validation Loss: 2951.3292127336777\n",
      "Epoch 30, Validation Loss: 2959.4095785958425\n",
      "Epoch 31, Validation Loss: 2943.1378871372767\n",
      "Epoch 32, Validation Loss: 2941.1339743477956\n",
      "Epoch 33, Validation Loss: 2927.3402099609375\n",
      "Epoch 34, Validation Loss: 2915.553378513881\n",
      "Epoch 35, Validation Loss: 2931.5579768589564\n",
      "Epoch 36, Validation Loss: 2953.218575613839\n",
      "Epoch 37, Validation Loss: 2959.5894317626953\n",
      "Epoch 38, Validation Loss: 2992.336199079241\n",
      "Epoch 39, Validation Loss: 3027.510984148298\n",
      "Epoch 40, Validation Loss: 3052.9893253871373\n",
      "Epoch 41, Validation Loss: 3076.109625680106\n",
      "Epoch 42, Validation Loss: 3091.029586791992\n",
      "Epoch 43, Validation Loss: 3112.426797049386\n",
      "Epoch 44, Validation Loss: 3122.359793526786\n",
      "Epoch 45, Validation Loss: 3143.5826416015625\n",
      "Epoch 46, Validation Loss: 3126.678080967494\n",
      "Epoch 47, Validation Loss: 3140.8497227260045\n",
      "Epoch 48, Validation Loss: 3144.9540187290736\n",
      "Epoch 49, Validation Loss: 3208.4365735735214\n",
      "Epoch 50, Validation Loss: 3222.00851222447\n",
      "Epoch 51, Validation Loss: 3188.4248940604075\n",
      "Epoch 52, Validation Loss: 3229.2696555001394\n",
      "Epoch 53, Validation Loss: 3243.385395595006\n",
      "Epoch 54, Validation Loss: 3214.569327218192\n",
      "Epoch 55, Validation Loss: 3179.7599182128906\n",
      "Epoch 56, Validation Loss: 3237.6002524239675\n",
      "Epoch 57, Validation Loss: 3286.6979435512\n",
      "Epoch 58, Validation Loss: 3281.835438319615\n",
      "Epoch 59, Validation Loss: 3304.025889805385\n",
      "Epoch 60, Validation Loss: 3346.982670375279\n",
      "Epoch 61, Validation Loss: 3346.825393676758\n",
      "Epoch 62, Validation Loss: 3325.194170270647\n",
      "Epoch 63, Validation Loss: 3284.8465467180527\n",
      "Epoch 64, Validation Loss: 3353.792005266462\n",
      "Epoch 65, Validation Loss: 3286.6732134137833\n",
      "Epoch 66, Validation Loss: 3332.679940359933\n",
      "Epoch 67, Validation Loss: 3368.041726248605\n",
      "Epoch 68, Validation Loss: 3412.5821947370255\n",
      "Epoch 69, Validation Loss: 3279.813295636858\n",
      "Epoch 70, Validation Loss: 3369.5960627964564\n",
      "Epoch 71, Validation Loss: 3411.168354579381\n",
      "Epoch 72, Validation Loss: 3382.9835139683314\n",
      "Epoch 73, Validation Loss: 3214.5633653913223\n",
      "Epoch 74, Validation Loss: 3432.3833836146764\n",
      "Epoch 75, Validation Loss: 3301.4508296421595\n",
      "Epoch 76, Validation Loss: 3418.2682495117188\n",
      "Epoch 77, Validation Loss: 3466.139127458845\n",
      "Epoch 78, Validation Loss: 3442.587417602539\n",
      "Epoch 79, Validation Loss: 3415.293234688895\n",
      "Epoch 80, Validation Loss: 3491.5379922049387\n",
      "Epoch 81, Validation Loss: 3488.3935546875\n",
      "Epoch 82, Validation Loss: 3402.775950840541\n",
      "Epoch 83, Validation Loss: 3461.43605695452\n",
      "Epoch 84, Validation Loss: 3444.740271432059\n",
      "Epoch 85, Validation Loss: 3593.8921857561386\n",
      "Epoch 86, Validation Loss: 3388.028485979353\n",
      "Epoch 87, Validation Loss: 3641.5149972098216\n",
      "Epoch 88, Validation Loss: 3396.3014003208705\n",
      "Epoch 89, Validation Loss: 3571.9688110351562\n",
      "Epoch 90, Validation Loss: 3602.7274475097656\n",
      "Epoch 91, Validation Loss: 3486.115487234933\n",
      "Epoch 92, Validation Loss: 3586.009983607701\n",
      "Epoch 93, Validation Loss: 3614.167711530413\n",
      "Epoch 94, Validation Loss: 3692.9022674560547\n",
      "Epoch 95, Validation Loss: 3533.435261317662\n",
      "Epoch 96, Validation Loss: 3492.276879446847\n",
      "Epoch 97, Validation Loss: 3680.223907470703\n",
      "Epoch 98, Validation Loss: 3582.4063611711777\n",
      "Epoch 99, Validation Loss: 3604.8712223597936\n",
      "Epoch 100, Validation Loss: 3525.44675554548\n",
      "Epoch 101, Validation Loss: 3612.5125340053014\n",
      "Epoch 102, Validation Loss: 3604.4012472970144\n",
      "Epoch 103, Validation Loss: 3581.241463797433\n",
      "Epoch 104, Validation Loss: 3503.7087903703964\n",
      "Epoch 105, Validation Loss: 3410.006796700614\n",
      "Epoch 106, Validation Loss: 3655.6371023995534\n",
      "Epoch 107, Validation Loss: 3540.9001988002233\n",
      "Epoch 108, Validation Loss: 3602.9639325823105\n",
      "Epoch 109, Validation Loss: 3607.718403407506\n",
      "Epoch 110, Validation Loss: 3674.979019165039\n",
      "Epoch 111, Validation Loss: 3658.6950007847377\n",
      "Epoch 112, Validation Loss: 3469.0780726841517\n",
      "Epoch 113, Validation Loss: 3625.171129499163\n",
      "Epoch 114, Validation Loss: 3693.522506713867\n",
      "Epoch 115, Validation Loss: 3595.7289079938614\n",
      "Epoch 116, Validation Loss: 3544.9321005684988\n",
      "Epoch 117, Validation Loss: 3635.908170427595\n",
      "Epoch 118, Validation Loss: 3613.3121882847377\n",
      "Epoch 119, Validation Loss: 3606.030744280134\n",
      "Epoch 120, Validation Loss: 3629.703321184431\n",
      "Epoch 121, Validation Loss: 3656.047315325056\n",
      "Epoch 122, Validation Loss: 3702.4366673060827\n",
      "Epoch 123, Validation Loss: 3708.733625139509\n",
      "Epoch 124, Validation Loss: 3614.125741141183\n",
      "Epoch 125, Validation Loss: 3802.757775442941\n",
      "Epoch 126, Validation Loss: 3396.7742309570312\n",
      "Epoch 127, Validation Loss: 3688.6988808768137\n",
      "Epoch 128, Validation Loss: 3797.36451285226\n",
      "Epoch 129, Validation Loss: 3684.7048775809153\n",
      "Epoch 130, Validation Loss: 3603.351146153041\n",
      "Epoch 131, Validation Loss: 3689.6610892159597\n",
      "Epoch 132, Validation Loss: 3328.511500767299\n",
      "Epoch 133, Validation Loss: 3422.3659123012\n",
      "Epoch 134, Validation Loss: 3446.7708195277623\n",
      "Epoch 135, Validation Loss: 3473.6837550571986\n",
      "Epoch 136, Validation Loss: 3441.467014857701\n",
      "Epoch 137, Validation Loss: 3540.5330810546875\n",
      "Epoch 138, Validation Loss: 3595.3360726492747\n",
      "Epoch 139, Validation Loss: 3534.525588989258\n",
      "Epoch 140, Validation Loss: 3486.88631766183\n",
      "Epoch 141, Validation Loss: 3536.5674896240234\n",
      "Epoch 142, Validation Loss: 3505.525639125279\n",
      "Epoch 143, Validation Loss: 3523.453903198242\n",
      "Epoch 144, Validation Loss: 3535.723066057478\n",
      "Epoch 145, Validation Loss: 3537.762538364955\n",
      "Epoch 146, Validation Loss: 3486.357214791434\n",
      "Epoch 147, Validation Loss: 3496.6515590122767\n",
      "Epoch 148, Validation Loss: 3510.585911342076\n",
      "Epoch 149, Validation Loss: 3522.8996080671036\n",
      "Epoch 150, Validation Loss: 3493.1272495814733\n",
      "Epoch 151, Validation Loss: 3519.4647325788223\n",
      "Epoch 152, Validation Loss: 3434.1307482038223\n",
      "Epoch 153, Validation Loss: 3412.4098292759486\n",
      "Epoch 154, Validation Loss: 3503.3265402657644\n",
      "Epoch 155, Validation Loss: 3418.9670933314733\n",
      "Epoch 156, Validation Loss: 3296.860081263951\n",
      "Epoch 157, Validation Loss: 3241.3506818498886\n",
      "Epoch 158, Validation Loss: 3196.3914729527064\n",
      "Epoch 159, Validation Loss: 3391.12746320452\n",
      "Epoch 160, Validation Loss: 3413.991753714425\n",
      "Epoch 161, Validation Loss: 3479.789629255022\n",
      "Epoch 162, Validation Loss: 3465.617847987584\n",
      "Epoch 163, Validation Loss: 3511.6163395472936\n",
      "Epoch 164, Validation Loss: 3450.189937046596\n",
      "Epoch 165, Validation Loss: 3523.632869175502\n",
      "Epoch 166, Validation Loss: 3507.9451490129745\n",
      "Epoch 167, Validation Loss: 3549.366686139788\n",
      "Epoch 168, Validation Loss: 3579.568926130022\n",
      "Epoch 169, Validation Loss: 3524.1153302873886\n",
      "Epoch 170, Validation Loss: 3533.0544716971262\n",
      "Epoch 171, Validation Loss: 3363.97947038923\n",
      "Epoch 172, Validation Loss: 3532.160858154297\n",
      "Epoch 173, Validation Loss: 3388.5121263776505\n",
      "Epoch 174, Validation Loss: 3585.414768763951\n",
      "Epoch 175, Validation Loss: 3410.581614903041\n",
      "Epoch 176, Validation Loss: 3493.7319532121933\n",
      "Epoch 177, Validation Loss: 3415.722161429269\n",
      "Epoch 178, Validation Loss: 3297.553270612444\n",
      "Epoch 179, Validation Loss: 3479.6789986746653\n",
      "Epoch 180, Validation Loss: 3371.5337589808873\n",
      "Epoch 181, Validation Loss: 3439.3320835658483\n",
      "Epoch 182, Validation Loss: 3376.2489057268417\n",
      "Epoch 183, Validation Loss: 3337.1075439453125\n",
      "Epoch 184, Validation Loss: 3460.2654985700333\n",
      "Epoch 185, Validation Loss: 3494.800750732422\n",
      "Epoch 186, Validation Loss: 3439.573808942522\n",
      "Epoch 187, Validation Loss: 3476.845751081194\n",
      "Epoch 188, Validation Loss: 3472.503339494978\n",
      "Epoch 189, Validation Loss: 3546.033909388951\n",
      "Epoch 190, Validation Loss: 3484.6429050990514\n",
      "Epoch 191, Validation Loss: 3439.990737915039\n",
      "Epoch 192, Validation Loss: 3441.023433140346\n",
      "Epoch 193, Validation Loss: 3336.3849291120255\n",
      "Epoch 194, Validation Loss: 3442.556891305106\n",
      "Epoch 195, Validation Loss: 3368.124219621931\n",
      "Epoch 196, Validation Loss: 3746.9298662458145\n",
      "Epoch 197, Validation Loss: 3756.5297328404017\n",
      "Epoch 198, Validation Loss: 3609.253646850586\n",
      "Epoch 199, Validation Loss: 3768.406707763672\n",
      "Epoch 200, Validation Loss: 3731.520765032087\n",
      "RMSE on test set: 47.106990814208984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  746\n",
      "len of segment_df is  2849\n",
      "len of segment_df is  2851\n",
      "len of segment_df is  250\n",
      "len of segment_df is  1268\n",
      "len of features_list 7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 317.9132371520996\n",
      "Epoch 2, Validation Loss: 328.7543151855469\n",
      "Epoch 3, Validation Loss: 312.585451965332\n",
      "Epoch 4, Validation Loss: 256.05824157714846\n",
      "Epoch 5, Validation Loss: 215.98680671691895\n",
      "Epoch 6, Validation Loss: 194.13294658660888\n",
      "Epoch 7, Validation Loss: 187.02995399475097\n",
      "Epoch 8, Validation Loss: 186.08731803894042\n",
      "Epoch 9, Validation Loss: 188.35708110809327\n",
      "Epoch 10, Validation Loss: 184.37469863891602\n",
      "Epoch 11, Validation Loss: 181.49646713256837\n",
      "Epoch 12, Validation Loss: 219.16097412109374\n",
      "Epoch 13, Validation Loss: 216.3224591064453\n",
      "Epoch 14, Validation Loss: 255.4842636871338\n",
      "Epoch 15, Validation Loss: 253.07322494506835\n",
      "Epoch 16, Validation Loss: 274.57790298461913\n",
      "Epoch 17, Validation Loss: 290.4440837097168\n",
      "Epoch 18, Validation Loss: 344.04937255859375\n",
      "Epoch 19, Validation Loss: 322.5074397277832\n",
      "Epoch 20, Validation Loss: 339.1958808898926\n",
      "Epoch 21, Validation Loss: 346.03726608276367\n",
      "Epoch 22, Validation Loss: 361.7270960998535\n",
      "Epoch 23, Validation Loss: 364.06401458740237\n",
      "Epoch 24, Validation Loss: 366.5221469116211\n",
      "Epoch 25, Validation Loss: 399.17770111083985\n",
      "Epoch 26, Validation Loss: 404.12109619140625\n",
      "Epoch 27, Validation Loss: 399.4679440307617\n",
      "Epoch 28, Validation Loss: 413.1746676635742\n",
      "Epoch 29, Validation Loss: 422.58354431152344\n",
      "Epoch 30, Validation Loss: 382.6795721435547\n",
      "Epoch 31, Validation Loss: 405.55918273925784\n",
      "Epoch 32, Validation Loss: 418.03980178833007\n",
      "Epoch 33, Validation Loss: 421.0926940917969\n",
      "Epoch 34, Validation Loss: 407.4474154663086\n",
      "Epoch 35, Validation Loss: 448.88434539794923\n",
      "Epoch 36, Validation Loss: 468.4405856323242\n",
      "Epoch 37, Validation Loss: 457.0914666748047\n",
      "Epoch 38, Validation Loss: 470.8339239501953\n",
      "Epoch 39, Validation Loss: 467.1138977050781\n",
      "Epoch 40, Validation Loss: 465.5432427978516\n",
      "Epoch 41, Validation Loss: 441.0619529724121\n",
      "Epoch 42, Validation Loss: 478.62044036865234\n",
      "Epoch 43, Validation Loss: 488.1017150878906\n",
      "Epoch 44, Validation Loss: 462.93526428222657\n",
      "Epoch 45, Validation Loss: 485.3545819091797\n",
      "Epoch 46, Validation Loss: 499.13475189208987\n",
      "Epoch 47, Validation Loss: 455.7341877746582\n",
      "Epoch 48, Validation Loss: 482.59816650390627\n",
      "Epoch 49, Validation Loss: 484.704375\n",
      "Epoch 50, Validation Loss: 491.3470684814453\n",
      "Epoch 51, Validation Loss: 497.01396423339844\n",
      "Epoch 52, Validation Loss: 478.57810943603516\n",
      "Epoch 53, Validation Loss: 445.1663659667969\n",
      "Epoch 54, Validation Loss: 457.90514190673827\n",
      "Epoch 55, Validation Loss: 463.681455078125\n",
      "Epoch 56, Validation Loss: 463.61617599487306\n",
      "Epoch 57, Validation Loss: 467.87068145751954\n",
      "Epoch 58, Validation Loss: 485.13385498046875\n",
      "Epoch 59, Validation Loss: 502.372795715332\n",
      "Epoch 60, Validation Loss: 461.81307495117187\n",
      "Epoch 61, Validation Loss: 478.7527163696289\n",
      "Epoch 62, Validation Loss: 459.1513821411133\n",
      "Epoch 63, Validation Loss: 461.28955139160155\n",
      "Epoch 64, Validation Loss: 514.0695449829102\n",
      "Epoch 65, Validation Loss: 488.9028549194336\n",
      "Epoch 66, Validation Loss: 404.8479995727539\n",
      "Epoch 67, Validation Loss: 514.5735382080078\n",
      "Epoch 68, Validation Loss: 519.0103381347657\n",
      "Epoch 69, Validation Loss: 515.686990661621\n",
      "Epoch 70, Validation Loss: 489.37853118896487\n",
      "Epoch 71, Validation Loss: 478.995007019043\n",
      "Epoch 72, Validation Loss: 524.1757986450195\n",
      "Epoch 73, Validation Loss: 515.9641943359375\n",
      "Epoch 74, Validation Loss: 495.67839477539064\n",
      "Epoch 75, Validation Loss: 516.5646981811524\n",
      "Epoch 76, Validation Loss: 500.27314422607424\n",
      "Epoch 77, Validation Loss: 523.3145462036133\n",
      "Epoch 78, Validation Loss: 519.1809341430665\n",
      "Epoch 79, Validation Loss: 520.3683041381836\n",
      "Epoch 80, Validation Loss: 507.48211486816405\n",
      "Epoch 81, Validation Loss: 487.68196655273437\n",
      "Epoch 82, Validation Loss: 509.76990112304685\n",
      "Epoch 83, Validation Loss: 495.9517352294922\n",
      "Epoch 84, Validation Loss: 496.9122915649414\n",
      "Epoch 85, Validation Loss: 496.25823272705077\n",
      "Epoch 86, Validation Loss: 472.5254302978516\n",
      "Epoch 87, Validation Loss: 482.2727200317383\n",
      "Epoch 88, Validation Loss: 495.8554470825195\n",
      "Epoch 89, Validation Loss: 472.85630889892576\n",
      "Epoch 90, Validation Loss: 472.1566775512695\n",
      "Epoch 91, Validation Loss: 502.6507452392578\n",
      "Epoch 92, Validation Loss: 480.7808999633789\n",
      "Epoch 93, Validation Loss: 480.56799774169923\n",
      "Epoch 94, Validation Loss: 492.63779113769533\n",
      "Epoch 95, Validation Loss: 461.95811950683594\n",
      "Epoch 96, Validation Loss: 472.287548828125\n",
      "Epoch 97, Validation Loss: 485.8116979980469\n",
      "Epoch 98, Validation Loss: 488.0128729248047\n",
      "Epoch 99, Validation Loss: 459.6085787963867\n",
      "Epoch 100, Validation Loss: 473.6932077026367\n",
      "Epoch 101, Validation Loss: 479.4609994506836\n",
      "Epoch 102, Validation Loss: 448.77007263183594\n",
      "Epoch 103, Validation Loss: 464.29039794921874\n",
      "Epoch 104, Validation Loss: 460.5113256835937\n",
      "Epoch 105, Validation Loss: 451.7905838012695\n",
      "Epoch 106, Validation Loss: 475.00690307617185\n",
      "Epoch 107, Validation Loss: 459.72865142822263\n",
      "Epoch 108, Validation Loss: 448.7348049926758\n",
      "Epoch 109, Validation Loss: 460.7053869628906\n",
      "Epoch 110, Validation Loss: 457.26421813964845\n",
      "Epoch 111, Validation Loss: 464.6158715820313\n",
      "Epoch 112, Validation Loss: 452.346591796875\n",
      "Epoch 113, Validation Loss: 451.9456884765625\n",
      "Epoch 114, Validation Loss: 434.6752178955078\n",
      "Epoch 115, Validation Loss: 449.31995880126954\n",
      "Epoch 116, Validation Loss: 447.44762817382815\n",
      "Epoch 117, Validation Loss: 460.18980529785154\n",
      "Epoch 118, Validation Loss: 448.1552587890625\n",
      "Epoch 119, Validation Loss: 444.8432098388672\n",
      "Epoch 120, Validation Loss: 445.604072265625\n",
      "Epoch 121, Validation Loss: 442.6111557006836\n",
      "Epoch 122, Validation Loss: 437.46455017089846\n",
      "Epoch 123, Validation Loss: 450.35051208496094\n",
      "Epoch 124, Validation Loss: 437.6968682861328\n",
      "Epoch 125, Validation Loss: 416.17571411132815\n",
      "Epoch 126, Validation Loss: 430.4612768554687\n",
      "Epoch 127, Validation Loss: 412.6714025878906\n",
      "Epoch 128, Validation Loss: 431.86010192871095\n",
      "Epoch 129, Validation Loss: 434.62951232910154\n",
      "Epoch 130, Validation Loss: 451.59649658203125\n",
      "Epoch 131, Validation Loss: 415.54151947021484\n",
      "Epoch 132, Validation Loss: 436.02286560058593\n",
      "Epoch 133, Validation Loss: 408.6969915771484\n",
      "Epoch 134, Validation Loss: 449.6683380126953\n",
      "Epoch 135, Validation Loss: 428.7668127441406\n",
      "Epoch 136, Validation Loss: 415.89531677246094\n",
      "Epoch 137, Validation Loss: 445.15033630371096\n",
      "Epoch 138, Validation Loss: 398.0016464233398\n",
      "Epoch 139, Validation Loss: 490.94585205078124\n",
      "Epoch 140, Validation Loss: 410.64611450195315\n",
      "Epoch 141, Validation Loss: 456.2859899902344\n",
      "Epoch 142, Validation Loss: 457.18911315917967\n",
      "Epoch 143, Validation Loss: 460.36163513183595\n",
      "Epoch 144, Validation Loss: 442.21174865722656\n",
      "Epoch 145, Validation Loss: 444.8431646728516\n",
      "Epoch 146, Validation Loss: 429.6995153808594\n",
      "Epoch 147, Validation Loss: 413.13774810791017\n",
      "Epoch 148, Validation Loss: 422.1250994873047\n",
      "Epoch 149, Validation Loss: 401.7727111816406\n",
      "Epoch 150, Validation Loss: 411.23378479003907\n",
      "Epoch 151, Validation Loss: 414.3870211791992\n",
      "Epoch 152, Validation Loss: 420.4828875732422\n",
      "Epoch 153, Validation Loss: 401.59626739501954\n",
      "Epoch 154, Validation Loss: 410.6903424072266\n",
      "Epoch 155, Validation Loss: 450.5644580078125\n",
      "Epoch 156, Validation Loss: 433.4170281982422\n",
      "Epoch 157, Validation Loss: 439.5520489501953\n",
      "Epoch 158, Validation Loss: 408.04173583984374\n",
      "Epoch 159, Validation Loss: 442.0270394897461\n",
      "Epoch 160, Validation Loss: 399.0163919067383\n",
      "Epoch 161, Validation Loss: 415.5118292236328\n",
      "Epoch 162, Validation Loss: 437.79091705322264\n",
      "Epoch 163, Validation Loss: 420.4114236450195\n",
      "Epoch 164, Validation Loss: 416.3944964599609\n",
      "Epoch 165, Validation Loss: 409.499748840332\n",
      "Epoch 166, Validation Loss: 416.6682440185547\n",
      "Epoch 167, Validation Loss: 435.51598754882815\n",
      "Epoch 168, Validation Loss: 436.50033477783205\n",
      "Epoch 169, Validation Loss: 397.000100402832\n",
      "Epoch 170, Validation Loss: 432.4211584472656\n",
      "Epoch 171, Validation Loss: 485.0107147216797\n",
      "Epoch 172, Validation Loss: 407.6450048828125\n",
      "Epoch 173, Validation Loss: 407.19922943115233\n",
      "Epoch 174, Validation Loss: 403.1573208618164\n",
      "Epoch 175, Validation Loss: 409.4433563232422\n",
      "Epoch 176, Validation Loss: 338.8727407836914\n",
      "Epoch 177, Validation Loss: 447.509404296875\n",
      "Epoch 178, Validation Loss: 353.4967309570313\n",
      "Epoch 179, Validation Loss: 456.5076089477539\n",
      "Epoch 180, Validation Loss: 393.5762661743164\n",
      "Epoch 181, Validation Loss: 389.2559161376953\n",
      "Epoch 182, Validation Loss: 361.32551727294924\n",
      "Epoch 183, Validation Loss: 374.0073335266113\n",
      "Epoch 184, Validation Loss: 413.1473648071289\n",
      "Epoch 185, Validation Loss: 305.6960316467285\n",
      "Epoch 186, Validation Loss: 338.03107360839846\n",
      "Epoch 187, Validation Loss: 450.73561614990234\n",
      "Epoch 188, Validation Loss: 337.5326849365234\n",
      "Epoch 189, Validation Loss: 340.67600891113284\n",
      "Epoch 190, Validation Loss: 370.2025173950195\n",
      "Epoch 191, Validation Loss: 435.52470764160154\n",
      "Epoch 192, Validation Loss: 372.9174905395508\n",
      "Epoch 193, Validation Loss: 368.60199645996096\n",
      "Epoch 194, Validation Loss: 376.4604641723633\n",
      "Epoch 195, Validation Loss: 350.46899871826173\n",
      "Epoch 196, Validation Loss: 340.1932928466797\n",
      "Epoch 197, Validation Loss: 374.67969482421876\n",
      "Epoch 198, Validation Loss: 380.68704193115235\n",
      "Epoch 199, Validation Loss: 333.1889996337891\n",
      "Epoch 200, Validation Loss: 364.4287405395508\n",
      "RMSE on test set: 19.779958724975586\n",
      "segment_3\n",
      "before:  2021-01-14 01:42:53\n",
      "after:  2021-01-14 02:17:52\n",
      "segment_3\n",
      "before:  2021-01-14 13:17:54\n",
      "after:  2021-01-14 13:47:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:13: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\3089230906.py:26: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\310685994.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.7' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.8' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_43360\\1314951947.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  1313\n",
      "len of segment_df is  1376\n",
      "len of segment_df is  1005\n",
      "len of segment_df is  288\n",
      "len of features_list 3898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 591.7467926832346\n",
      "Epoch 2, Validation Loss: 590.6058300458468\n",
      "Epoch 3, Validation Loss: 572.7371174005361\n",
      "Epoch 4, Validation Loss: 567.7586808571449\n",
      "Epoch 5, Validation Loss: 565.4738989609939\n",
      "Epoch 6, Validation Loss: 564.8606744912954\n",
      "Epoch 7, Validation Loss: 573.8574764545148\n",
      "Epoch 8, Validation Loss: 581.4133388812726\n",
      "Epoch 9, Validation Loss: 570.1227804330679\n",
      "Epoch 10, Validation Loss: 574.4056079571063\n",
      "Epoch 11, Validation Loss: 568.593009361854\n",
      "Epoch 12, Validation Loss: 548.7798960759089\n",
      "Epoch 13, Validation Loss: 541.4973919208234\n",
      "Epoch 14, Validation Loss: 542.5670758760892\n",
      "Epoch 15, Validation Loss: 534.2073273291954\n",
      "Epoch 16, Validation Loss: 533.038695702186\n",
      "Epoch 17, Validation Loss: 542.0703703073355\n",
      "Epoch 18, Validation Loss: 534.9570629413312\n",
      "Epoch 19, Validation Loss: 538.4214938237117\n",
      "Epoch 20, Validation Loss: 538.7254700293907\n",
      "Epoch 21, Validation Loss: 528.7686063326322\n",
      "Epoch 22, Validation Loss: 537.299686725323\n",
      "Epoch 23, Validation Loss: 536.5959560687726\n",
      "Epoch 24, Validation Loss: 539.4112478402944\n",
      "Epoch 25, Validation Loss: 538.1100663405198\n",
      "Epoch 26, Validation Loss: 538.009394132174\n",
      "Epoch 27, Validation Loss: 540.3063319279597\n",
      "Epoch 28, Validation Loss: 540.7466636070839\n",
      "Epoch 29, Validation Loss: 543.494749215933\n",
      "Epoch 30, Validation Loss: 542.11866408128\n",
      "Epoch 31, Validation Loss: 545.1339551485502\n",
      "Epoch 32, Validation Loss: 545.4129744309646\n",
      "Epoch 33, Validation Loss: 545.9129609328049\n",
      "Epoch 34, Validation Loss: 550.4372136042668\n",
      "Epoch 35, Validation Loss: 549.0255232590896\n",
      "Epoch 36, Validation Loss: 552.630362877479\n",
      "Epoch 37, Validation Loss: 556.7932962270884\n",
      "Epoch 38, Validation Loss: 563.184329693134\n",
      "Epoch 39, Validation Loss: 547.1934744027944\n",
      "Epoch 40, Validation Loss: 563.9089519794171\n",
      "Epoch 41, Validation Loss: 564.6030044555664\n",
      "Epoch 42, Validation Loss: 568.0391963078425\n",
      "Epoch 43, Validation Loss: 574.3179896428035\n",
      "Epoch 44, Validation Loss: 573.3945834820087\n",
      "Epoch 45, Validation Loss: 579.119626558744\n",
      "Epoch 46, Validation Loss: 574.6279666607196\n",
      "Epoch 47, Validation Loss: 592.7525705190806\n",
      "Epoch 48, Validation Loss: 595.1159198467548\n",
      "Epoch 49, Validation Loss: 610.6093303973859\n",
      "Epoch 50, Validation Loss: 623.0769265981821\n",
      "Epoch 51, Validation Loss: 631.8360713078425\n",
      "Epoch 52, Validation Loss: 629.3886483999399\n",
      "Epoch 53, Validation Loss: 634.2527712308444\n",
      "Epoch 54, Validation Loss: 627.6785501333384\n",
      "Epoch 55, Validation Loss: 643.9485461895282\n",
      "Epoch 56, Validation Loss: 625.7893993671124\n",
      "Epoch 57, Validation Loss: 640.0491614708534\n",
      "Epoch 58, Validation Loss: 648.8173487736628\n",
      "Epoch 59, Validation Loss: 670.244385939378\n",
      "Epoch 60, Validation Loss: 610.0913672814003\n",
      "Epoch 61, Validation Loss: 701.8108502901517\n",
      "Epoch 62, Validation Loss: 712.0929283728966\n",
      "Epoch 63, Validation Loss: 683.3563907329852\n",
      "Epoch 64, Validation Loss: 663.8038975642278\n",
      "Epoch 65, Validation Loss: 711.7079326923077\n",
      "Epoch 66, Validation Loss: 679.0372220552885\n",
      "Epoch 67, Validation Loss: 720.1671805748573\n",
      "Epoch 68, Validation Loss: 695.1400486872747\n",
      "Epoch 69, Validation Loss: 736.086037855882\n",
      "Epoch 70, Validation Loss: 741.0489038320688\n",
      "Epoch 71, Validation Loss: 742.5001848660983\n",
      "Epoch 72, Validation Loss: 716.6104348989634\n",
      "Epoch 73, Validation Loss: 702.1440265362079\n",
      "Epoch 74, Validation Loss: 600.1763282189003\n",
      "Epoch 75, Validation Loss: 689.6549635667068\n",
      "Epoch 76, Validation Loss: 708.1358360877404\n",
      "Epoch 77, Validation Loss: 719.724485544058\n",
      "Epoch 78, Validation Loss: 713.9862477229192\n",
      "Epoch 79, Validation Loss: 684.598150400015\n",
      "Epoch 80, Validation Loss: 754.4185673640325\n",
      "Epoch 81, Validation Loss: 711.8055772047776\n",
      "Epoch 82, Validation Loss: 737.3458803617037\n",
      "Epoch 83, Validation Loss: 753.5431418785682\n",
      "Epoch 84, Validation Loss: 766.1795930128831\n",
      "Epoch 85, Validation Loss: 756.4176518366887\n",
      "Epoch 86, Validation Loss: 755.0842161912185\n",
      "Epoch 87, Validation Loss: 701.5068734975962\n",
      "Epoch 88, Validation Loss: 642.0602622398964\n",
      "Epoch 89, Validation Loss: 631.7273442195012\n",
      "Epoch 90, Validation Loss: 740.3245157095103\n",
      "Epoch 91, Validation Loss: 705.6773341252253\n",
      "Epoch 92, Validation Loss: 707.9615566547101\n",
      "Epoch 93, Validation Loss: 718.9934921264648\n",
      "Epoch 94, Validation Loss: 748.4393991323618\n",
      "Epoch 95, Validation Loss: 720.9443652813251\n",
      "Epoch 96, Validation Loss: 720.1280136108398\n",
      "Epoch 97, Validation Loss: 756.2635134183444\n",
      "Epoch 98, Validation Loss: 730.8342731182391\n",
      "Epoch 99, Validation Loss: 769.2838228665865\n",
      "Epoch 100, Validation Loss: 781.3254464956431\n",
      "Epoch 101, Validation Loss: 693.6958395150991\n",
      "Epoch 102, Validation Loss: 728.2960580679087\n",
      "Epoch 103, Validation Loss: 692.1156311035156\n",
      "Epoch 104, Validation Loss: 652.7967810997596\n",
      "Epoch 105, Validation Loss: 629.4169663649338\n",
      "Epoch 106, Validation Loss: 731.753170306866\n",
      "Epoch 107, Validation Loss: 661.6907307551458\n",
      "Epoch 108, Validation Loss: 713.0099628155048\n",
      "Epoch 109, Validation Loss: 772.9165942852313\n",
      "Epoch 110, Validation Loss: 612.0875384990985\n",
      "Epoch 111, Validation Loss: 666.605459359976\n",
      "Epoch 112, Validation Loss: 746.1231654240535\n",
      "Epoch 113, Validation Loss: 702.5411658653846\n",
      "Epoch 114, Validation Loss: 670.9097923865685\n",
      "Epoch 115, Validation Loss: 660.5479172926682\n",
      "Epoch 116, Validation Loss: 733.3275228647085\n",
      "Epoch 117, Validation Loss: 688.5497307410607\n",
      "Epoch 118, Validation Loss: 664.3150452833909\n",
      "Epoch 119, Validation Loss: 640.3480400672325\n",
      "Epoch 120, Validation Loss: 595.662827711839\n",
      "Epoch 121, Validation Loss: 785.5611830491287\n",
      "Epoch 122, Validation Loss: 614.6735241229718\n",
      "Epoch 123, Validation Loss: 703.6494973989634\n",
      "Epoch 124, Validation Loss: 815.4287966214694\n",
      "Epoch 125, Validation Loss: 744.0347595214844\n",
      "Epoch 126, Validation Loss: 827.3976123516376\n",
      "Epoch 127, Validation Loss: 629.7783021193284\n",
      "Epoch 128, Validation Loss: 661.8165611853966\n",
      "Epoch 129, Validation Loss: 703.5139312744141\n",
      "Epoch 130, Validation Loss: 706.4921135535607\n",
      "Epoch 131, Validation Loss: 702.4300267146184\n",
      "Epoch 132, Validation Loss: 754.1238773052509\n",
      "Epoch 133, Validation Loss: 816.210941021259\n",
      "Epoch 134, Validation Loss: 798.6812720665565\n",
      "Epoch 135, Validation Loss: 909.753926203801\n",
      "Epoch 136, Validation Loss: 886.3593403742864\n",
      "Epoch 137, Validation Loss: 836.1863250732422\n",
      "Epoch 138, Validation Loss: 992.9951488788312\n",
      "Epoch 139, Validation Loss: 649.9874120858999\n",
      "Epoch 140, Validation Loss: 834.374005831205\n",
      "Epoch 141, Validation Loss: 846.5166719876803\n",
      "Epoch 142, Validation Loss: 818.7654360257662\n",
      "Epoch 143, Validation Loss: 1014.8941439115084\n",
      "Epoch 144, Validation Loss: 758.734024634728\n",
      "Epoch 145, Validation Loss: 917.627699631911\n",
      "Epoch 146, Validation Loss: 849.9813913198618\n",
      "Epoch 147, Validation Loss: 907.3506516676682\n",
      "Epoch 148, Validation Loss: 805.5992396428035\n",
      "Epoch 149, Validation Loss: 865.7231903076172\n",
      "Epoch 150, Validation Loss: 773.2696351271409\n",
      "Epoch 151, Validation Loss: 769.0473245474009\n",
      "Epoch 152, Validation Loss: 702.8143334021935\n",
      "Epoch 153, Validation Loss: 720.9825392503005\n",
      "Epoch 154, Validation Loss: 730.7394297673152\n",
      "Epoch 155, Validation Loss: 707.1259413499099\n",
      "Epoch 156, Validation Loss: 671.5806966928335\n",
      "Epoch 157, Validation Loss: 705.6385057889498\n",
      "Epoch 158, Validation Loss: 769.1158593984751\n",
      "Epoch 159, Validation Loss: 740.5455815241887\n",
      "Epoch 160, Validation Loss: 782.9979506272537\n",
      "Epoch 161, Validation Loss: 779.7064279409556\n",
      "Epoch 162, Validation Loss: 864.5747880202073\n",
      "Epoch 163, Validation Loss: 999.3467102050781\n",
      "Epoch 164, Validation Loss: 811.1416678795448\n",
      "Epoch 165, Validation Loss: 700.5071258544922\n",
      "Epoch 166, Validation Loss: 985.7159200815054\n",
      "Epoch 167, Validation Loss: 881.5656327467698\n",
      "Epoch 168, Validation Loss: 867.4286857018104\n",
      "Epoch 169, Validation Loss: 887.7532524695763\n",
      "Epoch 170, Validation Loss: 806.387454693134\n",
      "Epoch 171, Validation Loss: 717.1491112342247\n",
      "Epoch 172, Validation Loss: 802.5958686241737\n",
      "Epoch 173, Validation Loss: 878.5092926025391\n",
      "Epoch 174, Validation Loss: 746.4200228177584\n",
      "Epoch 175, Validation Loss: 796.8051652174729\n",
      "Epoch 176, Validation Loss: 756.9842969454252\n",
      "Epoch 177, Validation Loss: 756.9927151019757\n",
      "Epoch 178, Validation Loss: 793.1153517503005\n",
      "Epoch 179, Validation Loss: 767.0720895620493\n",
      "Epoch 180, Validation Loss: 787.5234732994667\n",
      "Epoch 181, Validation Loss: 813.3495143010066\n",
      "Epoch 182, Validation Loss: 841.3100222074069\n",
      "Epoch 183, Validation Loss: 913.8658693753756\n",
      "Epoch 184, Validation Loss: 901.5327782264122\n",
      "Epoch 185, Validation Loss: 894.6226759690505\n",
      "Epoch 186, Validation Loss: 919.9243463369517\n",
      "Epoch 187, Validation Loss: 946.4290489783654\n",
      "Epoch 188, Validation Loss: 914.115966796875\n",
      "Epoch 189, Validation Loss: 875.3873161902794\n",
      "Epoch 190, Validation Loss: 856.326899601863\n",
      "Epoch 191, Validation Loss: 862.387919499324\n",
      "Epoch 192, Validation Loss: 878.070536686824\n",
      "Epoch 193, Validation Loss: 865.7104621300331\n",
      "Epoch 194, Validation Loss: 831.4738077016978\n",
      "Epoch 195, Validation Loss: 870.0836756779597\n",
      "Epoch 196, Validation Loss: 833.305903508113\n",
      "Epoch 197, Validation Loss: 856.4149076021635\n",
      "Epoch 198, Validation Loss: 926.346678513747\n",
      "Epoch 199, Validation Loss: 807.5702608548678\n",
      "Epoch 200, Validation Loss: 947.3451338547927\n",
      "RMSE on test set: 35.62152099609375\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_rmse_list = []\n",
    "for ffile in overlap:\n",
    "\n",
    "    subject = pd.read_csv(f\"../LB_split/{ffile}\")\n",
    "    glucose = preprocess_t1dexi_cgm(f\"../LB_split/{ffile}\", False)\n",
    "    glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "    # Create the multi-channel database\n",
    "    g_data = []\n",
    "    for timestamp in glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': glucose_dict[timestamp],\n",
    "            # 'meal_type': None,\n",
    "            # 'meal_carbs': 0\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "    # Convert glucose values to numeric type for analysis\n",
    "    glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "    segments = segement_data_as_1hour(glucose_df)\n",
    "    interpolated_segements = detect_missing_and_spline_interpolate(segments)\n",
    "    meal = pd.read_csv(f\"../ML_split/{ffile}\")\n",
    "    selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "    meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "    meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    meal_df['assigned'] = False\n",
    "\n",
    "    # Extract unique dates\n",
    "    unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "    # Convert to list\n",
    "    meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "    cleaned_segments = {}\n",
    "\n",
    "    # Iterate through each segment and filter by unique dates\n",
    "    for segment_name, df in interpolated_segements.items():\n",
    "        # Convert timestamp column to datetime and then extract the date part\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        \n",
    "        # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "        filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "        \n",
    "        # Drop the 'date' column as it's no longer needed\n",
    "        filtered_df = filtered_df.drop(columns=['date'])\n",
    "        \n",
    "        # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "        cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "    # Update the segments with meal data\n",
    "    meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)\n",
    "\n",
    "    subject_facm = pd.read_csv(f\"../FACM_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_basal.reset_index(drop=True, inplace=True)\n",
    "    new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "    new_df_basal['assigned'] = False\n",
    "    new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "    \n",
    "    basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n",
    "\n",
    "    new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/{ffile}\", False)\n",
    "    bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)\n",
    "\n",
    "    features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "    # Assuming features_list and raw_glu_list are already defined\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(raw_glu_list)\n",
    "\n",
    "    # Step 1: Split into 80% train+val and 20% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Convert the splits to torch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "\n",
    "    # Example of how to define the loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
    "\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on test set: {rmse}')\n",
    "    test_rmse_list.append(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carbs</th>\n",
       "      <th>ts</th>\n",
       "      <th>assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carbs         ts  assigned\n",
       "0      3 2020-06-01     False\n",
       "1      3 2020-06-01     False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>glucose_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-04 00:04:32</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-04 00:09:32</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-04 00:14:34</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-04 00:19:31</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-04 00:24:32</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>2020-07-31 23:36:02</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>2020-07-31 23:41:02</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>2020-07-31 23:46:02</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5608</th>\n",
       "      <td>2020-07-31 23:51:02</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5609</th>\n",
       "      <td>2020-07-31 23:56:04</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5610 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  glucose_value\n",
       "0    2020-07-04 00:04:32           97.0\n",
       "1    2020-07-04 00:09:32          100.0\n",
       "2    2020-07-04 00:14:34          102.0\n",
       "3    2020-07-04 00:19:31          101.0\n",
       "4    2020-07-04 00:24:32           91.0\n",
       "...                  ...            ...\n",
       "5605 2020-07-31 23:36:02          145.0\n",
       "5606 2020-07-31 23:41:02          140.0\n",
       "5607 2020-07-31 23:46:02          140.0\n",
       "5608 2020-07-31 23:51:02          133.0\n",
       "5609 2020-07-31 23:56:04          137.0\n",
       "\n",
       "[5610 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'911.csv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['854.csv',\n",
       " '979.csv',\n",
       " '816.csv',\n",
       " '953.csv',\n",
       " '981.csv',\n",
       " '1617.csv',\n",
       " '1343.csv',\n",
       " '987.csv',\n",
       " '255.csv',\n",
       " '85.csv',\n",
       " '907.csv',\n",
       " '856.csv',\n",
       " '354.csv',\n",
       " '894.csv',\n",
       " '911.csv',\n",
       " '862.csv',\n",
       " '900.csv',\n",
       " '695.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_1': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_2': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_3': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carbs, basal_rate, bolus_dose]\n",
       " Index: []}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bolus_updated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dose</th>\n",
       "      <th>ts_begin</th>\n",
       "      <th>assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.566</td>\n",
       "      <td>2020-07-04 02:11:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.948</td>\n",
       "      <td>2020-07-04 03:15:54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.795</td>\n",
       "      <td>2020-07-04 04:21:35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.706</td>\n",
       "      <td>2020-07-04 14:23:12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.970</td>\n",
       "      <td>2020-07-04 14:34:54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1.697</td>\n",
       "      <td>2020-07-30 20:08:08</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2.423</td>\n",
       "      <td>2020-07-31 10:28:31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.422</td>\n",
       "      <td>2020-07-31 13:52:59</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1.271</td>\n",
       "      <td>2020-07-31 16:57:41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>7.720</td>\n",
       "      <td>2020-07-31 17:02:33</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dose            ts_begin  assigned\n",
       "0    1.566 2020-07-04 02:11:00     False\n",
       "1    0.948 2020-07-04 03:15:54     False\n",
       "2    1.795 2020-07-04 04:21:35     False\n",
       "3    1.706 2020-07-04 14:23:12     False\n",
       "4    7.970 2020-07-04 14:34:54     False\n",
       "..     ...                 ...       ...\n",
       "203  1.697 2020-07-30 20:08:08     False\n",
       "204  2.423 2020-07-31 10:28:31     False\n",
       "205  1.422 2020-07-31 13:52:59     False\n",
       "206  1.271 2020-07-31 16:57:41     False\n",
       "207  7.720 2020-07-31 17:02:33     False\n",
       "\n",
       "[208 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>glucose_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-07 00:04:54</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-07 00:09:54</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-07 00:14:54</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-07 00:19:54</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-07 00:24:54</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7469</th>\n",
       "      <td>2021-05-04 23:35:38</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7470</th>\n",
       "      <td>2021-05-04 23:40:38</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>2021-05-04 23:45:38</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>2021-05-04 23:50:38</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>2021-05-04 23:55:38</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7474 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  glucose_value\n",
       "0    2021-04-07 00:04:54           91.0\n",
       "1    2021-04-07 00:09:54           89.0\n",
       "2    2021-04-07 00:14:54           90.0\n",
       "3    2021-04-07 00:19:54           93.0\n",
       "4    2021-04-07 00:24:54           96.0\n",
       "...                  ...            ...\n",
       "7469 2021-05-04 23:35:38          229.0\n",
       "7470 2021-05-04 23:40:38          244.0\n",
       "7471 2021-05-04 23:45:38          256.0\n",
       "7472 2021-05-04 23:50:38          267.0\n",
       "7473 2021-05-04 23:55:38          276.0\n",
       "\n",
       "[7474 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
