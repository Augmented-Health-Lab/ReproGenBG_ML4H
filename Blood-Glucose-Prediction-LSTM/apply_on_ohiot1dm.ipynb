{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_window = 7 # *5\n",
    "prediction_horizon = 6 # *5 minute ahead\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_OhioT1DM(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)\n",
    "\n",
    "    res = []\n",
    "    for item in root.findall(\"glucose_level\"):\n",
    "        entry0 = item[0].attrib\n",
    "        res.append([float(entry0[\"value\"])])\n",
    "        for i in range(1, len(item)):\n",
    "            last_entry = item[i - 1].attrib\n",
    "            entry = item[i].attrib\n",
    "            t1 = datetime.datetime.strptime(entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            t0 = datetime.datetime.strptime(last_entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            delt = t1 - t0\n",
    "            if delt <= interval_timedelta:\n",
    "                res[-1].append(float(entry[\"value\"]))\n",
    "            else:\n",
    "                res.append([float(entry[\"value\"])])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, pred_step):\n",
    "    \"\"\"\n",
    "    Create sequences and targets from time series data.\n",
    "\n",
    "    Args:\n",
    "    data (list or ndarray): The time series data.\n",
    "    seq_length (int): Length of the sequence.\n",
    "    pred_step (int): Steps ahead to predict.\n",
    "\n",
    "    Returns:\n",
    "    tuple of torch.Tensors: sequences and targets.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length - pred_step + 1):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        targets.append(data[i+seq_length+pred_step-1])\n",
    "    return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR OHIOT1DM\n",
    "\n",
    "train_directory_path = r'C:\\Users\\anonymoususer\\OneDrive\\Desktop\\BGprediction\\OhioT1DM\\2018\\train'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "              if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        # print(\"Shape after RNN:\", out.shape)  # Debugging: Check shape after RNN\n",
    "        # out = out[:, -1, :]\n",
    "        # print(\"Shape before FC:\", out.shape)  # Debugging: Check shape before FC\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim, output_features):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_features, hidden_dim, batch_first=True)\n",
    "        self.relu = nn.ReLU()  # Define ReLU activation\n",
    "        self.fc = nn.Linear(hidden_dim, output_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Apply ReLU activation to the output of the LSTM\n",
    "        lstm_out = self.relu(lstm_out)  # Applying ReLU to only the last time step's output\n",
    "        # Pass the output through the fully connected layer\n",
    "        y_pred = self.fc(lstm_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        # Use the output of the last time step\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Assuming x_train_uni is a PyTorch tensor with shape (batch_size, seq_length, num_features)\n",
    "input_size = x_train_uni.shape[-1]  # Number of features\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "\n",
    "model = GRUModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the correct device\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data to the correct device\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_targets(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Ensure model is on the right device\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data & target to the same device as model\n",
    "            output = model(data)\n",
    "            predictions.extend(output.detach().cpu().numpy())  # Move output back to CPU for storage/manipulation\n",
    "            targets.extend(target.detach().cpu().numpy())\n",
    "    return predictions, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse_pytorch(predictions, targets):\n",
    "    # Convert predictions and targets to float tensors if they are not already\n",
    "    predictions = predictions.float()\n",
    "    targets = targets.float()\n",
    "    \n",
    "    # Calculate MSE\n",
    "    # mse = torch.mean((predictions - targets) ** 2)\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on RNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "rmse_list = []\n",
    "\n",
    "for subj in train_file_names:\n",
    "    res = preprocess_OhioT1DM(f\"../OhioT1DM/2018/train/{subj}.xml\")\n",
    "    # reconstruct_data = create_sequences(res[0], sequence_window, prediction_horizon)\n",
    "    # Loop thru all subjects to stack the data\n",
    "    X_stack_tensor = []\n",
    "    y_stack_tensor = []\n",
    "\n",
    "    for ii in res:\n",
    "        reconstruct_data = create_sequences(ii, sequence_window, prediction_horizon) # sequence length = 6(30min), pediction_horizon = 3(predict 15 minutes ahead)\n",
    "        X_stack_tensor.append(reconstruct_data[0])\n",
    "        y_stack_tensor.append(reconstruct_data[1])\n",
    "    \n",
    "    all_sequences = torch.cat(X_stack_tensor, dim=0) # yy_0 includes the X\n",
    "    all_targets = torch.cat(y_stack_tensor, dim=0) # yy_1 includes the y label\n",
    "    sequences_train, sequences_val, targets_train, targets_val = train_test_split(\n",
    "        all_sequences, all_targets, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Build the data loader and model\n",
    "    train_dataset = TensorDataset(sequences_train, targets_train)\n",
    "    val_dataset = TensorDataset(sequences_val, targets_val)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model parameters\n",
    "    input_size = sequences_train.shape[-1]  # Number of features\n",
    "\n",
    "    # Model instantiation\n",
    "    model_rnn = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_rnn.parameters())\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model_rnn, train_data_loader, optimizer, device)\n",
    "        val_loss = validate_epoch(model_rnn, val_data_loader, device)\n",
    "        # Print every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}')\n",
    "\n",
    "    # Assuming model and val_loader are already defined\n",
    "    predictions, targets = get_predictions_and_targets(model_rnn, val_data_loader)\n",
    "    rmse_value = rmse_pytorch(torch.tensor(predictions), torch.tensor(targets))\n",
    "    rmse_list.append(rmse_value)\n",
    "    print(f'RMSE: {rmse_value.item()}')\n",
    "    print(f\"=================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on LSTM \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "lstm_rmse_list = []\n",
    "\n",
    "for subj in train_file_names[:10]:\n",
    "    # res = preprocess_DiaTrend(f\"../DiaTrend/train/{subj}.csv\")\n",
    "    res = preprocess_OhioT1DM(f\"../OhioT1DM/2018/train/{subj}.xml\")\n",
    "    # reconstruct_data = create_sequences(res[0], sequence_window, prediction_horizon)\n",
    "    # Loop thru all subjects to stack the data\n",
    "    X_stack_tensor = []\n",
    "    y_stack_tensor = []\n",
    "\n",
    "    for ii in res:\n",
    "        reconstruct_data = create_sequences(ii, sequence_window, prediction_horizon) # sequence length = 6(30min), pediction_horizon = 3(predict 15 minutes ahead)\n",
    "        X_stack_tensor.append(reconstruct_data[0])\n",
    "        y_stack_tensor.append(reconstruct_data[1])\n",
    "    \n",
    "    all_sequences = torch.cat(X_stack_tensor, dim=0) # yy_0 includes the X\n",
    "    all_targets = torch.cat(y_stack_tensor, dim=0) # yy_1 includes the y label\n",
    "    sequences_train, sequences_val, targets_train, targets_val = train_test_split(\n",
    "        all_sequences, all_targets, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Build the data loader and model\n",
    "    train_dataset = TensorDataset(sequences_train, targets_train)\n",
    "    val_dataset = TensorDataset(sequences_val, targets_val)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model parameters\n",
    "    input_size = sequences_train.shape[-1]  # Number of features\n",
    "\n",
    "    # Model instantiation\n",
    "    model_lstm = LSTMModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_lstm.parameters())\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model_lstm, train_data_loader, optimizer, device)\n",
    "        val_loss = validate_epoch(model_lstm, val_data_loader, device)\n",
    "        # Print every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}')\n",
    "\n",
    "    # Assuming model and val_loader are already defined\n",
    "    predictions, targets = get_predictions_and_targets(model_lstm, val_data_loader)\n",
    "    rmse_value = rmse_pytorch(torch.tensor(predictions), torch.tensor(targets))\n",
    "    lstm_rmse_list.append(rmse_value)\n",
    "    print(f'RMSE: {rmse_value.item()}')\n",
    "    print(f\"=================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "gru_rmse_list = []\n",
    "\n",
    "for subj in train_file_names[:10]:\n",
    "    res = preprocess_OhioT1DM(f\"../OhioT1DM/2018/train/{subj}.xml\")\n",
    "    X_stack_tensor = []\n",
    "    y_stack_tensor = []\n",
    "\n",
    "    for ii in res:\n",
    "        reconstruct_data = create_sequences(ii, sequence_window, prediction_horizon)\n",
    "        X_stack_tensor.append(reconstruct_data[0])\n",
    "        y_stack_tensor.append(reconstruct_data[1])\n",
    "    \n",
    "    all_sequences = torch.cat(X_stack_tensor, dim=0)\n",
    "    all_targets = torch.cat(y_stack_tensor, dim=0)\n",
    "    sequences_train, sequences_val, targets_train, targets_val = train_test_split(\n",
    "        all_sequences, all_targets, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(sequences_train, targets_train)\n",
    "    val_dataset = TensorDataset(sequences_val, targets_val)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    input_size = sequences_train.shape[-1]\n",
    "\n",
    "    # Model instantiation\n",
    "    model_gru = GRUModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_gru.parameters())\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model_gru, train_data_loader, optimizer, device)\n",
    "        val_loss = validate_epoch(model_gru, val_data_loader, device)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}')\n",
    "\n",
    "    predictions, targets = get_predictions_and_targets(model_gru, val_data_loader)\n",
    "    rmse_value = rmse_pytorch(torch.tensor(predictions), torch.tensor(targets))\n",
    "    gru_rmse_list.append(rmse_value)\n",
    "    print(f'RMSE: {rmse_value.item()}')\n",
    "    print(f\"=================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "batch_size = 50\n",
    "num_epochs = 200  # Increased for better convergence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "gru_rmse_list = []\n",
    "\n",
    "for subj in train_file_names[:10]:\n",
    "    # ... existing data preprocessing code ...\n",
    "\n",
    "    # Modified DataLoader with new batch size and enabled shuffling\n",
    "    train_dataset = TensorDataset(sequences_train, targets_train)\n",
    "    val_dataset = TensorDataset(sequences_val, targets_val)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Enable shuffling\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = sequences_train.shape[-1]\n",
    "\n",
    "    # Create multiple optimizers for comparison\n",
    "    optimizers = {\n",
    "        'adamax': torch.optim.Adamax(model_gru.parameters(), lr=0.002),\n",
    "        'rmsprop': torch.optim.RMSprop(model_gru.parameters(), lr=0.001),\n",
    "        'adam': torch.optim.Adam(model_gru.parameters(), lr=0.001)\n",
    "    }\n",
    "\n",
    "    # Select optimizer (can be changed to test different optimizers)\n",
    "    optimizer = optimizers['adamax']  # or 'rmsprop' or 'adam'\n",
    "\n",
    "    # Training with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model_gru, train_data_loader, optimizer, device)\n",
    "        val_loss = validate_epoch(model_gru, val_data_loader, device)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}')\n",
    "\n",
    "    # ... rest of the evaluation code ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
