{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"R5MpYxGU834N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3IylnX-p8Hl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from collections import deque\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import xml.etree.ElementTree as ET\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKspEB0pp8Hl"},"outputs":[],"source":["# import glucose_transformer\n","\n","# from glucose_transformer import (\n","#     TimeSeriesDataset,\n","#     TransformerEncoder_version2,\n","#     TransformerEncoder,\n","#     load_ohio_series_train,\n","#     create_population_splits,\n","#     create_loocv_splits,\n","#     create_4fold_splits,\n","#     split_into_continuous_series,\n","#     create_train_val_datasets,\n","#     train_model,\n","#     evaluate_model,\n","#     evaluate_and_save_metrics_population,\n","#     evaluate_and_save_metrics,\n","#     save_model,\n","#     load_model\n","# )\n","%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"]},{"cell_type":"markdown","source":["# Train on DiaTrend"],"metadata":{"id":"SPf4KPJv1pfP"}},{"cell_type":"code","source":["%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"],"metadata":{"id":"HoC7NaAb1tcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5 fold CV"],"metadata":{"id":"ZiGrmhdi1uyK"}},{"cell_type":"code","source":["def create_5fold_splits(data_path):\n","    uids = [int(file.split('.')[0].split('processed_cgm_data_Subject')[1]) for file in os.listdir(data_path)]\n","    # print(uids)\n","    splits = [0, 11, 22, 33, 44, float('inf')]\n","    fold_splits = {}\n","    for fold in range(5):\n","        fold_name = f\"fold{fold+1}\"\n","\n","        # Create test and train sets\n","        test_files = [i for i in uids if splits[fold] < i <= splits[fold+1]]\n","        # print(\"test\", test_files)\n","        train_files = [i for i in uids if i not in test_files]\n","        # print(\"train\", train_files)\n","        # Add to splits dictionary\n","        fold_splits[fold_name] = {\n","            'test': ['processed_cgm_data_Subject'+str(i)+'.csv' for i in test_files],\n","            'train': ['processed_cgm_data_Subject'+str(i)+'.csv' for i in train_files]\n","        }\n","\n","        # break\n","\n","    return fold_splits"],"metadata":{"id":"rGsQOxIg1tez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/diatrend_processed/'\n","\n","fold_splits = create_5fold_splits(data_dir)\n","print(fold_splits)"],"metadata":{"id":"aG-cewuu18wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_datetime(date_str):\n","  try:\n","    return pd.to_datetime(date_str)\n","  except ValueError:\n","    return pd.to_datetime(date_str + ' 00:00:00')"],"metadata":{"id":"VqTk5D6U2gel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_train_data_by_fold(fold_name):\n","  data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/diatrend_processed/'\n","  train_df = pd.DataFrame()\n","\n","  for file in os.listdir(data_dir):\n","    if file in fold_splits[fold_name]['train']:\n","      df = pd.read_csv(os.path.join(data_dir, file))\n","      uid = file.split('.')[0].split('processed_cgm_data_Subject')[1]\n","      df = df.rename(columns={\"date\": \"timestamp\"})\n","      df['USUBJID'] = [uid] * len(df)\n","      df['timestamp'] = df['timestamp'].apply(convert_to_datetime)\n","\n","      df = df.loc[:, ['USUBJID', 'timestamp', 'mg/dl']]\n","      train_df = pd.concat([train_df, df])\n","      # break\n","      # print(train_df.USUBJID.unique())\n","  return train_df"],"metadata":{"id":"ptOiLwPo2gem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model train"],"metadata":{"id":"btx9TejX2kMO"}},{"cell_type":"code","source":["fold_lst = fold_splits.keys()\n","print(fold_lst)\n","\n","for fold in fold_lst:\n","  train_df = load_train_data_by_fold(fold)\n","  print(fold, '\\ntrain data shape:', train_df.shape)\n","  # break\n","\n","  # Move model to GPU if available\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","  # 4. Set hyperparameters\n","  past_sequence_length = 24\n","  future_offset = 6\n","  batch_size = 64\n","  max_interval_minutes = 30\n","\n","  # 5. Train model\n","  model = TransformerEncoder_version2(\n","      past_seq_len=past_sequence_length,\n","      num_layers=1,\n","      d_model=512,\n","      nhead=4,\n","      input_dim=1,\n","      dropout=0.2\n","  )\n","  model = model.to(device)\n","\n","  # Create datasets\n","  train_series_list = []\n","  for uid in train_df['USUBJID'].unique():\n","      cur_df = train_df[train_df['USUBJID'] == uid]\n","      cur_df.drop(columns=['USUBJID'], inplace=True)\n","      series_list = split_into_continuous_series(cur_df, past_sequence_length, future_offset, max_interval_minutes)\n","      train_series_list.extend(series_list)\n","\n","  train_dataset, val_dataset = create_train_val_datasets(\n","      train_series_list,\n","      train_ratio=0.8,\n","      past_seq_len=past_sequence_length,\n","      future_offset=future_offset\n","  )\n","\n","  # Create data loaders\n","  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","  # Train model\n","  train_losses, val_losses = train_model(\n","      model=model,\n","      train_loader=train_loader,\n","      val_loader=val_loader,\n","      num_epochs=200,\n","      learning_rate=1e-3\n","  )\n","\n","  model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","  sh = 'sh'+str(past_sequence_length)\n","\n","  # Save the trained model\n","  save_dir=os.path.join(model_dir, 'saved_models_diatrend/5_fold_'+sh+'/')\n","  os.makedirs(save_dir, exist_ok=True)\n","  save_model(model, sh+'_'+fold, save_dir)\n"],"metadata":{"id":"KEfKPYn0181X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## individual evaluation"],"metadata":{"id":"4C04eBOt2msd"}},{"cell_type":"code","source":["def evaluate_and_save_metrics_diatrend(model, test_df, save_dir=\"metrics\",\n","                            past_sequence_length=7, future_offset=6,\n","                            batch_size=32, max_interval_minutes=30):\n","    \"\"\"\n","    Evaluate model performance on test data and save metrics to file.\n","\n","    Args:\n","        model: The trained model\n","        save_dir: Directory to save metrics\n","        past_sequence_length: Length of input sequence\n","        future_offset: Prediction horizon\n","        batch_size: Batch size for testing\n","        max_interval_minutes: Maximum interval between readings to consider continuous\n","    \"\"\"\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Split into continuous series\n","    test_series_list = split_into_continuous_series(test_df, past_sequence_length, future_offset,max_interval_minutes)\n","\n","    # Create dataset and dataloader\n","    test_dataset, _ = create_train_val_datasets(\n","        test_series_list,\n","        train_ratio=0.9999,\n","        past_seq_len=past_sequence_length,\n","        future_offset=future_offset\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Evaluate model\n","    model.eval()\n","    predictions = []\n","    ground_truths = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to('cuda') if torch.cuda.is_available() else inputs\n","            targets = targets.to('cuda') if torch.cuda.is_available() else targets\n","\n","            outputs = model(inputs)\n","            predictions.extend(outputs.cpu().numpy())\n","            ground_truths.extend(targets.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    predictions = np.array(predictions).flatten()\n","    ground_truths = np.array(ground_truths).flatten()\n","\n","    # Calculate metrics\n","    rmse = np.sqrt(mean_squared_error(ground_truths, predictions))\n","    mae = np.mean(np.abs(predictions - ground_truths))\n","    mape = np.mean(np.abs((ground_truths - predictions) / ground_truths)) * 100\n","\n","    # Print metrics\n","    print(f'Test file: {uid}')\n","    print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n","    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n","    print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')\n","\n","    # Save metrics to file\n","    metrics_filename = f\"metrics_{uid}.txt\"\n","    metrics_path = os.path.join(save_dir, metrics_filename)\n","\n","    with open(metrics_path, 'w') as f:\n","        f.write(f\"Test File: {uid}\\n\")\n","        f.write(f\"RMSE: {rmse:.2f}\\n\")\n","        f.write(f\"MAE: {mae:.2f}\\n\")\n","        f.write(f\"MAPE: {mape:.2f}%\\n\")\n","\n","    # Create plots\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(predictions[:200], label='Predictions', color='r')\n","    plt.plot(ground_truths[:200], label='Ground Truth', color='b')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Value')\n","    plt.title('Predictions vs Ground Truth')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(ground_truths, predictions, alpha=0.5)\n","    plt.plot([min(ground_truths), max(ground_truths)],\n","             [min(ground_truths), max(ground_truths)],\n","             'r--', label='Perfect Prediction')\n","    plt.xlabel('Ground Truth')\n","    plt.ylabel('Predictions')\n","    plt.title(f'Scatter Plot (RMSE: {rmse:.2f})')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'mape': mape,\n","        'predictions': predictions,\n","        'ground_truths': ground_truths\n","    }"],"metadata":{"id":"yV0ibSCOPdzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGoUldGhPdzI"},"outputs":[],"source":["past_sequence_length = 24\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","sh = 'sh'+str(past_sequence_length)\n","model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/diatrend_processed/'\n","\n","test_eval = []\n","for fold in fold_splits.keys():\n","  print(fold, fold_splits[fold]['test'])\n","  # Load the saved model\n","  model = load_model_population(sh+'_'+fold, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_diatrend/5_fold_'+sh+'/'))\n","\n","  for test in fold_splits[fold]['test']:\n","    test_df = pd.read_csv(os.path.join(data_dir, test))\n","    uid = test.split('.')[0].split('processed_cgm_data_Subject')[1]\n","    test_df = test_df.rename(columns={\"date\": \"timestamp\"})\n","    # test_df['USUBJID'] = [uid] * len(test_df)\n","    test_df['timestamp'] = test_df['timestamp'].apply(convert_to_datetime)\n","    test_df = test_df.loc[:, ['timestamp', 'mg/dl']]\n","    # print(test_df.shape)\n","    # break\n","    metrics = evaluate_and_save_metrics_diatrend(\n","        model=model,\n","        test_df=test_df,\n","        save_dir=os.path.join(model_dir, 'evaluation_metrics_diatrend/5_fold_individual_'+sh+'/'),\n","        past_sequence_length=past_sequence_length,\n","        future_offset=future_offset,\n","        batch_size=batch_size,\n","        max_interval_minutes=max_interval_minutes\n","    )\n","\n","    test_eval.append([uid, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","    # print(f\"\\nResults for population model:\")\n","    print(f\"RMSE: {metrics['rmse']:.2f}\")\n","    print(f\"MAE: {metrics['mae']:.2f}\")\n","    print(f\"MAPE: {metrics['mape']:.2f}%\")\n","\n","  # break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpZHbBfrPdzI"},"outputs":[],"source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_diatrend/5_fold_test_eval_'+sh+'.csv'), index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"zIbwhqgq2rVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train on T1DEXI dataset"],"metadata":{"id":"xZS2zviJfMzI"}},{"cell_type":"code","source":["%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"],"metadata":{"id":"eb1VgiEl1o3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## population (archive)"],"metadata":{"id":"FwKknMczfSKQ"}},{"cell_type":"markdown","source":["### data preprocess"],"metadata":{"id":"qBaDbRBa0BGY"}},{"cell_type":"code","source":["def convert_to_datetime(date_str):\n","  try:\n","    return pd.to_datetime(date_str)\n","  except ValueError:\n","    return pd.to_datetime(date_str + ' 00:00:00')"],"metadata":{"id":"OXVnzg4VF8gR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","\n","train_df = pd.DataFrame()\n","test_df = pd.DataFrame()\n","\n","for file in os.listdir(data_dir):\n","  if file.endswith('.csv'):\n","    df = pd.read_csv(os.path.join(data_dir, file))\n","    # df.drop(columns=['USUBJID'], inplace=True)\n","    df = df.rename(columns={\"LBORRES\": \"mg/dl\", \"LBDTC\": \"timestamp\"})\n","    df['timestamp'] = df['timestamp'].apply(convert_to_datetime)\n","    df = df.loc[:, ['USUBJID', 'timestamp', 'mg/dl']] # reorder to keep the same format as Diatrend for future training\n","    num_train = int(len(df) * 0.8)\n","    cur_train_df = df.iloc[:num_train]\n","    cur_test_df = df.iloc[num_train:]\n","    train_df = pd.concat([train_df, cur_train_df])\n","    test_df = pd.concat([test_df, cur_test_df])\n","    # break\n","\n","population_data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_population/'\n","train_df.to_csv(os.path.join(population_data_dir, 'T1DEXI_train.csv'), index=False)\n","test_df.to_csv(os.path.join(population_data_dir, 'T1DEXI_test.csv'), index=False)"],"metadata":{"id":"avCeJBqufQrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape, test_df.shape)\n","train_df.head(3)"],"metadata":{"id":"A0X8ubHroF3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model train"],"metadata":{"id":"dVQAblVM0DIQ"}},{"cell_type":"code","source":["%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"],"metadata":{"id":"0KCaZnQM8LP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["population_data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_population/'\n","\n","df = pd.read_csv(population_data_dir + 'T1DEXI_train.csv')\n","df['timestamp'] = pd.to_datetime(df['timestamp'])"],"metadata":{"id":"qjo0-BJK6tjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 4. Set hyperparameters\n","past_sequence_length = 12\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","\n","# 5. Train model\n","model = TransformerEncoder_version2(\n","    past_seq_len=past_sequence_length,\n","    num_layers=1,\n","    d_model=512,\n","    nhead=4,\n","    input_dim=1,\n","    dropout=0.2\n",")\n","model = model.to(device)\n","\n","# Create datasets\n","train_series_list = []\n","for uid in df['USUBJID'].unique():\n","    cur_df = df[df['USUBJID'] == uid]\n","    cur_df.drop(columns=['USUBJID'], inplace=True)\n","    series_list = split_into_continuous_series(cur_df, past_sequence_length, future_offset, max_interval_minutes)\n","    train_series_list.extend(series_list)\n","\n","train_dataset, val_dataset = create_train_val_datasets(\n","    train_series_list,\n","    train_ratio=0.8,\n","    past_seq_len=past_sequence_length,\n","    future_offset=future_offset\n",")"],"metadata":{"id":"tdLq3w8Jzm4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Train model\n","train_losses, val_losses = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    num_epochs=200,\n","    learning_rate=1e-3\n",")"],"metadata":{"id":"mIwwg85EDOQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","sh = 'sh'+str(past_sequence_length)\n","\n","# Save the trained model\n","save_model(model, 'population_'+sh, save_dir=os.path.join(model_dir, 'saved_models_T1DEXI/'))"],"metadata":{"id":"_vRX_nlADSki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### individaul evaluation"],"metadata":{"id":"vlXxL_BuFF0B"}},{"cell_type":"code","source":["def evaluate_and_save_metrics_T1DEXI(model, test_df, save_dir=\"metrics\",\n","                            past_sequence_length=7, future_offset=6,\n","                            batch_size=32, max_interval_minutes=30):\n","    \"\"\"\n","    Evaluate model performance on test data and save metrics to file.\n","\n","    Args:\n","        model: The trained model\n","        save_dir: Directory to save metrics\n","        past_sequence_length: Length of input sequence\n","        future_offset: Prediction horizon\n","        batch_size: Batch size for testing\n","        max_interval_minutes: Maximum interval between readings to consider continuous\n","    \"\"\"\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Split into continuous series\n","    test_series_list = split_into_continuous_series(test_df, past_sequence_length, future_offset,max_interval_minutes)\n","\n","    # Create dataset and dataloader\n","    test_dataset, _ = create_train_val_datasets(\n","        test_series_list,\n","        train_ratio=0.99,\n","        past_seq_len=past_sequence_length,\n","        future_offset=future_offset\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Evaluate model\n","    model.eval()\n","    predictions = []\n","    ground_truths = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to('cuda') if torch.cuda.is_available() else inputs\n","            targets = targets.to('cuda') if torch.cuda.is_available() else targets\n","\n","            outputs = model(inputs)\n","            predictions.extend(outputs.cpu().numpy())\n","            ground_truths.extend(targets.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    predictions = np.array(predictions).flatten()\n","    ground_truths = np.array(ground_truths).flatten()\n","\n","    # Calculate metrics\n","    rmse = np.sqrt(mean_squared_error(ground_truths, predictions))\n","    mae = np.mean(np.abs(predictions - ground_truths))\n","    mape = np.mean(np.abs((ground_truths - predictions) / ground_truths)) * 100\n","\n","    # Print metrics\n","    print(f'Test file: {uid}')\n","    print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n","    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n","    print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')\n","\n","    # Save metrics to file\n","    metrics_filename = f\"metrics_{uid}.txt\"\n","    metrics_path = os.path.join(save_dir, metrics_filename)\n","\n","    with open(metrics_path, 'w') as f:\n","        f.write(f\"Test File: {uid}\\n\")\n","        f.write(f\"RMSE: {rmse:.2f}\\n\")\n","        f.write(f\"MAE: {mae:.2f}\\n\")\n","        f.write(f\"MAPE: {mape:.2f}%\\n\")\n","\n","    # Create plots\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(predictions[:200], label='Predictions', color='r')\n","    plt.plot(ground_truths[:200], label='Ground Truth', color='b')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Value')\n","    plt.title('Predictions vs Ground Truth')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(ground_truths, predictions, alpha=0.5)\n","    plt.plot([min(ground_truths), max(ground_truths)],\n","             [min(ground_truths), max(ground_truths)],\n","             'r--', label='Perfect Prediction')\n","    plt.xlabel('Ground Truth')\n","    plt.ylabel('Predictions')\n","    plt.title(f'Scatter Plot (RMSE: {rmse:.2f})')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'mape': mape,\n","        'predictions': predictions,\n","        'ground_truths': ground_truths\n","    }"],"metadata":{"id":"Oa27w-uHGkAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"po4u1Tj4Egef"},"outputs":[],"source":["past_sequence_length = 12\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","sh = 'sh'+str(past_sequence_length)\n","\n","# Load the saved model\n","model = load_model_population('population_'+sh, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_T1DEXI'))\n","test_eval = []\n","\n","test_df = pd.read_csv(population_data_dir + 'T1DEXI_test.csv')\n","test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n","\n","# for test in population_splits['test']:\n","#     print(test)\n","for uid in test_df['USUBJID'].unique():\n","    cur_df = test_df[test_df['USUBJID'] == uid]\n","    cur_df.drop(columns=['USUBJID'], inplace=True)\n","    # Evaluate on test data individually\n","    metrics = evaluate_and_save_metrics_T1DEXI(\n","        model=model,\n","        test_df=cur_df,\n","        save_dir=os.path.join(model_dir, 'evaluation_metrics_T1DEXI/80_20_individual_'+sh+'/'),\n","        past_sequence_length=past_sequence_length,\n","        future_offset=future_offset,\n","        batch_size=batch_size,\n","        max_interval_minutes=max_interval_minutes\n","    )\n","\n","    id = uid\n","    test_eval.append([id, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","    # print(f\"\\nResults for population model:\")\n","    print(f\"RMSE: {metrics['rmse']:.2f}\")\n","    print(f\"MAE: {metrics['mae']:.2f}\")\n","    print(f\"MAPE: {metrics['mape']:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbI-8wa5Egef"},"outputs":[],"source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_T1DEXI/80_20_test_eval_'+sh+'.csv'), index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"coyiu34OD-FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5-fold cross validation"],"metadata":{"id":"Xl3r74ucfUAW"}},{"cell_type":"markdown","source":["### data split"],"metadata":{"id":"-f90oIXdjMuR"}},{"cell_type":"code","source":["# def create_5fold_splits(data_path):\n","#     uids = [file for file in os.listdir(data_path)]\n","#     total_files = len(uids)\n","\n","#     # Calculate files per fold (rounded up for the first folds)\n","#     files_per_fold = total_files // 5\n","#     remainder = total_files % 5\n","\n","#     # Create splits\n","#     fold_splits = {}\n","#     start_idx = 0\n","\n","#     for fold in range(5):\n","#         fold_name = f\"fold{fold+1}\"\n","\n","#         # Calculate number of files for this fold's test set\n","#         if fold < remainder:\n","#             current_fold_size = files_per_fold + 1\n","#         else:\n","#             current_fold_size = files_per_fold\n","\n","#         # Get test indices for this fold\n","#         end_idx = start_idx + current_fold_size\n","#         test_indices = list(range(start_idx, end_idx))\n","#         # print(test_indices)\n","\n","#         # Create test and train sets\n","#         test_files = [uids[i] for i in test_indices]\n","#         train_files = [path for i, path in enumerate(uids) if i not in test_indices]\n","\n","#         # Add to splits dictionary\n","#         fold_splits[fold_name] = {\n","#             'test': test_files,\n","#             'train': train_files\n","#         }\n","\n","#         # Update start index for next fold\n","#         start_idx = end_idx\n","#         # break\n","\n","#     return fold_splits"],"metadata":{"id":"HLHsOFZIaUor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_5fold_splits(data_path):\n","    uids = [int(file.split('.')[0]) for file in os.listdir(data_path)]\n","    print(uids)\n","    splits = [0, 248, 1201, 1348, 1459, float('inf')]\n","    fold_splits = {}\n","    for fold in range(5):\n","        fold_name = f\"fold{fold+1}\"\n","\n","        # Create test and train sets\n","        test_files = [i for i in uids if splits[fold] < i <= splits[fold+1]]\n","        # print(test_files)\n","        train_files = [i for i in uids if i not in test_files]\n","\n","        # Add to splits dictionary\n","        fold_splits[fold_name] = {\n","            'test': [str(i)+'.csv' for i in test_files],\n","            'train': [str(i)+'.csv' for i in train_files]\n","        }\n","\n","        # break\n","\n","    return fold_splits"],"metadata":{"id":"sae2QL-YzDoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","\n","fold_splits = create_5fold_splits(data_dir)\n","print(fold_splits)"],"metadata":{"id":"31CPseO_bPWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_datetime(date_str):\n","  try:\n","    return pd.to_datetime(date_str)\n","  except ValueError:\n","    return pd.to_datetime(date_str + ' 00:00:00')"],"metadata":{"id":"-26ZVsxPaEAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_train_data_by_fold(fold_name):\n","  data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","  train_df = pd.DataFrame()\n","\n","  for file in os.listdir(data_dir):\n","    if file in fold_splits[fold_name]['train']:\n","      df = pd.read_csv(os.path.join(data_dir, file))\n","      # df.drop(columns=['USUBJID'], inplace=True)\n","      df = df.rename(columns={\"LBORRES\": \"mg/dl\", \"LBDTC\": \"timestamp\"})\n","      df['timestamp'] = df['timestamp'].apply(convert_to_datetime)\n","      df = df.loc[:, ['USUBJID', 'timestamp', 'mg/dl']] # reorder to keep the same format as Diatrend for future training\n","\n","      train_df = pd.concat([train_df, df])\n","      # break\n","  return train_df"],"metadata":{"id":"nDYNp1bbeu-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### model train"],"metadata":{"id":"NrrTsa4YjSSw"}},{"cell_type":"code","source":["%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"],"metadata":{"id":"22xe15MNd0kw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fold_lst = fold_splits.keys()\n","print(fold_lst)\n","\n","for fold in fold_lst:\n","  train_df = load_train_data_by_fold(fold)\n","  print(fold, '\\ntrain data shape:', train_df.shape)\n","\n","  # Move model to GPU if available\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","  # 4. Set hyperparameters\n","  past_sequence_length = 24\n","  future_offset = 6\n","  batch_size = 64\n","  max_interval_minutes = 30\n","\n","  # 5. Train model\n","  model = TransformerEncoder_version2(\n","      past_seq_len=past_sequence_length,\n","      num_layers=1,\n","      d_model=512,\n","      nhead=4,\n","      input_dim=1,\n","      dropout=0.2\n","  )\n","  model = model.to(device)\n","\n","  # Create datasets\n","  train_series_list = []\n","  for uid in train_df['USUBJID'].unique():\n","      cur_df = train_df[train_df['USUBJID'] == uid]\n","      cur_df.drop(columns=['USUBJID'], inplace=True)\n","      series_list = split_into_continuous_series(cur_df, past_sequence_length, future_offset, max_interval_minutes)\n","      train_series_list.extend(series_list)\n","\n","  train_dataset, val_dataset = create_train_val_datasets(\n","      train_series_list,\n","      train_ratio=0.8,\n","      past_seq_len=past_sequence_length,\n","      future_offset=future_offset\n","  )\n","\n","  # Create data loaders\n","  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","  # Train model\n","  train_losses, val_losses = train_model(\n","      model=model,\n","      train_loader=train_loader,\n","      val_loader=val_loader,\n","      num_epochs=200,\n","      learning_rate=1e-3\n","  )\n","\n","  model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","  sh = 'sh'+str(past_sequence_length)\n","\n","  # Save the trained model\n","  save_dir=os.path.join(model_dir, 'saved_models_T1DEXI/5_fold_'+sh+'/')\n","  os.makedirs(save_dir, exist_ok=True)\n","  save_model(model, sh+'_'+fold, save_dir)\n","\n","  # break"],"metadata":{"id":"qwY57cfDaEAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### individual evaluation"],"metadata":{"id":"QkD4k75JjWJM"}},{"cell_type":"code","source":["def evaluate_and_save_metrics_T1DEXI(model, test_df, save_dir=\"metrics\",\n","                            past_sequence_length=7, future_offset=6,\n","                            batch_size=32, max_interval_minutes=30):\n","    \"\"\"\n","    Evaluate model performance on test data and save metrics to file.\n","\n","    Args:\n","        model: The trained model\n","        save_dir: Directory to save metrics\n","        past_sequence_length: Length of input sequence\n","        future_offset: Prediction horizon\n","        batch_size: Batch size for testing\n","        max_interval_minutes: Maximum interval between readings to consider continuous\n","    \"\"\"\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Split into continuous series\n","    test_series_list = split_into_continuous_series(test_df, past_sequence_length, future_offset,max_interval_minutes)\n","\n","    # Create dataset and dataloader\n","    test_dataset, _ = create_train_val_datasets(\n","        test_series_list,\n","        train_ratio=0.9999,\n","        past_seq_len=past_sequence_length,\n","        future_offset=future_offset\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Evaluate model\n","    model.eval()\n","    predictions = []\n","    ground_truths = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to('cuda') if torch.cuda.is_available() else inputs\n","            targets = targets.to('cuda') if torch.cuda.is_available() else targets\n","\n","            outputs = model(inputs)\n","            predictions.extend(outputs.cpu().numpy())\n","            ground_truths.extend(targets.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    predictions = np.array(predictions).flatten()\n","    ground_truths = np.array(ground_truths).flatten()\n","\n","    # Calculate metrics\n","    rmse = np.sqrt(mean_squared_error(ground_truths, predictions))\n","    mae = np.mean(np.abs(predictions - ground_truths))\n","    mape = np.mean(np.abs((ground_truths - predictions) / ground_truths)) * 100\n","\n","    # Print metrics\n","    print(f'Test file: {uid}')\n","    print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n","    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n","    print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')\n","\n","    # Save metrics to file\n","    metrics_filename = f\"metrics_{uid}.txt\"\n","    metrics_path = os.path.join(save_dir, metrics_filename)\n","\n","    with open(metrics_path, 'w') as f:\n","        f.write(f\"Test File: {uid}\\n\")\n","        f.write(f\"RMSE: {rmse:.2f}\\n\")\n","        f.write(f\"MAE: {mae:.2f}\\n\")\n","        f.write(f\"MAPE: {mape:.2f}%\\n\")\n","\n","    # Create plots\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(predictions[:200], label='Predictions', color='r')\n","    plt.plot(ground_truths[:200], label='Ground Truth', color='b')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Value')\n","    plt.title('Predictions vs Ground Truth')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(ground_truths, predictions, alpha=0.5)\n","    plt.plot([min(ground_truths), max(ground_truths)],\n","             [min(ground_truths), max(ground_truths)],\n","             'r--', label='Perfect Prediction')\n","    plt.xlabel('Ground Truth')\n","    plt.ylabel('Predictions')\n","    plt.title(f'Scatter Plot (RMSE: {rmse:.2f})')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'mape': mape,\n","        'predictions': predictions,\n","        'ground_truths': ground_truths\n","    }"],"metadata":{"id":"Xiy9k0e4ydj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def load_test_data_individual(fold_name):\n","#   data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","#   train_df = pd.DataFrame()\n","\n","#   for file in os.listdir(data_dir):\n","#     if file in fold_splits[fold_name]['train']:\n","#       df = pd.read_csv(os.path.join(data_dir, file))\n","#       # df.drop(columns=['USUBJID'], inplace=True)\n","#       df = df.rename(columns={\"LBORRES\": \"mg/dl\", \"LBDTC\": \"timestamp\"})\n","#       df['timestamp'] = df['timestamp'].apply(convert_to_datetime)\n","#       df = df.loc[:, ['USUBJID', 'timestamp', 'mg/dl']] # reorder to keep the same format as Diatrend for future training\n","\n","#       train_df = pd.concat([train_df, df])\n","#       # break\n","#   return train_df"],"metadata":{"id":"phVOOM_6zr6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lpHyWeMydkA"},"outputs":[],"source":["past_sequence_length = 24\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","sh = 'sh'+str(past_sequence_length)\n","model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","\n","test_eval = []\n","for fold in fold_splits.keys():\n","  print(fold, fold_splits[fold]['test'])\n","  # Load the saved model\n","  model = load_model_population(sh+'_'+fold, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_T1DEXI/5_fold_'+sh+'/'))\n","\n","  for test in fold_splits[fold]['test']:\n","    uid = test.split('.')[0]\n","    test_df = pd.read_csv(os.path.join(data_dir, test))\n","    test_df = test_df.rename(columns={\"LBORRES\": \"mg/dl\", \"LBDTC\": \"timestamp\"})\n","    test_df['timestamp'] = test_df['timestamp'].apply(convert_to_datetime)\n","    test_df = test_df.loc[:, ['timestamp', 'mg/dl']]\n","    # print(test_df.shape)\n","    # break\n","    metrics = evaluate_and_save_metrics_T1DEXI(\n","        model=model,\n","        test_df=test_df,\n","        save_dir=os.path.join(model_dir, 'evaluation_metrics_T1DEXI/5_fold_individual_'+sh+'/'),\n","        past_sequence_length=past_sequence_length,\n","        future_offset=future_offset,\n","        batch_size=batch_size,\n","        max_interval_minutes=max_interval_minutes\n","    )\n","\n","    test_eval.append([uid, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","    # print(f\"\\nResults for population model:\")\n","    print(f\"RMSE: {metrics['rmse']:.2f}\")\n","    print(f\"MAE: {metrics['mae']:.2f}\")\n","    print(f\"MAPE: {metrics['mape']:.2f}%\")\n","\n","  # break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVs364KxydkA"},"outputs":[],"source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_T1DEXI/5_fold_test_eval_'+sh+'.csv'), index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"K2DsbUVAix_0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DUk0Ze-Op8Hl"},"source":["# Train on Ohio dataset"]},{"cell_type":"code","source":["%run /content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/glucose_transformer.py"],"metadata":{"id":"Ov-1kSg-tALK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fghOoKSjp8Hm"},"source":["## Population data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAXQudrUp8Hm"},"outputs":[],"source":["data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/'\n","folder_path_train_2018 = os.path.join(data_dir, \"./OhioT1DM 2020/2018/train\")\n","folder_path_train_2020 = os.path.join(data_dir,\"./OhioT1DM 2020/2020/train\")\n","train_files_2018 = [f for f in os.listdir(folder_path_train_2018) if f.endswith('.xml')]\n","train_files_2020 = [f for f in os.listdir(folder_path_train_2020) if f.endswith('.xml')]\n","\n","folder_path_test_2018 = os.path.join(data_dir,\"./OhioT1DM 2020/2018/test\")\n","folder_path_test_2020 = os.path.join(data_dir,\"./OhioT1DM 2020/2020/test\")\n","test_files_2018 = [f for f in os.listdir(folder_path_test_2018) if f.endswith('.xml')]\n","test_files_2020 = [f for f in os.listdir(folder_path_test_2020) if f.endswith('.xml')]\n","\n","population_splits = create_population_splits(\n","    folder_path_train_2018,\n","    folder_path_train_2020,\n","    train_files_2018,\n","    train_files_2020,\n","    folder_path_test_2018,\n","    folder_path_test_2020,\n","    test_files_2018,\n","    test_files_2020\n",")\n","\n","print(population_splits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9lSt2SDp8Hm"},"outputs":[],"source":["# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 4. Set hyperparameters\n","past_sequence_length = 24\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","\n","# 5. Train model\n","model = TransformerEncoder_version2(\n","    past_seq_len=past_sequence_length,\n","    num_layers=1,\n","    d_model=512,\n","    nhead=4,\n","    input_dim=1,\n","    dropout=0.2\n",")\n","model = model.to(device)\n","\n","# Load and process training data\n","train_dfs = []\n","for train_file in population_splits['train']:\n","    df = load_ohio_series_train(train_file, \"glucose_level\", \"value\")\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    train_dfs.append(df)\n","\n","# Create datasets\n","train_series_list = []\n","for df in train_dfs:\n","    series_list = split_into_continuous_series(df, past_sequence_length, future_offset, max_interval_minutes)\n","    train_series_list.extend(series_list)\n","\n","train_dataset, val_dataset = create_train_val_datasets(\n","    train_series_list,\n","    train_ratio=0.8,\n","    past_seq_len=past_sequence_length,\n","    future_offset=future_offset\n",")\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Train model\n","train_losses, val_losses = train_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    num_epochs=200,\n","    learning_rate=1e-3\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Je-HjZcsp8Hm"},"outputs":[],"source":["model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","sh = 'sh'+str(past_sequence_length)\n","\n","# Save the trained model\n","save_model(model, 'population_'+sh, save_dir=os.path.join(model_dir, 'saved_models_original_ohio/'))"]},{"cell_type":"code","source":["past_sequence_length = 24\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","\n","sh = 'sh'+str(past_sequence_length)\n","model = load_model_population('population_'+sh, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_original_ohio'))\n","\n","# Evaluate on test data\n","metrics = evaluate_and_save_metrics_population(\n","    model=model,\n","    test_file_path=population_splits['test'],\n","    save_dir=os.path.join(model_dir, 'evaluation_metrics_original_ohio'),\n","    past_sequence_length=past_sequence_length,\n","    future_offset=future_offset,\n","    batch_size=batch_size,\n","    max_interval_minutes=max_interval_minutes\n",")\n","\n","# evaluation on whole test set\n","print(f\"RMSE: {metrics['rmse']:.2f}\")\n","print(f\"MAE: {metrics['mae']:.2f}\")\n","print(f\"MAPE: {metrics['mape']:.2f}%\")"],"metadata":{"id":"wqLXr-RFIY-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6MVL9XXp8Hm"},"outputs":[],"source":["# Load the saved model\n","model = load_model_population('population_'+sh, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_original_ohio'))\n","test_eval = []\n","\n","for test in population_splits['test']:\n","    print(test)\n","    # Evaluate on test data individually\n","    metrics = evaluate_and_save_metrics(\n","        model=model,\n","        test_file_path=test,\n","        save_dir=os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_'+sh+'/'),\n","        past_sequence_length=past_sequence_length,\n","        future_offset=future_offset,\n","        batch_size=batch_size,\n","        max_interval_minutes=max_interval_minutes\n","    )\n","\n","    id = test.split('/')[-1].split('-')[0]\n","    test_eval.append([id, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","    # print(f\"\\nResults for population model:\")\n","    print(f\"RMSE: {metrics['rmse']:.2f}\")\n","    print(f\"MAE: {metrics['mae']:.2f}\")\n","    print(f\"MAPE: {metrics['mape']:.2f}%\")"]},{"cell_type":"markdown","source":["### save individual results to .csv file"],"metadata":{"id":"gUR9U7Vi0O6q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaN4vBfb81B2"},"outputs":[],"source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_test_eval_'+sh+'.csv'), index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"VGlG3lKMzGjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vHmBcfW2zGnN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ohio 60 mins model train on diatrend and T1DEXI"],"metadata":{"id":"xx_O-l4hPWzp"}},{"cell_type":"code","source":["def convert_to_datetime(date_str):\n","  try:\n","    return pd.to_datetime(date_str)\n","  except ValueError:\n","    return pd.to_datetime(date_str + ' 00:00:00')"],"metadata":{"id":"5EE9cRhLrhJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["past_sequence_length = 12\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","\n","model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","\n","sh = 'sh'+str(past_sequence_length)\n","model = load_model_population('population_'+sh, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_original_ohio'))"],"metadata":{"id":"AIwCR3CyPd9v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Diatrend"],"metadata":{"id":"pEulFTo3tL9c"}},{"cell_type":"code","source":["def evaluate_and_save_metrics_diatrend(model, test_df, save_dir=\"metrics\",\n","                            past_sequence_length=7, future_offset=6,\n","                            batch_size=32, max_interval_minutes=30):\n","    \"\"\"\n","    Evaluate model performance on test data and save metrics to file.\n","\n","    Args:\n","        model: The trained model\n","        save_dir: Directory to save metrics\n","        past_sequence_length: Length of input sequence\n","        future_offset: Prediction horizon\n","        batch_size: Batch size for testing\n","        max_interval_minutes: Maximum interval between readings to consider continuous\n","    \"\"\"\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Split into continuous series\n","    test_series_list = split_into_continuous_series(test_df, past_sequence_length, future_offset,max_interval_minutes)\n","\n","    # Create dataset and dataloader\n","    test_dataset, _ = create_train_val_datasets(\n","        test_series_list,\n","        train_ratio=0.9999,\n","        past_seq_len=past_sequence_length,\n","        future_offset=future_offset\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Evaluate model\n","    model.eval()\n","    predictions = []\n","    ground_truths = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to('cuda') if torch.cuda.is_available() else inputs\n","            targets = targets.to('cuda') if torch.cuda.is_available() else targets\n","\n","            outputs = model(inputs)\n","            predictions.extend(outputs.cpu().numpy())\n","            ground_truths.extend(targets.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    predictions = np.array(predictions).flatten()\n","    ground_truths = np.array(ground_truths).flatten()\n","\n","    # Calculate metrics\n","    rmse = np.sqrt(mean_squared_error(ground_truths, predictions))\n","    mae = np.mean(np.abs(predictions - ground_truths))\n","    mape = np.mean(np.abs((ground_truths - predictions) / ground_truths)) * 100\n","\n","    # Print metrics\n","    print(f'Test file: {uid}')\n","    print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n","    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n","    print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')\n","\n","    # Save metrics to file\n","    metrics_filename = f\"metrics_{uid}.txt\"\n","    metrics_path = os.path.join(save_dir, metrics_filename)\n","\n","    with open(metrics_path, 'w') as f:\n","        f.write(f\"Test File: {uid}\\n\")\n","        f.write(f\"RMSE: {rmse:.2f}\\n\")\n","        f.write(f\"MAE: {mae:.2f}\\n\")\n","        f.write(f\"MAPE: {mape:.2f}%\\n\")\n","\n","    # Create plots\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(predictions[:200], label='Predictions', color='r')\n","    plt.plot(ground_truths[:200], label='Ground Truth', color='b')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Value')\n","    plt.title('Predictions vs Ground Truth')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(ground_truths, predictions, alpha=0.5)\n","    plt.plot([min(ground_truths), max(ground_truths)],\n","             [min(ground_truths), max(ground_truths)],\n","             'r--', label='Perfect Prediction')\n","    plt.xlabel('Ground Truth')\n","    plt.ylabel('Predictions')\n","    plt.title(f'Scatter Plot (RMSE: {rmse:.2f})')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'mape': mape,\n","        'predictions': predictions,\n","        'ground_truths': ground_truths\n","    }"],"metadata":{"id":"zfQ85yVUtN2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["past_sequence_length = 12\n","future_offset = 6\n","batch_size = 64\n","max_interval_minutes = 30\n","\n","model_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/gdrive_version/'\n","\n","sh = 'sh'+str(past_sequence_length)\n","model = load_model_population('population_'+sh, past_sequence_length, save_dir=os.path.join(model_dir, 'saved_models_original_ohio'))"],"metadata":{"id":"LmN3_KuIt_B2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/diatrend_processed/'\n","test_eval = []\n","\n","for test in os.listdir(data_dir):\n","  test_df = pd.read_csv(os.path.join(data_dir, test))\n","  uid = test.split('.')[0].split('processed_cgm_data_Subject')[1]\n","  test_df = test_df.rename(columns={\"date\": \"timestamp\"})\n","  # test_df['USUBJID'] = [uid] * len(test_df)\n","  test_df['timestamp'] = test_df['timestamp'].apply(convert_to_datetime)\n","  test_df = test_df.loc[:, ['timestamp', 'mg/dl']]\n","  # print(test_df.shape)\n","  # break\n","  metrics = evaluate_and_save_metrics_diatrend(\n","      model=model,\n","      test_df=test_df,\n","      save_dir=os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_t1dexi_diatrend/diatrend_'+sh+'/'),\n","      past_sequence_length=past_sequence_length,\n","      future_offset=future_offset,\n","      batch_size=batch_size,\n","      max_interval_minutes=max_interval_minutes\n","  )\n","\n","  test_eval.append([uid, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","  # print(f\"\\nResults for population model:\")\n","  print(f\"RMSE: {metrics['rmse']:.2f}\")\n","  print(f\"MAE: {metrics['mae']:.2f}\")\n","  print(f\"MAPE: {metrics['mape']:.2f}%\")\n"],"metadata":{"id":"mGI9bwL4tO0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_t1dexi_diatrend/diatrend_eval_'+sh+'.csv'), index=False)"],"metadata":{"id":"8KW7q7WotPIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T1DEXI"],"metadata":{"id":"r06lPkbFtIP6"}},{"cell_type":"code","source":["def evaluate_and_save_metrics_T1DEXI(model, test_df, save_dir=\"metrics\",\n","                            past_sequence_length=7, future_offset=6,\n","                            batch_size=32, max_interval_minutes=30):\n","    \"\"\"\n","    Evaluate model performance on test data and save metrics to file.\n","\n","    Args:\n","        model: The trained model\n","        save_dir: Directory to save metrics\n","        past_sequence_length: Length of input sequence\n","        future_offset: Prediction horizon\n","        batch_size: Batch size for testing\n","        max_interval_minutes: Maximum interval between readings to consider continuous\n","    \"\"\"\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Split into continuous series\n","    test_series_list = split_into_continuous_series(test_df, past_sequence_length, future_offset,max_interval_minutes)\n","\n","    # Create dataset and dataloader\n","    test_dataset, _ = create_train_val_datasets(\n","        test_series_list,\n","        train_ratio=0.9999,\n","        past_seq_len=past_sequence_length,\n","        future_offset=future_offset\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Evaluate model\n","    model.eval()\n","    predictions = []\n","    ground_truths = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs = inputs.to('cuda') if torch.cuda.is_available() else inputs\n","            targets = targets.to('cuda') if torch.cuda.is_available() else targets\n","\n","            outputs = model(inputs)\n","            predictions.extend(outputs.cpu().numpy())\n","            ground_truths.extend(targets.cpu().numpy())\n","\n","    # Convert to numpy arrays\n","    predictions = np.array(predictions).flatten()\n","    ground_truths = np.array(ground_truths).flatten()\n","\n","    # Calculate metrics\n","    rmse = np.sqrt(mean_squared_error(ground_truths, predictions))\n","    mae = np.mean(np.abs(predictions - ground_truths))\n","    mape = np.mean(np.abs((ground_truths - predictions) / ground_truths)) * 100\n","\n","    # Print metrics\n","    print(f'Test file: {uid}')\n","    print(f'Root Mean Square Error (RMSE): {rmse:.2f}')\n","    print(f'Mean Absolute Error (MAE): {mae:.2f}')\n","    print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')\n","\n","    # Save metrics to file\n","    metrics_filename = f\"metrics_{uid}.txt\"\n","    metrics_path = os.path.join(save_dir, metrics_filename)\n","\n","    with open(metrics_path, 'w') as f:\n","        f.write(f\"Test File: {uid}\\n\")\n","        f.write(f\"RMSE: {rmse:.2f}\\n\")\n","        f.write(f\"MAE: {mae:.2f}\\n\")\n","        f.write(f\"MAPE: {mape:.2f}%\\n\")\n","\n","    # Create plots\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(predictions[:200], label='Predictions', color='r')\n","    plt.plot(ground_truths[:200], label='Ground Truth', color='b')\n","    plt.xlabel('Sample')\n","    plt.ylabel('Value')\n","    plt.title('Predictions vs Ground Truth')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.scatter(ground_truths, predictions, alpha=0.5)\n","    plt.plot([min(ground_truths), max(ground_truths)],\n","             [min(ground_truths), max(ground_truths)],\n","             'r--', label='Perfect Prediction')\n","    plt.xlabel('Ground Truth')\n","    plt.ylabel('Predictions')\n","    plt.title(f'Scatter Plot (RMSE: {rmse:.2f})')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return {\n","        'rmse': rmse,\n","        'mae': mae,\n","        'mape': mape,\n","        'predictions': predictions,\n","        'ground_truths': ground_truths\n","    }"],"metadata":{"id":"DFYWTHHbRejb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_eval = []\n","# for fold in fold_splits.keys():\n","#   print(fold, fold_splits[fold]['test'])\n","data_dir = '/content/drive/Shareddrives/Yanjun/ReproGenBG/ReproGenBG_Dataset/T1DEXI_processed/'\n","\n","for test in os.listdir(data_dir):\n","  # print(test)\n","  # # for test in fold_splits[fold]['test']:\n","  uid = test.split('.')[0]\n","  test_df = pd.read_csv(os.path.join(data_dir, test))\n","  test_df = test_df.rename(columns={\"LBORRES\": \"mg/dl\", \"LBDTC\": \"timestamp\"})\n","  test_df['timestamp'] = test_df['timestamp'].apply(convert_to_datetime)\n","  test_df = test_df.loc[:, ['timestamp', 'mg/dl']]\n","  # print(test_df.shape)\n","  # break\n","  metrics = evaluate_and_save_metrics_T1DEXI(\n","      model=model,\n","      test_df=test_df,\n","      save_dir=os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_t1dexi_diatrend/t1dexi_'+sh+'/'),\n","      past_sequence_length=past_sequence_length,\n","      future_offset=future_offset,\n","      batch_size=batch_size,\n","      max_interval_minutes=max_interval_minutes\n","  )\n","\n","  test_eval.append([uid, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n","\n","  # print(f\"\\nResults for population model:\")\n","  print(f\"RMSE: {metrics['rmse']:.2f}\")\n","  print(f\"MAE: {metrics['mae']:.2f}\")\n","  print(f\"MAPE: {metrics['mape']:.2f}%\")"],"metadata":{"id":"5krMAJg0pbiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_eval)\n","df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n","df.to_csv(os.path.join(model_dir, 'evaluation_metrics_original_ohio/individual_t1dexi_diatrend/t1dexi_eval_'+sh+'.csv'), index=False)"],"metadata":{"id":"yuudmQKIqXiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lId-JOSSsPy5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}