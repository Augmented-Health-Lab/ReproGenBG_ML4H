{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K3IylnX-p8Hl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AKspEB0pp8Hl"
   },
   "outputs": [],
   "source": [
    "# Import everything from glucose_transformer\n",
    "from glucose_transformer import (\n",
    "    TimeSeriesDataset,\n",
    "    TransformerEncoder_version2,\n",
    "    TransformerEncoder,\n",
    "    load_ohio_series_train,\n",
    "    create_population_splits,\n",
    "    create_loocv_splits,\n",
    "    create_4fold_splits,\n",
    "    split_into_continuous_series,\n",
    "    create_train_val_datasets,\n",
    "    train_model,\n",
    "    evaluate_model,\n",
    "    evaluate_and_save_metrics_population,\n",
    "    evaluate_and_save_metrics,\n",
    "    save_model,\n",
    "    load_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUk0Ze-Op8Hl"
   },
   "source": [
    "# Train on Ohio dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fghOoKSjp8Hm"
   },
   "source": [
    "## Population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAXQudrUp8Hm",
    "outputId": "137c8cf3-721b-413e-b360-b8a9757b4f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file: ['./OhioT1DM 2020/2018/test/559-ws-testing.xml', './OhioT1DM 2020/2018/test/588-ws-testing.xml', './OhioT1DM 2020/2018/test/570-ws-testing.xml', './OhioT1DM 2020/2018/test/563-ws-testing.xml', './OhioT1DM 2020/2018/test/591-ws-testing.xml', './OhioT1DM 2020/2018/test/575-ws-testing.xml', './OhioT1DM 2020/2020/test/552-ws-testing.xml', './OhioT1DM 2020/2020/test/540-ws-testing.xml', './OhioT1DM 2020/2020/test/544-ws-testing.xml', './OhioT1DM 2020/2020/test/596-ws-testing.xml', './OhioT1DM 2020/2020/test/584-ws-testing.xml', './OhioT1DM 2020/2020/test/567-ws-testing.xml']\n",
      "Number of training files: 12\n",
      "Training files:\n",
      "  575-ws-training.xml\n",
      "  563-ws-training.xml\n",
      "  559-ws-training.xml\n",
      "{'train': ['./OhioT1DM 2020/2018/train/575-ws-training.xml', './OhioT1DM 2020/2018/train/563-ws-training.xml', './OhioT1DM 2020/2018/train/559-ws-training.xml', './OhioT1DM 2020/2018/train/588-ws-training.xml', './OhioT1DM 2020/2018/train/570-ws-training.xml', './OhioT1DM 2020/2018/train/591-ws-training.xml', './OhioT1DM 2020/2020/train/584-ws-training.xml', './OhioT1DM 2020/2020/train/540-ws-training.xml', './OhioT1DM 2020/2020/train/596-ws-training.xml', './OhioT1DM 2020/2020/train/567-ws-training.xml', './OhioT1DM 2020/2020/train/552-ws-training.xml', './OhioT1DM 2020/2020/train/544-ws-training.xml'], 'test': ['./OhioT1DM 2020/2018/test/559-ws-testing.xml', './OhioT1DM 2020/2018/test/588-ws-testing.xml', './OhioT1DM 2020/2018/test/570-ws-testing.xml', './OhioT1DM 2020/2018/test/563-ws-testing.xml', './OhioT1DM 2020/2018/test/591-ws-testing.xml', './OhioT1DM 2020/2018/test/575-ws-testing.xml', './OhioT1DM 2020/2020/test/552-ws-testing.xml', './OhioT1DM 2020/2020/test/540-ws-testing.xml', './OhioT1DM 2020/2020/test/544-ws-testing.xml', './OhioT1DM 2020/2020/test/596-ws-testing.xml', './OhioT1DM 2020/2020/test/584-ws-testing.xml', './OhioT1DM 2020/2020/test/567-ws-testing.xml']}\n"
     ]
    }
   ],
   "source": [
    "folder_path_train_2018 = f\"./OhioT1DM 2020/2018/train\"\n",
    "folder_path_train_2020 = \"./OhioT1DM 2020/2020/train\"\n",
    "train_files_2018 = [f for f in os.listdir(folder_path_train_2018) if f.endswith('.xml')]\n",
    "train_files_2020 = [f for f in os.listdir(folder_path_train_2020) if f.endswith('.xml')]\n",
    "\n",
    "folder_path_test_2018 = f\"./OhioT1DM 2020/2018/test\"\n",
    "folder_path_test_2020 = \"./OhioT1DM 2020/2020/test\"\n",
    "test_files_2018 = [f for f in os.listdir(folder_path_test_2018) if f.endswith('.xml')]\n",
    "test_files_2020 = [f for f in os.listdir(folder_path_test_2020) if f.endswith('.xml')]\n",
    "\n",
    "population_splits = create_population_splits(\n",
    "    folder_path_train_2018,\n",
    "    folder_path_train_2020,\n",
    "    train_files_2018,\n",
    "    train_files_2020,\n",
    "    folder_path_test_2018,\n",
    "    folder_path_test_2020,\n",
    "    test_files_2018,\n",
    "    test_files_2020\n",
    ")\n",
    "\n",
    "print(population_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T9lSt2SDp8Hm",
    "outputId": "8a00d462-192d-40fb-be51-80d66e0a396b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanjuncui/Library/Python/3.8/lib/python/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 4342.886330, Val Loss: 3841.350531\n",
      "Epoch [6/200], Train Loss: 3613.103631, Val Loss: 3871.530792\n",
      "Epoch [11/200], Train Loss: 3614.899665, Val Loss: 3803.747021\n",
      "Epoch [16/200], Train Loss: 851.203074, Val Loss: 698.923365\n",
      "Epoch [21/200], Train Loss: 775.662844, Val Loss: 819.003807\n",
      "Epoch [26/200], Train Loss: 700.592793, Val Loss: 844.026089\n",
      "Epoch [31/200], Train Loss: 670.113331, Val Loss: 537.026765\n",
      "Epoch [36/200], Train Loss: 605.341914, Val Loss: 503.041718\n",
      "Epoch [41/200], Train Loss: 610.231651, Val Loss: 507.908971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-yanjun.cui.gr@dartmouth.edu/Shared drives/Yanjun/ReproGenBG/ReproGenBG_ML4H/GlucoseTransformer/glucose_transformer.py:440\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    438\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    439\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m--> 440\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m    443\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 4. Set hyperparameters\n",
    "past_sequence_length = 12\n",
    "future_offset = 6\n",
    "batch_size = 64\n",
    "max_interval_minutes = 30\n",
    "\n",
    "# 5. Train model\n",
    "model = TransformerEncoder_version2(\n",
    "    past_seq_len=past_sequence_length,\n",
    "    num_layers=1,\n",
    "    d_model=512,\n",
    "    nhead=4,\n",
    "    input_dim=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load and process training data\n",
    "train_dfs = []\n",
    "for train_file in population_splits['train']:\n",
    "    df = load_ohio_series_train(train_file, \"glucose_level\", \"value\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    train_dfs.append(df)\n",
    "\n",
    "# Create datasets\n",
    "train_series_list = []\n",
    "for df in train_dfs:\n",
    "    series_list = split_into_continuous_series(df, past_sequence_length, future_offset, max_interval_minutes)\n",
    "    train_series_list.extend(series_list)\n",
    "\n",
    "train_dataset, val_dataset = create_train_val_datasets(\n",
    "    train_series_list,\n",
    "    train_ratio=0.8,\n",
    "    past_seq_len=past_sequence_length,\n",
    "    future_offset=future_offset\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Train model\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=200,\n",
    "    learning_rate=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Je-HjZcsp8Hm",
    "outputId": "e0f68fb9-1dcc-4b24-9894-b2d88bea0729"
   },
   "outputs": [],
   "source": [
    "# 4. Set hyperparameters\n",
    "past_sequence_length = 12\n",
    "future_offset = 6\n",
    "batch_size = 64\n",
    "max_interval_minutes = 30\n",
    "test_eval = []\n",
    "\n",
    "# Evaluate on test data\n",
    "metrics = evaluate_and_save_metrics_population(\n",
    "    model=model,\n",
    "    test_file_path=population_splits['test'],\n",
    "    save_dir='evaluation_metrics',\n",
    "    past_sequence_length=past_sequence_length,\n",
    "    future_offset=future_offset,\n",
    "    batch_size=batch_size,\n",
    "    max_interval_minutes=max_interval_minutes\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model, 'population_version2', save_dir='saved_models_original_ohio')\n",
    "\n",
    "# evaluation on whole test set\n",
    "print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6MVL9XXp8Hm",
    "outputId": "ebfc8e65-8af5-48b4-c57a-987d011e7fd2"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = load_model('population_version2', save_dir='saved_models_original_ohio')\n",
    "\n",
    "# 4. Set hyperparameters\n",
    "past_sequence_length = 12\n",
    "future_offset = 6\n",
    "batch_size = 64\n",
    "max_interval_minutes = 30\n",
    "test_eval = []\n",
    "\n",
    "for test in population_splits['test']:\n",
    "    print(test)\n",
    "    # Evaluate on test data individually \n",
    "    metrics = evaluate_and_save_metrics(\n",
    "        model=model,\n",
    "        test_file_path=test,\n",
    "        save_dir='evaluation_metrics',\n",
    "        past_sequence_length=past_sequence_length,\n",
    "        future_offset=future_offset,\n",
    "        batch_size=batch_size,\n",
    "        max_interval_minutes=max_interval_minutes\n",
    "    )\n",
    "\n",
    "    id = test.split('/')[-1].split('-')[0]\n",
    "    test_eval.append([id, round(metrics['rmse'], 2), round(metrics['mae'], 2), round(metrics['mape'], 2)])\n",
    "\n",
    "    # print(f\"\\nResults for population model:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_eval)\n",
    "df = pd.DataFrame(test_eval, columns=['test patient', 'RMSE', 'MAE', 'MAPE'])\n",
    "df.to_csv('./evaluation_metrics/individual_test_eval2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zehQiy8Hp8Hn"
   },
   "source": [
    "## Leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V6yCFj4p8Hn",
    "outputId": "63ea3182-1127-4ce5-aeea-3a3c10f4b1e6"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# 1. Set up paths and get file lists\n",
    "folder_path_train_2018 = f\"./OhioT1DM 2020/2018/train\"\n",
    "folder_path_train_2020 = \"./OhioT1DM 2020/2020/train\"\n",
    "train_files_2018 = [f for f in os.listdir(folder_path_train_2018) if f.endswith('.xml')]\n",
    "train_files_2020 = [f for f in os.listdir(folder_path_train_2020) if f.endswith('.xml')]\n",
    "\n",
    "# 2. Create LOOCV splits\n",
    "loocv_splits = create_loocv_splits(\n",
    "    folder_path_train_2018,\n",
    "    folder_path_train_2020,\n",
    "    train_files_2018,\n",
    "    train_files_2020\n",
    ")\n",
    "\n",
    "# 3. Initialize model\n",
    "# model = TransformerEncoder(\n",
    "#     num_layers=3,\n",
    "#     d_model=64,\n",
    "#     nhead=4,\n",
    "#     input_dim=1,\n",
    "#     dim_feedforward=256,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 4. Set hyperparameters\n",
    "past_sequence_length = 12\n",
    "future_offset = 6\n",
    "batch_size = 64\n",
    "max_interval_minutes = 30\n",
    "\n",
    "# Get the items starting from fold9\n",
    "\n",
    "fold_items = dict(list(loocv_splits.items())[1:])\n",
    "\n",
    "# 5. Train and evaluate for each fold\n",
    "for fold_name, fold_data in fold_items.items():\n",
    "    model = TransformerEncoder(\n",
    "        num_layers=1,\n",
    "        d_model=512,\n",
    "        nhead=4,\n",
    "        input_dim=1,\n",
    "        dim_feedforward=256,\n",
    "        dropout=0.2\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(f\"\\nProcessing {fold_name}...\")\n",
    "\n",
    "    # Load and process training data\n",
    "    train_dfs = []\n",
    "    for train_file in fold_data['train']:\n",
    "        df = load_ohio_series_train(train_file, \"glucose_level\", \"value\")\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        train_dfs.append(df)\n",
    "\n",
    "    # Create datasets\n",
    "    train_series_list = []\n",
    "    for df in train_dfs:\n",
    "        series_list = split_into_continuous_series(df, past_sequence_length, future_offset, max_interval_minutes)\n",
    "        train_series_list.extend(series_list)\n",
    "\n",
    "    train_dataset, val_dataset = create_train_val_datasets(\n",
    "        train_series_list,\n",
    "        train_ratio=0.8,\n",
    "        past_seq_len=past_sequence_length,\n",
    "        future_offset=future_offset\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Train model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=200,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "\n",
    "    # Evaluate on test data\n",
    "    metrics = evaluate_and_save_metrics(\n",
    "        model=model,\n",
    "        test_file_path=fold_data['test'],\n",
    "        save_dir='evaluation_metrics',\n",
    "        past_sequence_length=past_sequence_length,\n",
    "        future_offset=future_offset,\n",
    "        batch_size=batch_size,\n",
    "        max_interval_minutes=max_interval_minutes\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, fold_data['test'][-19:], save_dir='saved_models_original_ohio')\n",
    "\n",
    "    print(f\"\\nResults for {fold_name}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J86ZIzsp8Hn",
    "outputId": "5a69e274-261f-492f-d857-91298fbab5d3"
   },
   "outputs": [],
   "source": [
    "fold_data['test'][-19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nShh3Sibp8Hn",
    "outputId": "93db6278-4b2e-42c7-aaf8-d41824ed9fa5"
   },
   "outputs": [],
   "source": [
    "save_model(model, fold_data['test'][-19:], save_dir='saved_models_original_ohio')\n",
    "\n",
    "print(f\"\\nResults for {fold_name}:\")\n",
    "print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3D7uSk6p8Hn",
    "outputId": "f098304c-8f74-42b3-8e55-d2285f47f325"
   },
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EkQv9zBp8Hn"
   },
   "outputs": [],
   "source": [
    "# Save model after training each fold\n",
    "def save_model(model, fold_name, save_dir='saved_models'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(save_dir, f'model_{fold_name}.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load model for evaluation\n",
    "def load_model(fold_name, model_class=TransformerEncoder, save_dir='saved_models'):\n",
    "    model_path = os.path.join(save_dir, f'model_{fold_name}.pth')\n",
    "\n",
    "    # Initialize a new model with the same architecture\n",
    "    model = model_class(\n",
    "        num_layers=3,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        input_dim=1,\n",
    "        dim_feedforward=256,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # Load the saved weights\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3f1C7tlp8Hn",
    "outputId": "5bad8a54-6016-41c5-a9dd-14d2c2c6962d"
   },
   "outputs": [],
   "source": [
    "fold_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3Ywc-NCp8Ho",
    "outputId": "4b81da18-45b9-4056-96a5-ccb1c718d1d9"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# 1. Set up paths and get file lists\n",
    "folder_path_train_2018 = \"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/OhioT1DM/2018/train\"\n",
    "folder_path_train_2020 = \"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/OhioT1DM/2020/train\"\n",
    "train_files_2018 = [f for f in os.listdir(folder_path_train_2018) if f.endswith('.xml')]\n",
    "train_files_2020 = [f for f in os.listdir(folder_path_train_2020) if f.endswith('.xml')]\n",
    "\n",
    "# 2. Create LOOCV splits\n",
    "loocv_splits = create_loocv_splits(\n",
    "    folder_path_train_2018,\n",
    "    folder_path_train_2020,\n",
    "    train_files_2018,\n",
    "    train_files_2020\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "# 4. Set hyperparameters\n",
    "past_sequence_length = 7\n",
    "future_offset = 6\n",
    "batch_size = 32\n",
    "max_interval_minutes = 30\n",
    "\n",
    "# Get the items starting from fold9\n",
    "starting_fold = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hInjyUhKp8Ho",
    "outputId": "0b5322b9-b63f-470c-8868-0420cdc864ba"
   },
   "outputs": [],
   "source": [
    "# For a specific fold\n",
    "fold_name = 'fold9'\n",
    "fold_data = loocv_splits[fold_name]\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(fold_name)\n",
    "\n",
    "metrics = evaluate_and_save_metrics(\n",
    "    model=model,\n",
    "    test_file_path=fold_data['test'],\n",
    "    save_dir='evaluation_metrics',\n",
    "    past_sequence_length=past_sequence_length,\n",
    "    future_offset=future_offset,\n",
    "    batch_size=batch_size,\n",
    "    max_interval_minutes=max_interval_minutes\n",
    ")\n",
    "\n",
    "print(f\"\\nResults for {fold_name}:\")\n",
    "print(f\"RMSE: {metrics['rmse']:.2f}\")\n",
    "print(f\"MAE: {metrics['mae']:.2f}\")\n",
    "print(f\"MAPE: {metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJD29tzXp8Ho"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
