{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from cgms_data_seg_t1dexi import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader_T1DEXI import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "List of devices available to TensorFlow:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# New method in TensorFlow 2.x:\n",
    "# This will list the devices TensorFlow recognizes\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"List of devices available to TensorFlow:\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The entire loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_training(subj):\n",
    "    subject_cgm = pd.read_csv(subj)\n",
    "\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    subject_cgm_concise = subject_cgm[[\"LBORRES\", \"LBDTC\"]]\n",
    "    return subject_cgm_concise\n",
    "\n",
    "def preporcess_T1DEXI(subject_id):\n",
    "    subject_cgm = read_preprocess_cgm_training(subject_id)\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject_cgm.empty:\n",
    "        current_group = [subject_cgm.iloc[0]['LBORRES']]\n",
    "        last_time = subject_cgm.iloc[0]['LBDTC']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject_cgm.iloc[1:].iterrows():\n",
    "        current_time = row['LBDTC']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['LBORRES'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['LBORRES']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11 00:01:17\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (7799, 6), Train/test: 1/7798\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-19 10:54:37\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (7552, 6), Train/test: 1/7551\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-10-04 00:01:17\n",
      "Reading 120 segments\n",
      "Building dataset, requesting data from 0 to 120\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 120 continuous time series\n",
      "Data shape: (6173, 6), Train/test: 1/6172\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-04-06 00:04:28\n",
      "Reading 174 segments\n",
      "Building dataset, requesting data from 0 to 174\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 174 continuous time series\n",
      "Data shape: (5701, 6), Train/test: 1/5700\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-07-06 00:01:08\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (7807, 6), Train/test: 1/7806\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-11-10 00:03:50\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (7890, 6), Train/test: 1/7889\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-01-16 00:00:05\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7970, 6), Train/test: 1/7969\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-04-22 00:04:43\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (7827, 6), Train/test: 1/7826\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-18 00:02:19\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7880, 6), Train/test: 1/7879\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-07-13 00:03:34\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7091, 6), Train/test: 1/7090\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-07-28 00:13:50\n",
      "Reading 38 segments\n",
      "Building dataset, requesting data from 0 to 38\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 38 continuous time series\n",
      "Data shape: (5446, 6), Train/test: 1/5445\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-08-06 00:02:21\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (7700, 6), Train/test: 1/7699\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-06-19 00:04:40\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 21 continuous time series\n",
      "Data shape: (7667, 6), Train/test: 1/7666\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-07-18 00:04:25\n",
      "Reading 6 segments\n",
      "Building dataset, requesting data from 0 to 6\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 6 continuous time series\n",
      "Data shape: (7931, 6), Train/test: 1/7930\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-10-26 00:04:06\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (7816, 6), Train/test: 1/7815\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-06 00:01:25\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 3 continuous time series\n",
      "Data shape: (7959, 6), Train/test: 1/7958\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-09-08 00:02:13\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7893, 6), Train/test: 1/7892\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-11-22 00:04:56\n",
      "Reading 24 segments\n",
      "Building dataset, requesting data from 0 to 24\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 24 continuous time series\n",
      "Data shape: (7510, 6), Train/test: 1/7509\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-04-07 00:01:50\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7871, 6), Train/test: 1/7870\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-07-09 00:01:58\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (7913, 6), Train/test: 1/7912\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-02-13 00:00:14\n",
      "Reading 18 segments\n",
      "Building dataset, requesting data from 0 to 18\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 18 continuous time series\n",
      "Data shape: (7691, 6), Train/test: 1/7690\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-15 00:03:32\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (7753, 6), Train/test: 1/7752\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-05-04 00:04:58\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7960, 6), Train/test: 1/7959\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-08-04 00:02:15\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7981, 6), Train/test: 1/7980\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-10-18 00:02:38\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 27 continuous time series\n",
      "Data shape: (7515, 6), Train/test: 1/7514\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-09-01 00:00:02\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7907, 6), Train/test: 1/7906\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-10-22 00:02:53\n",
      "Reading 13 segments\n",
      "Building dataset, requesting data from 0 to 13\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 13 continuous time series\n",
      "Data shape: (7747, 6), Train/test: 1/7746\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-07-15 00:04:15\n",
      "Reading 29 segments\n",
      "Building dataset, requesting data from 0 to 29\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 29 continuous time series\n",
      "Data shape: (7374, 6), Train/test: 1/7373\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-06-20 00:01:54\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (7535, 6), Train/test: 1/7534\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-10-03 01:34:01\n",
      "Reading 14 segments\n",
      "Building dataset, requesting data from 0 to 14\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 14 continuous time series\n",
      "Data shape: (7785, 6), Train/test: 1/7784\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-09 00:03:55\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (7703, 6), Train/test: 1/7702\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-01-14 00:03:12\n",
      "Reading 39 segments\n",
      "Building dataset, requesting data from 0 to 39\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 39 continuous time series\n",
      "Data shape: (6423, 6), Train/test: 1/6422\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-03-01 00:00:28\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 23 continuous time series\n",
      "Data shape: (7708, 6), Train/test: 1/7707\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-09-16 00:02:42\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (7833, 6), Train/test: 1/7832\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-03-13 00:04:05\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (7791, 6), Train/test: 1/7790\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-01-03 00:03:00\n",
      "Reading 14 segments\n",
      "Building dataset, requesting data from 0 to 14\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 14 continuous time series\n",
      "Data shape: (7438, 6), Train/test: 1/7437\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-09-16 00:00:07\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (7444, 6), Train/test: 1/7443\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-05-10 00:01:14\n",
      "Reading 39 segments\n",
      "Building dataset, requesting data from 0 to 39\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 39 continuous time series\n",
      "Data shape: (7246, 6), Train/test: 1/7245\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-23 00:03:21\n",
      "Reading 34 segments\n",
      "Building dataset, requesting data from 0 to 34\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 34 continuous time series\n",
      "Data shape: (7431, 6), Train/test: 1/7430\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-11-09 00:04:01\n",
      "Reading 15 segments\n",
      "Building dataset, requesting data from 0 to 15\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 15 continuous time series\n",
      "Data shape: (7716, 6), Train/test: 1/7715\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-05 00:03:28\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (7096, 6), Train/test: 1/7095\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-04-03 00:03:04\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (7456, 6), Train/test: 1/7455\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-06-16 00:04:06\n",
      "Reading 13 segments\n",
      "Building dataset, requesting data from 0 to 13\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 13 continuous time series\n",
      "Data shape: (7817, 6), Train/test: 1/7816\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-06-03 00:01:58\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (7607, 6), Train/test: 1/7606\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-01-29 00:02:34\n",
      "Reading 124 segments\n",
      "Building dataset, requesting data from 0 to 124\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 124 continuous time series\n",
      "Data shape: (6249, 6), Train/test: 1/6248\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-11-05 00:01:03\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (7855, 6), Train/test: 1/7854\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-21 00:04:19\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (7869, 6), Train/test: 1/7868\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-06-15 00:01:09\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7971, 6), Train/test: 1/7970\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-11-03 00:04:25\n",
      "Reading 55 segments\n",
      "Building dataset, requesting data from 0 to 55\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 55 continuous time series\n",
      "Data shape: (6893, 6), Train/test: 1/6892\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-05-31 00:04:40\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7379, 6), Train/test: 1/7378\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-11-10 23:53:18\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7674, 6), Train/test: 1/7673\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-08 00:04:00\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (7709, 6), Train/test: 1/7708\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-09-16 00:02:56\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (7886, 6), Train/test: 1/7885\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-02-07 00:03:52\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7581, 6), Train/test: 1/7580\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-09-27 00:04:24\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (7927, 6), Train/test: 1/7926\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-11-04 00:01:45\n",
      "Reading 6 segments\n",
      "Building dataset, requesting data from 0 to 6\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 6 continuous time series\n",
      "Data shape: (7887, 6), Train/test: 1/7886\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-01-02 00:01:38\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 11 continuous time series\n",
      "Data shape: (7806, 6), Train/test: 1/7805\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-08-02 00:03:15\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (7944, 6), Train/test: 1/7943\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-03-10 00:32:01\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (7871, 6), Train/test: 1/7870\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-07-17 00:36:46\n",
      "Reading 19 segments\n",
      "Building dataset, requesting data from 0 to 19\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 19 continuous time series\n",
      "Data shape: (7685, 6), Train/test: 1/7684\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-03-04 00:02:50\n",
      "Reading 215 segments\n",
      "Building dataset, requesting data from 0 to 215\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 215 continuous time series\n",
      "Data shape: (5699, 6), Train/test: 1/5698\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-08-04 00:02:31\n",
      "Reading 28 segments\n",
      "Building dataset, requesting data from 0 to 28\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 28 continuous time series\n",
      "Data shape: (7443, 6), Train/test: 1/7442\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-07-31 00:00:14\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (7399, 6), Train/test: 1/7398\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_sh6_cross_check\\pretrain\n"
     ]
    }
   ],
   "source": [
    "epoch = 80\n",
    "ph = 6\n",
    "sh = 6\n",
    "path = \"../t1dexi_results\"\n",
    "# Correct path with raw string\n",
    "# for sh in [6, 12, 18, 24]:\n",
    "#     for fold_num in range(1, 6):\n",
    "#         train_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_training'\n",
    "#         # List files without their extensions\n",
    "#         train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "#                             if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "        # cleaned_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in train_file_names]\n",
    "\n",
    "\n",
    "        # Define the directory path\n",
    "test_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed'  # Use a raw string for paths on Windows\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "            if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "# cleaned_test_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in test_file_names]\n",
    "\n",
    "# train_data = dict()\n",
    "# for subj in train_file_names:\n",
    "\n",
    "#     subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_training/{subj}.csv'\n",
    "#     reader = preporcess_T1DEXI(subj_path)\n",
    "#     train_data[subj] = reader\n",
    "\n",
    "# test_data = dict()\n",
    "# for subj in test_file_names:\n",
    "\n",
    "#     subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_test/{subj}.csv'\n",
    "#     reader = preporcess_T1DEXI(subj_path)\n",
    "#     test_data[subj] = reader\n",
    "\n",
    "# a dumb dataset instance \n",
    "# train_dataset = CGMSDataSeg(\n",
    "#     \"t1dexi\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold1_training/252.csv\", 5\n",
    "# )\n",
    "sampling_horizon = sh\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "# with open(f'../t1dexi_results/config.json') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "# argv = (\n",
    "#     config[\"k_size\"],\n",
    "#     config[\"nblock\"],\n",
    "#     config[\"nn_size\"],\n",
    "#     config[\"nn_layer\"],\n",
    "#     config[\"learning_rate\"],\n",
    "#     config[\"batch_size\"],\n",
    "#     epoch,\n",
    "#     config[\"beta\"],\n",
    "# )\n",
    "# l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_cross_check\")\n",
    "# if not os.path.exists(outdir):\n",
    "#     os.makedirs(outdir)\n",
    "# all_errs = []\n",
    "\n",
    "# # Train on the training fold\n",
    "# train_file_names.sort()\n",
    "standard = False  \n",
    "\n",
    "# train_pids = set(train_file_names)\n",
    "# local_train_data = []\n",
    "# for k in train_pids:\n",
    "#     local_train_data += train_data[k]\n",
    "\n",
    "\n",
    "# train_dataset.data = local_train_data\n",
    "# train_dataset.set_cutpoint = -1\n",
    "# train_dataset.reset(\n",
    "#     sampling_horizon,\n",
    "#     prediction_horizon,\n",
    "#     scale,\n",
    "#     100,\n",
    "#     False,\n",
    "#     outtype,\n",
    "#     1,\n",
    "#     standard,\n",
    "# )\n",
    "# regressor(train_dataset, *argv, l_type, outdir)\n",
    "\n",
    "# Evaluate on the test patients\n",
    "all_errs = []\n",
    "for pid in test_file_names: # First 9 as subset, can be an example\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"t1dexi\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/{pid}.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        [err],\n",
    "        fmt=\"%.4f\",\n",
    "    )\n",
    "    all_errs.append([str(pid), err]) \n",
    "all_errs = np.array(all_errs, dtype=object)  # Use dtype=object to handle mixed types\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f\")\n",
    "        # label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-31 00:00:14\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 918\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 28186/349018\n",
      "Found 918 continuous time series\n",
      "Data shape: (377206, 6), Train/test: 377204/2\n",
      "Train test ratio: 188602.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  5893\n",
      "Epoch 0, train loss: 0.241195\n",
      "Epoch 1, train loss: 0.295681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 85\u001b[0m\n\u001b[0;32m     74\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mset_cutpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mreset(\n\u001b[0;32m     76\u001b[0m     sampling_horizon,\n\u001b[0;32m     77\u001b[0m     prediction_horizon,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     standard,\n\u001b[0;32m     84\u001b[0m )\n\u001b[1;32m---> 85\u001b[0m \u001b[43mregressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluate on the test patients\u001b[39;00m\n\u001b[0;32m     88\u001b[0m all_errs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\AccurateBG\\accurate_bg\\cnn_ohio.py:143\u001b[0m, in \u001b[0;36mregressor\u001b[1;34m(low_fid_data, k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta, loss_type, outdir)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(low_fid_data\u001b[38;5;241m.\u001b[39mtrain_n \u001b[38;5;241m/\u001b[39m batch_size)):\n\u001b[0;32m    142\u001b[0m     d \u001b[38;5;241m=\u001b[39m low_fid_data\u001b[38;5;241m.\u001b[39mtrain_next_batch(batch_size)\n\u001b[1;32m--> 143\u001b[0m     \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m err \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(loss, feed_dict\u001b[38;5;241m=\u001b[39m{x: d[\u001b[38;5;241m0\u001b[39m], y_: d[\u001b[38;5;241m1\u001b[39m], weights: d[\u001b[38;5;241m2\u001b[39m]})\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, train loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (i, err))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 80\n",
    "ph = 6\n",
    "sh = 6\n",
    "fold_num = 1\n",
    "path = \"../t1dexi_results\"\n",
    "train_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_training'\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "                    if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "# cleaned_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in train_file_names]\n",
    "\n",
    "\n",
    "# Define the directory path\n",
    "test_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_test'  # Use a raw string for paths on Windows\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "            if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "# cleaned_test_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in test_file_names]\n",
    "\n",
    "train_data = dict()\n",
    "for subj in train_file_names:\n",
    "\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_training/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    train_data[subj] = reader\n",
    "\n",
    "test_data = dict()\n",
    "for subj in test_file_names:\n",
    "\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_test/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    test_data[subj] = reader\n",
    "\n",
    "# a dumb dataset instance \n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"t1dexi\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold1_training/252.csv\", 5\n",
    ")\n",
    "sampling_horizon = sh\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(f'../t1dexi_results/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_fold{fold_num}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []\n",
    "\n",
    "# Train on the training fold\n",
    "train_file_names.sort()\n",
    "standard = False  \n",
    "\n",
    "train_pids = set(train_file_names)\n",
    "local_train_data = []\n",
    "for k in train_pids:\n",
    "    local_train_data += train_data[k]\n",
    "\n",
    "\n",
    "train_dataset.data = local_train_data\n",
    "train_dataset.set_cutpoint = -1\n",
    "train_dataset.reset(\n",
    "    sampling_horizon,\n",
    "    prediction_horizon,\n",
    "    scale,\n",
    "    100,\n",
    "    False,\n",
    "    outtype,\n",
    "    1,\n",
    "    standard,\n",
    ")\n",
    "regressor(train_dataset, *argv, l_type, outdir)\n",
    "\n",
    "# Evaluate on the test patients\n",
    "all_errs = []\n",
    "for pid in test_file_names: # First 9 as subset, can be an example\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"t1dexi\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_t1dexi_subset/T1DEXI_cgm_processed/fold{fold_num}_test/{pid}.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        [err],\n",
    "        fmt=\"%.4f\",\n",
    "    )\n",
    "    all_errs.append([str(pid), err]) \n",
    "all_errs = np.array(all_errs, dtype=object)  # Use dtype=object to handle mixed types\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_data(subj):\n",
    "    subject = pd.read_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/{subj}.csv')\n",
    "    subject_cgm = subject.loc[subject['LBCAT'] == 'CGM']\n",
    "    # Convert the 'LBDTC' column to datetime\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    return subject_cgm\n",
    "    # Count the number of occurrences of each unique date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854',\n",
    " '979',\n",
    " '816',\n",
    " '953',\n",
    " '981',\n",
    " '1617',\n",
    " '1343',\n",
    " '987',\n",
    " '255',\n",
    " '907',\n",
    " '856',\n",
    " '354',\n",
    " '894',\n",
    " '862',\n",
    " '900',\n",
    " '695']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide training and testing\n",
    "# Generate folder for training and test data\n",
    "\n",
    "for subj in overlap:\n",
    "    subject = pd.read_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/{subj}.csv')\n",
    "    subject_cgm = subject.loc[subject['LBCAT'] == 'CGM']\n",
    "    \n",
    "    split_index = int(len(subject_cgm) * 0.8)\n",
    "    # Split the DataFrame\n",
    "    train_df = subject[:split_index]\n",
    "    test_df = subject[split_index:]\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    train_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/training/{subj}_training_data.csv', index=False)\n",
    "    test_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/testing/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "    # Optionally, confirm the operation\n",
    "    print(f\"Training data saved with {len(train_df)} records.\")\n",
    "    print(f\"Testing data saved with {len(test_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_training(subj):\n",
    "    subject_cgm = pd.read_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/training/{subj}_training_data.csv')\n",
    "\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    subject_cgm_concise = subject_cgm[[\"LBORRES\", \"LBDTC\"]]\n",
    "    return subject_cgm_concise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example \n",
    "subject_cgm = read_preprocess_cgm_training(overlap[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Segment the data into several pieces\n",
    "# Assuming self.interval_timedelta is set, for example:\n",
    "interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "# Create a list to store the results\n",
    "res = []\n",
    "\n",
    "# Initialize the first group\n",
    "if not subject_cgm.empty:\n",
    "    current_group = [subject_cgm.iloc[0]['LBORRES']]\n",
    "    last_time = subject_cgm.iloc[0]['LBDTC']\n",
    "\n",
    "# Iterate over rows in DataFrame starting from the second row\n",
    "for index, row in subject_cgm.iloc[1:].iterrows():\n",
    "    current_time = row['LBDTC']\n",
    "    if (current_time - last_time) <= interval_timedelta:\n",
    "        # If the time difference is within the limit, add to the current group\n",
    "        current_group.append(row['LBORRES'])\n",
    "    else:\n",
    "        # Otherwise, start a new group\n",
    "        res.append(current_group)\n",
    "        current_group = [row['LBORRES']]\n",
    "    last_time = current_time\n",
    "\n",
    "# Add the last group if it's not empty\n",
    "if current_group:\n",
    "    res.append(current_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_training(subj):\n",
    "    subject_cgm = pd.read_csv(subj)\n",
    "\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    subject_cgm_concise = subject_cgm[[\"LBORRES\", \"LBDTC\"]]\n",
    "    return subject_cgm_concise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preporcess_T1DEXI(subject_id):\n",
    "    subject_cgm = read_preprocess_cgm_training(subject_id)\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject_cgm.empty:\n",
    "        current_group = [subject_cgm.iloc[0]['LBORRES']]\n",
    "        last_time = subject_cgm.iloc[0]['LBDTC']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject_cgm.iloc[1:].iterrows():\n",
    "        current_time = row['LBDTC']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['LBORRES'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['LBORRES']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loop to generate res for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "train_directory_path = r'C:\\Users\\username\\OneDrive\\Desktop\\BGprediction\\LB_split\\training'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "              if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1343_testing_data', '1617_testing_data', '255_testing_data', '354_testing_data', '695_testing_data', '816_testing_data', '854_testing_data', '856_testing_data', '862_testing_data', '894_testing_data', '900_testing_data', '907_testing_data', '953_testing_data', '979_testing_data', '981_testing_data', '987_testing_data']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "test_directory_path = r'C:\\Users\\username\\OneDrive\\Desktop\\BGprediction\\LB_split\\testing'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "              if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343_training_data\n",
      "1617_training_data\n",
      "255_training_data\n",
      "354_training_data\n",
      "695_training_data\n",
      "816_training_data\n",
      "854_training_data\n",
      "856_training_data\n",
      "862_training_data\n",
      "894_training_data\n",
      "900_training_data\n",
      "907_training_data\n",
      "953_training_data\n",
      "979_training_data\n",
      "981_training_data\n",
      "987_training_data\n"
     ]
    }
   ],
   "source": [
    "train_data = dict()\n",
    "for subj in train_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/training/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    train_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343_testing_data\n",
      "1617_testing_data\n",
      "255_testing_data\n",
      "354_testing_data\n",
      "695_testing_data\n",
      "816_testing_data\n",
      "854_testing_data\n",
      "856_testing_data\n",
      "862_testing_data\n",
      "894_testing_data\n",
      "900_testing_data\n",
      "907_testing_data\n",
      "953_testing_data\n",
      "979_testing_data\n",
      "981_testing_data\n",
      "987_testing_data\n"
     ]
    }
   ],
   "source": [
    "# Have not been run\n",
    "test_data = dict()\n",
    "for subj in test_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/testing/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    test_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "\n",
    "ph = 6 # Prediction horizon\n",
    "path = \"../t1dexi_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_subjects = [s.replace(\"_training_data\", \"\") for s in train_file_names]\n",
    "cleaned_subjects.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01 00:01:51\n",
      "Reading 8 segments\n"
     ]
    }
   ],
   "source": [
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/training/255_training_data.csv\", 5\n",
    ")\n",
    "sampling_horizon = 7\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(f'../t1dexi_results/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "standard = False  # do not use standard\n",
    "all_errs = []\n",
    "for pid in overlap:\n",
    "    # local_train_data = []\n",
    "    \n",
    "    # for k in train_data.keys():\n",
    "    #     if k != pid:\n",
    "    #         local_train_data += train_data[k]\n",
    "    train_pids = set(overlap) - set([pid])\n",
    "    local_train_data = []\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k + \"_training_data\"]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    # target_test_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    # )\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/testing/{pid}_testing_data.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    # target_train_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    # )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/LB_split/training/{pid}_training_data.csv\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    print(transfer_res)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\", #%.4f %.4f %.4f %.4f\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f %.4f\") # %.4f %.4f\n",
    "# label pair:(groundTruth, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['854', '0.15074661', '0.14032947'],\n",
       "       ['979', '0.10964842', '0.1068475'],\n",
       "       ['816', '0.17296007', '0.15637168'],\n",
       "       ['953', '0.16875254', '0.16648282'],\n",
       "       ['981', '0.18782552', '0.1781756'],\n",
       "       ['1617', '0.18750052', '0.17405139'],\n",
       "       ['1343', '0.1479887', '0.12966709'],\n",
       "       ['987', '0.1956755', '0.18187545'],\n",
       "       ['255', '0.112035595', '0.11363354'],\n",
       "       ['907', '0.2046883', '0.18899176'],\n",
       "       ['856', '0.16526204', '0.15994464'],\n",
       "       ['354', '0.21569382', '0.21845779'],\n",
       "       ['894', '0.1430959', '0.14199'],\n",
       "       ['862', '0.2226933', '0.19469088'],\n",
       "       ['900', '0.14939056', '0.12725726'],\n",
       "       ['695', '0.16643564', '0.16699977']], dtype='<U32')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.16877456468750002\n",
      "Average of the third column: 0.159110415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The altimate result please go into the result folder to check the evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run into an error, but all result txt files are ready, please run this section for evaluation\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "files_and_directories = os.listdir('./t1dexi_results')\n",
    "\n",
    "# Filter for files that end with .txt\n",
    "txt_files = [file for file in files_and_directories if file.endswith('.txt')]\n",
    "\n",
    "# Read the data from the text file\n",
    "def calcuate_rmse(file):\n",
    "    data = np.loadtxt(file)  # Make sure to replace 'data.txt' with your actual file path\n",
    "    print(file)\n",
    "    # Splitting the data into groundtruth and predictions\n",
    "    groundtruth = data[:, 0]  # First column as ground truth (also same as third column)\n",
    "    predictions_1 = data[:, 1]  # Second column as predictions from method 1\n",
    "    predictions_2 = data[:, 3]  # Fourth column as predictions from method 2\n",
    "\n",
    "    # Function to calculate RMSE\n",
    "    def calculate_rmse(true_values, predictions):\n",
    "        mse = np.mean((true_values - predictions) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    # Calculate RMSE for each method\n",
    "    rmse_method_1 = calculate_rmse(groundtruth, predictions_1)\n",
    "    rmse_method_2 = calculate_rmse(groundtruth, predictions_2)\n",
    "\n",
    "    print(\"RMSE for Method 1:\", rmse_method_1)\n",
    "    print(\"RMSE for Method 2:\", rmse_method_2)\n",
    "    return rmse_method_1\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "for f in txt_files[:-1]:\n",
    "    rmse1 = calcuate_rmse(f)\n",
    "    print(rmse1)\n",
    "    rmse_list.append(rmse1)\n",
    "\n",
    "print(np.average(rmse_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
