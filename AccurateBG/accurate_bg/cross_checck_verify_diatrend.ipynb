{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "List of devices available to TensorFlow:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from cgms_data_seg_diatrend import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader_DiaTrend import DataReader\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# New method in TensorFlow 2.x:\n",
    "# This will list the devices TensorFlow recognizes\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"List of devices available to TensorFlow:\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_DiaTrend(path):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    subject['date'] = pd.to_datetime(subject['date'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    print(subject['date'][0])\n",
    "    subject.sort_values('date', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    # Assuming self.interval_timedelta is set, for example:\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject.empty:\n",
    "        current_group = [subject.iloc[0]['mg/dl']]\n",
    "        last_time = subject.iloc[0]['date']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject.iloc[1:].iterrows():\n",
    "        current_time = row['date']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['mg/dl'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['mg/dl']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "    \n",
    "    # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 80\n",
    "ph = 6\n",
    "path = \"../diatrend_results\"\n",
    "sh = 6\n",
    "# Correct path with raw string\n",
    "# for sh in [6, 12, 18, 24]:\n",
    "#     for fold_num in range(1, 6):\n",
    "        # train_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold{fold_num}_training'\n",
    "        # # List files without their extensions\n",
    "        # train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "        #                     if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "        # cleaned_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in train_file_names]\n",
    "\n",
    "\n",
    "        # Define the directory path\n",
    "test_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset'  # Use a raw string for paths on Windows\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "            if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "cleaned_test_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in test_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-05 00:00:51.037000\n",
      "Reading 209 segments\n",
      "Building dataset, requesting data from 0 to 209\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 209 continuous time series\n",
      "Data shape: (8611, 6), Train/test: 1/8610\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2018-09-30 00:03:02\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (11829, 6), Train/test: 1/11828\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2018-06-20 00:04:30\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (11623, 6), Train/test: 1/11622\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-09 00:04:51\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (11783, 6), Train/test: 1/11782\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-04 02:21:39\n",
      "Reading 48 segments\n",
      "Building dataset, requesting data from 0 to 48\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 48 continuous time series\n",
      "Data shape: (10688, 6), Train/test: 1/10687\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-09-05 00:01:34\n",
      "Reading 18 segments\n",
      "Building dataset, requesting data from 0 to 18\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 18 continuous time series\n",
      "Data shape: (11428, 6), Train/test: 1/11427\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-10-02 00:01:19\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (11857, 6), Train/test: 1/11856\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-12-25 00:02:52\n",
      "Reading 18 segments\n",
      "Building dataset, requesting data from 0 to 18\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 18 continuous time series\n",
      "Data shape: (11736, 6), Train/test: 1/11735\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-01-01 00:03:08\n",
      "Reading 6 segments\n",
      "Building dataset, requesting data from 0 to 6\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 6 continuous time series\n",
      "Data shape: (11929, 6), Train/test: 1/11928\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-11-24 00:04:24\n",
      "Reading 9 segments\n",
      "Building dataset, requesting data from 0 to 9\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 9 continuous time series\n",
      "Data shape: (11771, 6), Train/test: 1/11770\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-04-09 00:00:29\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 11 continuous time series\n",
      "Data shape: (11771, 6), Train/test: 1/11770\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2015-12-05 00:01:23.363000\n",
      "Reading 418 segments\n",
      "Building dataset, requesting data from 0 to 418\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 418 continuous time series\n",
      "Data shape: (7434, 6), Train/test: 1/7433\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-05-14 00:00:34\n",
      "Reading 25 segments\n",
      "Building dataset, requesting data from 0 to 25\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 25 continuous time series\n",
      "Data shape: (11437, 6), Train/test: 1/11436\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-02-14 00:04:12\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 27 continuous time series\n",
      "Data shape: (11432, 6), Train/test: 1/11431\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-09-16 00:03:53\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (11868, 6), Train/test: 1/11867\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2020-06-26 00:03:03\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 21 continuous time series\n",
      "Data shape: (11617, 6), Train/test: 1/11616\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-03-14 00:02:30\n",
      "Reading 59 segments\n",
      "Building dataset, requesting data from 0 to 59\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 59 continuous time series\n",
      "Data shape: (9961, 6), Train/test: 1/9960\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-06-30 00:00:12\n",
      "Reading 9 segments\n",
      "Building dataset, requesting data from 0 to 9\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 9 continuous time series\n",
      "Data shape: (11796, 6), Train/test: 1/11795\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-02-26 00:03:54\n",
      "Reading 14 segments\n",
      "Building dataset, requesting data from 0 to 14\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 14 continuous time series\n",
      "Data shape: (11691, 6), Train/test: 1/11690\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-06-12 00:04:25\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (11879, 6), Train/test: 1/11878\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-11-06 00:03:22\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (11810, 6), Train/test: 1/11809\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2018-12-05 00:02:57\n",
      "Reading 548 segments\n",
      "Building dataset, requesting data from 0 to 548\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 548 continuous time series\n",
      "Data shape: (6578, 6), Train/test: 1/6577\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2016-08-25 00:39:41.582000\n",
      "Reading 346 segments\n",
      "Building dataset, requesting data from 0 to 346\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 346 continuous time series\n",
      "Data shape: (8101, 6), Train/test: 1/8100\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-05-25 00:04:25\n",
      "Reading 13 segments\n",
      "Building dataset, requesting data from 0 to 13\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 13 continuous time series\n",
      "Data shape: (11790, 6), Train/test: 1/11789\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-01-01 00:02:49\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (11941, 6), Train/test: 1/11940\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-02-01 00:00:24\n",
      "Reading 19 segments\n",
      "Building dataset, requesting data from 0 to 19\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 19 continuous time series\n",
      "Data shape: (11706, 6), Train/test: 1/11705\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-07-14 00:00:17\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (11705, 6), Train/test: 1/11704\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-05-23 00:01:16\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (11523, 6), Train/test: 1/11522\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-12-09 06:22:05\n",
      "Reading 9 segments\n",
      "Building dataset, requesting data from 0 to 9\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 9 continuous time series\n",
      "Data shape: (11733, 6), Train/test: 1/11732\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-03-11 00:00:21\n",
      "Reading 28 segments\n",
      "Building dataset, requesting data from 0 to 28\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 28 continuous time series\n",
      "Data shape: (11313, 6), Train/test: 1/11312\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-06 00:01:42\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (11596, 6), Train/test: 1/11595\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-14 00:54:21\n",
      "Reading 15 segments\n",
      "Building dataset, requesting data from 0 to 15\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 15 continuous time series\n",
      "Data shape: (11621, 6), Train/test: 1/11620\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-03-11 00:04:15\n",
      "Reading 38 segments\n",
      "Building dataset, requesting data from 0 to 38\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 38 continuous time series\n",
      "Data shape: (11293, 6), Train/test: 1/11292\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2017-06-16 00:04:15\n",
      "Reading 20 segments\n",
      "Building dataset, requesting data from 0 to 20\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 20 continuous time series\n",
      "Data shape: (11449, 6), Train/test: 1/11448\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-12-11 00:02:00\n",
      "Reading 31 segments\n",
      "Building dataset, requesting data from 0 to 31\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 31 continuous time series\n",
      "Data shape: (11563, 6), Train/test: 1/11562\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2022-02-16 00:02:15\n",
      "Reading 29 segments\n",
      "Building dataset, requesting data from 0 to 29\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 29 continuous time series\n",
      "Data shape: (11671, 6), Train/test: 1/11670\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-04-05 00:04:13\n",
      "Reading 28 segments\n",
      "Building dataset, requesting data from 0 to 28\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 28 continuous time series\n",
      "Data shape: (11591, 6), Train/test: 1/11590\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2022-02-25 00:02:54\n",
      "Reading 39 segments\n",
      "Building dataset, requesting data from 0 to 39\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 39 continuous time series\n",
      "Data shape: (11514, 6), Train/test: 1/11513\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-09-19 00:00:16\n",
      "Reading 34 segments\n",
      "Building dataset, requesting data from 0 to 34\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 34 continuous time series\n",
      "Data shape: (10180, 6), Train/test: 1/10179\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-17 00:00:56\n",
      "Reading 35 segments\n",
      "Building dataset, requesting data from 0 to 35\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 35 continuous time series\n",
      "Data shape: (11315, 6), Train/test: 1/11314\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-16 00:00:35\n",
      "Reading 24 segments\n",
      "Building dataset, requesting data from 0 to 24\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 24 continuous time series\n",
      "Data shape: (11499, 6), Train/test: 1/11498\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-27 00:00:03\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (11715, 6), Train/test: 1/11714\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2021-11-27 11:48:00\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (11756, 6), Train/test: 1/11755\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-27 00:00:53\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (10747, 6), Train/test: 1/10746\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2017-03-17 00:03:34\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 44 continuous time series\n",
      "Data shape: (10753, 6), Train/test: 1/10752\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-17 11:32:32\n",
      "Reading 395 segments\n",
      "Building dataset, requesting data from 0 to 395\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 395 continuous time series\n",
      "Data shape: (6287, 6), Train/test: 1/6286\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-27 00:02:26\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (8945, 6), Train/test: 1/8944\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-08-16 00:01:20\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (8889, 6), Train/test: 1/8888\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-06-09 00:00:34\n",
      "Reading 30 segments\n",
      "Building dataset, requesting data from 0 to 30\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 30 continuous time series\n",
      "Data shape: (8093, 6), Train/test: 1/8092\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2017-11-11 00:03:09\n",
      "Reading 24 segments\n",
      "Building dataset, requesting data from 0 to 24\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 24 continuous time series\n",
      "Data shape: (11541, 6), Train/test: 1/11540\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2017-10-04 00:02:14\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (11389, 6), Train/test: 1/11388\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2018-08-14 04:16:19\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 21 continuous time series\n",
      "Data shape: (11340, 6), Train/test: 1/11339\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n",
      "2019-04-10 01:29:52\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (11744, 6), Train/test: 1/11743\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_cross_check\\pretrain\n"
     ]
    }
   ],
   "source": [
    "epoch = 80\n",
    "ph = 6\n",
    "path = \"../diatrend_results\"\n",
    "sh = 6\n",
    "# Correct path with raw string\n",
    "# for sh in [6, 12, 18, 24]:\n",
    "#     for fold_num in range(1, 6):\n",
    "        # train_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold{fold_num}_training'\n",
    "        # # List files without their extensions\n",
    "        # train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "        #                     if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "        # cleaned_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in train_file_names]\n",
    "\n",
    "\n",
    "        # Define the directory path\n",
    "test_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset'  # Use a raw string for paths on Windows\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "            if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "cleaned_test_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in test_file_names]\n",
    "\n",
    "# train_data = dict()\n",
    "# for subj in train_file_names:\n",
    "\n",
    "#     subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold{fold_num}_training/{subj}.csv'\n",
    "#     reader = preprocess_DiaTrend(subj_path)\n",
    "#     train_data[subj] = reader\n",
    "\n",
    "# test_data = dict()\n",
    "# for subj in test_file_names:\n",
    "\n",
    "#     subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold{fold_num}_test/{subj}.csv'\n",
    "#     reader = preprocess_DiaTrend(subj_path)\n",
    "#     test_data[subj] = reader\n",
    "\n",
    "# # a dumb dataset instance \n",
    "# train_dataset = CGMSDataSeg(\n",
    "#     \"diatrend\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_training/processed_cgm_data_Subject12.csv\", 5\n",
    "# )\n",
    "sampling_horizon = sh\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "# with open(f'../diatrend_results/config.json') as json_file:\n",
    "#     config = json.load(json_file)\n",
    "# argv = (\n",
    "#     config[\"k_size\"],\n",
    "#     config[\"nblock\"],\n",
    "#     config[\"nn_size\"],\n",
    "#     config[\"nn_layer\"],\n",
    "#     config[\"learning_rate\"],\n",
    "#     config[\"batch_size\"],\n",
    "#     epoch,\n",
    "#     config[\"beta\"],\n",
    "# )\n",
    "# l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_cross_check\")\n",
    "# if not os.path.exists(outdir):\n",
    "#     os.makedirs(outdir)\n",
    "# all_errs = []\n",
    "\n",
    "# # Train on the training fold\n",
    "# cleaned_subjects.sort()\n",
    "standard = False  \n",
    "\n",
    "# train_pids = set(cleaned_subjects)\n",
    "# local_train_data = []\n",
    "# for k in train_pids:\n",
    "#     local_train_data += train_data[\"processed_cgm_data_\" + k]\n",
    "\n",
    "\n",
    "# train_dataset.data = local_train_data\n",
    "# train_dataset.set_cutpoint = -1\n",
    "# train_dataset.reset(\n",
    "#     sampling_horizon,\n",
    "#     prediction_horizon,\n",
    "#     scale,\n",
    "#     100,\n",
    "#     False,\n",
    "#     outtype,\n",
    "#     1,\n",
    "#     standard,\n",
    "# )\n",
    "# regressor(train_dataset, *argv, l_type, outdir)\n",
    "\n",
    "# Evaluate on the test patients\n",
    "all_errs = []\n",
    "for pid in cleaned_test_subjects: # First 9 as subset, can be an example\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"diatrend\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/processed_cgm_data_{pid}.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        [err],\n",
    "        fmt=\"%.4f\",\n",
    "    )\n",
    "    all_errs.append([str(pid), err]) \n",
    "all_errs = np.array(all_errs, dtype=object)  # Use dtype=object to handle mixed types\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f\")\n",
    "        # label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj in file_names:\n",
    "    subject = pd.read_excel(f\"C:/Users/username/OneDrive/Desktop/BGprediction/DiaTrend/dataset/{subj}.xlsx\",\"CGM\")\n",
    "    split_index = int(len(subject) * 0.8)\n",
    "    # Split the DataFrame\n",
    "    train_df = subject[:split_index]\n",
    "    test_df = subject[split_index:]\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    train_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/DiaTrend/train/{subj}_training_data.csv', index=False)\n",
    "    test_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/DiaTrend/test/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "    # Optionally, confirm the operation\n",
    "    print(f\"Training data saved with {len(train_df)} records.\")\n",
    "    print(f\"Testing data saved with {len(test_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do the train_test split\n",
    "# First 80% in training\n",
    "# Last 20% in test\n",
    "# split_index = int(len(subject) * 0.8)\n",
    "# # Split the DataFrame\n",
    "# train_df = subject[:split_index]\n",
    "# test_df = subject[split_index:]\n",
    "\n",
    "# # Save the DataFrames to CSV files\n",
    "# train_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/DiaTrend/train/{subj}_training_data.csv', index=False)\n",
    "# test_df.to_csv(f'C:/Users/username/OneDrive/Desktop/BGprediction/DiaTrend/test/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "# # Optionally, confirm the operation\n",
    "# print(f\"Training data saved with {len(train_df)} records.\")\n",
    "# print(f\"Testing data saved with {len(test_df)} records.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "subject['date'] = pd.to_datetime(subject['date'])  # Convert 'date' column to datetime if not already\n",
    "subject.sort_values('date', inplace=True)  # Sort the DataFrame by the 'date' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming self.interval_timedelta is set, for example:\n",
    "interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "# Create a list to store the results\n",
    "res = []\n",
    "\n",
    "# Initialize the first group\n",
    "if not subject.empty:\n",
    "    current_group = [subject.iloc[0]['mg/dl']]\n",
    "    last_time = subject.iloc[0]['date']\n",
    "\n",
    "# Iterate over rows in DataFrame starting from the second row\n",
    "for index, row in subject.iloc[1:].iterrows():\n",
    "    current_time = row['date']\n",
    "    if (current_time - last_time) <= interval_timedelta:\n",
    "        # If the time difference is within the limit, add to the current group\n",
    "        current_group.append(row['mg/dl'])\n",
    "    else:\n",
    "        # Otherwise, start a new group\n",
    "        res.append(current_group)\n",
    "        current_group = [row['mg/dl']]\n",
    "    last_time = current_time\n",
    "\n",
    "# Add the last group if it's not empty\n",
    "if current_group:\n",
    "    res.append(current_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_DiaTrend(path):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    subject['date'] = pd.to_datetime(subject['date'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    print(subject['date'][0])\n",
    "    subject.sort_values('date', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    # Assuming self.interval_timedelta is set, for example:\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject.empty:\n",
    "        current_group = [subject.iloc[0]['mg/dl']]\n",
    "        last_time = subject.iloc[0]['date']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject.iloc[1:].iterrows():\n",
    "        current_time = row['date']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['mg/dl'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['mg/dl']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "    \n",
    "    # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loop to generate res for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_cgm_data_Subject1', 'processed_cgm_data_Subject10', 'processed_cgm_data_Subject11', 'processed_cgm_data_Subject2', 'processed_cgm_data_Subject23', 'processed_cgm_data_Subject24', 'processed_cgm_data_Subject25', 'processed_cgm_data_Subject26', 'processed_cgm_data_Subject27', 'processed_cgm_data_Subject28', 'processed_cgm_data_Subject29', 'processed_cgm_data_Subject3', 'processed_cgm_data_Subject30', 'processed_cgm_data_Subject31', 'processed_cgm_data_Subject32', 'processed_cgm_data_Subject33', 'processed_cgm_data_Subject34', 'processed_cgm_data_Subject35', 'processed_cgm_data_Subject36', 'processed_cgm_data_Subject37', 'processed_cgm_data_Subject38', 'processed_cgm_data_Subject39', 'processed_cgm_data_Subject4', 'processed_cgm_data_Subject40', 'processed_cgm_data_Subject41', 'processed_cgm_data_Subject42', 'processed_cgm_data_Subject43', 'processed_cgm_data_Subject44', 'processed_cgm_data_Subject45', 'processed_cgm_data_Subject46', 'processed_cgm_data_Subject47', 'processed_cgm_data_Subject48', 'processed_cgm_data_Subject49', 'processed_cgm_data_Subject5', 'processed_cgm_data_Subject50', 'processed_cgm_data_Subject51', 'processed_cgm_data_Subject53', 'processed_cgm_data_Subject54', 'processed_cgm_data_Subject6', 'processed_cgm_data_Subject7', 'processed_cgm_data_Subject8', 'processed_cgm_data_Subject9']\n"
     ]
    }
   ],
   "source": [
    "# Fomulate a loop to create a list to include all the files in train and test datset and generate the res for each of them seperately\n",
    "# C:/Users/username/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_training/processed_cgm_data_Subject12.csv\n",
    "fold_num = 2\n",
    "\n",
    "# Correct path with raw string\n",
    "train_directory_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold{fold_num}_training'\n",
    "\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "                    if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(train_file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject12', 'Subject13', 'Subject14', 'Subject15', 'Subject16', 'Subject17', 'Subject18', 'Subject19', 'Subject20', 'Subject21', 'Subject22', 'Subject23', 'Subject24', 'Subject25', 'Subject26', 'Subject27', 'Subject28', 'Subject29', 'Subject30', 'Subject31', 'Subject32', 'Subject33', 'Subject34', 'Subject35', 'Subject36', 'Subject37', 'Subject38', 'Subject39', 'Subject40', 'Subject41', 'Subject42', 'Subject43', 'Subject44', 'Subject45', 'Subject46', 'Subject47', 'Subject48', 'Subject49', 'Subject50', 'Subject51', 'Subject53', 'Subject54']\n"
     ]
    }
   ],
   "source": [
    "cleaned_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in train_file_names]\n",
    "\n",
    "print(cleaned_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_cgm_data_Subject1', 'processed_cgm_data_Subject10', 'processed_cgm_data_Subject11', 'processed_cgm_data_Subject2', 'processed_cgm_data_Subject3', 'processed_cgm_data_Subject4', 'processed_cgm_data_Subject5', 'processed_cgm_data_Subject6', 'processed_cgm_data_Subject7', 'processed_cgm_data_Subject8', 'processed_cgm_data_Subject9']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "test_directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_test'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "              if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject1', 'Subject10', 'Subject11', 'Subject2', 'Subject3', 'Subject4', 'Subject5', 'Subject6', 'Subject7', 'Subject8', 'Subject9']\n"
     ]
    }
   ],
   "source": [
    "cleaned_test_subjects = [s.replace(\"processed_cgm_data_\", \"\") for s in test_file_names]\n",
    "\n",
    "print(cleaned_test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_cgm_data_Subject12\n",
      "2019-06-09 00:04:51\n",
      "processed_cgm_data_Subject13\n",
      "2019-06-04 02:21:39\n",
      "processed_cgm_data_Subject14\n",
      "2019-09-05 00:01:34\n",
      "processed_cgm_data_Subject15\n",
      "2019-10-02 00:01:19\n",
      "processed_cgm_data_Subject16\n",
      "2019-12-25 00:02:52\n",
      "processed_cgm_data_Subject17\n",
      "2020-01-01 00:03:08\n",
      "processed_cgm_data_Subject18\n",
      "2019-11-24 00:04:24\n",
      "processed_cgm_data_Subject19\n",
      "2020-04-09 00:00:29\n",
      "processed_cgm_data_Subject20\n",
      "2020-05-14 00:00:34\n",
      "processed_cgm_data_Subject21\n",
      "2021-02-14 00:04:12\n",
      "processed_cgm_data_Subject22\n",
      "2020-09-16 00:03:53\n",
      "processed_cgm_data_Subject23\n",
      "2020-06-26 00:03:03\n",
      "processed_cgm_data_Subject24\n",
      "2021-03-14 00:02:30\n",
      "processed_cgm_data_Subject25\n",
      "2021-06-30 00:00:12\n",
      "processed_cgm_data_Subject26\n",
      "2021-02-26 00:03:54\n",
      "processed_cgm_data_Subject27\n",
      "2021-06-12 00:04:25\n",
      "processed_cgm_data_Subject28\n",
      "2021-11-06 00:03:22\n",
      "processed_cgm_data_Subject29\n",
      "2018-12-05 00:02:57\n",
      "processed_cgm_data_Subject30\n",
      "2019-05-25 00:04:25\n",
      "processed_cgm_data_Subject31\n",
      "2019-01-01 00:02:49\n",
      "processed_cgm_data_Subject32\n",
      "2021-02-01 00:00:24\n",
      "processed_cgm_data_Subject33\n",
      "2021-07-14 00:00:17\n",
      "processed_cgm_data_Subject34\n",
      "2021-05-23 00:01:16\n",
      "processed_cgm_data_Subject35\n",
      "2021-12-09 06:22:05\n",
      "processed_cgm_data_Subject36\n",
      "2019-03-11 00:00:21\n",
      "processed_cgm_data_Subject37\n",
      "2019-08-06 00:01:42\n",
      "processed_cgm_data_Subject38\n",
      "2019-08-14 00:54:21\n",
      "processed_cgm_data_Subject39\n",
      "2019-03-11 00:04:15\n",
      "processed_cgm_data_Subject40\n",
      "2021-12-11 00:02:00\n",
      "processed_cgm_data_Subject41\n",
      "2022-02-16 00:02:15\n",
      "processed_cgm_data_Subject42\n",
      "2019-04-05 00:04:13\n",
      "processed_cgm_data_Subject43\n",
      "2022-02-25 00:02:54\n",
      "processed_cgm_data_Subject44\n",
      "2021-09-19 00:00:16\n",
      "processed_cgm_data_Subject45\n",
      "2019-08-17 00:00:56\n",
      "processed_cgm_data_Subject46\n",
      "2019-08-16 00:00:35\n",
      "processed_cgm_data_Subject47\n",
      "2019-06-27 00:00:03\n",
      "processed_cgm_data_Subject48\n",
      "2021-11-27 11:48:00\n",
      "processed_cgm_data_Subject49\n",
      "2019-06-27 00:00:53\n",
      "processed_cgm_data_Subject50\n",
      "2019-08-17 11:32:32\n",
      "processed_cgm_data_Subject51\n",
      "2019-06-27 00:02:26\n",
      "processed_cgm_data_Subject53\n",
      "2019-08-16 00:01:20\n",
      "processed_cgm_data_Subject54\n",
      "2019-06-09 00:00:34\n"
     ]
    }
   ],
   "source": [
    "train_data = dict()\n",
    "for subj in train_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_training/{subj}.csv'\n",
    "    reader = preprocess_DiaTrend(subj_path)\n",
    "    train_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_cgm_data_Subject1\n",
      "2015-12-05 00:00:51.037000\n",
      "processed_cgm_data_Subject10\n",
      "2018-09-30 00:03:02\n",
      "processed_cgm_data_Subject11\n",
      "2018-06-20 00:04:30\n",
      "processed_cgm_data_Subject2\n",
      "2015-12-05 00:01:23.363000\n",
      "processed_cgm_data_Subject3\n",
      "2016-08-25 00:39:41.582000\n",
      "processed_cgm_data_Subject4\n",
      "2017-06-16 00:04:15\n",
      "processed_cgm_data_Subject5\n",
      "2017-03-17 00:03:34\n",
      "processed_cgm_data_Subject6\n",
      "2017-11-11 00:03:09\n",
      "processed_cgm_data_Subject7\n",
      "2017-10-04 00:02:14\n",
      "processed_cgm_data_Subject8\n",
      "2018-08-14 04:16:19\n",
      "processed_cgm_data_Subject9\n",
      "2019-04-10 01:29:52\n"
     ]
    }
   ],
   "source": [
    "# Have not been run\n",
    "test_data = dict()\n",
    "for subj in test_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_test/{subj}.csv'\n",
    "    reader = preprocess_DiaTrend(subj_path)\n",
    "    test_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 80\n",
    "sh = 6\n",
    "ph = 6\n",
    "path = \"../diatrend_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 00:04:51\n",
      "Reading 12 segments\n"
     ]
    }
   ],
   "source": [
    "# a dumb dataset instance \"C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\processed_cgm_data_Subject12.csv\"\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"diatrend\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_training/processed_cgm_data_Subject12.csv\", 5\n",
    ")\n",
    "sampling_horizon = sh\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(f'../diatrend_results/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "cleaned_subjects.sort()\n",
    "standard = False  # do not use standard\n",
    "\n",
    "train_pids = set(cleaned_subjects)\n",
    "local_train_data = []\n",
    "for k in train_pids:\n",
    "    local_train_data += train_data[\"processed_cgm_data_\" + k]\n",
    "print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "train_dataset.data = local_train_data\n",
    "train_dataset.set_cutpoint = -1\n",
    "train_dataset.reset(\n",
    "    sampling_horizon,\n",
    "    prediction_horizon,\n",
    "    scale,\n",
    "    100,\n",
    "    False,\n",
    "    outtype,\n",
    "    1,\n",
    "    standard,\n",
    ")\n",
    "regressor(train_dataset, *argv, l_type, outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-05 00:00:51.037000\n",
      "Reading 209 segments\n",
      "Building dataset, requesting data from 0 to 209\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 209 continuous time series\n",
      "Data shape: (8611, 6), Train/test: 1/8610\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2018-09-30 00:03:02\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (11829, 6), Train/test: 1/11828\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2018-06-20 00:04:30\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 22 continuous time series\n",
      "Data shape: (11623, 6), Train/test: 1/11622\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2015-12-05 00:01:23.363000\n",
      "Reading 418 segments\n",
      "Building dataset, requesting data from 0 to 418\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 418 continuous time series\n",
      "Data shape: (7434, 6), Train/test: 1/7433\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2016-08-25 00:39:41.582000\n",
      "Reading 346 segments\n",
      "Building dataset, requesting data from 0 to 346\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 346 continuous time series\n",
      "Data shape: (8101, 6), Train/test: 1/8100\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2017-06-16 00:04:15\n",
      "Reading 20 segments\n",
      "Building dataset, requesting data from 0 to 20\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 20 continuous time series\n",
      "Data shape: (11449, 6), Train/test: 1/11448\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2017-03-17 00:03:34\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 44 continuous time series\n",
      "Data shape: (10753, 6), Train/test: 1/10752\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2017-11-11 00:03:09\n",
      "Reading 24 segments\n",
      "Building dataset, requesting data from 0 to 24\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 24 continuous time series\n",
      "Data shape: (11541, 6), Train/test: 1/11540\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2017-10-04 00:02:14\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (11389, 6), Train/test: 1/11388\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2018-08-14 04:16:19\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 21 continuous time series\n",
      "Data shape: (11340, 6), Train/test: 1/11339\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n",
      "2019-04-10 01:29:52\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (11744, 6), Train/test: 1/11743\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_sh6_rmse\\pretrain\n"
     ]
    }
   ],
   "source": [
    "all_errs = []\n",
    "for pid in cleaned_test_subjects: # First 9 as subset, can be an example\n",
    "    # train_pids = set(cleaned_subjects)\n",
    "    # local_train_data = []\n",
    "    # for k in train_pids:\n",
    "    #     local_train_data += train_data[k + \"_training_data\"]\n",
    "    # print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "    \n",
    "    # train_dataset.data = local_train_data\n",
    "    # train_dataset.set_cutpoint = -1\n",
    "    # train_dataset.reset(\n",
    "    #     sampling_horizon,\n",
    "    #     prediction_horizon,\n",
    "    #     scale,\n",
    "    #     100,\n",
    "    #     False,\n",
    "    #     outtype,\n",
    "    #     1,\n",
    "    #     standard,\n",
    "    # )\n",
    "    # regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"diatrend\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_test/processed_cgm_data_{pid}.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    # target_train_dataset = CGMSDataSeg(\n",
    "    # \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/modified_diatrend_subset/fold1_training/{pid}.csv\", 5\n",
    "    # )\n",
    "    # target_train_dataset.set_cutpoint = -1\n",
    "    # target_train_dataset.reset(\n",
    "    #     sampling_horizon,\n",
    "    #     prediction_horizon,\n",
    "    #     scale,\n",
    "    #     100,\n",
    "    #     False,\n",
    "    #     outtype,\n",
    "    #     1,\n",
    "    #     standard,\n",
    "    # )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    # errs = [err]\n",
    "    # transfer_res = [labels]\n",
    "    # for i in range(1, 2):\n",
    "    #     err, labels = regressor_transfer(\n",
    "    #         target_train_dataset,\n",
    "    #         target_test_dataset,\n",
    "    #         config[\"batch_size\"],\n",
    "    #         epoch,\n",
    "    #         outdir,\n",
    "    #         i,\n",
    "    #     )\n",
    "    #     errs.append(err)\n",
    "    #     transfer_res.append(labels)\n",
    "    # transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        [err],\n",
    "        fmt=\"%.4f\",\n",
    "    )\n",
    "    all_errs.append([str(pid), err]) \n",
    "all_errs = np.array(all_errs, dtype=object)  # Use dtype=object to handle mixed types\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.23931833875\n",
      "Average of the third column: 0.23320475625\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, if the previous cell run into an issue but all result txt files are ready\n",
    "# You can run this to evaluate:\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "files_and_directories = os.listdir('.')\n",
    "\n",
    "# Filter for files that end with .txt\n",
    "txt_files = [file for file in files_and_directories if file.endswith('.txt')]\n",
    "\n",
    "\n",
    "# Read the data from the text file\n",
    "def calcuate_rmse(file):\n",
    "    data = np.loadtxt(file)  # Make sure to replace 'data.txt' with your actual file path\n",
    "    print(file)\n",
    "    # Splitting the data into groundtruth and predictions\n",
    "    groundtruth = data[:, 0]  # First column as ground truth (also same as third column)\n",
    "    predictions_1 = data[:, 1]  # Second column as predictions from method 1\n",
    "    predictions_2 = data[:, 3]  # Fourth column as predictions from method 2\n",
    "\n",
    "    # Function to calculate RMSE\n",
    "    def calculate_rmse(true_values, predictions):\n",
    "        mse = np.mean((true_values - predictions) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    # Calculate RMSE for each method\n",
    "    rmse_method_1 = calculate_rmse(groundtruth, predictions_1)\n",
    "    rmse_method_2 = calculate_rmse(groundtruth, predictions_2)\n",
    "\n",
    "    print(\"RMSE for Method 1:\", rmse_method_1)\n",
    "    print(\"RMSE for Method 2:\", rmse_method_2)\n",
    "    return rmse_method_1\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "for f in txt_files[1:]:\n",
    "    rmse1 = calcuate_rmse(f)\n",
    "    print(rmse1)\n",
    "    rmse_list.append(rmse1)\n",
    "\n",
    "print(np.average(rmse_list))\n",
    "\n",
    "print(rmse_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
