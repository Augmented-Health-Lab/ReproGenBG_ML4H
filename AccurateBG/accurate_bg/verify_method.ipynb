{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from cgms_data_seg import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader import DataReader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option --epoch 150 --prediction_horizon 12\n",
    "epoch = 80\n",
    "ph = 6\n",
    "\n",
    "path = \"../ohio_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 44 segments\n",
      "Pretrain data: 19625082.0\n",
      "Building dataset, requesting data from 0 to 798\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7492/107922\n",
      "Found 798 continuous time series\n",
      "Data shape: (115416, 6), Train/test: 115414/2\n",
      "Train test ratio: 57707.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1803\n",
      "Epoch 0, train loss: 0.209016\n",
      "Epoch 1, train loss: 0.229581\n",
      "Epoch 2, train loss: 0.251364\n",
      "Epoch 3, train loss: 0.184529\n",
      "Epoch 4, train loss: 0.212756\n",
      "Epoch 5, train loss: 0.210902\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\training\\saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch 6, train loss: 0.216322\n",
      "Epoch 7, train loss: 0.203105\n",
      "Epoch 8, train loss: 0.200279\n",
      "Epoch 9, train loss: 0.224561\n",
      "Epoch 10, train loss: 0.237487\n",
      "Epoch 11, train loss: 0.263298\n",
      "Epoch 12, train loss: 0.168068\n",
      "Epoch 13, train loss: 0.195813\n",
      "Epoch 14, train loss: 0.206115\n",
      "Epoch 15, train loss: 0.205281\n",
      "Epoch 16, train loss: 0.280951\n",
      "Epoch 17, train loss: 0.163397\n",
      "Epoch 18, train loss: 0.195635\n",
      "Epoch 19, train loss: 0.180986\n",
      "Epoch 20, train loss: 0.193306\n",
      "Epoch 21, train loss: 0.183756\n",
      "Epoch 22, train loss: 0.203460\n",
      "Epoch 23, train loss: 0.179051\n",
      "Epoch 24, train loss: 0.169442\n",
      "Epoch 25, train loss: 0.160566\n",
      "Epoch 26, train loss: 0.197393\n",
      "Epoch 27, train loss: 0.210976\n",
      "Epoch 28, train loss: 0.176954\n",
      "Epoch 29, train loss: 0.198282\n",
      "Epoch 30, train loss: 0.204194\n",
      "Epoch 31, train loss: 0.207512\n",
      "Epoch 32, train loss: 0.209963\n",
      "Epoch 33, train loss: 0.175516\n",
      "Epoch 34, train loss: 0.182608\n",
      "Epoch 35, train loss: 0.259594\n",
      "Epoch 36, train loss: 0.214083\n",
      "Epoch 37, train loss: 0.224523\n",
      "Epoch 38, train loss: 0.228922\n",
      "Epoch 39, train loss: 0.154553\n",
      "Epoch 40, train loss: 0.162879\n",
      "Epoch 41, train loss: 0.193703\n",
      "Epoch 42, train loss: 0.234339\n",
      "Epoch 43, train loss: 0.212141\n",
      "Epoch 44, train loss: 0.242298\n",
      "Epoch 45, train loss: 0.202469\n",
      "Epoch 46, train loss: 0.229800\n",
      "Epoch 47, train loss: 0.252446\n",
      "Epoch 48, train loss: 0.206407\n",
      "Epoch 49, train loss: 0.243836\n",
      "Epoch 50, train loss: 0.171167\n",
      "Epoch 51, train loss: 0.241006\n",
      "Epoch 52, train loss: 0.224309\n",
      "Epoch 53, train loss: 0.224164\n",
      "Epoch 54, train loss: 0.213719\n",
      "Epoch 55, train loss: 0.227425\n",
      "Epoch 56, train loss: 0.154979\n",
      "Epoch 57, train loss: 0.200428\n",
      "Epoch 58, train loss: 0.188257\n",
      "Epoch 59, train loss: 0.174647\n",
      "Epoch 60, train loss: 0.161500\n",
      "Epoch 61, train loss: 0.140694\n",
      "Epoch 62, train loss: 0.236020\n",
      "Epoch 63, train loss: 0.211135\n",
      "Epoch 64, train loss: 0.174981\n",
      "Epoch 65, train loss: 0.223745\n",
      "Epoch 66, train loss: 0.185354\n",
      "Epoch 67, train loss: 0.216867\n",
      "Epoch 68, train loss: 0.186597\n",
      "Epoch 69, train loss: 0.232078\n",
      "Epoch 70, train loss: 0.267820\n",
      "Epoch 71, train loss: 0.163566\n",
      "Epoch 72, train loss: 0.191416\n",
      "Epoch 73, train loss: 0.199140\n",
      "Epoch 74, train loss: 0.166427\n",
      "Epoch 75, train loss: 0.180156\n",
      "Epoch 76, train loss: 0.197701\n",
      "Epoch 77, train loss: 0.255474\n",
      "Epoch 78, train loss: 0.214509\n",
      "Epoch 79, train loss: 0.239793\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "x here is\n",
      "[[179. 183. 187. 191. 195. 199.]\n",
      " [183. 187. 191. 195. 199. 204.]\n",
      " [187. 191. 195. 199. 204. 209.]\n",
      " ...\n",
      " [188. 187. 186. 186. 186. 187.]\n",
      " [187. 186. 186. 186. 187. 188.]\n",
      " [186. 186. 186. 187. 188. 187.]]\n",
      "y here is\n",
      "[[215. 215. 215. 215. 215. 215.]\n",
      " [225. 225. 225. 225. 225. 225.]\n",
      " [233. 233. 233. 233. 233. 233.]\n",
      " ...\n",
      " [182. 182. 182. 182. 182. 182.]\n",
      " [180. 180. 180. 180. 180. 180.]\n",
      " [177. 177. 177. 177. 177. 177.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (2382, 6), Train/test: 1/2381\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "x here is\n",
      "[[101.  98. 104. 112. 120. 127.]\n",
      " [ 98. 104. 112. 120. 127. 135.]\n",
      " [104. 112. 120. 127. 135. 142.]\n",
      " ...\n",
      " [148. 148. 148. 149. 150. 152.]\n",
      " [148. 148. 149. 150. 152. 155.]\n",
      " [148. 149. 150. 152. 155. 156.]]\n",
      "y here is\n",
      "[[151. 151. 151. 151. 151. 151.]\n",
      " [150. 150. 150. 150. 150. 150.]\n",
      " [124. 124. 124. 124. 124. 124.]\n",
      " ...\n",
      " [168. 168. 168. 168. 168. 168.]\n",
      " [172. 172. 172. 172. 172. 172.]\n",
      " [176. 176. 176. 176. 176. 176.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 793/9517\n",
      "Found 44 continuous time series\n",
      "Data shape: (10312, 6), Train/test: 10310/2\n",
      "Train test ratio: 5155.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2943CC070>\n",
      "Epoch 0, test loss: 0.190432\n",
      "Epoch 1, test loss: 0.190800\n",
      "Epoch 2, test loss: 0.191263\n",
      "Epoch 3, test loss: 0.195052\n",
      "Epoch 4, test loss: 0.194216\n",
      "Epoch 5, test loss: 0.190034\n",
      "Epoch 6, test loss: 0.191675\n",
      "Epoch 7, test loss: 0.189391\n",
      "Epoch 8, test loss: 0.188595\n",
      "Epoch 9, test loss: 0.188492\n",
      "Epoch 10, test loss: 0.190545\n",
      "Epoch 11, test loss: 0.191125\n",
      "Epoch 12, test loss: 0.192512\n",
      "Epoch 13, test loss: 0.188277\n",
      "Epoch 14, test loss: 0.194685\n",
      "Epoch 15, test loss: 0.189043\n",
      "Epoch 16, test loss: 0.190732\n",
      "Epoch 17, test loss: 0.193049\n",
      "Epoch 18, test loss: 0.191484\n",
      "Epoch 19, test loss: 0.188368\n",
      "Epoch 20, test loss: 0.188595\n",
      "Epoch 21, test loss: 0.193555\n",
      "Epoch 22, test loss: 0.199315\n",
      "Epoch 23, test loss: 0.191291\n",
      "Epoch 24, test loss: 0.191987\n",
      "Epoch 25, test loss: 0.194128\n",
      "Epoch 26, test loss: 0.190422\n",
      "Epoch 27, test loss: 0.201356\n",
      "Epoch 28, test loss: 0.188818\n",
      "Epoch 29, test loss: 0.188768\n",
      "Epoch 30, test loss: 0.189162\n",
      "Epoch 31, test loss: 0.188393\n",
      "Epoch 32, test loss: 0.200175\n",
      "Epoch 33, test loss: 0.189747\n",
      "Epoch 34, test loss: 0.192278\n",
      "Epoch 35, test loss: 0.195913\n",
      "Epoch 36, test loss: 0.193164\n",
      "Epoch 37, test loss: 0.188857\n",
      "Epoch 38, test loss: 0.188404\n",
      "Epoch 39, test loss: 0.189097\n",
      "Epoch 40, test loss: 0.197376\n",
      "Epoch 41, test loss: 0.188673\n",
      "Epoch 42, test loss: 0.192721\n",
      "Epoch 43, test loss: 0.188076\n",
      "Epoch 44, test loss: 0.189738\n",
      "Epoch 45, test loss: 0.209328\n",
      "Epoch 46, test loss: 0.188687\n",
      "Epoch 47, test loss: 0.188083\n",
      "Epoch 48, test loss: 0.188470\n",
      "Epoch 49, test loss: 0.189465\n",
      "Epoch 50, test loss: 0.195338\n",
      "Epoch 51, test loss: 0.189330\n",
      "Epoch 52, test loss: 0.195670\n",
      "Epoch 53, test loss: 0.188121\n",
      "Epoch 54, test loss: 0.190794\n",
      "Epoch 55, test loss: 0.189452\n",
      "Epoch 56, test loss: 0.193004\n",
      "Epoch 57, test loss: 0.187938\n",
      "Epoch 58, test loss: 0.188359\n",
      "Epoch 59, test loss: 0.190063\n",
      "Epoch 60, test loss: 0.188829\n",
      "Epoch 61, test loss: 0.188417\n",
      "Epoch 62, test loss: 0.188146\n",
      "Epoch 63, test loss: 0.197909\n",
      "Epoch 64, test loss: 0.190260\n",
      "Epoch 65, test loss: 0.188730\n",
      "Epoch 66, test loss: 0.187698\n",
      "Epoch 67, test loss: 0.192345\n",
      "Epoch 68, test loss: 0.189318\n",
      "Epoch 69, test loss: 0.188356\n",
      "Epoch 70, test loss: 0.188723\n",
      "Epoch 71, test loss: 0.194728\n",
      "Epoch 72, test loss: 0.191912\n",
      "Epoch 73, test loss: 0.188851\n",
      "Epoch 74, test loss: 0.188324\n",
      "Epoch 75, test loss: 0.188356\n",
      "Epoch 76, test loss: 0.188022\n",
      "Epoch 77, test loss: 0.194164\n",
      "Epoch 78, test loss: 0.190354\n",
      "Epoch 79, test loss: 0.188548\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2943CC070>\n",
      "Epoch 0, test loss: 0.192054\n",
      "Epoch 1, test loss: 0.188577\n",
      "Epoch 2, test loss: 0.188518\n",
      "Epoch 3, test loss: 0.188251\n",
      "Epoch 4, test loss: 0.204421\n",
      "Epoch 5, test loss: 0.189883\n",
      "Epoch 6, test loss: 0.188506\n",
      "Epoch 7, test loss: 0.189068\n",
      "Epoch 8, test loss: 0.189068\n",
      "Epoch 9, test loss: 0.189092\n",
      "Epoch 10, test loss: 0.188222\n",
      "Epoch 11, test loss: 0.187837\n",
      "Epoch 12, test loss: 0.191657\n",
      "Epoch 13, test loss: 0.189564\n",
      "Epoch 14, test loss: 0.188818\n",
      "Epoch 15, test loss: 0.188392\n",
      "Epoch 16, test loss: 0.196009\n",
      "Epoch 17, test loss: 0.189892\n",
      "Epoch 18, test loss: 0.194609\n",
      "Epoch 19, test loss: 0.188728\n",
      "Epoch 20, test loss: 0.190849\n",
      "Epoch 21, test loss: 0.193729\n",
      "Epoch 22, test loss: 0.197352\n",
      "Epoch 23, test loss: 0.191488\n",
      "Epoch 24, test loss: 0.189627\n",
      "Epoch 25, test loss: 0.194992\n",
      "Epoch 26, test loss: 0.189956\n",
      "Epoch 27, test loss: 0.189722\n",
      "Epoch 28, test loss: 0.193290\n",
      "Epoch 29, test loss: 0.194990\n",
      "Epoch 30, test loss: 0.189211\n",
      "Epoch 31, test loss: 0.188678\n",
      "Epoch 32, test loss: 0.190679\n",
      "Epoch 33, test loss: 0.188387\n",
      "Epoch 34, test loss: 0.189359\n",
      "Epoch 35, test loss: 0.191527\n",
      "Epoch 36, test loss: 0.190087\n",
      "Epoch 37, test loss: 0.189061\n",
      "Epoch 38, test loss: 0.189125\n",
      "Epoch 39, test loss: 0.188070\n",
      "Epoch 40, test loss: 0.188121\n",
      "Epoch 41, test loss: 0.192684\n",
      "Epoch 42, test loss: 0.189238\n",
      "Epoch 43, test loss: 0.190736\n",
      "Epoch 44, test loss: 0.188431\n",
      "Epoch 45, test loss: 0.190611\n",
      "Epoch 46, test loss: 0.188795\n",
      "Epoch 47, test loss: 0.190691\n",
      "Epoch 48, test loss: 0.188301\n",
      "Epoch 49, test loss: 0.190853\n",
      "Epoch 50, test loss: 0.190476\n",
      "Epoch 51, test loss: 0.188970\n",
      "Epoch 52, test loss: 0.189524\n",
      "Epoch 53, test loss: 0.194206\n",
      "Epoch 54, test loss: 0.206069\n",
      "Epoch 55, test loss: 0.194959\n",
      "Epoch 56, test loss: 0.191409\n",
      "Epoch 57, test loss: 0.188964\n",
      "Epoch 58, test loss: 0.189245\n",
      "Epoch 59, test loss: 0.192998\n",
      "Epoch 60, test loss: 0.188357\n",
      "Epoch 61, test loss: 0.188196\n",
      "Epoch 62, test loss: 0.189985\n",
      "Epoch 63, test loss: 0.190973\n",
      "Epoch 64, test loss: 0.192672\n",
      "Epoch 65, test loss: 0.191870\n",
      "Epoch 66, test loss: 0.189558\n",
      "Epoch 67, test loss: 0.189924\n",
      "Epoch 68, test loss: 0.192740\n",
      "Epoch 69, test loss: 0.193613\n",
      "Epoch 70, test loss: 0.192408\n",
      "Epoch 71, test loss: 0.195564\n",
      "Epoch 72, test loss: 0.192351\n",
      "Epoch 73, test loss: 0.192042\n",
      "Epoch 74, test loss: 0.192253\n",
      "Epoch 75, test loss: 0.187895\n",
      "Epoch 76, test loss: 0.193332\n",
      "Epoch 77, test loss: 0.189388\n",
      "Epoch 78, test loss: 0.202208\n",
      "Epoch 79, test loss: 0.188503\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2943CC070>\n",
      "Epoch 0, test loss: 0.239891\n",
      "Epoch 1, test loss: 0.214254\n",
      "Epoch 2, test loss: 0.199237\n",
      "Epoch 3, test loss: 0.197117\n",
      "Epoch 4, test loss: 0.202794\n",
      "Epoch 5, test loss: 0.208400\n",
      "Epoch 6, test loss: 0.197151\n",
      "Epoch 7, test loss: 0.194633\n",
      "Epoch 8, test loss: 0.193933\n",
      "Epoch 9, test loss: 0.195736\n",
      "Epoch 10, test loss: 0.195556\n",
      "Epoch 11, test loss: 0.201488\n",
      "Epoch 12, test loss: 0.192664\n",
      "Epoch 13, test loss: 0.199628\n",
      "Epoch 14, test loss: 0.192765\n",
      "Epoch 15, test loss: 0.207722\n",
      "Epoch 16, test loss: 0.196448\n",
      "Epoch 17, test loss: 0.192776\n",
      "Epoch 18, test loss: 0.193826\n",
      "Epoch 19, test loss: 0.193082\n",
      "Epoch 20, test loss: 0.196053\n",
      "Epoch 21, test loss: 0.192182\n",
      "Epoch 22, test loss: 0.192439\n",
      "Epoch 23, test loss: 0.193344\n",
      "Epoch 24, test loss: 0.191516\n",
      "Epoch 25, test loss: 0.192416\n",
      "Epoch 26, test loss: 0.191168\n",
      "Epoch 27, test loss: 0.194577\n",
      "Epoch 28, test loss: 0.194459\n",
      "Epoch 29, test loss: 0.192192\n",
      "Epoch 30, test loss: 0.196326\n",
      "Epoch 31, test loss: 0.192307\n",
      "Epoch 32, test loss: 0.191328\n",
      "Epoch 33, test loss: 0.190542\n",
      "Epoch 34, test loss: 0.196139\n",
      "Epoch 35, test loss: 0.198611\n",
      "Epoch 36, test loss: 0.191489\n",
      "Epoch 37, test loss: 0.194792\n",
      "Epoch 38, test loss: 0.192007\n",
      "Epoch 39, test loss: 0.192175\n",
      "Epoch 40, test loss: 0.200922\n",
      "Epoch 41, test loss: 0.190872\n",
      "Epoch 42, test loss: 0.194537\n",
      "Epoch 43, test loss: 0.199311\n",
      "Epoch 44, test loss: 0.190553\n",
      "Epoch 45, test loss: 0.190498\n",
      "Epoch 46, test loss: 0.191325\n",
      "Epoch 47, test loss: 0.192632\n",
      "Epoch 48, test loss: 0.194177\n",
      "Epoch 49, test loss: 0.191237\n",
      "Epoch 50, test loss: 0.197101\n",
      "Epoch 51, test loss: 0.191207\n",
      "Epoch 52, test loss: 0.201746\n",
      "Epoch 53, test loss: 0.192622\n",
      "Epoch 54, test loss: 0.191222\n",
      "Epoch 55, test loss: 0.193872\n",
      "Epoch 56, test loss: 0.190960\n",
      "Epoch 57, test loss: 0.190732\n",
      "Epoch 58, test loss: 0.195381\n",
      "Epoch 59, test loss: 0.194670\n",
      "Epoch 60, test loss: 0.195175\n",
      "Epoch 61, test loss: 0.194135\n",
      "Epoch 62, test loss: 0.190789\n",
      "Epoch 63, test loss: 0.192180\n",
      "Epoch 64, test loss: 0.192315\n",
      "Epoch 65, test loss: 0.190970\n",
      "Epoch 66, test loss: 0.197527\n",
      "Epoch 67, test loss: 0.198184\n",
      "Epoch 68, test loss: 0.201250\n",
      "Epoch 69, test loss: 0.192964\n",
      "Epoch 70, test loss: 0.197652\n",
      "Epoch 71, test loss: 0.191222\n",
      "Epoch 72, test loss: 0.191191\n",
      "Epoch 73, test loss: 0.190492\n",
      "Epoch 74, test loss: 0.195509\n",
      "Epoch 75, test loss: 0.190924\n",
      "Epoch 76, test loss: 0.201283\n",
      "Epoch 77, test loss: 0.190257\n",
      "Epoch 78, test loss: 0.190756\n",
      "Epoch 79, test loss: 0.196060\n",
      "Epoch 80, test loss: 0.192968\n",
      "Epoch 81, test loss: 0.191928\n",
      "Epoch 82, test loss: 0.192278\n",
      "Epoch 83, test loss: 0.192995\n",
      "Epoch 84, test loss: 0.196825\n",
      "Epoch 85, test loss: 0.200016\n",
      "Epoch 86, test loss: 0.191083\n",
      "Epoch 87, test loss: 0.203390\n",
      "Epoch 88, test loss: 0.199619\n",
      "Epoch 89, test loss: 0.191817\n",
      "Epoch 90, test loss: 0.192662\n",
      "Epoch 91, test loss: 0.192486\n",
      "Epoch 92, test loss: 0.191525\n",
      "Epoch 93, test loss: 0.191605\n",
      "Epoch 94, test loss: 0.190679\n",
      "Epoch 95, test loss: 0.196441\n",
      "Epoch 96, test loss: 0.191871\n",
      "Epoch 97, test loss: 0.197747\n",
      "Epoch 98, test loss: 0.190846\n",
      "Epoch 99, test loss: 0.200321\n",
      "Epoch 100, test loss: 0.192137\n",
      "Epoch 101, test loss: 0.190285\n",
      "Epoch 102, test loss: 0.192087\n",
      "Epoch 103, test loss: 0.190626\n",
      "Epoch 104, test loss: 0.190773\n",
      "Epoch 105, test loss: 0.190612\n",
      "Epoch 106, test loss: 0.190917\n",
      "Epoch 107, test loss: 0.191093\n",
      "Epoch 108, test loss: 0.191019\n",
      "Epoch 109, test loss: 0.190876\n",
      "Epoch 110, test loss: 0.190373\n",
      "Epoch 111, test loss: 0.191176\n",
      "Epoch 112, test loss: 0.195131\n",
      "Epoch 113, test loss: 0.192710\n",
      "Epoch 114, test loss: 0.190548\n",
      "Epoch 115, test loss: 0.191557\n",
      "Epoch 116, test loss: 0.190813\n",
      "Epoch 117, test loss: 0.191278\n",
      "Epoch 118, test loss: 0.190758\n",
      "Epoch 119, test loss: 0.196770\n",
      "Epoch 120, test loss: 0.190884\n",
      "Epoch 121, test loss: 0.195004\n",
      "Epoch 122, test loss: 0.191912\n",
      "Epoch 123, test loss: 0.200226\n",
      "Epoch 124, test loss: 0.191990\n",
      "Epoch 125, test loss: 0.196798\n",
      "Epoch 126, test loss: 0.191860\n",
      "Epoch 127, test loss: 0.191675\n",
      "Epoch 128, test loss: 0.190670\n",
      "Epoch 129, test loss: 0.190507\n",
      "Epoch 130, test loss: 0.191289\n",
      "Epoch 131, test loss: 0.191244\n",
      "Epoch 132, test loss: 0.191405\n",
      "Epoch 133, test loss: 0.202891\n",
      "Epoch 134, test loss: 0.190974\n",
      "Epoch 135, test loss: 0.191417\n",
      "Epoch 136, test loss: 0.191571\n",
      "Epoch 137, test loss: 0.198148\n",
      "Epoch 138, test loss: 0.191233\n",
      "Epoch 139, test loss: 0.196434\n",
      "Epoch 140, test loss: 0.194538\n",
      "Epoch 141, test loss: 0.190623\n",
      "Epoch 142, test loss: 0.200892\n",
      "Epoch 143, test loss: 0.193335\n",
      "Epoch 144, test loss: 0.190897\n",
      "Epoch 145, test loss: 0.194347\n",
      "Epoch 146, test loss: 0.191710\n",
      "Epoch 147, test loss: 0.191687\n",
      "Epoch 148, test loss: 0.191906\n",
      "Epoch 149, test loss: 0.190972\n",
      "Epoch 150, test loss: 0.192686\n",
      "Epoch 151, test loss: 0.191234\n",
      "Epoch 152, test loss: 0.190589\n",
      "Epoch 153, test loss: 0.192691\n",
      "Epoch 154, test loss: 0.195653\n",
      "Epoch 155, test loss: 0.191684\n",
      "Epoch 156, test loss: 0.194260\n",
      "Epoch 157, test loss: 0.191187\n",
      "Epoch 158, test loss: 0.201140\n",
      "Epoch 159, test loss: 0.197218\n",
      "Epoch 160, test loss: 0.191024\n",
      "Epoch 161, test loss: 0.191379\n",
      "Epoch 162, test loss: 0.191421\n",
      "Epoch 163, test loss: 0.192421\n",
      "Epoch 164, test loss: 0.192365\n",
      "Epoch 165, test loss: 0.192282\n",
      "Epoch 166, test loss: 0.192391\n",
      "Epoch 167, test loss: 0.191973\n",
      "Epoch 168, test loss: 0.191810\n",
      "Epoch 169, test loss: 0.191179\n",
      "Epoch 170, test loss: 0.192523\n",
      "Epoch 171, test loss: 0.195296\n",
      "Epoch 172, test loss: 0.197406\n",
      "Epoch 173, test loss: 0.192891\n",
      "Epoch 174, test loss: 0.193931\n",
      "Epoch 175, test loss: 0.193533\n",
      "Epoch 176, test loss: 0.193946\n",
      "Epoch 177, test loss: 0.194994\n",
      "Epoch 178, test loss: 0.192559\n",
      "Epoch 179, test loss: 0.190692\n",
      "Epoch 180, test loss: 0.199018\n",
      "Epoch 181, test loss: 0.191641\n",
      "Epoch 182, test loss: 0.192386\n",
      "Epoch 183, test loss: 0.196563\n",
      "Epoch 184, test loss: 0.202643\n",
      "Epoch 185, test loss: 0.197493\n",
      "Epoch 186, test loss: 0.191170\n",
      "Epoch 187, test loss: 0.200981\n",
      "Epoch 188, test loss: 0.190399\n",
      "Epoch 189, test loss: 0.193245\n",
      "Epoch 190, test loss: 0.194617\n",
      "Epoch 191, test loss: 0.205796\n",
      "Epoch 192, test loss: 0.198909\n",
      "Epoch 193, test loss: 0.190827\n",
      "Epoch 194, test loss: 0.208102\n",
      "Epoch 195, test loss: 0.197493\n",
      "Epoch 196, test loss: 0.198425\n",
      "Epoch 197, test loss: 0.190978\n",
      "Epoch 198, test loss: 0.194233\n",
      "Epoch 199, test loss: 0.190579\n",
      "Epoch 200, test loss: 0.193114\n",
      "Epoch 201, test loss: 0.193762\n",
      "Epoch 202, test loss: 0.192098\n",
      "Epoch 203, test loss: 0.191794\n",
      "Epoch 204, test loss: 0.192632\n",
      "Epoch 205, test loss: 0.190532\n",
      "Epoch 206, test loss: 0.190779\n",
      "Epoch 207, test loss: 0.194668\n",
      "Epoch 208, test loss: 0.192072\n",
      "Epoch 209, test loss: 0.191063\n",
      "Epoch 210, test loss: 0.191089\n",
      "Epoch 211, test loss: 0.192371\n",
      "Epoch 212, test loss: 0.191945\n",
      "Epoch 213, test loss: 0.190824\n",
      "Epoch 214, test loss: 0.191241\n",
      "Epoch 215, test loss: 0.192620\n",
      "Epoch 216, test loss: 0.190835\n",
      "Epoch 217, test loss: 0.190834\n",
      "Epoch 218, test loss: 0.192521\n",
      "Epoch 219, test loss: 0.192705\n",
      "Epoch 220, test loss: 0.191339\n",
      "Epoch 221, test loss: 0.190665\n",
      "Epoch 222, test loss: 0.191085\n",
      "Epoch 223, test loss: 0.190873\n",
      "Epoch 224, test loss: 0.198425\n",
      "Epoch 225, test loss: 0.196359\n",
      "Epoch 226, test loss: 0.196240\n",
      "Epoch 227, test loss: 0.192532\n",
      "Epoch 228, test loss: 0.198427\n",
      "Epoch 229, test loss: 0.190720\n",
      "Epoch 230, test loss: 0.195884\n",
      "Epoch 231, test loss: 0.190679\n",
      "Epoch 232, test loss: 0.191661\n",
      "Epoch 233, test loss: 0.200788\n",
      "Epoch 234, test loss: 0.192755\n",
      "Epoch 235, test loss: 0.193317\n",
      "Epoch 236, test loss: 0.196268\n",
      "Epoch 237, test loss: 0.198354\n",
      "Epoch 238, test loss: 0.193833\n",
      "Epoch 239, test loss: 0.195309\n",
      "Pretrain data: 19653653.0\n",
      "Building dataset, requesting data from 0 to 820\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7592/106252\n",
      "Found 820 continuous time series\n",
      "Data shape: (113846, 6), Train/test: 113844/2\n",
      "Train test ratio: 56922.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1778\n",
      "Epoch 0, train loss: 0.207981\n",
      "Epoch 1, train loss: 0.228150\n",
      "Epoch 2, train loss: 0.244868\n",
      "Epoch 3, train loss: 0.390650\n",
      "Epoch 4, train loss: 0.244976\n",
      "Epoch 5, train loss: 0.240508\n",
      "Epoch 6, train loss: 0.195189\n",
      "Epoch 7, train loss: 0.190200\n",
      "Epoch 8, train loss: 0.266778\n",
      "Epoch 9, train loss: 0.211916\n",
      "Epoch 10, train loss: 0.179836\n",
      "Epoch 11, train loss: 0.200822\n",
      "Epoch 12, train loss: 0.280081\n",
      "Epoch 13, train loss: 0.208814\n",
      "Epoch 14, train loss: 0.180756\n",
      "Epoch 15, train loss: 0.204961\n",
      "Epoch 16, train loss: 0.269000\n",
      "Epoch 17, train loss: 0.228601\n",
      "Epoch 18, train loss: 0.265869\n",
      "Epoch 19, train loss: 0.245510\n",
      "Epoch 20, train loss: 0.175581\n",
      "Epoch 21, train loss: 0.145277\n",
      "Epoch 22, train loss: 0.179725\n",
      "Epoch 23, train loss: 0.191148\n",
      "Epoch 24, train loss: 0.222822\n",
      "Epoch 25, train loss: 0.220093\n",
      "Epoch 26, train loss: 0.177662\n",
      "Epoch 27, train loss: 0.249832\n",
      "Epoch 28, train loss: 0.304390\n",
      "Epoch 29, train loss: 0.239265\n",
      "Epoch 30, train loss: 0.240630\n",
      "Epoch 31, train loss: 0.295691\n",
      "Epoch 32, train loss: 0.195783\n",
      "Epoch 33, train loss: 0.142965\n",
      "Epoch 34, train loss: 0.242095\n",
      "Epoch 35, train loss: 0.238567\n",
      "Epoch 36, train loss: 0.259645\n",
      "Epoch 37, train loss: 0.222102\n",
      "Epoch 38, train loss: 0.232259\n",
      "Epoch 39, train loss: 0.256717\n",
      "Epoch 40, train loss: 0.213444\n",
      "Epoch 41, train loss: 0.228935\n",
      "Epoch 42, train loss: 0.200455\n",
      "Epoch 43, train loss: 0.193997\n",
      "Epoch 44, train loss: 0.212151\n",
      "Epoch 45, train loss: 0.189610\n",
      "Epoch 46, train loss: 0.209794\n",
      "Epoch 47, train loss: 0.248306\n",
      "Epoch 48, train loss: 0.189573\n",
      "Epoch 49, train loss: 0.226228\n",
      "Epoch 50, train loss: 0.234746\n",
      "Epoch 51, train loss: 0.210389\n",
      "Epoch 52, train loss: 0.199841\n",
      "Epoch 53, train loss: 0.203093\n",
      "Epoch 54, train loss: 0.264558\n",
      "Epoch 55, train loss: 0.226437\n",
      "Epoch 56, train loss: 0.166696\n",
      "Epoch 57, train loss: 0.296035\n",
      "Epoch 58, train loss: 0.254767\n",
      "Epoch 59, train loss: 0.170277\n",
      "Epoch 60, train loss: 0.236839\n",
      "Epoch 61, train loss: 0.165027\n",
      "Epoch 62, train loss: 0.248854\n",
      "Epoch 63, train loss: 0.177106\n",
      "Epoch 64, train loss: 0.167884\n",
      "Epoch 65, train loss: 0.182320\n",
      "Epoch 66, train loss: 0.180856\n",
      "Epoch 67, train loss: 0.169560\n",
      "Epoch 68, train loss: 0.230093\n",
      "Epoch 69, train loss: 0.250746\n",
      "Epoch 70, train loss: 0.281836\n",
      "Epoch 71, train loss: 0.256045\n",
      "Epoch 72, train loss: 0.222106\n",
      "Epoch 73, train loss: 0.195302\n",
      "Epoch 74, train loss: 0.161105\n",
      "Epoch 75, train loss: 0.259761\n",
      "Epoch 76, train loss: 0.211854\n",
      "Epoch 77, train loss: 0.195411\n",
      "Epoch 78, train loss: 0.195610\n",
      "Epoch 79, train loss: 0.195259\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "x here is\n",
      "[[239. 238. 235. 233. 231. 229.]\n",
      " [238. 235. 233. 231. 229. 227.]\n",
      " [235. 233. 231. 229. 227. 222.]\n",
      " ...\n",
      " [140. 151. 154. 152. 151. 149.]\n",
      " [151. 154. 152. 151. 149. 149.]\n",
      " [154. 152. 151. 149. 149. 145.]]\n",
      "y here is\n",
      "[[208. 208. 208. 208. 208. 208.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " [204. 204. 204. 204. 204. 204.]\n",
      " ...\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [140. 140. 140. 140. 140. 140.]\n",
      " [145. 145. 145. 145. 145. 145.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (2526, 6), Train/test: 1/2525\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "x here is\n",
      "[[219. 229. 224. 221. 215. 209.]\n",
      " [229. 224. 221. 215. 209. 203.]\n",
      " [224. 221. 215. 209. 203. 199.]\n",
      " ...\n",
      " [220. 225. 232. 237. 241. 243.]\n",
      " [225. 232. 237. 241. 243. 251.]\n",
      " [232. 237. 241. 243. 251. 257.]]\n",
      "y here is\n",
      "[[202. 202. 202. 202. 202. 202.]\n",
      " [192. 192. 192. 192. 192. 192.]\n",
      " [194. 194. 194. 194. 194. 194.]\n",
      " ...\n",
      " [250. 250. 250. 250. 250. 250.]\n",
      " [246. 246. 246. 246. 246. 246.]\n",
      " [240. 240. 240. 240. 240. 240.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 693/11187\n",
      "Found 22 continuous time series\n",
      "Data shape: (11882, 6), Train/test: 11880/2\n",
      "Train test ratio: 5940.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2934FC370>\n",
      "Epoch 0, test loss: 0.181581\n",
      "Epoch 1, test loss: 0.181637\n",
      "Epoch 2, test loss: 0.182155\n",
      "Epoch 3, test loss: 0.181012\n",
      "Epoch 4, test loss: 0.182223\n",
      "Epoch 5, test loss: 0.182922\n",
      "Epoch 6, test loss: 0.180669\n",
      "Epoch 7, test loss: 0.179997\n",
      "Epoch 8, test loss: 0.182405\n",
      "Epoch 9, test loss: 0.179427\n",
      "Epoch 10, test loss: 0.181282\n",
      "Epoch 11, test loss: 0.180219\n",
      "Epoch 12, test loss: 0.180188\n",
      "Epoch 13, test loss: 0.180687\n",
      "Epoch 14, test loss: 0.181716\n",
      "Epoch 15, test loss: 0.184240\n",
      "Epoch 16, test loss: 0.181069\n",
      "Epoch 17, test loss: 0.181594\n",
      "Epoch 18, test loss: 0.181891\n",
      "Epoch 19, test loss: 0.179668\n",
      "Epoch 20, test loss: 0.179454\n",
      "Epoch 21, test loss: 0.185871\n",
      "Epoch 22, test loss: 0.188779\n",
      "Epoch 23, test loss: 0.179210\n",
      "Epoch 24, test loss: 0.179701\n",
      "Epoch 25, test loss: 0.183583\n",
      "Epoch 26, test loss: 0.179831\n",
      "Epoch 27, test loss: 0.179922\n",
      "Epoch 28, test loss: 0.180011\n",
      "Epoch 29, test loss: 0.179813\n",
      "Epoch 30, test loss: 0.180197\n",
      "Epoch 31, test loss: 0.184638\n",
      "Epoch 32, test loss: 0.179736\n",
      "Epoch 33, test loss: 0.182171\n",
      "Epoch 34, test loss: 0.180089\n",
      "Epoch 35, test loss: 0.180068\n",
      "Epoch 36, test loss: 0.179117\n",
      "Epoch 37, test loss: 0.179412\n",
      "Epoch 38, test loss: 0.184045\n",
      "Epoch 39, test loss: 0.188311\n",
      "Epoch 40, test loss: 0.179863\n",
      "Epoch 41, test loss: 0.179185\n",
      "Epoch 42, test loss: 0.179232\n",
      "Epoch 43, test loss: 0.181143\n",
      "Epoch 44, test loss: 0.179155\n",
      "Epoch 45, test loss: 0.179519\n",
      "Epoch 46, test loss: 0.183882\n",
      "Epoch 47, test loss: 0.181179\n",
      "Epoch 48, test loss: 0.180567\n",
      "Epoch 49, test loss: 0.180767\n",
      "Epoch 50, test loss: 0.180045\n",
      "Epoch 51, test loss: 0.191400\n",
      "Epoch 52, test loss: 0.182707\n",
      "Epoch 53, test loss: 0.184648\n",
      "Epoch 54, test loss: 0.179769\n",
      "Epoch 55, test loss: 0.179595\n",
      "Epoch 56, test loss: 0.178420\n",
      "Epoch 57, test loss: 0.179720\n",
      "Epoch 58, test loss: 0.179428\n",
      "Epoch 59, test loss: 0.179477\n",
      "Epoch 60, test loss: 0.178977\n",
      "Epoch 61, test loss: 0.180035\n",
      "Epoch 62, test loss: 0.179948\n",
      "Epoch 63, test loss: 0.178515\n",
      "Epoch 64, test loss: 0.179753\n",
      "Epoch 65, test loss: 0.180124\n",
      "Epoch 66, test loss: 0.185245\n",
      "Epoch 67, test loss: 0.178838\n",
      "Epoch 68, test loss: 0.179173\n",
      "Epoch 69, test loss: 0.179945\n",
      "Epoch 70, test loss: 0.180342\n",
      "Epoch 71, test loss: 0.180984\n",
      "Epoch 72, test loss: 0.179879\n",
      "Epoch 73, test loss: 0.179542\n",
      "Epoch 74, test loss: 0.187320\n",
      "Epoch 75, test loss: 0.180156\n",
      "Epoch 76, test loss: 0.181028\n",
      "Epoch 77, test loss: 0.181577\n",
      "Epoch 78, test loss: 0.179914\n",
      "Epoch 79, test loss: 0.180107\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2934FC370>\n",
      "Epoch 0, test loss: 0.182104\n",
      "Epoch 1, test loss: 0.181835\n",
      "Epoch 2, test loss: 0.183848\n",
      "Epoch 3, test loss: 0.181975\n",
      "Epoch 4, test loss: 0.182104\n",
      "Epoch 5, test loss: 0.181748\n",
      "Epoch 6, test loss: 0.180270\n",
      "Epoch 7, test loss: 0.185735\n",
      "Epoch 8, test loss: 0.180178\n",
      "Epoch 9, test loss: 0.181853\n",
      "Epoch 10, test loss: 0.179691\n",
      "Epoch 11, test loss: 0.181051\n",
      "Epoch 12, test loss: 0.182373\n",
      "Epoch 13, test loss: 0.180018\n",
      "Epoch 14, test loss: 0.180166\n",
      "Epoch 15, test loss: 0.182054\n",
      "Epoch 16, test loss: 0.180641\n",
      "Epoch 17, test loss: 0.181764\n",
      "Epoch 18, test loss: 0.181384\n",
      "Epoch 19, test loss: 0.181438\n",
      "Epoch 20, test loss: 0.180780\n",
      "Epoch 21, test loss: 0.186490\n",
      "Epoch 22, test loss: 0.181504\n",
      "Epoch 23, test loss: 0.180513\n",
      "Epoch 24, test loss: 0.183917\n",
      "Epoch 25, test loss: 0.180695\n",
      "Epoch 26, test loss: 0.179976\n",
      "Epoch 27, test loss: 0.179767\n",
      "Epoch 28, test loss: 0.180243\n",
      "Epoch 29, test loss: 0.179685\n",
      "Epoch 30, test loss: 0.180064\n",
      "Epoch 31, test loss: 0.184904\n",
      "Epoch 32, test loss: 0.180648\n",
      "Epoch 33, test loss: 0.184495\n",
      "Epoch 34, test loss: 0.185137\n",
      "Epoch 35, test loss: 0.180188\n",
      "Epoch 36, test loss: 0.180001\n",
      "Epoch 37, test loss: 0.181491\n",
      "Epoch 38, test loss: 0.180835\n",
      "Epoch 39, test loss: 0.181224\n",
      "Epoch 40, test loss: 0.180335\n",
      "Epoch 41, test loss: 0.179827\n",
      "Epoch 42, test loss: 0.181070\n",
      "Epoch 43, test loss: 0.179836\n",
      "Epoch 44, test loss: 0.182666\n",
      "Epoch 45, test loss: 0.181581\n",
      "Epoch 46, test loss: 0.183035\n",
      "Epoch 47, test loss: 0.189151\n",
      "Epoch 48, test loss: 0.182913\n",
      "Epoch 49, test loss: 0.180795\n",
      "Epoch 50, test loss: 0.180866\n",
      "Epoch 51, test loss: 0.180292\n",
      "Epoch 52, test loss: 0.180819\n",
      "Epoch 53, test loss: 0.187149\n",
      "Epoch 54, test loss: 0.181100\n",
      "Epoch 55, test loss: 0.180579\n",
      "Epoch 56, test loss: 0.180296\n",
      "Epoch 57, test loss: 0.180214\n",
      "Epoch 58, test loss: 0.180887\n",
      "Epoch 59, test loss: 0.180436\n",
      "Epoch 60, test loss: 0.179946\n",
      "Epoch 61, test loss: 0.180827\n",
      "Epoch 62, test loss: 0.180535\n",
      "Epoch 63, test loss: 0.181077\n",
      "Epoch 64, test loss: 0.182266\n",
      "Epoch 65, test loss: 0.181947\n",
      "Epoch 66, test loss: 0.180372\n",
      "Epoch 67, test loss: 0.182835\n",
      "Epoch 68, test loss: 0.180864\n",
      "Epoch 69, test loss: 0.184594\n",
      "Epoch 70, test loss: 0.181568\n",
      "Epoch 71, test loss: 0.181392\n",
      "Epoch 72, test loss: 0.180576\n",
      "Epoch 73, test loss: 0.180463\n",
      "Epoch 74, test loss: 0.181391\n",
      "Epoch 75, test loss: 0.180940\n",
      "Epoch 76, test loss: 0.180253\n",
      "Epoch 77, test loss: 0.180271\n",
      "Epoch 78, test loss: 0.180878\n",
      "Epoch 79, test loss: 0.180853\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2934FC370>\n",
      "Epoch 0, test loss: 0.207932\n",
      "Epoch 1, test loss: 0.185952\n",
      "Epoch 2, test loss: 0.188837\n",
      "Epoch 3, test loss: 0.186277\n",
      "Epoch 4, test loss: 0.186376\n",
      "Epoch 5, test loss: 0.187426\n",
      "Epoch 6, test loss: 0.186577\n",
      "Epoch 7, test loss: 0.190554\n",
      "Epoch 8, test loss: 0.188497\n",
      "Epoch 9, test loss: 0.187831\n",
      "Epoch 10, test loss: 0.188971\n",
      "Epoch 11, test loss: 0.189289\n",
      "Epoch 12, test loss: 0.197128\n",
      "Epoch 13, test loss: 0.188924\n",
      "Epoch 14, test loss: 0.189698\n",
      "Epoch 15, test loss: 0.187684\n",
      "Epoch 16, test loss: 0.187358\n",
      "Epoch 17, test loss: 0.188790\n",
      "Epoch 18, test loss: 0.191472\n",
      "Epoch 19, test loss: 0.187775\n",
      "Epoch 20, test loss: 0.187440\n",
      "Epoch 21, test loss: 0.187338\n",
      "Epoch 22, test loss: 0.188226\n",
      "Epoch 23, test loss: 0.188257\n",
      "Epoch 24, test loss: 0.190271\n",
      "Epoch 25, test loss: 0.187164\n",
      "Epoch 26, test loss: 0.188105\n",
      "Epoch 27, test loss: 0.187406\n",
      "Epoch 28, test loss: 0.187584\n",
      "Epoch 29, test loss: 0.187163\n",
      "Epoch 30, test loss: 0.188588\n",
      "Epoch 31, test loss: 0.187070\n",
      "Epoch 32, test loss: 0.187340\n",
      "Epoch 33, test loss: 0.187635\n",
      "Epoch 34, test loss: 0.188640\n",
      "Epoch 35, test loss: 0.188378\n",
      "Epoch 36, test loss: 0.187201\n",
      "Epoch 37, test loss: 0.188410\n",
      "Epoch 38, test loss: 0.189563\n",
      "Epoch 39, test loss: 0.186915\n",
      "Epoch 40, test loss: 0.187290\n",
      "Epoch 41, test loss: 0.187311\n",
      "Epoch 42, test loss: 0.188674\n",
      "Epoch 43, test loss: 0.186596\n",
      "Epoch 44, test loss: 0.186964\n",
      "Epoch 45, test loss: 0.187829\n",
      "Epoch 46, test loss: 0.188503\n",
      "Epoch 47, test loss: 0.195657\n",
      "Epoch 48, test loss: 0.187926\n",
      "Epoch 49, test loss: 0.187681\n",
      "Epoch 50, test loss: 0.187989\n",
      "Epoch 51, test loss: 0.186252\n",
      "Epoch 52, test loss: 0.186391\n",
      "Epoch 53, test loss: 0.185363\n",
      "Epoch 54, test loss: 0.185841\n",
      "Epoch 55, test loss: 0.191660\n",
      "Epoch 56, test loss: 0.185006\n",
      "Epoch 57, test loss: 0.185347\n",
      "Epoch 58, test loss: 0.185898\n",
      "Epoch 59, test loss: 0.185745\n",
      "Epoch 60, test loss: 0.185060\n",
      "Epoch 61, test loss: 0.187382\n",
      "Epoch 62, test loss: 0.186210\n",
      "Epoch 63, test loss: 0.185841\n",
      "Epoch 64, test loss: 0.188900\n",
      "Epoch 65, test loss: 0.186468\n",
      "Epoch 66, test loss: 0.186481\n",
      "Epoch 67, test loss: 0.187225\n",
      "Epoch 68, test loss: 0.187680\n",
      "Epoch 69, test loss: 0.186162\n",
      "Epoch 70, test loss: 0.186618\n",
      "Epoch 71, test loss: 0.188931\n",
      "Epoch 72, test loss: 0.186883\n",
      "Epoch 73, test loss: 0.187684\n",
      "Epoch 74, test loss: 0.187491\n",
      "Epoch 75, test loss: 0.186915\n",
      "Epoch 76, test loss: 0.188479\n",
      "Epoch 77, test loss: 0.187506\n",
      "Epoch 78, test loss: 0.188420\n",
      "Epoch 79, test loss: 0.188602\n",
      "Epoch 80, test loss: 0.189784\n",
      "Epoch 81, test loss: 0.188130\n",
      "Epoch 82, test loss: 0.187149\n",
      "Epoch 83, test loss: 0.186901\n",
      "Epoch 84, test loss: 0.187306\n",
      "Epoch 85, test loss: 0.187547\n",
      "Epoch 86, test loss: 0.186395\n",
      "Epoch 87, test loss: 0.186811\n",
      "Epoch 88, test loss: 0.187617\n",
      "Epoch 89, test loss: 0.186661\n",
      "Epoch 90, test loss: 0.187450\n",
      "Epoch 91, test loss: 0.186867\n",
      "Epoch 92, test loss: 0.187378\n",
      "Epoch 93, test loss: 0.186292\n",
      "Epoch 94, test loss: 0.189974\n",
      "Epoch 95, test loss: 0.186441\n",
      "Epoch 96, test loss: 0.186878\n",
      "Epoch 97, test loss: 0.187404\n",
      "Epoch 98, test loss: 0.187532\n",
      "Epoch 99, test loss: 0.187100\n",
      "Epoch 100, test loss: 0.187436\n",
      "Epoch 101, test loss: 0.187210\n",
      "Epoch 102, test loss: 0.187574\n",
      "Epoch 103, test loss: 0.186353\n",
      "Epoch 104, test loss: 0.188546\n",
      "Epoch 105, test loss: 0.187135\n",
      "Epoch 106, test loss: 0.187478\n",
      "Epoch 107, test loss: 0.186428\n",
      "Epoch 108, test loss: 0.186179\n",
      "Epoch 109, test loss: 0.186954\n",
      "Epoch 110, test loss: 0.186754\n",
      "Epoch 111, test loss: 0.186365\n",
      "Epoch 112, test loss: 0.185949\n",
      "Epoch 113, test loss: 0.187906\n",
      "Epoch 114, test loss: 0.187022\n",
      "Epoch 115, test loss: 0.187778\n",
      "Epoch 116, test loss: 0.186007\n",
      "Epoch 117, test loss: 0.186443\n",
      "Epoch 118, test loss: 0.187269\n",
      "Epoch 119, test loss: 0.186048\n",
      "Epoch 120, test loss: 0.186092\n",
      "Epoch 121, test loss: 0.186258\n",
      "Epoch 122, test loss: 0.186334\n",
      "Epoch 123, test loss: 0.189881\n",
      "Epoch 124, test loss: 0.186736\n",
      "Epoch 125, test loss: 0.185457\n",
      "Epoch 126, test loss: 0.186064\n",
      "Epoch 127, test loss: 0.187722\n",
      "Epoch 128, test loss: 0.185440\n",
      "Epoch 129, test loss: 0.185691\n",
      "Epoch 130, test loss: 0.186132\n",
      "Epoch 131, test loss: 0.186494\n",
      "Epoch 132, test loss: 0.188510\n",
      "Epoch 133, test loss: 0.185725\n",
      "Epoch 134, test loss: 0.185636\n",
      "Epoch 135, test loss: 0.187515\n",
      "Epoch 136, test loss: 0.188702\n",
      "Epoch 137, test loss: 0.186227\n",
      "Epoch 138, test loss: 0.188539\n",
      "Epoch 139, test loss: 0.185899\n",
      "Epoch 140, test loss: 0.186958\n",
      "Epoch 141, test loss: 0.185077\n",
      "Epoch 142, test loss: 0.187148\n",
      "Epoch 143, test loss: 0.186185\n",
      "Epoch 144, test loss: 0.186149\n",
      "Epoch 145, test loss: 0.188225\n",
      "Epoch 146, test loss: 0.186314\n",
      "Epoch 147, test loss: 0.185771\n",
      "Epoch 148, test loss: 0.185701\n",
      "Epoch 149, test loss: 0.185986\n",
      "Epoch 150, test loss: 0.186374\n",
      "Epoch 151, test loss: 0.185597\n",
      "Epoch 152, test loss: 0.184971\n",
      "Epoch 153, test loss: 0.185677\n",
      "Epoch 154, test loss: 0.188558\n",
      "Epoch 155, test loss: 0.185061\n",
      "Epoch 156, test loss: 0.186630\n",
      "Epoch 157, test loss: 0.184792\n",
      "Epoch 158, test loss: 0.186461\n",
      "Epoch 159, test loss: 0.185192\n",
      "Epoch 160, test loss: 0.185978\n",
      "Epoch 161, test loss: 0.185434\n",
      "Epoch 162, test loss: 0.186600\n",
      "Epoch 163, test loss: 0.188706\n",
      "Epoch 164, test loss: 0.184676\n",
      "Epoch 165, test loss: 0.185044\n",
      "Epoch 166, test loss: 0.187208\n",
      "Epoch 167, test loss: 0.188089\n",
      "Epoch 168, test loss: 0.184929\n",
      "Epoch 169, test loss: 0.187227\n",
      "Epoch 170, test loss: 0.186729\n",
      "Epoch 171, test loss: 0.189013\n",
      "Epoch 172, test loss: 0.185667\n",
      "Epoch 173, test loss: 0.184497\n",
      "Epoch 174, test loss: 0.187459\n",
      "Epoch 175, test loss: 0.185113\n",
      "Epoch 176, test loss: 0.185263\n",
      "Epoch 177, test loss: 0.185230\n",
      "Epoch 178, test loss: 0.184678\n",
      "Epoch 179, test loss: 0.188962\n",
      "Epoch 180, test loss: 0.185809\n",
      "Epoch 181, test loss: 0.185121\n",
      "Epoch 182, test loss: 0.184913\n",
      "Epoch 183, test loss: 0.186865\n",
      "Epoch 184, test loss: 0.185085\n",
      "Epoch 185, test loss: 0.184806\n",
      "Epoch 186, test loss: 0.186300\n",
      "Epoch 187, test loss: 0.187161\n",
      "Epoch 188, test loss: 0.187946\n",
      "Epoch 189, test loss: 0.186404\n",
      "Epoch 190, test loss: 0.185392\n",
      "Epoch 191, test loss: 0.185812\n",
      "Epoch 192, test loss: 0.184770\n",
      "Epoch 193, test loss: 0.185972\n",
      "Epoch 194, test loss: 0.185032\n",
      "Epoch 195, test loss: 0.185412\n",
      "Epoch 196, test loss: 0.185459\n",
      "Epoch 197, test loss: 0.185706\n",
      "Epoch 198, test loss: 0.184972\n",
      "Epoch 199, test loss: 0.184471\n",
      "Epoch 200, test loss: 0.187153\n",
      "Epoch 201, test loss: 0.185752\n",
      "Epoch 202, test loss: 0.193266\n",
      "Epoch 203, test loss: 0.184786\n",
      "Epoch 204, test loss: 0.185285\n",
      "Epoch 205, test loss: 0.185730\n",
      "Epoch 206, test loss: 0.184464\n",
      "Epoch 207, test loss: 0.184446\n",
      "Epoch 208, test loss: 0.184274\n",
      "Epoch 209, test loss: 0.188744\n",
      "Epoch 210, test loss: 0.187935\n",
      "Epoch 211, test loss: 0.187885\n",
      "Epoch 212, test loss: 0.186198\n",
      "Epoch 213, test loss: 0.184682\n",
      "Epoch 214, test loss: 0.184661\n",
      "Epoch 215, test loss: 0.185601\n",
      "Epoch 216, test loss: 0.184869\n",
      "Epoch 217, test loss: 0.188880\n",
      "Epoch 218, test loss: 0.189533\n",
      "Epoch 219, test loss: 0.184569\n",
      "Epoch 220, test loss: 0.186146\n",
      "Epoch 221, test loss: 0.185216\n",
      "Epoch 222, test loss: 0.185090\n",
      "Epoch 223, test loss: 0.184677\n",
      "Epoch 224, test loss: 0.184032\n",
      "Epoch 225, test loss: 0.184312\n",
      "Epoch 226, test loss: 0.189209\n",
      "Epoch 227, test loss: 0.186486\n",
      "Epoch 228, test loss: 0.185769\n",
      "Epoch 229, test loss: 0.185137\n",
      "Epoch 230, test loss: 0.185619\n",
      "Epoch 231, test loss: 0.185441\n",
      "Epoch 232, test loss: 0.189133\n",
      "Epoch 233, test loss: 0.185084\n",
      "Epoch 234, test loss: 0.184310\n",
      "Epoch 235, test loss: 0.185253\n",
      "Epoch 236, test loss: 0.189529\n",
      "Epoch 237, test loss: 0.187021\n",
      "Epoch 238, test loss: 0.184350\n",
      "Epoch 239, test loss: 0.184926\n",
      "Pretrain data: 19365644.0\n",
      "Building dataset, requesting data from 0 to 821\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7865/107110\n",
      "Found 821 continuous time series\n",
      "Data shape: (114977, 6), Train/test: 114975/2\n",
      "Train test ratio: 57487.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1796\n",
      "Epoch 0, train loss: 0.215005\n",
      "Epoch 1, train loss: 0.207568\n",
      "Epoch 2, train loss: 0.191716\n",
      "Epoch 3, train loss: 0.216492\n",
      "Epoch 4, train loss: 0.168726\n",
      "Epoch 5, train loss: 0.232102\n",
      "Epoch 6, train loss: 0.217297\n",
      "Epoch 7, train loss: 0.189651\n",
      "Epoch 8, train loss: 0.218484\n",
      "Epoch 9, train loss: 0.223173\n",
      "Epoch 10, train loss: 0.223689\n",
      "Epoch 11, train loss: 0.195743\n",
      "Epoch 12, train loss: 0.242338\n",
      "Epoch 13, train loss: 0.218957\n",
      "Epoch 14, train loss: 0.190556\n",
      "Epoch 15, train loss: 0.257139\n",
      "Epoch 16, train loss: 0.193700\n",
      "Epoch 17, train loss: 0.209747\n",
      "Epoch 18, train loss: 0.241179\n",
      "Epoch 19, train loss: 0.203892\n",
      "Epoch 20, train loss: 0.230439\n",
      "Epoch 21, train loss: 0.244712\n",
      "Epoch 22, train loss: 0.233229\n",
      "Epoch 23, train loss: 0.213737\n",
      "Epoch 24, train loss: 0.175603\n",
      "Epoch 25, train loss: 0.182161\n",
      "Epoch 26, train loss: 0.185831\n",
      "Epoch 27, train loss: 0.208612\n",
      "Epoch 28, train loss: 0.195283\n",
      "Epoch 29, train loss: 0.182356\n",
      "Epoch 30, train loss: 0.199344\n",
      "Epoch 31, train loss: 0.223864\n",
      "Epoch 32, train loss: 0.165431\n",
      "Epoch 33, train loss: 0.224237\n",
      "Epoch 34, train loss: 0.213810\n",
      "Epoch 35, train loss: 0.187097\n",
      "Epoch 36, train loss: 0.200289\n",
      "Epoch 37, train loss: 0.147504\n",
      "Epoch 38, train loss: 0.226371\n",
      "Epoch 39, train loss: 0.173611\n",
      "Epoch 40, train loss: 0.213685\n",
      "Epoch 41, train loss: 0.228087\n",
      "Epoch 42, train loss: 0.182011\n",
      "Epoch 43, train loss: 0.228131\n",
      "Epoch 44, train loss: 0.267237\n",
      "Epoch 45, train loss: 0.277799\n",
      "Epoch 46, train loss: 0.194573\n",
      "Epoch 47, train loss: 0.231261\n",
      "Epoch 48, train loss: 0.199721\n",
      "Epoch 49, train loss: 0.190996\n",
      "Epoch 50, train loss: 0.208678\n",
      "Epoch 51, train loss: 0.221831\n",
      "Epoch 52, train loss: 0.197798\n",
      "Epoch 53, train loss: 0.208988\n",
      "Epoch 54, train loss: 0.156621\n",
      "Epoch 55, train loss: 0.201737\n",
      "Epoch 56, train loss: 0.166238\n",
      "Epoch 57, train loss: 0.171093\n",
      "Epoch 58, train loss: 0.224522\n",
      "Epoch 59, train loss: 0.196590\n",
      "Epoch 60, train loss: 0.167332\n",
      "Epoch 61, train loss: 0.197946\n",
      "Epoch 62, train loss: 0.177922\n",
      "Epoch 63, train loss: 0.206034\n",
      "Epoch 64, train loss: 0.206393\n",
      "Epoch 65, train loss: 0.196606\n",
      "Epoch 66, train loss: 0.201535\n",
      "Epoch 67, train loss: 0.185070\n",
      "Epoch 68, train loss: 0.210333\n",
      "Epoch 69, train loss: 0.173654\n",
      "Epoch 70, train loss: 0.166871\n",
      "Epoch 71, train loss: 0.263541\n",
      "Epoch 72, train loss: 0.234589\n",
      "Epoch 73, train loss: 0.226287\n",
      "Epoch 74, train loss: 0.236046\n",
      "Epoch 75, train loss: 0.222101\n",
      "Epoch 76, train loss: 0.197505\n",
      "Epoch 77, train loss: 0.183413\n",
      "Epoch 78, train loss: 0.308193\n",
      "Epoch 79, train loss: 0.209786\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "x here is\n",
      "[[135. 143. 152. 159. 166. 172.]\n",
      " [143. 152. 159. 166. 172. 178.]\n",
      " [152. 159. 166. 172. 178. 184.]\n",
      " ...\n",
      " [203. 205. 208. 204. 202. 201.]\n",
      " [205. 208. 204. 202. 201. 201.]\n",
      " [208. 204. 202. 201. 201. 201.]]\n",
      "y here is\n",
      "[[203. 203. 203. 203. 203. 203.]\n",
      " [204. 204. 204. 204. 204. 204.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " ...\n",
      " [212. 212. 212. 212. 212. 212.]\n",
      " [218. 218. 218. 218. 218. 218.]\n",
      " [224. 224. 224. 224. 224. 224.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (2635, 6), Train/test: 1/2634\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "x here is\n",
      "[[101. 100. 100.  99.  98.  98.]\n",
      " [100. 100.  99.  98.  98.  95.]\n",
      " [100.  99.  98.  98.  95.  94.]\n",
      " ...\n",
      " [ 71.  73.  74.  74.  76.  79.]\n",
      " [ 73.  74.  74.  76.  79.  87.]\n",
      " [ 74.  74.  76.  79.  87.  95.]]\n",
      "y here is\n",
      "[[ 86.  86.  86.  86.  86.  86.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " ...\n",
      " [120. 120. 120. 120. 120. 120.]\n",
      " [123. 123. 123. 123. 123. 123.]\n",
      " [128. 128. 128. 128. 128. 128.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 420/10329\n",
      "Found 21 continuous time series\n",
      "Data shape: (10751, 6), Train/test: 10749/2\n",
      "Train test ratio: 5374.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310FE20>\n",
      "Epoch 0, test loss: 0.161112\n",
      "Epoch 1, test loss: 0.160639\n",
      "Epoch 2, test loss: 0.162003\n",
      "Epoch 3, test loss: 0.158216\n",
      "Epoch 4, test loss: 0.158812\n",
      "Epoch 5, test loss: 0.159207\n",
      "Epoch 6, test loss: 0.160505\n",
      "Epoch 7, test loss: 0.164298\n",
      "Epoch 8, test loss: 0.161241\n",
      "Epoch 9, test loss: 0.158475\n",
      "Epoch 10, test loss: 0.159304\n",
      "Epoch 11, test loss: 0.160286\n",
      "Epoch 12, test loss: 0.158587\n",
      "Epoch 13, test loss: 0.159685\n",
      "Epoch 14, test loss: 0.158310\n",
      "Epoch 15, test loss: 0.158834\n",
      "Epoch 16, test loss: 0.160085\n",
      "Epoch 17, test loss: 0.158113\n",
      "Epoch 18, test loss: 0.158730\n",
      "Epoch 19, test loss: 0.158257\n",
      "Epoch 20, test loss: 0.158888\n",
      "Epoch 21, test loss: 0.157805\n",
      "Epoch 22, test loss: 0.158546\n",
      "Epoch 23, test loss: 0.160155\n",
      "Epoch 24, test loss: 0.158718\n",
      "Epoch 25, test loss: 0.158988\n",
      "Epoch 26, test loss: 0.158597\n",
      "Epoch 27, test loss: 0.159624\n",
      "Epoch 28, test loss: 0.157675\n",
      "Epoch 29, test loss: 0.159141\n",
      "Epoch 30, test loss: 0.159811\n",
      "Epoch 31, test loss: 0.158994\n",
      "Epoch 32, test loss: 0.157971\n",
      "Epoch 33, test loss: 0.157926\n",
      "Epoch 34, test loss: 0.166045\n",
      "Epoch 35, test loss: 0.158204\n",
      "Epoch 36, test loss: 0.157830\n",
      "Epoch 37, test loss: 0.158125\n",
      "Epoch 38, test loss: 0.158585\n",
      "Epoch 39, test loss: 0.158664\n",
      "Epoch 40, test loss: 0.157950\n",
      "Epoch 41, test loss: 0.159245\n",
      "Epoch 42, test loss: 0.157919\n",
      "Epoch 43, test loss: 0.158164\n",
      "Epoch 44, test loss: 0.158540\n",
      "Epoch 45, test loss: 0.158532\n",
      "Epoch 46, test loss: 0.159289\n",
      "Epoch 47, test loss: 0.157779\n",
      "Epoch 48, test loss: 0.158438\n",
      "Epoch 49, test loss: 0.162603\n",
      "Epoch 50, test loss: 0.159989\n",
      "Epoch 51, test loss: 0.157630\n",
      "Epoch 52, test loss: 0.157946\n",
      "Epoch 53, test loss: 0.157984\n",
      "Epoch 54, test loss: 0.158364\n",
      "Epoch 55, test loss: 0.157970\n",
      "Epoch 56, test loss: 0.160183\n",
      "Epoch 57, test loss: 0.158716\n",
      "Epoch 58, test loss: 0.157789\n",
      "Epoch 59, test loss: 0.158155\n",
      "Epoch 60, test loss: 0.159422\n",
      "Epoch 61, test loss: 0.158729\n",
      "Epoch 62, test loss: 0.161435\n",
      "Epoch 63, test loss: 0.158565\n",
      "Epoch 64, test loss: 0.161656\n",
      "Epoch 65, test loss: 0.158759\n",
      "Epoch 66, test loss: 0.160154\n",
      "Epoch 67, test loss: 0.158265\n",
      "Epoch 68, test loss: 0.158155\n",
      "Epoch 69, test loss: 0.158207\n",
      "Epoch 70, test loss: 0.160085\n",
      "Epoch 71, test loss: 0.158375\n",
      "Epoch 72, test loss: 0.162962\n",
      "Epoch 73, test loss: 0.158183\n",
      "Epoch 74, test loss: 0.160270\n",
      "Epoch 75, test loss: 0.158743\n",
      "Epoch 76, test loss: 0.158395\n",
      "Epoch 77, test loss: 0.158715\n",
      "Epoch 78, test loss: 0.160360\n",
      "Epoch 79, test loss: 0.160668\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310FE20>\n",
      "Epoch 0, test loss: 0.158146\n",
      "Epoch 1, test loss: 0.157069\n",
      "Epoch 2, test loss: 0.157675\n",
      "Epoch 3, test loss: 0.158317\n",
      "Epoch 4, test loss: 0.157969\n",
      "Epoch 5, test loss: 0.158156\n",
      "Epoch 6, test loss: 0.158789\n",
      "Epoch 7, test loss: 0.158342\n",
      "Epoch 8, test loss: 0.160835\n",
      "Epoch 9, test loss: 0.158595\n",
      "Epoch 10, test loss: 0.158477\n",
      "Epoch 11, test loss: 0.159595\n",
      "Epoch 12, test loss: 0.158055\n",
      "Epoch 13, test loss: 0.158314\n",
      "Epoch 14, test loss: 0.158788\n",
      "Epoch 15, test loss: 0.160303\n",
      "Epoch 16, test loss: 0.158620\n",
      "Epoch 17, test loss: 0.159573\n",
      "Epoch 18, test loss: 0.158610\n",
      "Epoch 19, test loss: 0.158630\n",
      "Epoch 20, test loss: 0.159960\n",
      "Epoch 21, test loss: 0.158086\n",
      "Epoch 22, test loss: 0.159647\n",
      "Epoch 23, test loss: 0.160977\n",
      "Epoch 24, test loss: 0.158681\n",
      "Epoch 25, test loss: 0.161063\n",
      "Epoch 26, test loss: 0.159382\n",
      "Epoch 27, test loss: 0.161898\n",
      "Epoch 28, test loss: 0.159917\n",
      "Epoch 29, test loss: 0.158512\n",
      "Epoch 30, test loss: 0.158567\n",
      "Epoch 31, test loss: 0.158873\n",
      "Epoch 32, test loss: 0.158555\n",
      "Epoch 33, test loss: 0.157989\n",
      "Epoch 34, test loss: 0.159360\n",
      "Epoch 35, test loss: 0.160180\n",
      "Epoch 36, test loss: 0.158508\n",
      "Epoch 37, test loss: 0.159093\n",
      "Epoch 38, test loss: 0.160673\n",
      "Epoch 39, test loss: 0.158524\n",
      "Epoch 40, test loss: 0.167264\n",
      "Epoch 41, test loss: 0.159399\n",
      "Epoch 42, test loss: 0.159676\n",
      "Epoch 43, test loss: 0.160080\n",
      "Epoch 44, test loss: 0.158156\n",
      "Epoch 45, test loss: 0.160407\n",
      "Epoch 46, test loss: 0.158373\n",
      "Epoch 47, test loss: 0.158966\n",
      "Epoch 48, test loss: 0.158358\n",
      "Epoch 49, test loss: 0.159600\n",
      "Epoch 50, test loss: 0.158702\n",
      "Epoch 51, test loss: 0.158445\n",
      "Epoch 52, test loss: 0.158305\n",
      "Epoch 53, test loss: 0.161307\n",
      "Epoch 54, test loss: 0.160043\n",
      "Epoch 55, test loss: 0.158237\n",
      "Epoch 56, test loss: 0.160301\n",
      "Epoch 57, test loss: 0.158455\n",
      "Epoch 58, test loss: 0.159839\n",
      "Epoch 59, test loss: 0.159567\n",
      "Epoch 60, test loss: 0.158454\n",
      "Epoch 61, test loss: 0.158707\n",
      "Epoch 62, test loss: 0.159221\n",
      "Epoch 63, test loss: 0.160264\n",
      "Epoch 64, test loss: 0.159655\n",
      "Epoch 65, test loss: 0.158849\n",
      "Epoch 66, test loss: 0.159482\n",
      "Epoch 67, test loss: 0.158476\n",
      "Epoch 68, test loss: 0.158823\n",
      "Epoch 69, test loss: 0.172377\n",
      "Epoch 70, test loss: 0.160831\n",
      "Epoch 71, test loss: 0.161101\n",
      "Epoch 72, test loss: 0.159071\n",
      "Epoch 73, test loss: 0.158713\n",
      "Epoch 74, test loss: 0.160229\n",
      "Epoch 75, test loss: 0.159745\n",
      "Epoch 76, test loss: 0.158703\n",
      "Epoch 77, test loss: 0.158697\n",
      "Epoch 78, test loss: 0.159968\n",
      "Epoch 79, test loss: 0.161354\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310FE20>\n",
      "Epoch 0, test loss: 0.198044\n",
      "Epoch 1, test loss: 0.178585\n",
      "Epoch 2, test loss: 0.174063\n",
      "Epoch 3, test loss: 0.172361\n",
      "Epoch 4, test loss: 0.174187\n",
      "Epoch 5, test loss: 0.168812\n",
      "Epoch 6, test loss: 0.169379\n",
      "Epoch 7, test loss: 0.168078\n",
      "Epoch 8, test loss: 0.166320\n",
      "Epoch 9, test loss: 0.165882\n",
      "Epoch 10, test loss: 0.166060\n",
      "Epoch 11, test loss: 0.165620\n",
      "Epoch 12, test loss: 0.164414\n",
      "Epoch 13, test loss: 0.163945\n",
      "Epoch 14, test loss: 0.163999\n",
      "Epoch 15, test loss: 0.164724\n",
      "Epoch 16, test loss: 0.163210\n",
      "Epoch 17, test loss: 0.163063\n",
      "Epoch 18, test loss: 0.162882\n",
      "Epoch 19, test loss: 0.164551\n",
      "Epoch 20, test loss: 0.162460\n",
      "Epoch 21, test loss: 0.162712\n",
      "Epoch 22, test loss: 0.165382\n",
      "Epoch 23, test loss: 0.165054\n",
      "Epoch 24, test loss: 0.162612\n",
      "Epoch 25, test loss: 0.163441\n",
      "Epoch 26, test loss: 0.162559\n",
      "Epoch 27, test loss: 0.162228\n",
      "Epoch 28, test loss: 0.162267\n",
      "Epoch 29, test loss: 0.166342\n",
      "Epoch 30, test loss: 0.162350\n",
      "Epoch 31, test loss: 0.162565\n",
      "Epoch 32, test loss: 0.172345\n",
      "Epoch 33, test loss: 0.163919\n",
      "Epoch 34, test loss: 0.163172\n",
      "Epoch 35, test loss: 0.162321\n",
      "Epoch 36, test loss: 0.161578\n",
      "Epoch 37, test loss: 0.161700\n",
      "Epoch 38, test loss: 0.161789\n",
      "Epoch 39, test loss: 0.162883\n",
      "Epoch 40, test loss: 0.161655\n",
      "Epoch 41, test loss: 0.163584\n",
      "Epoch 42, test loss: 0.161437\n",
      "Epoch 43, test loss: 0.161193\n",
      "Epoch 44, test loss: 0.161860\n",
      "Epoch 45, test loss: 0.161223\n",
      "Epoch 46, test loss: 0.161236\n",
      "Epoch 47, test loss: 0.160982\n",
      "Epoch 48, test loss: 0.161045\n",
      "Epoch 49, test loss: 0.163082\n",
      "Epoch 50, test loss: 0.162872\n",
      "Epoch 51, test loss: 0.163693\n",
      "Epoch 52, test loss: 0.161235\n",
      "Epoch 53, test loss: 0.161132\n",
      "Epoch 54, test loss: 0.165345\n",
      "Epoch 55, test loss: 0.161286\n",
      "Epoch 56, test loss: 0.161713\n",
      "Epoch 57, test loss: 0.161327\n",
      "Epoch 58, test loss: 0.163088\n",
      "Epoch 59, test loss: 0.161429\n",
      "Epoch 60, test loss: 0.161812\n",
      "Epoch 61, test loss: 0.164094\n",
      "Epoch 62, test loss: 0.161035\n",
      "Epoch 63, test loss: 0.168404\n",
      "Epoch 64, test loss: 0.161036\n",
      "Epoch 65, test loss: 0.160686\n",
      "Epoch 66, test loss: 0.164807\n",
      "Epoch 67, test loss: 0.163172\n",
      "Epoch 68, test loss: 0.160825\n",
      "Epoch 69, test loss: 0.160996\n",
      "Epoch 70, test loss: 0.166897\n",
      "Epoch 71, test loss: 0.163992\n",
      "Epoch 72, test loss: 0.161102\n",
      "Epoch 73, test loss: 0.160636\n",
      "Epoch 74, test loss: 0.161125\n",
      "Epoch 75, test loss: 0.161111\n",
      "Epoch 76, test loss: 0.162599\n",
      "Epoch 77, test loss: 0.160445\n",
      "Epoch 78, test loss: 0.161396\n",
      "Epoch 79, test loss: 0.161903\n",
      "Epoch 80, test loss: 0.161554\n",
      "Epoch 81, test loss: 0.161988\n",
      "Epoch 82, test loss: 0.164635\n",
      "Epoch 83, test loss: 0.160594\n",
      "Epoch 84, test loss: 0.162174\n",
      "Epoch 85, test loss: 0.160956\n",
      "Epoch 86, test loss: 0.160887\n",
      "Epoch 87, test loss: 0.161708\n",
      "Epoch 88, test loss: 0.161133\n",
      "Epoch 89, test loss: 0.161126\n",
      "Epoch 90, test loss: 0.161407\n",
      "Epoch 91, test loss: 0.160725\n",
      "Epoch 92, test loss: 0.160670\n",
      "Epoch 93, test loss: 0.160575\n",
      "Epoch 94, test loss: 0.161123\n",
      "Epoch 95, test loss: 0.167647\n",
      "Epoch 96, test loss: 0.161260\n",
      "Epoch 97, test loss: 0.161261\n",
      "Epoch 98, test loss: 0.161709\n",
      "Epoch 99, test loss: 0.161137\n",
      "Epoch 100, test loss: 0.166461\n",
      "Epoch 101, test loss: 0.165728\n",
      "Epoch 102, test loss: 0.161929\n",
      "Epoch 103, test loss: 0.162723\n",
      "Epoch 104, test loss: 0.161349\n",
      "Epoch 105, test loss: 0.161079\n",
      "Epoch 106, test loss: 0.160599\n",
      "Epoch 107, test loss: 0.160764\n",
      "Epoch 108, test loss: 0.160529\n",
      "Epoch 109, test loss: 0.162238\n",
      "Epoch 110, test loss: 0.162114\n",
      "Epoch 111, test loss: 0.163887\n",
      "Epoch 112, test loss: 0.165596\n",
      "Epoch 113, test loss: 0.160980\n",
      "Epoch 114, test loss: 0.160980\n",
      "Epoch 115, test loss: 0.160500\n",
      "Epoch 116, test loss: 0.162184\n",
      "Epoch 117, test loss: 0.160948\n",
      "Epoch 118, test loss: 0.162156\n",
      "Epoch 119, test loss: 0.163914\n",
      "Epoch 120, test loss: 0.164359\n",
      "Epoch 121, test loss: 0.161214\n",
      "Epoch 122, test loss: 0.163244\n",
      "Epoch 123, test loss: 0.163007\n",
      "Epoch 124, test loss: 0.162824\n",
      "Epoch 125, test loss: 0.175678\n",
      "Epoch 126, test loss: 0.161200\n",
      "Epoch 127, test loss: 0.161431\n",
      "Epoch 128, test loss: 0.168291\n",
      "Epoch 129, test loss: 0.160673\n",
      "Epoch 130, test loss: 0.163469\n",
      "Epoch 131, test loss: 0.162490\n",
      "Epoch 132, test loss: 0.161085\n",
      "Epoch 133, test loss: 0.160909\n",
      "Epoch 134, test loss: 0.160544\n",
      "Epoch 135, test loss: 0.161322\n",
      "Epoch 136, test loss: 0.161882\n",
      "Epoch 137, test loss: 0.161078\n",
      "Epoch 138, test loss: 0.161146\n",
      "Epoch 139, test loss: 0.161092\n",
      "Epoch 140, test loss: 0.161166\n",
      "Epoch 141, test loss: 0.161777\n",
      "Epoch 142, test loss: 0.160773\n",
      "Epoch 143, test loss: 0.161596\n",
      "Epoch 144, test loss: 0.160982\n",
      "Epoch 145, test loss: 0.166649\n",
      "Epoch 146, test loss: 0.160724\n",
      "Epoch 147, test loss: 0.163151\n",
      "Epoch 148, test loss: 0.161209\n",
      "Epoch 149, test loss: 0.160771\n",
      "Epoch 150, test loss: 0.168245\n",
      "Epoch 151, test loss: 0.161013\n",
      "Epoch 152, test loss: 0.162063\n",
      "Epoch 153, test loss: 0.161579\n",
      "Epoch 154, test loss: 0.161260\n",
      "Epoch 155, test loss: 0.162624\n",
      "Epoch 156, test loss: 0.161418\n",
      "Epoch 157, test loss: 0.162801\n",
      "Epoch 158, test loss: 0.162065\n",
      "Epoch 159, test loss: 0.161883\n",
      "Epoch 160, test loss: 0.161171\n",
      "Epoch 161, test loss: 0.161160\n",
      "Epoch 162, test loss: 0.161925\n",
      "Epoch 163, test loss: 0.162158\n",
      "Epoch 164, test loss: 0.167070\n",
      "Epoch 165, test loss: 0.166015\n",
      "Epoch 166, test loss: 0.161763\n",
      "Epoch 167, test loss: 0.162306\n",
      "Epoch 168, test loss: 0.165003\n",
      "Epoch 169, test loss: 0.160675\n",
      "Epoch 170, test loss: 0.161342\n",
      "Epoch 171, test loss: 0.161113\n",
      "Epoch 172, test loss: 0.164199\n",
      "Epoch 173, test loss: 0.161636\n",
      "Epoch 174, test loss: 0.165988\n",
      "Epoch 175, test loss: 0.163886\n",
      "Epoch 176, test loss: 0.161767\n",
      "Epoch 177, test loss: 0.164987\n",
      "Epoch 178, test loss: 0.161596\n",
      "Epoch 179, test loss: 0.161233\n",
      "Epoch 180, test loss: 0.161903\n",
      "Epoch 181, test loss: 0.162609\n",
      "Epoch 182, test loss: 0.161823\n",
      "Epoch 183, test loss: 0.161227\n",
      "Epoch 184, test loss: 0.163184\n",
      "Epoch 185, test loss: 0.160853\n",
      "Epoch 186, test loss: 0.160911\n",
      "Epoch 187, test loss: 0.161759\n",
      "Epoch 188, test loss: 0.160913\n",
      "Epoch 189, test loss: 0.161289\n",
      "Epoch 190, test loss: 0.162411\n",
      "Epoch 191, test loss: 0.169150\n",
      "Epoch 192, test loss: 0.164965\n",
      "Epoch 193, test loss: 0.161490\n",
      "Epoch 194, test loss: 0.165894\n",
      "Epoch 195, test loss: 0.162495\n",
      "Epoch 196, test loss: 0.166751\n",
      "Epoch 197, test loss: 0.160873\n",
      "Epoch 198, test loss: 0.172972\n",
      "Epoch 199, test loss: 0.161725\n",
      "Epoch 200, test loss: 0.162316\n",
      "Epoch 201, test loss: 0.170539\n",
      "Epoch 202, test loss: 0.160761\n",
      "Epoch 203, test loss: 0.161392\n",
      "Epoch 204, test loss: 0.176346\n",
      "Epoch 205, test loss: 0.161271\n",
      "Epoch 206, test loss: 0.161458\n",
      "Epoch 207, test loss: 0.161470\n",
      "Epoch 208, test loss: 0.162081\n",
      "Epoch 209, test loss: 0.164214\n",
      "Epoch 210, test loss: 0.161711\n",
      "Epoch 211, test loss: 0.165729\n",
      "Epoch 212, test loss: 0.163286\n",
      "Epoch 213, test loss: 0.161618\n",
      "Epoch 214, test loss: 0.164114\n",
      "Epoch 215, test loss: 0.162371\n",
      "Epoch 216, test loss: 0.161450\n",
      "Epoch 217, test loss: 0.162025\n",
      "Epoch 218, test loss: 0.163748\n",
      "Epoch 219, test loss: 0.162262\n",
      "Epoch 220, test loss: 0.162348\n",
      "Epoch 221, test loss: 0.162540\n",
      "Epoch 222, test loss: 0.160920\n",
      "Epoch 223, test loss: 0.161317\n",
      "Epoch 224, test loss: 0.162435\n",
      "Epoch 225, test loss: 0.162292\n",
      "Epoch 226, test loss: 0.161849\n",
      "Epoch 227, test loss: 0.163038\n",
      "Epoch 228, test loss: 0.161621\n",
      "Epoch 229, test loss: 0.161064\n",
      "Epoch 230, test loss: 0.161430\n",
      "Epoch 231, test loss: 0.162008\n",
      "Epoch 232, test loss: 0.165297\n",
      "Epoch 233, test loss: 0.162493\n",
      "Epoch 234, test loss: 0.164866\n",
      "Epoch 235, test loss: 0.161429\n",
      "Epoch 236, test loss: 0.161318\n",
      "Epoch 237, test loss: 0.163147\n",
      "Epoch 238, test loss: 0.162230\n",
      "Epoch 239, test loss: 0.162419\n",
      "Pretrain data: 19339240.0\n",
      "Building dataset, requesting data from 0 to 831\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7998/105206\n",
      "Found 831 continuous time series\n",
      "Data shape: (113206, 6), Train/test: 113204/2\n",
      "Train test ratio: 56602.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1768\n",
      "Epoch 0, train loss: 0.165177\n",
      "Epoch 1, train loss: 0.179370\n",
      "Epoch 2, train loss: 0.190599\n",
      "Epoch 3, train loss: 0.206209\n",
      "Epoch 4, train loss: 0.219158\n",
      "Epoch 5, train loss: 0.230786\n",
      "Epoch 6, train loss: 0.178159\n",
      "Epoch 7, train loss: 0.265103\n",
      "Epoch 8, train loss: 0.195706\n",
      "Epoch 9, train loss: 0.219052\n",
      "Epoch 10, train loss: 0.187636\n",
      "Epoch 11, train loss: 0.279813\n",
      "Epoch 12, train loss: 0.204119\n",
      "Epoch 13, train loss: 0.161274\n",
      "Epoch 14, train loss: 0.204300\n",
      "Epoch 15, train loss: 0.230421\n",
      "Epoch 16, train loss: 0.215094\n",
      "Epoch 17, train loss: 0.172975\n",
      "Epoch 18, train loss: 0.234213\n",
      "Epoch 19, train loss: 0.155063\n",
      "Epoch 20, train loss: 0.297464\n",
      "Epoch 21, train loss: 0.206191\n",
      "Epoch 22, train loss: 0.210612\n",
      "Epoch 23, train loss: 0.222821\n",
      "Epoch 24, train loss: 0.214440\n",
      "Epoch 25, train loss: 0.184775\n",
      "Epoch 26, train loss: 0.160953\n",
      "Epoch 27, train loss: 0.296348\n",
      "Epoch 28, train loss: 0.205897\n",
      "Epoch 29, train loss: 0.168726\n",
      "Epoch 30, train loss: 0.182157\n",
      "Epoch 31, train loss: 0.211009\n",
      "Epoch 32, train loss: 0.287133\n",
      "Epoch 33, train loss: 0.248853\n",
      "Epoch 34, train loss: 0.209202\n",
      "Epoch 35, train loss: 0.207520\n",
      "Epoch 36, train loss: 0.148737\n",
      "Epoch 37, train loss: 0.264600\n",
      "Epoch 38, train loss: 0.191909\n",
      "Epoch 39, train loss: 0.170579\n",
      "Epoch 40, train loss: 0.209681\n",
      "Epoch 41, train loss: 0.169689\n",
      "Epoch 42, train loss: 0.345564\n",
      "Epoch 43, train loss: 0.214703\n",
      "Epoch 44, train loss: 0.220487\n",
      "Epoch 45, train loss: 0.221859\n",
      "Epoch 46, train loss: 0.180493\n",
      "Epoch 47, train loss: 0.166728\n",
      "Epoch 48, train loss: 0.161844\n",
      "Epoch 49, train loss: 0.190045\n",
      "Epoch 50, train loss: 0.156735\n",
      "Epoch 51, train loss: 0.228167\n",
      "Epoch 52, train loss: 0.209573\n",
      "Epoch 53, train loss: 0.211393\n",
      "Epoch 54, train loss: 0.197489\n",
      "Epoch 55, train loss: 0.246532\n",
      "Epoch 56, train loss: 0.220390\n",
      "Epoch 57, train loss: 0.258875\n",
      "Epoch 58, train loss: 0.257190\n",
      "Epoch 59, train loss: 0.181013\n",
      "Epoch 60, train loss: 0.159881\n",
      "Epoch 61, train loss: 0.187980\n",
      "Epoch 62, train loss: 0.219529\n",
      "Epoch 63, train loss: 0.211871\n",
      "Epoch 64, train loss: 0.232825\n",
      "Epoch 65, train loss: 0.209192\n",
      "Epoch 66, train loss: 0.264790\n",
      "Epoch 67, train loss: 0.216084\n",
      "Epoch 68, train loss: 0.200024\n",
      "Epoch 69, train loss: 0.228271\n",
      "Epoch 70, train loss: 0.182848\n",
      "Epoch 71, train loss: 0.168282\n",
      "Epoch 72, train loss: 0.227372\n",
      "Epoch 73, train loss: 0.197309\n",
      "Epoch 74, train loss: 0.239683\n",
      "Epoch 75, train loss: 0.152216\n",
      "Epoch 76, train loss: 0.211115\n",
      "Epoch 77, train loss: 0.219233\n",
      "Epoch 78, train loss: 0.171910\n",
      "Epoch 79, train loss: 0.202722\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "x here is\n",
      "[[127. 123. 118. 112. 108. 106.]\n",
      " [123. 118. 112. 108. 106. 103.]\n",
      " [118. 112. 108. 106. 103.  98.]\n",
      " ...\n",
      " [238. 243. 250. 254. 263. 280.]\n",
      " [243. 250. 254. 263. 280. 288.]\n",
      " [250. 254. 263. 280. 288. 301.]]\n",
      "y here is\n",
      "[[ 89.  89.  89.  89.  89.  89.]\n",
      " [ 87.  87.  87.  87.  87.  87.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " ...\n",
      " [307. 307. 307. 307. 307. 307.]\n",
      " [311. 311. 311. 311. 311. 311.]\n",
      " [321. 321. 321. 321. 321. 321.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (2758, 6), Train/test: 1/2757\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[116. 117. 119. 116. 111. 110.]\n",
      " [117. 119. 116. 111. 110. 111.]\n",
      " [119. 116. 111. 110. 111. 113.]\n",
      " ...\n",
      " [198. 191. 186. 180. 175. 171.]\n",
      " [191. 186. 180. 175. 171. 168.]\n",
      " [186. 180. 175. 171. 168. 162.]]\n",
      "y here is\n",
      "[[126. 126. 126. 126. 126. 126.]\n",
      " [131. 131. 131. 131. 131. 131.]\n",
      " [136. 136. 136. 136. 136. 136.]\n",
      " ...\n",
      " [140. 140. 140. 140. 140. 140.]\n",
      " [137. 137. 137. 137. 137. 137.]\n",
      " [132. 132. 132. 132. 132. 132.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 287/12233\n",
      "Found 11 continuous time series\n",
      "Data shape: (12522, 6), Train/test: 12520/2\n",
      "Train test ratio: 6260.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2968AAA10>\n",
      "Epoch 0, test loss: 0.184641\n",
      "Epoch 1, test loss: 0.188096\n",
      "Epoch 2, test loss: 0.185153\n",
      "Epoch 3, test loss: 0.191027\n",
      "Epoch 4, test loss: 0.186072\n",
      "Epoch 5, test loss: 0.187813\n",
      "Epoch 6, test loss: 0.185556\n",
      "Epoch 7, test loss: 0.188057\n",
      "Epoch 8, test loss: 0.190768\n",
      "Epoch 9, test loss: 0.185009\n",
      "Epoch 10, test loss: 0.187050\n",
      "Epoch 11, test loss: 0.187866\n",
      "Epoch 12, test loss: 0.189398\n",
      "Epoch 13, test loss: 0.187898\n",
      "Epoch 14, test loss: 0.188235\n",
      "Epoch 15, test loss: 0.185510\n",
      "Epoch 16, test loss: 0.186104\n",
      "Epoch 17, test loss: 0.186332\n",
      "Epoch 18, test loss: 0.188442\n",
      "Epoch 19, test loss: 0.186428\n",
      "Epoch 20, test loss: 0.191091\n",
      "Epoch 21, test loss: 0.185440\n",
      "Epoch 22, test loss: 0.185839\n",
      "Epoch 23, test loss: 0.190868\n",
      "Epoch 24, test loss: 0.187028\n",
      "Epoch 25, test loss: 0.186598\n",
      "Epoch 26, test loss: 0.185782\n",
      "Epoch 27, test loss: 0.189818\n",
      "Epoch 28, test loss: 0.190714\n",
      "Epoch 29, test loss: 0.186260\n",
      "Epoch 30, test loss: 0.189070\n",
      "Epoch 31, test loss: 0.185039\n",
      "Epoch 32, test loss: 0.186594\n",
      "Epoch 33, test loss: 0.185920\n",
      "Epoch 34, test loss: 0.187039\n",
      "Epoch 35, test loss: 0.190926\n",
      "Epoch 36, test loss: 0.185490\n",
      "Epoch 37, test loss: 0.187429\n",
      "Epoch 38, test loss: 0.185130\n",
      "Epoch 39, test loss: 0.192549\n",
      "Epoch 40, test loss: 0.185499\n",
      "Epoch 41, test loss: 0.187980\n",
      "Epoch 42, test loss: 0.185439\n",
      "Epoch 43, test loss: 0.189584\n",
      "Epoch 44, test loss: 0.190037\n",
      "Epoch 45, test loss: 0.187017\n",
      "Epoch 46, test loss: 0.185604\n",
      "Epoch 47, test loss: 0.186535\n",
      "Epoch 48, test loss: 0.195266\n",
      "Epoch 49, test loss: 0.185970\n",
      "Epoch 50, test loss: 0.189617\n",
      "Epoch 51, test loss: 0.185867\n",
      "Epoch 52, test loss: 0.188545\n",
      "Epoch 53, test loss: 0.186113\n",
      "Epoch 54, test loss: 0.185299\n",
      "Epoch 55, test loss: 0.185748\n",
      "Epoch 56, test loss: 0.185538\n",
      "Epoch 57, test loss: 0.193228\n",
      "Epoch 58, test loss: 0.185637\n",
      "Epoch 59, test loss: 0.191090\n",
      "Epoch 60, test loss: 0.185988\n",
      "Epoch 61, test loss: 0.186120\n",
      "Epoch 62, test loss: 0.185770\n",
      "Epoch 63, test loss: 0.191828\n",
      "Epoch 64, test loss: 0.186794\n",
      "Epoch 65, test loss: 0.186982\n",
      "Epoch 66, test loss: 0.186174\n",
      "Epoch 67, test loss: 0.185660\n",
      "Epoch 68, test loss: 0.186245\n",
      "Epoch 69, test loss: 0.186839\n",
      "Epoch 70, test loss: 0.187412\n",
      "Epoch 71, test loss: 0.189294\n",
      "Epoch 72, test loss: 0.186199\n",
      "Epoch 73, test loss: 0.189777\n",
      "Epoch 74, test loss: 0.196774\n",
      "Epoch 75, test loss: 0.187085\n",
      "Epoch 76, test loss: 0.185542\n",
      "Epoch 77, test loss: 0.186214\n",
      "Epoch 78, test loss: 0.186307\n",
      "Epoch 79, test loss: 0.194098\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2968AAA10>\n",
      "Epoch 0, test loss: 0.185100\n",
      "Epoch 1, test loss: 0.185438\n",
      "Epoch 2, test loss: 0.189828\n",
      "Epoch 3, test loss: 0.186701\n",
      "Epoch 4, test loss: 0.187404\n",
      "Epoch 5, test loss: 0.187842\n",
      "Epoch 6, test loss: 0.186550\n",
      "Epoch 7, test loss: 0.187699\n",
      "Epoch 8, test loss: 0.186166\n",
      "Epoch 9, test loss: 0.185971\n",
      "Epoch 10, test loss: 0.187022\n",
      "Epoch 11, test loss: 0.186907\n",
      "Epoch 12, test loss: 0.187720\n",
      "Epoch 13, test loss: 0.190050\n",
      "Epoch 14, test loss: 0.186236\n",
      "Epoch 15, test loss: 0.188571\n",
      "Epoch 16, test loss: 0.185103\n",
      "Epoch 17, test loss: 0.191011\n",
      "Epoch 18, test loss: 0.187652\n",
      "Epoch 19, test loss: 0.185628\n",
      "Epoch 20, test loss: 0.185551\n",
      "Epoch 21, test loss: 0.185329\n",
      "Epoch 22, test loss: 0.185502\n",
      "Epoch 23, test loss: 0.185770\n",
      "Epoch 24, test loss: 0.185205\n",
      "Epoch 25, test loss: 0.190548\n",
      "Epoch 26, test loss: 0.186434\n",
      "Epoch 27, test loss: 0.186189\n",
      "Epoch 28, test loss: 0.186196\n",
      "Epoch 29, test loss: 0.190277\n",
      "Epoch 30, test loss: 0.186065\n",
      "Epoch 31, test loss: 0.186798\n",
      "Epoch 32, test loss: 0.187248\n",
      "Epoch 33, test loss: 0.185539\n",
      "Epoch 34, test loss: 0.185921\n",
      "Epoch 35, test loss: 0.188151\n",
      "Epoch 36, test loss: 0.186633\n",
      "Epoch 37, test loss: 0.186988\n",
      "Epoch 38, test loss: 0.189352\n",
      "Epoch 39, test loss: 0.186025\n",
      "Epoch 40, test loss: 0.186155\n",
      "Epoch 41, test loss: 0.185778\n",
      "Epoch 42, test loss: 0.190325\n",
      "Epoch 43, test loss: 0.186403\n",
      "Epoch 44, test loss: 0.186310\n",
      "Epoch 45, test loss: 0.191173\n",
      "Epoch 46, test loss: 0.186315\n",
      "Epoch 47, test loss: 0.188016\n",
      "Epoch 48, test loss: 0.185633\n",
      "Epoch 49, test loss: 0.187893\n",
      "Epoch 50, test loss: 0.189546\n",
      "Epoch 51, test loss: 0.191097\n",
      "Epoch 52, test loss: 0.188505\n",
      "Epoch 53, test loss: 0.186581\n",
      "Epoch 54, test loss: 0.193698\n",
      "Epoch 55, test loss: 0.186341\n",
      "Epoch 56, test loss: 0.185510\n",
      "Epoch 57, test loss: 0.186113\n",
      "Epoch 58, test loss: 0.185320\n",
      "Epoch 59, test loss: 0.187554\n",
      "Epoch 60, test loss: 0.186721\n",
      "Epoch 61, test loss: 0.185751\n",
      "Epoch 62, test loss: 0.189591\n",
      "Epoch 63, test loss: 0.184885\n",
      "Epoch 64, test loss: 0.190823\n",
      "Epoch 65, test loss: 0.186338\n",
      "Epoch 66, test loss: 0.190073\n",
      "Epoch 67, test loss: 0.185652\n",
      "Epoch 68, test loss: 0.186894\n",
      "Epoch 69, test loss: 0.185516\n",
      "Epoch 70, test loss: 0.186076\n",
      "Epoch 71, test loss: 0.189069\n",
      "Epoch 72, test loss: 0.186244\n",
      "Epoch 73, test loss: 0.196429\n",
      "Epoch 74, test loss: 0.186392\n",
      "Epoch 75, test loss: 0.185639\n",
      "Epoch 76, test loss: 0.185963\n",
      "Epoch 77, test loss: 0.188656\n",
      "Epoch 78, test loss: 0.188631\n",
      "Epoch 79, test loss: 0.189021\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2968AAA10>\n",
      "Epoch 0, test loss: 0.217805\n",
      "Epoch 1, test loss: 0.207019\n",
      "Epoch 2, test loss: 0.202731\n",
      "Epoch 3, test loss: 0.201666\n",
      "Epoch 4, test loss: 0.197751\n",
      "Epoch 5, test loss: 0.197228\n",
      "Epoch 6, test loss: 0.194857\n",
      "Epoch 7, test loss: 0.194001\n",
      "Epoch 8, test loss: 0.193406\n",
      "Epoch 9, test loss: 0.194494\n",
      "Epoch 10, test loss: 0.191690\n",
      "Epoch 11, test loss: 0.190887\n",
      "Epoch 12, test loss: 0.190384\n",
      "Epoch 13, test loss: 0.189419\n",
      "Epoch 14, test loss: 0.190866\n",
      "Epoch 15, test loss: 0.192098\n",
      "Epoch 16, test loss: 0.189624\n",
      "Epoch 17, test loss: 0.195587\n",
      "Epoch 18, test loss: 0.188674\n",
      "Epoch 19, test loss: 0.191547\n",
      "Epoch 20, test loss: 0.191592\n",
      "Epoch 21, test loss: 0.191319\n",
      "Epoch 22, test loss: 0.188782\n",
      "Epoch 23, test loss: 0.187694\n",
      "Epoch 24, test loss: 0.194030\n",
      "Epoch 25, test loss: 0.188269\n",
      "Epoch 26, test loss: 0.188760\n",
      "Epoch 27, test loss: 0.188595\n",
      "Epoch 28, test loss: 0.190769\n",
      "Epoch 29, test loss: 0.188357\n",
      "Epoch 30, test loss: 0.187760\n",
      "Epoch 31, test loss: 0.188320\n",
      "Epoch 32, test loss: 0.191350\n",
      "Epoch 33, test loss: 0.187621\n",
      "Epoch 34, test loss: 0.189148\n",
      "Epoch 35, test loss: 0.194618\n",
      "Epoch 36, test loss: 0.187843\n",
      "Epoch 37, test loss: 0.191451\n",
      "Epoch 38, test loss: 0.193825\n",
      "Epoch 39, test loss: 0.187878\n",
      "Epoch 40, test loss: 0.188420\n",
      "Epoch 41, test loss: 0.187638\n",
      "Epoch 42, test loss: 0.188036\n",
      "Epoch 43, test loss: 0.189100\n",
      "Epoch 44, test loss: 0.187441\n",
      "Epoch 45, test loss: 0.189830\n",
      "Epoch 46, test loss: 0.189052\n",
      "Epoch 47, test loss: 0.194463\n",
      "Epoch 48, test loss: 0.188692\n",
      "Epoch 49, test loss: 0.187365\n",
      "Epoch 50, test loss: 0.193747\n",
      "Epoch 51, test loss: 0.187450\n",
      "Epoch 52, test loss: 0.187799\n",
      "Epoch 53, test loss: 0.187393\n",
      "Epoch 54, test loss: 0.193005\n",
      "Epoch 55, test loss: 0.187893\n",
      "Epoch 56, test loss: 0.187385\n",
      "Epoch 57, test loss: 0.187289\n",
      "Epoch 58, test loss: 0.193502\n",
      "Epoch 59, test loss: 0.190950\n",
      "Epoch 60, test loss: 0.187326\n",
      "Epoch 61, test loss: 0.191387\n",
      "Epoch 62, test loss: 0.189828\n",
      "Epoch 63, test loss: 0.187162\n",
      "Epoch 64, test loss: 0.189471\n",
      "Epoch 65, test loss: 0.187095\n",
      "Epoch 66, test loss: 0.187713\n",
      "Epoch 67, test loss: 0.187355\n",
      "Epoch 68, test loss: 0.189625\n",
      "Epoch 69, test loss: 0.191558\n",
      "Epoch 70, test loss: 0.187135\n",
      "Epoch 71, test loss: 0.187048\n",
      "Epoch 72, test loss: 0.191254\n",
      "Epoch 73, test loss: 0.187010\n",
      "Epoch 74, test loss: 0.187967\n",
      "Epoch 75, test loss: 0.190228\n",
      "Epoch 76, test loss: 0.187664\n",
      "Epoch 77, test loss: 0.190087\n",
      "Epoch 78, test loss: 0.187161\n",
      "Epoch 79, test loss: 0.194565\n",
      "Epoch 80, test loss: 0.191222\n",
      "Epoch 81, test loss: 0.189807\n",
      "Epoch 82, test loss: 0.191671\n",
      "Epoch 83, test loss: 0.188616\n",
      "Epoch 84, test loss: 0.186941\n",
      "Epoch 85, test loss: 0.187027\n",
      "Epoch 86, test loss: 0.187427\n",
      "Epoch 87, test loss: 0.189321\n",
      "Epoch 88, test loss: 0.189704\n",
      "Epoch 89, test loss: 0.188389\n",
      "Epoch 90, test loss: 0.191342\n",
      "Epoch 91, test loss: 0.188376\n",
      "Epoch 92, test loss: 0.189185\n",
      "Epoch 93, test loss: 0.189552\n",
      "Epoch 94, test loss: 0.192048\n",
      "Epoch 95, test loss: 0.188189\n",
      "Epoch 96, test loss: 0.187728\n",
      "Epoch 97, test loss: 0.187635\n",
      "Epoch 98, test loss: 0.187572\n",
      "Epoch 99, test loss: 0.192410\n",
      "Epoch 100, test loss: 0.191754\n",
      "Epoch 101, test loss: 0.188622\n",
      "Epoch 102, test loss: 0.188230\n",
      "Epoch 103, test loss: 0.186775\n",
      "Epoch 104, test loss: 0.188364\n",
      "Epoch 105, test loss: 0.197547\n",
      "Epoch 106, test loss: 0.188059\n",
      "Epoch 107, test loss: 0.186791\n",
      "Epoch 108, test loss: 0.187260\n",
      "Epoch 109, test loss: 0.187570\n",
      "Epoch 110, test loss: 0.186753\n",
      "Epoch 111, test loss: 0.186965\n",
      "Epoch 112, test loss: 0.186867\n",
      "Epoch 113, test loss: 0.192524\n",
      "Epoch 114, test loss: 0.193382\n",
      "Epoch 115, test loss: 0.188321\n",
      "Epoch 116, test loss: 0.191991\n",
      "Epoch 117, test loss: 0.191360\n",
      "Epoch 118, test loss: 0.190315\n",
      "Epoch 119, test loss: 0.190298\n",
      "Epoch 120, test loss: 0.186847\n",
      "Epoch 121, test loss: 0.194851\n",
      "Epoch 122, test loss: 0.187081\n",
      "Epoch 123, test loss: 0.190579\n",
      "Epoch 124, test loss: 0.187870\n",
      "Epoch 125, test loss: 0.192197\n",
      "Epoch 126, test loss: 0.190030\n",
      "Epoch 127, test loss: 0.187109\n",
      "Epoch 128, test loss: 0.187956\n",
      "Epoch 129, test loss: 0.188088\n",
      "Epoch 130, test loss: 0.187008\n",
      "Epoch 131, test loss: 0.189082\n",
      "Epoch 132, test loss: 0.187975\n",
      "Epoch 133, test loss: 0.186903\n",
      "Epoch 134, test loss: 0.186933\n",
      "Epoch 135, test loss: 0.188437\n",
      "Epoch 136, test loss: 0.191562\n",
      "Epoch 137, test loss: 0.197290\n",
      "Epoch 138, test loss: 0.190733\n",
      "Epoch 139, test loss: 0.189783\n",
      "Epoch 140, test loss: 0.187778\n",
      "Epoch 141, test loss: 0.186802\n",
      "Epoch 142, test loss: 0.186883\n",
      "Epoch 143, test loss: 0.191482\n",
      "Epoch 144, test loss: 0.186937\n",
      "Epoch 145, test loss: 0.186978\n",
      "Epoch 146, test loss: 0.188239\n",
      "Epoch 147, test loss: 0.186729\n",
      "Epoch 148, test loss: 0.187086\n",
      "Epoch 149, test loss: 0.191687\n",
      "Epoch 150, test loss: 0.187333\n",
      "Epoch 151, test loss: 0.187725\n",
      "Epoch 152, test loss: 0.188751\n",
      "Epoch 153, test loss: 0.186723\n",
      "Epoch 154, test loss: 0.188117\n",
      "Epoch 155, test loss: 0.187051\n",
      "Epoch 156, test loss: 0.186821\n",
      "Epoch 157, test loss: 0.186958\n",
      "Epoch 158, test loss: 0.188832\n",
      "Epoch 159, test loss: 0.189136\n",
      "Epoch 160, test loss: 0.202516\n",
      "Epoch 161, test loss: 0.188800\n",
      "Epoch 162, test loss: 0.186833\n",
      "Epoch 163, test loss: 0.188531\n",
      "Epoch 164, test loss: 0.186938\n",
      "Epoch 165, test loss: 0.189572\n",
      "Epoch 166, test loss: 0.192507\n",
      "Epoch 167, test loss: 0.186852\n",
      "Epoch 168, test loss: 0.189778\n",
      "Epoch 169, test loss: 0.187425\n",
      "Epoch 170, test loss: 0.187022\n",
      "Epoch 171, test loss: 0.188131\n",
      "Epoch 172, test loss: 0.194416\n",
      "Epoch 173, test loss: 0.188515\n",
      "Epoch 174, test loss: 0.187385\n",
      "Epoch 175, test loss: 0.190360\n",
      "Epoch 176, test loss: 0.190151\n",
      "Epoch 177, test loss: 0.191670\n",
      "Epoch 178, test loss: 0.187852\n",
      "Epoch 179, test loss: 0.192443\n",
      "Epoch 180, test loss: 0.187656\n",
      "Epoch 181, test loss: 0.189435\n",
      "Epoch 182, test loss: 0.191609\n",
      "Epoch 183, test loss: 0.191081\n",
      "Epoch 184, test loss: 0.187972\n",
      "Epoch 185, test loss: 0.190693\n",
      "Epoch 186, test loss: 0.191689\n",
      "Epoch 187, test loss: 0.187999\n",
      "Epoch 188, test loss: 0.186739\n",
      "Epoch 189, test loss: 0.185976\n",
      "Epoch 190, test loss: 0.189153\n",
      "Epoch 191, test loss: 0.187582\n",
      "Epoch 192, test loss: 0.188996\n",
      "Epoch 193, test loss: 0.189699\n",
      "Epoch 194, test loss: 0.189450\n",
      "Epoch 195, test loss: 0.185627\n",
      "Epoch 196, test loss: 0.187493\n",
      "Epoch 197, test loss: 0.190060\n",
      "Epoch 198, test loss: 0.196919\n",
      "Epoch 199, test loss: 0.187771\n",
      "Epoch 200, test loss: 0.187474\n",
      "Epoch 201, test loss: 0.185934\n",
      "Epoch 202, test loss: 0.187653\n",
      "Epoch 203, test loss: 0.190415\n",
      "Epoch 204, test loss: 0.188776\n",
      "Epoch 205, test loss: 0.187342\n",
      "Epoch 206, test loss: 0.186955\n",
      "Epoch 207, test loss: 0.185456\n",
      "Epoch 208, test loss: 0.186981\n",
      "Epoch 209, test loss: 0.186710\n",
      "Epoch 210, test loss: 0.186585\n",
      "Epoch 211, test loss: 0.187194\n",
      "Epoch 212, test loss: 0.186622\n",
      "Epoch 213, test loss: 0.188519\n",
      "Epoch 214, test loss: 0.190494\n",
      "Epoch 215, test loss: 0.188627\n",
      "Epoch 216, test loss: 0.187692\n",
      "Epoch 217, test loss: 0.186102\n",
      "Epoch 218, test loss: 0.187010\n",
      "Epoch 219, test loss: 0.188175\n",
      "Epoch 220, test loss: 0.187166\n",
      "Epoch 221, test loss: 0.188808\n",
      "Epoch 222, test loss: 0.191250\n",
      "Epoch 223, test loss: 0.186548\n",
      "Epoch 224, test loss: 0.185307\n",
      "Epoch 225, test loss: 0.185836\n",
      "Epoch 226, test loss: 0.187086\n",
      "Epoch 227, test loss: 0.185684\n",
      "Epoch 228, test loss: 0.197344\n",
      "Epoch 229, test loss: 0.186558\n",
      "Epoch 230, test loss: 0.186681\n",
      "Epoch 231, test loss: 0.187234\n",
      "Epoch 232, test loss: 0.185580\n",
      "Epoch 233, test loss: 0.186138\n",
      "Epoch 234, test loss: 0.191003\n",
      "Epoch 235, test loss: 0.187608\n",
      "Epoch 236, test loss: 0.187489\n",
      "Epoch 237, test loss: 0.185667\n",
      "Epoch 238, test loss: 0.186133\n",
      "Epoch 239, test loss: 0.192386\n",
      "Pretrain data: 19742408.0\n",
      "Building dataset, requesting data from 0 to 769\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [266. 263. 259. 254. 250. 254.]\n",
      " [263. 259. 254. 250. 254. 261.]\n",
      " [259. 254. 250. 254. 261. 267.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [258. 258. 258. 258. 258. 258.]\n",
      " [257. 257. 257. 257. 257. 257.]\n",
      " [255. 255. 255. 255. 255. 255.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6845/107816\n",
      "Found 769 continuous time series\n",
      "Data shape: (114663, 6), Train/test: 114661/2\n",
      "Train test ratio: 57330.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1791\n",
      "Epoch 0, train loss: 0.244806\n",
      "Epoch 1, train loss: 0.255314\n",
      "Epoch 2, train loss: 0.178708\n",
      "Epoch 3, train loss: 0.194482\n",
      "Epoch 4, train loss: 0.194815\n",
      "Epoch 5, train loss: 0.180498\n",
      "Epoch 6, train loss: 0.210965\n",
      "Epoch 7, train loss: 0.212547\n",
      "Epoch 8, train loss: 0.208615\n",
      "Epoch 9, train loss: 0.191699\n",
      "Epoch 10, train loss: 0.204257\n",
      "Epoch 11, train loss: 0.187835\n",
      "Epoch 12, train loss: 0.256302\n",
      "Epoch 13, train loss: 0.252463\n",
      "Epoch 14, train loss: 0.268732\n",
      "Epoch 15, train loss: 0.221108\n",
      "Epoch 16, train loss: 0.217649\n",
      "Epoch 17, train loss: 0.248417\n",
      "Epoch 18, train loss: 0.206576\n",
      "Epoch 19, train loss: 0.240635\n",
      "Epoch 20, train loss: 0.177899\n",
      "Epoch 21, train loss: 0.222851\n",
      "Epoch 22, train loss: 0.255122\n",
      "Epoch 23, train loss: 0.169805\n",
      "Epoch 24, train loss: 0.188930\n",
      "Epoch 25, train loss: 0.190920\n",
      "Epoch 26, train loss: 0.203162\n",
      "Epoch 27, train loss: 0.186380\n",
      "Epoch 28, train loss: 0.270800\n",
      "Epoch 29, train loss: 0.189937\n",
      "Epoch 30, train loss: 0.258836\n",
      "Epoch 31, train loss: 0.205274\n",
      "Epoch 32, train loss: 0.157445\n",
      "Epoch 33, train loss: 0.196362\n",
      "Epoch 34, train loss: 0.192353\n",
      "Epoch 35, train loss: 0.157039\n",
      "Epoch 36, train loss: 0.192161\n",
      "Epoch 37, train loss: 0.180890\n",
      "Epoch 38, train loss: 0.200868\n",
      "Epoch 39, train loss: 0.210111\n",
      "Epoch 40, train loss: 0.183023\n",
      "Epoch 41, train loss: 0.188400\n",
      "Epoch 42, train loss: 0.176489\n",
      "Epoch 43, train loss: 0.169019\n",
      "Epoch 44, train loss: 0.189938\n",
      "Epoch 45, train loss: 0.201150\n",
      "Epoch 46, train loss: 0.164453\n",
      "Epoch 47, train loss: 0.193744\n",
      "Epoch 48, train loss: 0.187249\n",
      "Epoch 49, train loss: 0.180335\n",
      "Epoch 50, train loss: 0.179950\n",
      "Epoch 51, train loss: 0.375732\n",
      "Epoch 52, train loss: 0.150231\n",
      "Epoch 53, train loss: 0.245977\n",
      "Epoch 54, train loss: 0.194399\n",
      "Epoch 55, train loss: 0.197245\n",
      "Epoch 56, train loss: 0.210567\n",
      "Epoch 57, train loss: 0.230067\n",
      "Epoch 58, train loss: 0.248554\n",
      "Epoch 59, train loss: 0.158110\n",
      "Epoch 60, train loss: 0.156118\n",
      "Epoch 61, train loss: 0.197452\n",
      "Epoch 62, train loss: 0.318381\n",
      "Epoch 63, train loss: 0.188750\n",
      "Epoch 64, train loss: 0.219712\n",
      "Epoch 65, train loss: 0.234071\n",
      "Epoch 66, train loss: 0.154683\n",
      "Epoch 67, train loss: 0.257336\n",
      "Epoch 68, train loss: 0.209013\n",
      "Epoch 69, train loss: 0.216951\n",
      "Epoch 70, train loss: 0.227415\n",
      "Epoch 71, train loss: 0.159724\n",
      "Epoch 72, train loss: 0.181386\n",
      "Epoch 73, train loss: 0.217723\n",
      "Epoch 74, train loss: 0.205333\n",
      "Epoch 75, train loss: 0.176753\n",
      "Epoch 76, train loss: 0.207972\n",
      "Epoch 77, train loss: 0.227410\n",
      "Epoch 78, train loss: 0.245319\n",
      "Epoch 79, train loss: 0.239309\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[214. 217. 217. 212. 209. 208.]\n",
      " [217. 217. 212. 209. 208. 209.]\n",
      " [217. 212. 209. 208. 209. 209.]\n",
      " ...\n",
      " [128. 124. 126. 128. 130. 124.]\n",
      " [124. 126. 128. 130. 124. 120.]\n",
      " [126. 128. 130. 124. 120. 117.]]\n",
      "y here is\n",
      "[[190. 190. 190. 190. 190. 190.]\n",
      " [183. 183. 183. 183. 183. 183.]\n",
      " [179. 179. 179. 179. 179. 179.]\n",
      " ...\n",
      " [129. 129. 129. 129. 129. 129.]\n",
      " [139. 139. 139. 139. 139. 139.]\n",
      " [157. 157. 157. 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (2469, 6), Train/test: 1/2468\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 73 segments\n",
      "Building dataset, requesting data from 0 to 73\n",
      "x here is\n",
      "[[128. 123. 120. 124. 121. 120.]\n",
      " [123. 120. 124. 121. 120. 121.]\n",
      " [120. 124. 121. 120. 121. 121.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[118. 118. 118. 118. 118. 118.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1440/9623\n",
      "Found 73 continuous time series\n",
      "Data shape: (11065, 6), Train/test: 11063/2\n",
      "Train test ratio: 5531.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293B82140>\n",
      "Epoch 0, test loss: 0.223220\n",
      "Epoch 1, test loss: 0.223600\n",
      "Epoch 2, test loss: 0.224698\n",
      "Epoch 3, test loss: 0.223190\n",
      "Epoch 4, test loss: 0.224352\n",
      "Epoch 5, test loss: 0.224568\n",
      "Epoch 6, test loss: 0.223515\n",
      "Epoch 7, test loss: 0.223467\n",
      "Epoch 8, test loss: 0.226294\n",
      "Epoch 9, test loss: 0.224120\n",
      "Epoch 10, test loss: 0.225271\n",
      "Epoch 11, test loss: 0.223955\n",
      "Epoch 12, test loss: 0.223318\n",
      "Epoch 13, test loss: 0.225111\n",
      "Epoch 14, test loss: 0.223989\n",
      "Epoch 15, test loss: 0.223303\n",
      "Epoch 16, test loss: 0.237073\n",
      "Epoch 17, test loss: 0.230269\n",
      "Epoch 18, test loss: 0.224094\n",
      "Epoch 19, test loss: 0.228370\n",
      "Epoch 20, test loss: 0.227579\n",
      "Epoch 21, test loss: 0.224512\n",
      "Epoch 22, test loss: 0.222935\n",
      "Epoch 23, test loss: 0.225047\n",
      "Epoch 24, test loss: 0.228682\n",
      "Epoch 25, test loss: 0.225340\n",
      "Epoch 26, test loss: 0.224082\n",
      "Epoch 27, test loss: 0.223788\n",
      "Epoch 28, test loss: 0.225899\n",
      "Epoch 29, test loss: 0.223955\n",
      "Epoch 30, test loss: 0.225255\n",
      "Epoch 31, test loss: 0.229246\n",
      "Epoch 32, test loss: 0.224962\n",
      "Epoch 33, test loss: 0.227105\n",
      "Epoch 34, test loss: 0.223662\n",
      "Epoch 35, test loss: 0.225361\n",
      "Epoch 36, test loss: 0.224317\n",
      "Epoch 37, test loss: 0.224292\n",
      "Epoch 38, test loss: 0.224386\n",
      "Epoch 39, test loss: 0.222836\n",
      "Epoch 40, test loss: 0.223330\n",
      "Epoch 41, test loss: 0.224771\n",
      "Epoch 42, test loss: 0.223397\n",
      "Epoch 43, test loss: 0.224708\n",
      "Epoch 44, test loss: 0.226041\n",
      "Epoch 45, test loss: 0.225325\n",
      "Epoch 46, test loss: 0.225060\n",
      "Epoch 47, test loss: 0.225049\n",
      "Epoch 48, test loss: 0.225704\n",
      "Epoch 49, test loss: 0.223763\n",
      "Epoch 50, test loss: 0.224473\n",
      "Epoch 51, test loss: 0.223401\n",
      "Epoch 52, test loss: 0.224330\n",
      "Epoch 53, test loss: 0.226868\n",
      "Epoch 54, test loss: 0.225519\n",
      "Epoch 55, test loss: 0.224016\n",
      "Epoch 56, test loss: 0.232140\n",
      "Epoch 57, test loss: 0.233423\n",
      "Epoch 58, test loss: 0.227597\n",
      "Epoch 59, test loss: 0.224878\n",
      "Epoch 60, test loss: 0.224865\n",
      "Epoch 61, test loss: 0.225820\n",
      "Epoch 62, test loss: 0.235005\n",
      "Epoch 63, test loss: 0.226461\n",
      "Epoch 64, test loss: 0.225496\n",
      "Epoch 65, test loss: 0.230867\n",
      "Epoch 66, test loss: 0.225885\n",
      "Epoch 67, test loss: 0.225319\n",
      "Epoch 68, test loss: 0.225616\n",
      "Epoch 69, test loss: 0.223998\n",
      "Epoch 70, test loss: 0.225959\n",
      "Epoch 71, test loss: 0.226285\n",
      "Epoch 72, test loss: 0.224363\n",
      "Epoch 73, test loss: 0.227871\n",
      "Epoch 74, test loss: 0.224996\n",
      "Epoch 75, test loss: 0.226773\n",
      "Epoch 76, test loss: 0.224973\n",
      "Epoch 77, test loss: 0.225002\n",
      "Epoch 78, test loss: 0.228661\n",
      "Epoch 79, test loss: 0.223906\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293B82140>\n",
      "Epoch 0, test loss: 0.223407\n",
      "Epoch 1, test loss: 0.224752\n",
      "Epoch 2, test loss: 0.227142\n",
      "Epoch 3, test loss: 0.224092\n",
      "Epoch 4, test loss: 0.226593\n",
      "Epoch 5, test loss: 0.225842\n",
      "Epoch 6, test loss: 0.224871\n",
      "Epoch 7, test loss: 0.224171\n",
      "Epoch 8, test loss: 0.224974\n",
      "Epoch 9, test loss: 0.224605\n",
      "Epoch 10, test loss: 0.226055\n",
      "Epoch 11, test loss: 0.225699\n",
      "Epoch 12, test loss: 0.224548\n",
      "Epoch 13, test loss: 0.224313\n",
      "Epoch 14, test loss: 0.225941\n",
      "Epoch 15, test loss: 0.227111\n",
      "Epoch 16, test loss: 0.225009\n",
      "Epoch 17, test loss: 0.224091\n",
      "Epoch 18, test loss: 0.225466\n",
      "Epoch 19, test loss: 0.224821\n",
      "Epoch 20, test loss: 0.225640\n",
      "Epoch 21, test loss: 0.224439\n",
      "Epoch 22, test loss: 0.226462\n",
      "Epoch 23, test loss: 0.224971\n",
      "Epoch 24, test loss: 0.225284\n",
      "Epoch 25, test loss: 0.225977\n",
      "Epoch 26, test loss: 0.224796\n",
      "Epoch 27, test loss: 0.225097\n",
      "Epoch 28, test loss: 0.225446\n",
      "Epoch 29, test loss: 0.226247\n",
      "Epoch 30, test loss: 0.225464\n",
      "Epoch 31, test loss: 0.225101\n",
      "Epoch 32, test loss: 0.225636\n",
      "Epoch 33, test loss: 0.225250\n",
      "Epoch 34, test loss: 0.225087\n",
      "Epoch 35, test loss: 0.226009\n",
      "Epoch 36, test loss: 0.226362\n",
      "Epoch 37, test loss: 0.226025\n",
      "Epoch 38, test loss: 0.225808\n",
      "Epoch 39, test loss: 0.227032\n",
      "Epoch 40, test loss: 0.226556\n",
      "Epoch 41, test loss: 0.227969\n",
      "Epoch 42, test loss: 0.225930\n",
      "Epoch 43, test loss: 0.232125\n",
      "Epoch 44, test loss: 0.224940\n",
      "Epoch 45, test loss: 0.225201\n",
      "Epoch 46, test loss: 0.224868\n",
      "Epoch 47, test loss: 0.225758\n",
      "Epoch 48, test loss: 0.225488\n",
      "Epoch 49, test loss: 0.227523\n",
      "Epoch 50, test loss: 0.224967\n",
      "Epoch 51, test loss: 0.228670\n",
      "Epoch 52, test loss: 0.226041\n",
      "Epoch 53, test loss: 0.225776\n",
      "Epoch 54, test loss: 0.226197\n",
      "Epoch 55, test loss: 0.224716\n",
      "Epoch 56, test loss: 0.227118\n",
      "Epoch 57, test loss: 0.225074\n",
      "Epoch 58, test loss: 0.228274\n",
      "Epoch 59, test loss: 0.226306\n",
      "Epoch 60, test loss: 0.226539\n",
      "Epoch 61, test loss: 0.223517\n",
      "Epoch 62, test loss: 0.225620\n",
      "Epoch 63, test loss: 0.225768\n",
      "Epoch 64, test loss: 0.224321\n",
      "Epoch 65, test loss: 0.226208\n",
      "Epoch 66, test loss: 0.227179\n",
      "Epoch 67, test loss: 0.226361\n",
      "Epoch 68, test loss: 0.225694\n",
      "Epoch 69, test loss: 0.224949\n",
      "Epoch 70, test loss: 0.224119\n",
      "Epoch 71, test loss: 0.225603\n",
      "Epoch 72, test loss: 0.225388\n",
      "Epoch 73, test loss: 0.227692\n",
      "Epoch 74, test loss: 0.226620\n",
      "Epoch 75, test loss: 0.224880\n",
      "Epoch 76, test loss: 0.226098\n",
      "Epoch 77, test loss: 0.225231\n",
      "Epoch 78, test loss: 0.224412\n",
      "Epoch 79, test loss: 0.225972\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293B82140>\n",
      "Epoch 0, test loss: 0.391048\n",
      "Epoch 1, test loss: 0.252950\n",
      "Epoch 2, test loss: 0.250680\n",
      "Epoch 3, test loss: 0.249873\n",
      "Epoch 4, test loss: 0.247409\n",
      "Epoch 5, test loss: 0.244944\n",
      "Epoch 6, test loss: 0.241819\n",
      "Epoch 7, test loss: 0.240944\n",
      "Epoch 8, test loss: 0.236694\n",
      "Epoch 9, test loss: 0.233232\n",
      "Epoch 10, test loss: 0.235487\n",
      "Epoch 11, test loss: 0.232348\n",
      "Epoch 12, test loss: 0.230160\n",
      "Epoch 13, test loss: 0.231053\n",
      "Epoch 14, test loss: 0.229725\n",
      "Epoch 15, test loss: 0.231528\n",
      "Epoch 16, test loss: 0.228953\n",
      "Epoch 17, test loss: 0.232217\n",
      "Epoch 18, test loss: 0.228980\n",
      "Epoch 19, test loss: 0.233179\n",
      "Epoch 20, test loss: 0.229452\n",
      "Epoch 21, test loss: 0.230470\n",
      "Epoch 22, test loss: 0.226842\n",
      "Epoch 23, test loss: 0.226991\n",
      "Epoch 24, test loss: 0.228204\n",
      "Epoch 25, test loss: 0.227203\n",
      "Epoch 26, test loss: 0.227491\n",
      "Epoch 27, test loss: 0.227097\n",
      "Epoch 28, test loss: 0.226365\n",
      "Epoch 29, test loss: 0.225499\n",
      "Epoch 30, test loss: 0.226877\n",
      "Epoch 31, test loss: 0.226194\n",
      "Epoch 32, test loss: 0.227638\n",
      "Epoch 33, test loss: 0.225931\n",
      "Epoch 34, test loss: 0.227790\n",
      "Epoch 35, test loss: 0.225774\n",
      "Epoch 36, test loss: 0.225037\n",
      "Epoch 37, test loss: 0.225419\n",
      "Epoch 38, test loss: 0.227118\n",
      "Epoch 39, test loss: 0.225499\n",
      "Epoch 40, test loss: 0.226357\n",
      "Epoch 41, test loss: 0.226912\n",
      "Epoch 42, test loss: 0.225017\n",
      "Epoch 43, test loss: 0.226848\n",
      "Epoch 44, test loss: 0.229525\n",
      "Epoch 45, test loss: 0.226123\n",
      "Epoch 46, test loss: 0.224247\n",
      "Epoch 47, test loss: 0.226182\n",
      "Epoch 48, test loss: 0.226189\n",
      "Epoch 49, test loss: 0.224817\n",
      "Epoch 50, test loss: 0.225507\n",
      "Epoch 51, test loss: 0.225527\n",
      "Epoch 52, test loss: 0.226894\n",
      "Epoch 53, test loss: 0.229335\n",
      "Epoch 54, test loss: 0.224565\n",
      "Epoch 55, test loss: 0.224893\n",
      "Epoch 56, test loss: 0.225978\n",
      "Epoch 57, test loss: 0.225184\n",
      "Epoch 58, test loss: 0.225803\n",
      "Epoch 59, test loss: 0.226074\n",
      "Epoch 60, test loss: 0.227825\n",
      "Epoch 61, test loss: 0.224824\n",
      "Epoch 62, test loss: 0.224354\n",
      "Epoch 63, test loss: 0.226099\n",
      "Epoch 64, test loss: 0.226922\n",
      "Epoch 65, test loss: 0.224021\n",
      "Epoch 66, test loss: 0.225706\n",
      "Epoch 67, test loss: 0.224130\n",
      "Epoch 68, test loss: 0.224635\n",
      "Epoch 69, test loss: 0.226777\n",
      "Epoch 70, test loss: 0.224304\n",
      "Epoch 71, test loss: 0.224343\n",
      "Epoch 72, test loss: 0.226277\n",
      "Epoch 73, test loss: 0.225996\n",
      "Epoch 74, test loss: 0.226010\n",
      "Epoch 75, test loss: 0.227541\n",
      "Epoch 76, test loss: 0.223987\n",
      "Epoch 77, test loss: 0.225054\n",
      "Epoch 78, test loss: 0.228993\n",
      "Epoch 79, test loss: 0.225762\n",
      "Epoch 80, test loss: 0.223364\n",
      "Epoch 81, test loss: 0.224307\n",
      "Epoch 82, test loss: 0.224570\n",
      "Epoch 83, test loss: 0.225080\n",
      "Epoch 84, test loss: 0.226144\n",
      "Epoch 85, test loss: 0.225225\n",
      "Epoch 86, test loss: 0.227259\n",
      "Epoch 87, test loss: 0.224276\n",
      "Epoch 88, test loss: 0.224879\n",
      "Epoch 89, test loss: 0.224943\n",
      "Epoch 90, test loss: 0.224873\n",
      "Epoch 91, test loss: 0.222958\n",
      "Epoch 92, test loss: 0.224056\n",
      "Epoch 93, test loss: 0.227026\n",
      "Epoch 94, test loss: 0.224444\n",
      "Epoch 95, test loss: 0.230395\n",
      "Epoch 96, test loss: 0.224641\n",
      "Epoch 97, test loss: 0.226031\n",
      "Epoch 98, test loss: 0.224340\n",
      "Epoch 99, test loss: 0.225366\n",
      "Epoch 100, test loss: 0.223937\n",
      "Epoch 101, test loss: 0.225028\n",
      "Epoch 102, test loss: 0.225325\n",
      "Epoch 103, test loss: 0.224702\n",
      "Epoch 104, test loss: 0.227307\n",
      "Epoch 105, test loss: 0.226104\n",
      "Epoch 106, test loss: 0.225785\n",
      "Epoch 107, test loss: 0.226077\n",
      "Epoch 108, test loss: 0.223205\n",
      "Epoch 109, test loss: 0.227122\n",
      "Epoch 110, test loss: 0.224534\n",
      "Epoch 111, test loss: 0.225033\n",
      "Epoch 112, test loss: 0.225148\n",
      "Epoch 113, test loss: 0.225554\n",
      "Epoch 114, test loss: 0.223927\n",
      "Epoch 115, test loss: 0.223615\n",
      "Epoch 116, test loss: 0.224827\n",
      "Epoch 117, test loss: 0.223293\n",
      "Epoch 118, test loss: 0.222658\n",
      "Epoch 119, test loss: 0.223270\n",
      "Epoch 120, test loss: 0.224554\n",
      "Epoch 121, test loss: 0.228494\n",
      "Epoch 122, test loss: 0.223462\n",
      "Epoch 123, test loss: 0.224588\n",
      "Epoch 124, test loss: 0.224686\n",
      "Epoch 125, test loss: 0.224905\n",
      "Epoch 126, test loss: 0.223392\n",
      "Epoch 127, test loss: 0.224367\n",
      "Epoch 128, test loss: 0.223928\n",
      "Epoch 129, test loss: 0.223910\n",
      "Epoch 130, test loss: 0.225208\n",
      "Epoch 131, test loss: 0.227291\n",
      "Epoch 132, test loss: 0.224064\n",
      "Epoch 133, test loss: 0.224249\n",
      "Epoch 134, test loss: 0.224752\n",
      "Epoch 135, test loss: 0.226252\n",
      "Epoch 136, test loss: 0.225309\n",
      "Epoch 137, test loss: 0.224289\n",
      "Epoch 138, test loss: 0.227115\n",
      "Epoch 139, test loss: 0.225158\n",
      "Epoch 140, test loss: 0.228259\n",
      "Epoch 141, test loss: 0.227594\n",
      "Epoch 142, test loss: 0.223944\n",
      "Epoch 143, test loss: 0.224357\n",
      "Epoch 144, test loss: 0.224326\n",
      "Epoch 145, test loss: 0.222975\n",
      "Epoch 146, test loss: 0.227836\n",
      "Epoch 147, test loss: 0.224208\n",
      "Epoch 148, test loss: 0.224468\n",
      "Epoch 149, test loss: 0.225625\n",
      "Epoch 150, test loss: 0.226426\n",
      "Epoch 151, test loss: 0.225577\n",
      "Epoch 152, test loss: 0.224874\n",
      "Epoch 153, test loss: 0.224494\n",
      "Epoch 154, test loss: 0.225848\n",
      "Epoch 155, test loss: 0.224055\n",
      "Epoch 156, test loss: 0.224384\n",
      "Epoch 157, test loss: 0.225436\n",
      "Epoch 158, test loss: 0.224535\n",
      "Epoch 159, test loss: 0.224326\n",
      "Epoch 160, test loss: 0.224017\n",
      "Epoch 161, test loss: 0.226261\n",
      "Epoch 162, test loss: 0.225015\n",
      "Epoch 163, test loss: 0.226168\n",
      "Epoch 164, test loss: 0.225845\n",
      "Epoch 165, test loss: 0.223827\n",
      "Epoch 166, test loss: 0.223771\n",
      "Epoch 167, test loss: 0.224653\n",
      "Epoch 168, test loss: 0.224860\n",
      "Epoch 169, test loss: 0.226711\n",
      "Epoch 170, test loss: 0.228186\n",
      "Epoch 171, test loss: 0.224659\n",
      "Epoch 172, test loss: 0.225588\n",
      "Epoch 173, test loss: 0.224870\n",
      "Epoch 174, test loss: 0.225223\n",
      "Epoch 175, test loss: 0.224521\n",
      "Epoch 176, test loss: 0.224211\n",
      "Epoch 177, test loss: 0.225269\n",
      "Epoch 178, test loss: 0.224984\n",
      "Epoch 179, test loss: 0.222867\n",
      "Epoch 180, test loss: 0.225540\n",
      "Epoch 181, test loss: 0.225312\n",
      "Epoch 182, test loss: 0.225549\n",
      "Epoch 183, test loss: 0.224464\n",
      "Epoch 184, test loss: 0.224810\n",
      "Epoch 185, test loss: 0.225374\n",
      "Epoch 186, test loss: 0.225105\n",
      "Epoch 187, test loss: 0.224522\n",
      "Epoch 188, test loss: 0.224890\n",
      "Epoch 189, test loss: 0.226088\n",
      "Epoch 190, test loss: 0.224826\n",
      "Epoch 191, test loss: 0.225163\n",
      "Epoch 192, test loss: 0.224850\n",
      "Epoch 193, test loss: 0.227050\n",
      "Epoch 194, test loss: 0.226365\n",
      "Epoch 195, test loss: 0.226645\n",
      "Epoch 196, test loss: 0.226064\n",
      "Epoch 197, test loss: 0.225936\n",
      "Epoch 198, test loss: 0.224445\n",
      "Epoch 199, test loss: 0.223340\n",
      "Epoch 200, test loss: 0.223644\n",
      "Epoch 201, test loss: 0.224641\n",
      "Epoch 202, test loss: 0.226464\n",
      "Epoch 203, test loss: 0.225624\n",
      "Epoch 204, test loss: 0.227943\n",
      "Epoch 205, test loss: 0.227883\n",
      "Epoch 206, test loss: 0.225889\n",
      "Epoch 207, test loss: 0.225748\n",
      "Epoch 208, test loss: 0.227749\n",
      "Epoch 209, test loss: 0.224191\n",
      "Epoch 210, test loss: 0.224221\n",
      "Epoch 211, test loss: 0.224727\n",
      "Epoch 212, test loss: 0.225264\n",
      "Epoch 213, test loss: 0.225768\n",
      "Epoch 214, test loss: 0.228418\n",
      "Epoch 215, test loss: 0.224668\n",
      "Epoch 216, test loss: 0.224605\n",
      "Epoch 217, test loss: 0.223943\n",
      "Epoch 218, test loss: 0.226184\n",
      "Epoch 219, test loss: 0.224295\n",
      "Epoch 220, test loss: 0.225558\n",
      "Epoch 221, test loss: 0.224981\n",
      "Epoch 222, test loss: 0.227007\n",
      "Epoch 223, test loss: 0.223989\n",
      "Epoch 224, test loss: 0.225221\n",
      "Epoch 225, test loss: 0.225292\n",
      "Epoch 226, test loss: 0.226467\n",
      "Epoch 227, test loss: 0.225239\n",
      "Epoch 228, test loss: 0.225292\n",
      "Epoch 229, test loss: 0.226148\n",
      "Epoch 230, test loss: 0.225195\n",
      "Epoch 231, test loss: 0.223471\n",
      "Epoch 232, test loss: 0.225208\n",
      "Epoch 233, test loss: 0.228075\n",
      "Epoch 234, test loss: 0.226841\n",
      "Epoch 235, test loss: 0.224812\n",
      "Epoch 236, test loss: 0.225889\n",
      "Epoch 237, test loss: 0.226127\n",
      "Epoch 238, test loss: 0.226191\n",
      "Epoch 239, test loss: 0.226202\n",
      "Pretrain data: 19732321.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7547/107625\n",
      "Found 815 continuous time series\n",
      "Data shape: (115174, 6), Train/test: 115172/2\n",
      "Train test ratio: 57586.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1799\n",
      "Epoch 0, train loss: 0.233186\n",
      "Epoch 1, train loss: 0.238566\n",
      "Epoch 2, train loss: 0.240794\n",
      "Epoch 3, train loss: 0.197505\n",
      "Epoch 4, train loss: 0.218458\n",
      "Epoch 5, train loss: 0.285295\n",
      "Epoch 6, train loss: 0.214858\n",
      "Epoch 7, train loss: 0.189026\n",
      "Epoch 8, train loss: 0.170089\n",
      "Epoch 9, train loss: 0.188080\n",
      "Epoch 10, train loss: 0.174377\n",
      "Epoch 11, train loss: 0.243548\n",
      "Epoch 12, train loss: 0.195639\n",
      "Epoch 13, train loss: 0.165660\n",
      "Epoch 14, train loss: 0.260470\n",
      "Epoch 15, train loss: 0.183721\n",
      "Epoch 16, train loss: 0.234467\n",
      "Epoch 17, train loss: 0.204404\n",
      "Epoch 18, train loss: 0.202419\n",
      "Epoch 19, train loss: 0.168633\n",
      "Epoch 20, train loss: 0.227826\n",
      "Epoch 21, train loss: 0.207848\n",
      "Epoch 22, train loss: 0.240492\n",
      "Epoch 23, train loss: 0.175835\n",
      "Epoch 24, train loss: 0.200102\n",
      "Epoch 25, train loss: 0.203221\n",
      "Epoch 26, train loss: 0.184698\n",
      "Epoch 27, train loss: 0.197536\n",
      "Epoch 28, train loss: 0.208189\n",
      "Epoch 29, train loss: 0.204028\n",
      "Epoch 30, train loss: 0.199146\n",
      "Epoch 31, train loss: 0.243622\n",
      "Epoch 32, train loss: 0.169541\n",
      "Epoch 33, train loss: 0.191683\n",
      "Epoch 34, train loss: 0.375209\n",
      "Epoch 35, train loss: 0.217428\n",
      "Epoch 36, train loss: 0.297095\n",
      "Epoch 37, train loss: 0.206860\n",
      "Epoch 38, train loss: 0.208990\n",
      "Epoch 39, train loss: 0.166694\n",
      "Epoch 40, train loss: 0.193433\n",
      "Epoch 41, train loss: 0.198230\n",
      "Epoch 42, train loss: 0.193761\n",
      "Epoch 43, train loss: 0.156993\n",
      "Epoch 44, train loss: 0.161419\n",
      "Epoch 45, train loss: 0.212265\n",
      "Epoch 46, train loss: 0.213637\n",
      "Epoch 47, train loss: 0.182419\n",
      "Epoch 48, train loss: 0.202017\n",
      "Epoch 49, train loss: 0.228356\n",
      "Epoch 50, train loss: 0.182870\n",
      "Epoch 51, train loss: 0.188223\n",
      "Epoch 52, train loss: 0.226725\n",
      "Epoch 53, train loss: 0.158450\n",
      "Epoch 54, train loss: 0.215478\n",
      "Epoch 55, train loss: 0.196989\n",
      "Epoch 56, train loss: 0.216203\n",
      "Epoch 57, train loss: 0.166306\n",
      "Epoch 58, train loss: 0.153727\n",
      "Epoch 59, train loss: 0.176290\n",
      "Epoch 60, train loss: 0.215669\n",
      "Epoch 61, train loss: 0.199801\n",
      "Epoch 62, train loss: 0.190751\n",
      "Epoch 63, train loss: 0.189962\n",
      "Epoch 64, train loss: 0.179598\n",
      "Epoch 65, train loss: 0.206811\n",
      "Epoch 66, train loss: 0.218205\n",
      "Epoch 67, train loss: 0.214845\n",
      "Epoch 68, train loss: 0.253791\n",
      "Epoch 69, train loss: 0.197208\n",
      "Epoch 70, train loss: 0.206854\n",
      "Epoch 71, train loss: 0.209070\n",
      "Epoch 72, train loss: 0.217198\n",
      "Epoch 73, train loss: 0.211128\n",
      "Epoch 74, train loss: 0.197046\n",
      "Epoch 75, train loss: 0.182728\n",
      "Epoch 76, train loss: 0.274805\n",
      "Epoch 77, train loss: 0.198036\n",
      "Epoch 78, train loss: 0.239286\n",
      "Epoch 79, train loss: 0.207913\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "x here is\n",
      "[[283. 282. 281. 277. 267. 258.]\n",
      " [282. 281. 277. 267. 258. 251.]\n",
      " [281. 277. 267. 258. 251. 237.]\n",
      " ...\n",
      " [164. 163. 160. 159. 159. 156.]\n",
      " [163. 160. 159. 159. 156. 154.]\n",
      " [160. 159. 159. 156. 154. 152.]]\n",
      "y here is\n",
      "[[201. 201. 201. 201. 201. 201.]\n",
      " [195. 195. 195. 195. 195. 195.]\n",
      " [189. 189. 189. 189. 189. 189.]\n",
      " ...\n",
      " [151. 151. 151. 151. 151. 151.]\n",
      " [149. 149. 149. 149. 149. 149.]\n",
      " [144. 144. 144. 144. 144. 144.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (2705, 6), Train/test: 1/2704\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[160. 158. 160. 166. 175. 182.]\n",
      " [158. 160. 166. 175. 182. 188.]\n",
      " [160. 166. 175. 182. 188. 192.]\n",
      " ...\n",
      " [227. 223. 219. 211. 202. 196.]\n",
      " [223. 219. 211. 202. 196. 199.]\n",
      " [219. 211. 202. 196. 199. 224.]]\n",
      "y here is\n",
      "[[218. 218. 218. 218. 218. 218.]\n",
      " [226. 226. 226. 226. 226. 226.]\n",
      " [231. 231. 231. 231. 231. 231.]\n",
      " ...\n",
      " [268. 268. 268. 268. 268. 268.]\n",
      " [301. 301. 301. 301. 301. 301.]\n",
      " [290. 290. 290. 290. 290. 290.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 738/9814\n",
      "Found 27 continuous time series\n",
      "Data shape: (10554, 6), Train/test: 10552/2\n",
      "Train test ratio: 5276.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290183BB0>\n",
      "Epoch 0, test loss: 0.210529\n",
      "Epoch 1, test loss: 0.210058\n",
      "Epoch 2, test loss: 0.209274\n",
      "Epoch 3, test loss: 0.210980\n",
      "Epoch 4, test loss: 0.211119\n",
      "Epoch 5, test loss: 0.209831\n",
      "Epoch 6, test loss: 0.209410\n",
      "Epoch 7, test loss: 0.209550\n",
      "Epoch 8, test loss: 0.209549\n",
      "Epoch 9, test loss: 0.210364\n",
      "Epoch 10, test loss: 0.209927\n",
      "Epoch 11, test loss: 0.210531\n",
      "Epoch 12, test loss: 0.209833\n",
      "Epoch 13, test loss: 0.209804\n",
      "Epoch 14, test loss: 0.210479\n",
      "Epoch 15, test loss: 0.210171\n",
      "Epoch 16, test loss: 0.209917\n",
      "Epoch 17, test loss: 0.210464\n",
      "Epoch 18, test loss: 0.210611\n",
      "Epoch 19, test loss: 0.209675\n",
      "Epoch 20, test loss: 0.211666\n",
      "Epoch 21, test loss: 0.210603\n",
      "Epoch 22, test loss: 0.211866\n",
      "Epoch 23, test loss: 0.213516\n",
      "Epoch 24, test loss: 0.210098\n",
      "Epoch 25, test loss: 0.210411\n",
      "Epoch 26, test loss: 0.210423\n",
      "Epoch 27, test loss: 0.210224\n",
      "Epoch 28, test loss: 0.210552\n",
      "Epoch 29, test loss: 0.210840\n",
      "Epoch 30, test loss: 0.211126\n",
      "Epoch 31, test loss: 0.210894\n",
      "Epoch 32, test loss: 0.210538\n",
      "Epoch 33, test loss: 0.210198\n",
      "Epoch 34, test loss: 0.211892\n",
      "Epoch 35, test loss: 0.210491\n",
      "Epoch 36, test loss: 0.209863\n",
      "Epoch 37, test loss: 0.211200\n",
      "Epoch 38, test loss: 0.210779\n",
      "Epoch 39, test loss: 0.210058\n",
      "Epoch 40, test loss: 0.211684\n",
      "Epoch 41, test loss: 0.209822\n",
      "Epoch 42, test loss: 0.210061\n",
      "Epoch 43, test loss: 0.210264\n",
      "Epoch 44, test loss: 0.210048\n",
      "Epoch 45, test loss: 0.210555\n",
      "Epoch 46, test loss: 0.210514\n",
      "Epoch 47, test loss: 0.209940\n",
      "Epoch 48, test loss: 0.210825\n",
      "Epoch 49, test loss: 0.210123\n",
      "Epoch 50, test loss: 0.212339\n",
      "Epoch 51, test loss: 0.210141\n",
      "Epoch 52, test loss: 0.211048\n",
      "Epoch 53, test loss: 0.209600\n",
      "Epoch 54, test loss: 0.210156\n",
      "Epoch 55, test loss: 0.210906\n",
      "Epoch 56, test loss: 0.210542\n",
      "Epoch 57, test loss: 0.209703\n",
      "Epoch 58, test loss: 0.211137\n",
      "Epoch 59, test loss: 0.210513\n",
      "Epoch 60, test loss: 0.210401\n",
      "Epoch 61, test loss: 0.210053\n",
      "Epoch 62, test loss: 0.212999\n",
      "Epoch 63, test loss: 0.210510\n",
      "Epoch 64, test loss: 0.209945\n",
      "Epoch 65, test loss: 0.210860\n",
      "Epoch 66, test loss: 0.210923\n",
      "Epoch 67, test loss: 0.209894\n",
      "Epoch 68, test loss: 0.210657\n",
      "Epoch 69, test loss: 0.209720\n",
      "Epoch 70, test loss: 0.211384\n",
      "Epoch 71, test loss: 0.210253\n",
      "Epoch 72, test loss: 0.209898\n",
      "Epoch 73, test loss: 0.214083\n",
      "Epoch 74, test loss: 0.209745\n",
      "Epoch 75, test loss: 0.210051\n",
      "Epoch 76, test loss: 0.210135\n",
      "Epoch 77, test loss: 0.211502\n",
      "Epoch 78, test loss: 0.210438\n",
      "Epoch 79, test loss: 0.211831\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290183BB0>\n",
      "Epoch 0, test loss: 0.210589\n",
      "Epoch 1, test loss: 0.209815\n",
      "Epoch 2, test loss: 0.208999\n",
      "Epoch 3, test loss: 0.209459\n",
      "Epoch 4, test loss: 0.209457\n",
      "Epoch 5, test loss: 0.209533\n",
      "Epoch 6, test loss: 0.210346\n",
      "Epoch 7, test loss: 0.209909\n",
      "Epoch 8, test loss: 0.212095\n",
      "Epoch 9, test loss: 0.211293\n",
      "Epoch 10, test loss: 0.210499\n",
      "Epoch 11, test loss: 0.210157\n",
      "Epoch 12, test loss: 0.210171\n",
      "Epoch 13, test loss: 0.211005\n",
      "Epoch 14, test loss: 0.209732\n",
      "Epoch 15, test loss: 0.210281\n",
      "Epoch 16, test loss: 0.209889\n",
      "Epoch 17, test loss: 0.210035\n",
      "Epoch 18, test loss: 0.210170\n",
      "Epoch 19, test loss: 0.209783\n",
      "Epoch 20, test loss: 0.211960\n",
      "Epoch 21, test loss: 0.210499\n",
      "Epoch 22, test loss: 0.209832\n",
      "Epoch 23, test loss: 0.209725\n",
      "Epoch 24, test loss: 0.210019\n",
      "Epoch 25, test loss: 0.210544\n",
      "Epoch 26, test loss: 0.209786\n",
      "Epoch 27, test loss: 0.209946\n",
      "Epoch 28, test loss: 0.209920\n",
      "Epoch 29, test loss: 0.209752\n",
      "Epoch 30, test loss: 0.211715\n",
      "Epoch 31, test loss: 0.210476\n",
      "Epoch 32, test loss: 0.210415\n",
      "Epoch 33, test loss: 0.210025\n",
      "Epoch 34, test loss: 0.209970\n",
      "Epoch 35, test loss: 0.210186\n",
      "Epoch 36, test loss: 0.211109\n",
      "Epoch 37, test loss: 0.209680\n",
      "Epoch 38, test loss: 0.211228\n",
      "Epoch 39, test loss: 0.210656\n",
      "Epoch 40, test loss: 0.210102\n",
      "Epoch 41, test loss: 0.210228\n",
      "Epoch 42, test loss: 0.210144\n",
      "Epoch 43, test loss: 0.210698\n",
      "Epoch 44, test loss: 0.211339\n",
      "Epoch 45, test loss: 0.210598\n",
      "Epoch 46, test loss: 0.210848\n",
      "Epoch 47, test loss: 0.211588\n",
      "Epoch 48, test loss: 0.212492\n",
      "Epoch 49, test loss: 0.210302\n",
      "Epoch 50, test loss: 0.210037\n",
      "Epoch 51, test loss: 0.210356\n",
      "Epoch 52, test loss: 0.211572\n",
      "Epoch 53, test loss: 0.210084\n",
      "Epoch 54, test loss: 0.210027\n",
      "Epoch 55, test loss: 0.209844\n",
      "Epoch 56, test loss: 0.210780\n",
      "Epoch 57, test loss: 0.210016\n",
      "Epoch 58, test loss: 0.210326\n",
      "Epoch 59, test loss: 0.209884\n",
      "Epoch 60, test loss: 0.210525\n",
      "Epoch 61, test loss: 0.209947\n",
      "Epoch 62, test loss: 0.210246\n",
      "Epoch 63, test loss: 0.210470\n",
      "Epoch 64, test loss: 0.210162\n",
      "Epoch 65, test loss: 0.210048\n",
      "Epoch 66, test loss: 0.210212\n",
      "Epoch 67, test loss: 0.210134\n",
      "Epoch 68, test loss: 0.210871\n",
      "Epoch 69, test loss: 0.211597\n",
      "Epoch 70, test loss: 0.210048\n",
      "Epoch 71, test loss: 0.210914\n",
      "Epoch 72, test loss: 0.210988\n",
      "Epoch 73, test loss: 0.210106\n",
      "Epoch 74, test loss: 0.210081\n",
      "Epoch 75, test loss: 0.213535\n",
      "Epoch 76, test loss: 0.210247\n",
      "Epoch 77, test loss: 0.210124\n",
      "Epoch 78, test loss: 0.210058\n",
      "Epoch 79, test loss: 0.210134\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290183BB0>\n",
      "Epoch 0, test loss: 0.249835\n",
      "Epoch 1, test loss: 0.232755\n",
      "Epoch 2, test loss: 0.218807\n",
      "Epoch 3, test loss: 0.216449\n",
      "Epoch 4, test loss: 0.215132\n",
      "Epoch 5, test loss: 0.215923\n",
      "Epoch 6, test loss: 0.215462\n",
      "Epoch 7, test loss: 0.215928\n",
      "Epoch 8, test loss: 0.214572\n",
      "Epoch 9, test loss: 0.214613\n",
      "Epoch 10, test loss: 0.214862\n",
      "Epoch 11, test loss: 0.214493\n",
      "Epoch 12, test loss: 0.215146\n",
      "Epoch 13, test loss: 0.213961\n",
      "Epoch 14, test loss: 0.214603\n",
      "Epoch 15, test loss: 0.214359\n",
      "Epoch 16, test loss: 0.215094\n",
      "Epoch 17, test loss: 0.216802\n",
      "Epoch 18, test loss: 0.212839\n",
      "Epoch 19, test loss: 0.216010\n",
      "Epoch 20, test loss: 0.213433\n",
      "Epoch 21, test loss: 0.211905\n",
      "Epoch 22, test loss: 0.211575\n",
      "Epoch 23, test loss: 0.212141\n",
      "Epoch 24, test loss: 0.211655\n",
      "Epoch 25, test loss: 0.217175\n",
      "Epoch 26, test loss: 0.213004\n",
      "Epoch 27, test loss: 0.211151\n",
      "Epoch 28, test loss: 0.211713\n",
      "Epoch 29, test loss: 0.213132\n",
      "Epoch 30, test loss: 0.211010\n",
      "Epoch 31, test loss: 0.220183\n",
      "Epoch 32, test loss: 0.211158\n",
      "Epoch 33, test loss: 0.212314\n",
      "Epoch 34, test loss: 0.215641\n",
      "Epoch 35, test loss: 0.210917\n",
      "Epoch 36, test loss: 0.211465\n",
      "Epoch 37, test loss: 0.213844\n",
      "Epoch 38, test loss: 0.214614\n",
      "Epoch 39, test loss: 0.212816\n",
      "Epoch 40, test loss: 0.210473\n",
      "Epoch 41, test loss: 0.210821\n",
      "Epoch 42, test loss: 0.212415\n",
      "Epoch 43, test loss: 0.211324\n",
      "Epoch 44, test loss: 0.210609\n",
      "Epoch 45, test loss: 0.210564\n",
      "Epoch 46, test loss: 0.213422\n",
      "Epoch 47, test loss: 0.210790\n",
      "Epoch 48, test loss: 0.210717\n",
      "Epoch 49, test loss: 0.210448\n",
      "Epoch 50, test loss: 0.210614\n",
      "Epoch 51, test loss: 0.211261\n",
      "Epoch 52, test loss: 0.210193\n",
      "Epoch 53, test loss: 0.211479\n",
      "Epoch 54, test loss: 0.210774\n",
      "Epoch 55, test loss: 0.210824\n",
      "Epoch 56, test loss: 0.210472\n",
      "Epoch 57, test loss: 0.210941\n",
      "Epoch 58, test loss: 0.210669\n",
      "Epoch 59, test loss: 0.212018\n",
      "Epoch 60, test loss: 0.210521\n",
      "Epoch 61, test loss: 0.212081\n",
      "Epoch 62, test loss: 0.210958\n",
      "Epoch 63, test loss: 0.216840\n",
      "Epoch 64, test loss: 0.210732\n",
      "Epoch 65, test loss: 0.211508\n",
      "Epoch 66, test loss: 0.210994\n",
      "Epoch 67, test loss: 0.214308\n",
      "Epoch 68, test loss: 0.213717\n",
      "Epoch 69, test loss: 0.210869\n",
      "Epoch 70, test loss: 0.210391\n",
      "Epoch 71, test loss: 0.217286\n",
      "Epoch 72, test loss: 0.210819\n",
      "Epoch 73, test loss: 0.213978\n",
      "Epoch 74, test loss: 0.210806\n",
      "Epoch 75, test loss: 0.210456\n",
      "Epoch 76, test loss: 0.211340\n",
      "Epoch 77, test loss: 0.210993\n",
      "Epoch 78, test loss: 0.210134\n",
      "Epoch 79, test loss: 0.210316\n",
      "Epoch 80, test loss: 0.211903\n",
      "Epoch 81, test loss: 0.210204\n",
      "Epoch 82, test loss: 0.210441\n",
      "Epoch 83, test loss: 0.210185\n",
      "Epoch 84, test loss: 0.210808\n",
      "Epoch 85, test loss: 0.212396\n",
      "Epoch 86, test loss: 0.210301\n",
      "Epoch 87, test loss: 0.210005\n",
      "Epoch 88, test loss: 0.209858\n",
      "Epoch 89, test loss: 0.211262\n",
      "Epoch 90, test loss: 0.209796\n",
      "Epoch 91, test loss: 0.212384\n",
      "Epoch 92, test loss: 0.210580\n",
      "Epoch 93, test loss: 0.210257\n",
      "Epoch 94, test loss: 0.211341\n",
      "Epoch 95, test loss: 0.210422\n",
      "Epoch 96, test loss: 0.209815\n",
      "Epoch 97, test loss: 0.213236\n",
      "Epoch 98, test loss: 0.209944\n",
      "Epoch 99, test loss: 0.211346\n",
      "Epoch 100, test loss: 0.210369\n",
      "Epoch 101, test loss: 0.209568\n",
      "Epoch 102, test loss: 0.210044\n",
      "Epoch 103, test loss: 0.211393\n",
      "Epoch 104, test loss: 0.210085\n",
      "Epoch 105, test loss: 0.212164\n",
      "Epoch 106, test loss: 0.209838\n",
      "Epoch 107, test loss: 0.210813\n",
      "Epoch 108, test loss: 0.209681\n",
      "Epoch 109, test loss: 0.214281\n",
      "Epoch 110, test loss: 0.212395\n",
      "Epoch 111, test loss: 0.209505\n",
      "Epoch 112, test loss: 0.209883\n",
      "Epoch 113, test loss: 0.210418\n",
      "Epoch 114, test loss: 0.210835\n",
      "Epoch 115, test loss: 0.211382\n",
      "Epoch 116, test loss: 0.209643\n",
      "Epoch 117, test loss: 0.212490\n",
      "Epoch 118, test loss: 0.210280\n",
      "Epoch 119, test loss: 0.209905\n",
      "Epoch 120, test loss: 0.210289\n",
      "Epoch 121, test loss: 0.215245\n",
      "Epoch 122, test loss: 0.211463\n",
      "Epoch 123, test loss: 0.210649\n",
      "Epoch 124, test loss: 0.210709\n",
      "Epoch 125, test loss: 0.210805\n",
      "Epoch 126, test loss: 0.210374\n",
      "Epoch 127, test loss: 0.212549\n",
      "Epoch 128, test loss: 0.211681\n",
      "Epoch 129, test loss: 0.210694\n",
      "Epoch 130, test loss: 0.213267\n",
      "Epoch 131, test loss: 0.214517\n",
      "Epoch 132, test loss: 0.210010\n",
      "Epoch 133, test loss: 0.209706\n",
      "Epoch 134, test loss: 0.211219\n",
      "Epoch 135, test loss: 0.210327\n",
      "Epoch 136, test loss: 0.210195\n",
      "Epoch 137, test loss: 0.218736\n",
      "Epoch 138, test loss: 0.209814\n",
      "Epoch 139, test loss: 0.213124\n",
      "Epoch 140, test loss: 0.212972\n",
      "Epoch 141, test loss: 0.212783\n",
      "Epoch 142, test loss: 0.211471\n",
      "Epoch 143, test loss: 0.211821\n",
      "Epoch 144, test loss: 0.209862\n",
      "Epoch 145, test loss: 0.209958\n",
      "Epoch 146, test loss: 0.214739\n",
      "Epoch 147, test loss: 0.211419\n",
      "Epoch 148, test loss: 0.215360\n",
      "Epoch 149, test loss: 0.212332\n",
      "Epoch 150, test loss: 0.210849\n",
      "Epoch 151, test loss: 0.211202\n",
      "Epoch 152, test loss: 0.212920\n",
      "Epoch 153, test loss: 0.210522\n",
      "Epoch 154, test loss: 0.215010\n",
      "Epoch 155, test loss: 0.211254\n",
      "Epoch 156, test loss: 0.211217\n",
      "Epoch 157, test loss: 0.210284\n",
      "Epoch 158, test loss: 0.211895\n",
      "Epoch 159, test loss: 0.211422\n",
      "Epoch 160, test loss: 0.211556\n",
      "Epoch 161, test loss: 0.211951\n",
      "Epoch 162, test loss: 0.209945\n",
      "Epoch 163, test loss: 0.210344\n",
      "Epoch 164, test loss: 0.210116\n",
      "Epoch 165, test loss: 0.212518\n",
      "Epoch 166, test loss: 0.210260\n",
      "Epoch 167, test loss: 0.210619\n",
      "Epoch 168, test loss: 0.210436\n",
      "Epoch 169, test loss: 0.210858\n",
      "Epoch 170, test loss: 0.210213\n",
      "Epoch 171, test loss: 0.211213\n",
      "Epoch 172, test loss: 0.213997\n",
      "Epoch 173, test loss: 0.211331\n",
      "Epoch 174, test loss: 0.210127\n",
      "Epoch 175, test loss: 0.209834\n",
      "Epoch 176, test loss: 0.213478\n",
      "Epoch 177, test loss: 0.210567\n",
      "Epoch 178, test loss: 0.210110\n",
      "Epoch 179, test loss: 0.211453\n",
      "Epoch 180, test loss: 0.211382\n",
      "Epoch 181, test loss: 0.210751\n",
      "Epoch 182, test loss: 0.214422\n",
      "Epoch 183, test loss: 0.210737\n",
      "Epoch 184, test loss: 0.211999\n",
      "Epoch 185, test loss: 0.211537\n",
      "Epoch 186, test loss: 0.212108\n",
      "Epoch 187, test loss: 0.211276\n",
      "Epoch 188, test loss: 0.210931\n",
      "Epoch 189, test loss: 0.209898\n",
      "Epoch 190, test loss: 0.211046\n",
      "Epoch 191, test loss: 0.214445\n",
      "Epoch 192, test loss: 0.211221\n",
      "Epoch 193, test loss: 0.212309\n",
      "Epoch 194, test loss: 0.209917\n",
      "Epoch 195, test loss: 0.210835\n",
      "Epoch 196, test loss: 0.210725\n",
      "Epoch 197, test loss: 0.210957\n",
      "Epoch 198, test loss: 0.212879\n",
      "Epoch 199, test loss: 0.209909\n",
      "Epoch 200, test loss: 0.210058\n",
      "Epoch 201, test loss: 0.210219\n",
      "Epoch 202, test loss: 0.210126\n",
      "Epoch 203, test loss: 0.212587\n",
      "Epoch 204, test loss: 0.212281\n",
      "Epoch 205, test loss: 0.210407\n",
      "Epoch 206, test loss: 0.209726\n",
      "Epoch 207, test loss: 0.212337\n",
      "Epoch 208, test loss: 0.210166\n",
      "Epoch 209, test loss: 0.212410\n",
      "Epoch 210, test loss: 0.210336\n",
      "Epoch 211, test loss: 0.212945\n",
      "Epoch 212, test loss: 0.210284\n",
      "Epoch 213, test loss: 0.210025\n",
      "Epoch 214, test loss: 0.210828\n",
      "Epoch 215, test loss: 0.210919\n",
      "Epoch 216, test loss: 0.210028\n",
      "Epoch 217, test loss: 0.210713\n",
      "Epoch 218, test loss: 0.209599\n",
      "Epoch 219, test loss: 0.213299\n",
      "Epoch 220, test loss: 0.210649\n",
      "Epoch 221, test loss: 0.212332\n",
      "Epoch 222, test loss: 0.210215\n",
      "Epoch 223, test loss: 0.210129\n",
      "Epoch 224, test loss: 0.211440\n",
      "Epoch 225, test loss: 0.211319\n",
      "Epoch 226, test loss: 0.210727\n",
      "Epoch 227, test loss: 0.211318\n",
      "Epoch 228, test loss: 0.210157\n",
      "Epoch 229, test loss: 0.210208\n",
      "Epoch 230, test loss: 0.210388\n",
      "Epoch 231, test loss: 0.209951\n",
      "Epoch 232, test loss: 0.209826\n",
      "Epoch 233, test loss: 0.212617\n",
      "Epoch 234, test loss: 0.211631\n",
      "Epoch 235, test loss: 0.211429\n",
      "Epoch 236, test loss: 0.209725\n",
      "Epoch 237, test loss: 0.210668\n",
      "Epoch 238, test loss: 0.211110\n",
      "Epoch 239, test loss: 0.210042\n",
      "Pretrain data: 19786775.0\n",
      "Building dataset, requesting data from 0 to 665\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6955/108716\n",
      "Found 665 continuous time series\n",
      "Data shape: (115673, 6), Train/test: 115671/2\n",
      "Train test ratio: 57835.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1807\n",
      "Epoch 0, train loss: 0.227687\n",
      "Epoch 1, train loss: 0.247404\n",
      "Epoch 2, train loss: 0.177586\n",
      "Epoch 3, train loss: 0.309396\n",
      "Epoch 4, train loss: 0.293460\n",
      "Epoch 5, train loss: 0.184639\n",
      "Epoch 6, train loss: 0.154186\n",
      "Epoch 7, train loss: 0.182213\n",
      "Epoch 8, train loss: 0.163769\n",
      "Epoch 9, train loss: 0.239113\n",
      "Epoch 10, train loss: 0.218363\n",
      "Epoch 11, train loss: 0.245200\n",
      "Epoch 12, train loss: 0.276269\n",
      "Epoch 13, train loss: 0.196914\n",
      "Epoch 14, train loss: 0.152816\n",
      "Epoch 15, train loss: 0.230610\n",
      "Epoch 16, train loss: 0.207943\n",
      "Epoch 17, train loss: 0.184509\n",
      "Epoch 18, train loss: 0.283007\n",
      "Epoch 19, train loss: 0.222463\n",
      "Epoch 20, train loss: 0.207586\n",
      "Epoch 21, train loss: 0.159829\n",
      "Epoch 22, train loss: 0.237902\n",
      "Epoch 23, train loss: 0.199627\n",
      "Epoch 24, train loss: 0.177487\n",
      "Epoch 25, train loss: 0.173914\n",
      "Epoch 26, train loss: 0.222609\n",
      "Epoch 27, train loss: 0.173346\n",
      "Epoch 28, train loss: 0.278194\n",
      "Epoch 29, train loss: 0.265688\n",
      "Epoch 30, train loss: 0.230789\n",
      "Epoch 31, train loss: 0.176411\n",
      "Epoch 32, train loss: 0.196525\n",
      "Epoch 33, train loss: 0.229042\n",
      "Epoch 34, train loss: 0.205187\n",
      "Epoch 35, train loss: 0.155746\n",
      "Epoch 36, train loss: 0.168949\n",
      "Epoch 37, train loss: 0.221669\n",
      "Epoch 38, train loss: 0.180794\n",
      "Epoch 39, train loss: 0.145666\n",
      "Epoch 40, train loss: 0.187796\n",
      "Epoch 41, train loss: 0.282454\n",
      "Epoch 42, train loss: 0.216964\n",
      "Epoch 43, train loss: 0.220405\n",
      "Epoch 44, train loss: 0.206569\n",
      "Epoch 45, train loss: 0.192350\n",
      "Epoch 46, train loss: 0.225804\n",
      "Epoch 47, train loss: 0.225491\n",
      "Epoch 48, train loss: 0.234752\n",
      "Epoch 49, train loss: 0.190238\n",
      "Epoch 50, train loss: 0.187810\n",
      "Epoch 51, train loss: 0.182919\n",
      "Epoch 52, train loss: 0.195601\n",
      "Epoch 53, train loss: 0.228030\n",
      "Epoch 54, train loss: 0.237342\n",
      "Epoch 55, train loss: 0.199987\n",
      "Epoch 56, train loss: 0.189096\n",
      "Epoch 57, train loss: 0.201596\n",
      "Epoch 58, train loss: 0.195228\n",
      "Epoch 59, train loss: 0.165608\n",
      "Epoch 60, train loss: 0.191114\n",
      "Epoch 61, train loss: 0.208789\n",
      "Epoch 62, train loss: 0.185176\n",
      "Epoch 63, train loss: 0.241522\n",
      "Epoch 64, train loss: 0.218786\n",
      "Epoch 65, train loss: 0.211510\n",
      "Epoch 66, train loss: 0.236948\n",
      "Epoch 67, train loss: 0.212325\n",
      "Epoch 68, train loss: 0.219000\n",
      "Epoch 69, train loss: 0.190900\n",
      "Epoch 70, train loss: 0.198496\n",
      "Epoch 71, train loss: 0.183849\n",
      "Epoch 72, train loss: 0.157433\n",
      "Epoch 73, train loss: 0.187881\n",
      "Epoch 74, train loss: 0.197795\n",
      "Epoch 75, train loss: 0.182123\n",
      "Epoch 76, train loss: 0.231855\n",
      "Epoch 77, train loss: 0.178686\n",
      "Epoch 78, train loss: 0.169749\n",
      "Epoch 79, train loss: 0.204026\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "x here is\n",
      "[[254. 250. 249. 247. 242. 235.]\n",
      " [250. 249. 247. 242. 235. 229.]\n",
      " [249. 247. 242. 235. 229. 224.]\n",
      " ...\n",
      " [118. 122. 130. 143. 159. 177.]\n",
      " [122. 130. 143. 159. 177. 181.]\n",
      " [130. 143. 159. 177. 181. 205.]]\n",
      "y here is\n",
      "[[212. 212. 212. 212. 212. 212.]\n",
      " [209. 209. 209. 209. 209. 209.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " ...\n",
      " [254. 254. 254. 254. 254. 254.]\n",
      " [248. 248. 248. 248. 248. 248.]\n",
      " [242. 242. 242. 242. 242. 242.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 45 continuous time series\n",
      "Data shape: (2401, 6), Train/test: 1/2400\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "x here is\n",
      "[[ 76.  72.  68.  65.  63.  66.]\n",
      " [ 72.  68.  65.  63.  66.  71.]\n",
      " [ 68.  65.  63.  66.  71.  78.]\n",
      " ...\n",
      " [266. 263. 259. 254. 250. 254.]\n",
      " [263. 259. 254. 250. 254. 261.]\n",
      " [259. 254. 250. 254. 261. 267.]]\n",
      "y here is\n",
      "[[121. 121. 121. 121. 121. 121.]\n",
      " [131. 131. 131. 131. 131. 131.]\n",
      " [137. 137. 137. 137. 137. 137.]\n",
      " ...\n",
      " [258. 258. 258. 258. 258. 258.]\n",
      " [257. 257. 257. 257. 257. 257.]\n",
      " [255. 255. 255. 255. 255. 255.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1330/8723\n",
      "Found 177 continuous time series\n",
      "Data shape: (10055, 6), Train/test: 10053/2\n",
      "Train test ratio: 5026.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293BC54B0>\n",
      "Epoch 0, test loss: 0.216725\n",
      "Epoch 1, test loss: 0.215311\n",
      "Epoch 2, test loss: 0.216981\n",
      "Epoch 3, test loss: 0.217515\n",
      "Epoch 4, test loss: 0.215010\n",
      "Epoch 5, test loss: 0.216038\n",
      "Epoch 6, test loss: 0.220579\n",
      "Epoch 7, test loss: 0.215286\n",
      "Epoch 8, test loss: 0.216818\n",
      "Epoch 9, test loss: 0.217037\n",
      "Epoch 10, test loss: 0.222576\n",
      "Epoch 11, test loss: 0.217132\n",
      "Epoch 12, test loss: 0.217439\n",
      "Epoch 13, test loss: 0.215042\n",
      "Epoch 14, test loss: 0.216226\n",
      "Epoch 15, test loss: 0.215740\n",
      "Epoch 16, test loss: 0.216352\n",
      "Epoch 17, test loss: 0.215496\n",
      "Epoch 18, test loss: 0.222868\n",
      "Epoch 19, test loss: 0.217928\n",
      "Epoch 20, test loss: 0.215577\n",
      "Epoch 21, test loss: 0.215515\n",
      "Epoch 22, test loss: 0.216075\n",
      "Epoch 23, test loss: 0.215235\n",
      "Epoch 24, test loss: 0.216353\n",
      "Epoch 25, test loss: 0.215077\n",
      "Epoch 26, test loss: 0.215055\n",
      "Epoch 27, test loss: 0.214998\n",
      "Epoch 28, test loss: 0.215463\n",
      "Epoch 29, test loss: 0.218286\n",
      "Epoch 30, test loss: 0.227067\n",
      "Epoch 31, test loss: 0.217324\n",
      "Epoch 32, test loss: 0.216359\n",
      "Epoch 33, test loss: 0.217260\n",
      "Epoch 34, test loss: 0.216186\n",
      "Epoch 35, test loss: 0.215670\n",
      "Epoch 36, test loss: 0.222571\n",
      "Epoch 37, test loss: 0.224287\n",
      "Epoch 38, test loss: 0.217723\n",
      "Epoch 39, test loss: 0.216423\n",
      "Epoch 40, test loss: 0.215142\n",
      "Epoch 41, test loss: 0.218134\n",
      "Epoch 42, test loss: 0.232902\n",
      "Epoch 43, test loss: 0.218009\n",
      "Epoch 44, test loss: 0.215547\n",
      "Epoch 45, test loss: 0.217246\n",
      "Epoch 46, test loss: 0.217489\n",
      "Epoch 47, test loss: 0.228407\n",
      "Epoch 48, test loss: 0.216174\n",
      "Epoch 49, test loss: 0.216261\n",
      "Epoch 50, test loss: 0.220450\n",
      "Epoch 51, test loss: 0.215737\n",
      "Epoch 52, test loss: 0.215034\n",
      "Epoch 53, test loss: 0.216456\n",
      "Epoch 54, test loss: 0.215197\n",
      "Epoch 55, test loss: 0.215234\n",
      "Epoch 56, test loss: 0.215359\n",
      "Epoch 57, test loss: 0.215899\n",
      "Epoch 58, test loss: 0.217829\n",
      "Epoch 59, test loss: 0.218509\n",
      "Epoch 60, test loss: 0.215522\n",
      "Epoch 61, test loss: 0.215020\n",
      "Epoch 62, test loss: 0.215625\n",
      "Epoch 63, test loss: 0.215568\n",
      "Epoch 64, test loss: 0.215160\n",
      "Epoch 65, test loss: 0.227272\n",
      "Epoch 66, test loss: 0.215452\n",
      "Epoch 67, test loss: 0.215661\n",
      "Epoch 68, test loss: 0.215717\n",
      "Epoch 69, test loss: 0.218788\n",
      "Epoch 70, test loss: 0.217970\n",
      "Epoch 71, test loss: 0.218755\n",
      "Epoch 72, test loss: 0.217814\n",
      "Epoch 73, test loss: 0.221644\n",
      "Epoch 74, test loss: 0.215863\n",
      "Epoch 75, test loss: 0.222757\n",
      "Epoch 76, test loss: 0.215412\n",
      "Epoch 77, test loss: 0.215294\n",
      "Epoch 78, test loss: 0.217421\n",
      "Epoch 79, test loss: 0.215215\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293BC54B0>\n",
      "Epoch 0, test loss: 0.219288\n",
      "Epoch 1, test loss: 0.217539\n",
      "Epoch 2, test loss: 0.221749\n",
      "Epoch 3, test loss: 0.216179\n",
      "Epoch 4, test loss: 0.215390\n",
      "Epoch 5, test loss: 0.215403\n",
      "Epoch 6, test loss: 0.219049\n",
      "Epoch 7, test loss: 0.218101\n",
      "Epoch 8, test loss: 0.215767\n",
      "Epoch 9, test loss: 0.217935\n",
      "Epoch 10, test loss: 0.218608\n",
      "Epoch 11, test loss: 0.216409\n",
      "Epoch 12, test loss: 0.217000\n",
      "Epoch 13, test loss: 0.217717\n",
      "Epoch 14, test loss: 0.217018\n",
      "Epoch 15, test loss: 0.217901\n",
      "Epoch 16, test loss: 0.215553\n",
      "Epoch 17, test loss: 0.215778\n",
      "Epoch 18, test loss: 0.217953\n",
      "Epoch 19, test loss: 0.217013\n",
      "Epoch 20, test loss: 0.215703\n",
      "Epoch 21, test loss: 0.215264\n",
      "Epoch 22, test loss: 0.216349\n",
      "Epoch 23, test loss: 0.214743\n",
      "Epoch 24, test loss: 0.218356\n",
      "Epoch 25, test loss: 0.218588\n",
      "Epoch 26, test loss: 0.215064\n",
      "Epoch 27, test loss: 0.215828\n",
      "Epoch 28, test loss: 0.215288\n",
      "Epoch 29, test loss: 0.215620\n",
      "Epoch 30, test loss: 0.218881\n",
      "Epoch 31, test loss: 0.218497\n",
      "Epoch 32, test loss: 0.217407\n",
      "Epoch 33, test loss: 0.216390\n",
      "Epoch 34, test loss: 0.214811\n",
      "Epoch 35, test loss: 0.216141\n",
      "Epoch 36, test loss: 0.215036\n",
      "Epoch 37, test loss: 0.215319\n",
      "Epoch 38, test loss: 0.224335\n",
      "Epoch 39, test loss: 0.216323\n",
      "Epoch 40, test loss: 0.215323\n",
      "Epoch 41, test loss: 0.215288\n",
      "Epoch 42, test loss: 0.217118\n",
      "Epoch 43, test loss: 0.218557\n",
      "Epoch 44, test loss: 0.214699\n",
      "Epoch 45, test loss: 0.215459\n",
      "Epoch 46, test loss: 0.215326\n",
      "Epoch 47, test loss: 0.214678\n",
      "Epoch 48, test loss: 0.216273\n",
      "Epoch 49, test loss: 0.216575\n",
      "Epoch 50, test loss: 0.217870\n",
      "Epoch 51, test loss: 0.216008\n",
      "Epoch 52, test loss: 0.217633\n",
      "Epoch 53, test loss: 0.216030\n",
      "Epoch 54, test loss: 0.216322\n",
      "Epoch 55, test loss: 0.215009\n",
      "Epoch 56, test loss: 0.216600\n",
      "Epoch 57, test loss: 0.215830\n",
      "Epoch 58, test loss: 0.215362\n",
      "Epoch 59, test loss: 0.214967\n",
      "Epoch 60, test loss: 0.215600\n",
      "Epoch 61, test loss: 0.216102\n",
      "Epoch 62, test loss: 0.215325\n",
      "Epoch 63, test loss: 0.217288\n",
      "Epoch 64, test loss: 0.215046\n",
      "Epoch 65, test loss: 0.215599\n",
      "Epoch 66, test loss: 0.215220\n",
      "Epoch 67, test loss: 0.216704\n",
      "Epoch 68, test loss: 0.215063\n",
      "Epoch 69, test loss: 0.216063\n",
      "Epoch 70, test loss: 0.217030\n",
      "Epoch 71, test loss: 0.215728\n",
      "Epoch 72, test loss: 0.219376\n",
      "Epoch 73, test loss: 0.216137\n",
      "Epoch 74, test loss: 0.216475\n",
      "Epoch 75, test loss: 0.214696\n",
      "Epoch 76, test loss: 0.217975\n",
      "Epoch 77, test loss: 0.218749\n",
      "Epoch 78, test loss: 0.217341\n",
      "Epoch 79, test loss: 0.215355\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293BC54B0>\n",
      "Epoch 0, test loss: 0.269273\n",
      "Epoch 1, test loss: 0.247369\n",
      "Epoch 2, test loss: 0.227882\n",
      "Epoch 3, test loss: 0.223429\n",
      "Epoch 4, test loss: 0.221227\n",
      "Epoch 5, test loss: 0.220905\n",
      "Epoch 6, test loss: 0.223982\n",
      "Epoch 7, test loss: 0.224824\n",
      "Epoch 8, test loss: 0.220816\n",
      "Epoch 9, test loss: 0.219149\n",
      "Epoch 10, test loss: 0.224340\n",
      "Epoch 11, test loss: 0.218666\n",
      "Epoch 12, test loss: 0.217997\n",
      "Epoch 13, test loss: 0.219716\n",
      "Epoch 14, test loss: 0.223790\n",
      "Epoch 15, test loss: 0.218301\n",
      "Epoch 16, test loss: 0.218345\n",
      "Epoch 17, test loss: 0.219695\n",
      "Epoch 18, test loss: 0.218119\n",
      "Epoch 19, test loss: 0.217730\n",
      "Epoch 20, test loss: 0.223772\n",
      "Epoch 21, test loss: 0.223159\n",
      "Epoch 22, test loss: 0.217424\n",
      "Epoch 23, test loss: 0.218855\n",
      "Epoch 24, test loss: 0.218641\n",
      "Epoch 25, test loss: 0.217675\n",
      "Epoch 26, test loss: 0.216943\n",
      "Epoch 27, test loss: 0.219918\n",
      "Epoch 28, test loss: 0.216988\n",
      "Epoch 29, test loss: 0.217756\n",
      "Epoch 30, test loss: 0.219319\n",
      "Epoch 31, test loss: 0.216957\n",
      "Epoch 32, test loss: 0.221413\n",
      "Epoch 33, test loss: 0.217277\n",
      "Epoch 34, test loss: 0.217177\n",
      "Epoch 35, test loss: 0.216863\n",
      "Epoch 36, test loss: 0.218184\n",
      "Epoch 37, test loss: 0.217163\n",
      "Epoch 38, test loss: 0.221234\n",
      "Epoch 39, test loss: 0.217983\n",
      "Epoch 40, test loss: 0.217320\n",
      "Epoch 41, test loss: 0.223244\n",
      "Epoch 42, test loss: 0.217563\n",
      "Epoch 43, test loss: 0.217173\n",
      "Epoch 44, test loss: 0.216964\n",
      "Epoch 45, test loss: 0.217488\n",
      "Epoch 46, test loss: 0.223742\n",
      "Epoch 47, test loss: 0.218591\n",
      "Epoch 48, test loss: 0.219113\n",
      "Epoch 49, test loss: 0.216712\n",
      "Epoch 50, test loss: 0.220215\n",
      "Epoch 51, test loss: 0.216239\n",
      "Epoch 52, test loss: 0.218824\n",
      "Epoch 53, test loss: 0.215985\n",
      "Epoch 54, test loss: 0.218837\n",
      "Epoch 55, test loss: 0.217521\n",
      "Epoch 56, test loss: 0.216637\n",
      "Epoch 57, test loss: 0.217185\n",
      "Epoch 58, test loss: 0.216996\n",
      "Epoch 59, test loss: 0.217361\n",
      "Epoch 60, test loss: 0.216804\n",
      "Epoch 61, test loss: 0.217801\n",
      "Epoch 62, test loss: 0.218236\n",
      "Epoch 63, test loss: 0.216905\n",
      "Epoch 64, test loss: 0.217131\n",
      "Epoch 65, test loss: 0.216853\n",
      "Epoch 66, test loss: 0.217928\n",
      "Epoch 67, test loss: 0.220203\n",
      "Epoch 68, test loss: 0.216199\n",
      "Epoch 69, test loss: 0.217844\n",
      "Epoch 70, test loss: 0.216530\n",
      "Epoch 71, test loss: 0.216369\n",
      "Epoch 72, test loss: 0.216138\n",
      "Epoch 73, test loss: 0.224031\n",
      "Epoch 74, test loss: 0.220012\n",
      "Epoch 75, test loss: 0.216312\n",
      "Epoch 76, test loss: 0.218057\n",
      "Epoch 77, test loss: 0.219994\n",
      "Epoch 78, test loss: 0.215879\n",
      "Epoch 79, test loss: 0.216972\n",
      "Epoch 80, test loss: 0.221883\n",
      "Epoch 81, test loss: 0.219790\n",
      "Epoch 82, test loss: 0.216668\n",
      "Epoch 83, test loss: 0.219259\n",
      "Epoch 84, test loss: 0.216293\n",
      "Epoch 85, test loss: 0.217059\n",
      "Epoch 86, test loss: 0.216759\n",
      "Epoch 87, test loss: 0.215960\n",
      "Epoch 88, test loss: 0.216464\n",
      "Epoch 89, test loss: 0.220216\n",
      "Epoch 90, test loss: 0.218911\n",
      "Epoch 91, test loss: 0.217753\n",
      "Epoch 92, test loss: 0.216799\n",
      "Epoch 93, test loss: 0.217059\n",
      "Epoch 94, test loss: 0.218111\n",
      "Epoch 95, test loss: 0.217789\n",
      "Epoch 96, test loss: 0.219608\n",
      "Epoch 97, test loss: 0.217657\n",
      "Epoch 98, test loss: 0.215905\n",
      "Epoch 99, test loss: 0.217903\n",
      "Epoch 100, test loss: 0.219198\n",
      "Epoch 101, test loss: 0.216261\n",
      "Epoch 102, test loss: 0.217141\n",
      "Epoch 103, test loss: 0.215767\n",
      "Epoch 104, test loss: 0.216049\n",
      "Epoch 105, test loss: 0.216053\n",
      "Epoch 106, test loss: 0.216282\n",
      "Epoch 107, test loss: 0.216004\n",
      "Epoch 108, test loss: 0.215740\n",
      "Epoch 109, test loss: 0.217073\n",
      "Epoch 110, test loss: 0.219652\n",
      "Epoch 111, test loss: 0.215262\n",
      "Epoch 112, test loss: 0.216260\n",
      "Epoch 113, test loss: 0.216527\n",
      "Epoch 114, test loss: 0.216814\n",
      "Epoch 115, test loss: 0.216426\n",
      "Epoch 116, test loss: 0.216149\n",
      "Epoch 117, test loss: 0.224478\n",
      "Epoch 118, test loss: 0.218401\n",
      "Epoch 119, test loss: 0.214765\n",
      "Epoch 120, test loss: 0.223924\n",
      "Epoch 121, test loss: 0.216316\n",
      "Epoch 122, test loss: 0.217783\n",
      "Epoch 123, test loss: 0.218427\n",
      "Epoch 124, test loss: 0.223963\n",
      "Epoch 125, test loss: 0.215717\n",
      "Epoch 126, test loss: 0.217179\n",
      "Epoch 127, test loss: 0.223907\n",
      "Epoch 128, test loss: 0.218094\n",
      "Epoch 129, test loss: 0.216212\n",
      "Epoch 130, test loss: 0.221297\n",
      "Epoch 131, test loss: 0.217246\n",
      "Epoch 132, test loss: 0.218721\n",
      "Epoch 133, test loss: 0.215433\n",
      "Epoch 134, test loss: 0.215812\n",
      "Epoch 135, test loss: 0.219951\n",
      "Epoch 136, test loss: 0.216272\n",
      "Epoch 137, test loss: 0.216394\n",
      "Epoch 138, test loss: 0.216043\n",
      "Epoch 139, test loss: 0.221329\n",
      "Epoch 140, test loss: 0.217005\n",
      "Epoch 141, test loss: 0.217578\n",
      "Epoch 142, test loss: 0.215421\n",
      "Epoch 143, test loss: 0.216632\n",
      "Epoch 144, test loss: 0.216482\n",
      "Epoch 145, test loss: 0.219673\n",
      "Epoch 146, test loss: 0.217921\n",
      "Epoch 147, test loss: 0.216262\n",
      "Epoch 148, test loss: 0.219291\n",
      "Epoch 149, test loss: 0.215691\n",
      "Epoch 150, test loss: 0.217014\n",
      "Epoch 151, test loss: 0.215756\n",
      "Epoch 152, test loss: 0.215554\n",
      "Epoch 153, test loss: 0.218842\n",
      "Epoch 154, test loss: 0.215411\n",
      "Epoch 155, test loss: 0.216879\n",
      "Epoch 156, test loss: 0.216955\n",
      "Epoch 157, test loss: 0.214927\n",
      "Epoch 158, test loss: 0.215208\n",
      "Epoch 159, test loss: 0.216258\n",
      "Epoch 160, test loss: 0.218789\n",
      "Epoch 161, test loss: 0.215424\n",
      "Epoch 162, test loss: 0.219935\n",
      "Epoch 163, test loss: 0.219537\n",
      "Epoch 164, test loss: 0.215935\n",
      "Epoch 165, test loss: 0.216420\n",
      "Epoch 166, test loss: 0.218418\n",
      "Epoch 167, test loss: 0.215476\n",
      "Epoch 168, test loss: 0.215184\n",
      "Epoch 169, test loss: 0.215364\n",
      "Epoch 170, test loss: 0.216369\n",
      "Epoch 171, test loss: 0.216792\n",
      "Epoch 172, test loss: 0.216651\n",
      "Epoch 173, test loss: 0.217650\n",
      "Epoch 174, test loss: 0.215471\n",
      "Epoch 175, test loss: 0.215312\n",
      "Epoch 176, test loss: 0.215879\n",
      "Epoch 177, test loss: 0.219438\n",
      "Epoch 178, test loss: 0.217470\n",
      "Epoch 179, test loss: 0.215451\n",
      "Epoch 180, test loss: 0.219330\n",
      "Epoch 181, test loss: 0.214928\n",
      "Epoch 182, test loss: 0.219136\n",
      "Epoch 183, test loss: 0.217361\n",
      "Epoch 184, test loss: 0.215401\n",
      "Epoch 185, test loss: 0.215439\n",
      "Epoch 186, test loss: 0.216607\n",
      "Epoch 187, test loss: 0.216470\n",
      "Epoch 188, test loss: 0.215659\n",
      "Epoch 189, test loss: 0.217982\n",
      "Epoch 190, test loss: 0.216026\n",
      "Epoch 191, test loss: 0.215397\n",
      "Epoch 192, test loss: 0.215071\n",
      "Epoch 193, test loss: 0.216646\n",
      "Epoch 194, test loss: 0.218350\n",
      "Epoch 195, test loss: 0.216771\n",
      "Epoch 196, test loss: 0.225475\n",
      "Epoch 197, test loss: 0.214970\n",
      "Epoch 198, test loss: 0.215085\n",
      "Epoch 199, test loss: 0.219355\n",
      "Epoch 200, test loss: 0.217914\n",
      "Epoch 201, test loss: 0.219571\n",
      "Epoch 202, test loss: 0.215628\n",
      "Epoch 203, test loss: 0.222939\n",
      "Epoch 204, test loss: 0.216551\n",
      "Epoch 205, test loss: 0.216340\n",
      "Epoch 206, test loss: 0.217792\n",
      "Epoch 207, test loss: 0.220289\n",
      "Epoch 208, test loss: 0.215754\n",
      "Epoch 209, test loss: 0.218233\n",
      "Epoch 210, test loss: 0.214976\n",
      "Epoch 211, test loss: 0.215253\n",
      "Epoch 212, test loss: 0.216128\n",
      "Epoch 213, test loss: 0.222493\n",
      "Epoch 214, test loss: 0.215724\n",
      "Epoch 215, test loss: 0.216129\n",
      "Epoch 216, test loss: 0.216389\n",
      "Epoch 217, test loss: 0.217066\n",
      "Epoch 218, test loss: 0.215385\n",
      "Epoch 219, test loss: 0.217399\n",
      "Epoch 220, test loss: 0.214729\n",
      "Epoch 221, test loss: 0.215695\n",
      "Epoch 222, test loss: 0.215169\n",
      "Epoch 223, test loss: 0.215273\n",
      "Epoch 224, test loss: 0.217123\n",
      "Epoch 225, test loss: 0.215325\n",
      "Epoch 226, test loss: 0.215894\n",
      "Epoch 227, test loss: 0.215251\n",
      "Epoch 228, test loss: 0.215295\n",
      "Epoch 229, test loss: 0.214298\n",
      "Epoch 230, test loss: 0.215221\n",
      "Epoch 231, test loss: 0.215292\n",
      "Epoch 232, test loss: 0.219957\n",
      "Epoch 233, test loss: 0.215430\n",
      "Epoch 234, test loss: 0.216778\n",
      "Epoch 235, test loss: 0.215512\n",
      "Epoch 236, test loss: 0.221207\n",
      "Epoch 237, test loss: 0.215442\n",
      "Epoch 238, test loss: 0.216822\n",
      "Epoch 239, test loss: 0.214705\n",
      "Pretrain data: 20092707.0\n",
      "Building dataset, requesting data from 0 to 672\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7734/110770\n",
      "Found 672 continuous time series\n",
      "Data shape: (118506, 6), Train/test: 118504/2\n",
      "Train test ratio: 59252.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1851\n",
      "Epoch 0, train loss: 0.248939\n",
      "Epoch 1, train loss: 0.286904\n",
      "Epoch 2, train loss: 0.218257\n",
      "Epoch 3, train loss: 0.230975\n",
      "Epoch 4, train loss: 0.184232\n",
      "Epoch 5, train loss: 0.191911\n",
      "Epoch 6, train loss: 0.179937\n",
      "Epoch 7, train loss: 0.220885\n",
      "Epoch 8, train loss: 0.191128\n",
      "Epoch 9, train loss: 0.181924\n",
      "Epoch 10, train loss: 0.174070\n",
      "Epoch 11, train loss: 0.217312\n",
      "Epoch 12, train loss: 0.225015\n",
      "Epoch 13, train loss: 0.171755\n",
      "Epoch 14, train loss: 0.195742\n",
      "Epoch 15, train loss: 0.206581\n",
      "Epoch 16, train loss: 0.226057\n",
      "Epoch 17, train loss: 0.211629\n",
      "Epoch 18, train loss: 0.189260\n",
      "Epoch 19, train loss: 0.165395\n",
      "Epoch 20, train loss: 0.209835\n",
      "Epoch 21, train loss: 0.175346\n",
      "Epoch 22, train loss: 0.211654\n",
      "Epoch 23, train loss: 0.208190\n",
      "Epoch 24, train loss: 0.238976\n",
      "Epoch 25, train loss: 0.189843\n",
      "Epoch 26, train loss: 0.233976\n",
      "Epoch 27, train loss: 0.146450\n",
      "Epoch 28, train loss: 0.210516\n",
      "Epoch 29, train loss: 0.194146\n",
      "Epoch 30, train loss: 0.207572\n",
      "Epoch 31, train loss: 0.200252\n",
      "Epoch 32, train loss: 0.247354\n",
      "Epoch 33, train loss: 0.210493\n",
      "Epoch 34, train loss: 0.249188\n",
      "Epoch 35, train loss: 0.210528\n",
      "Epoch 36, train loss: 0.193615\n",
      "Epoch 37, train loss: 0.273673\n",
      "Epoch 38, train loss: 0.186698\n",
      "Epoch 39, train loss: 0.183155\n",
      "Epoch 40, train loss: 0.193164\n",
      "Epoch 41, train loss: 0.198868\n",
      "Epoch 42, train loss: 0.190575\n",
      "Epoch 43, train loss: 0.224757\n",
      "Epoch 44, train loss: 0.182334\n",
      "Epoch 45, train loss: 0.249509\n",
      "Epoch 46, train loss: 0.169802\n",
      "Epoch 47, train loss: 0.239524\n",
      "Epoch 48, train loss: 0.190146\n",
      "Epoch 49, train loss: 0.181329\n",
      "Epoch 50, train loss: 0.207915\n",
      "Epoch 51, train loss: 0.225331\n",
      "Epoch 52, train loss: 0.176231\n",
      "Epoch 53, train loss: 0.204717\n",
      "Epoch 54, train loss: 0.202787\n",
      "Epoch 55, train loss: 0.296633\n",
      "Epoch 56, train loss: 0.263846\n",
      "Epoch 57, train loss: 0.172612\n",
      "Epoch 58, train loss: 0.182161\n",
      "Epoch 59, train loss: 0.208772\n",
      "Epoch 60, train loss: 0.158523\n",
      "Epoch 61, train loss: 0.242364\n",
      "Epoch 62, train loss: 0.171580\n",
      "Epoch 63, train loss: 0.200418\n",
      "Epoch 64, train loss: 0.215895\n",
      "Epoch 65, train loss: 0.154718\n",
      "Epoch 66, train loss: 0.186251\n",
      "Epoch 67, train loss: 0.215845\n",
      "Epoch 68, train loss: 0.208737\n",
      "Epoch 69, train loss: 0.220194\n",
      "Epoch 70, train loss: 0.208856\n",
      "Epoch 71, train loss: 0.221195\n",
      "Epoch 72, train loss: 0.211488\n",
      "Epoch 73, train loss: 0.211663\n",
      "Epoch 74, train loss: 0.188836\n",
      "Epoch 75, train loss: 0.314391\n",
      "Epoch 76, train loss: 0.201483\n",
      "Epoch 77, train loss: 0.193278\n",
      "Epoch 78, train loss: 0.217183\n",
      "Epoch 79, train loss: 0.251357\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[181. 180. 178. 178. 176. 173.]\n",
      " [180. 178. 178. 176. 173. 168.]\n",
      " [178. 178. 176. 173. 168. 163.]\n",
      " ...\n",
      " [293. 289. 287. 283. 276. 269.]\n",
      " [289. 287. 283. 276. 269. 261.]\n",
      " [287. 283. 276. 269. 261. 252.]]\n",
      "y here is\n",
      "[[154. 154. 154. 154. 154. 154.]\n",
      " [153. 153. 153. 153. 153. 153.]\n",
      " [151. 151. 151. 151. 151. 151.]\n",
      " ...\n",
      " [229. 229. 229. 229. 229. 229.]\n",
      " [224. 224. 224. 224. 224. 224.]\n",
      " [215. 215. 215. 215. 215. 215.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1891, 6), Train/test: 1/1890\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 170 segments\n",
      "Building dataset, requesting data from 0 to 170\n",
      "x here is\n",
      "[[ 95.  86.  81.  81.  82.  82.]\n",
      " [ 86.  81.  81.  82.  82.  84.]\n",
      " [ 81.  81.  82.  82.  84.  88.]\n",
      " ...\n",
      " [277. 261. 245. 232. 222. 221.]\n",
      " [261. 245. 232. 222. 221. 222.]\n",
      " [245. 232. 222. 221. 222. 223.]]\n",
      "y here is\n",
      "[[ 97.  97.  97.  97.  97.  97.]\n",
      " [ 97.  97.  97.  97.  97.  97.]\n",
      " [ 95.  95.  95.  95.  95.  95.]\n",
      " ...\n",
      " [225. 225. 225. 225. 225. 225.]\n",
      " [227. 227. 227. 227. 227. 227.]\n",
      " [226. 226. 226. 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 551/6669\n",
      "Found 170 continuous time series\n",
      "Data shape: (7222, 6), Train/test: 7220/2\n",
      "Train test ratio: 3610.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A296749C00>\n",
      "Epoch 0, test loss: 0.166752\n",
      "Epoch 1, test loss: 0.168037\n",
      "Epoch 2, test loss: 0.169906\n",
      "Epoch 3, test loss: 0.166164\n",
      "Epoch 4, test loss: 0.165400\n",
      "Epoch 5, test loss: 0.177816\n",
      "Epoch 6, test loss: 0.164870\n",
      "Epoch 7, test loss: 0.165272\n",
      "Epoch 8, test loss: 0.165869\n",
      "Epoch 9, test loss: 0.165785\n",
      "Epoch 10, test loss: 0.167639\n",
      "Epoch 11, test loss: 0.166084\n",
      "Epoch 12, test loss: 0.165113\n",
      "Epoch 13, test loss: 0.165519\n",
      "Epoch 14, test loss: 0.177459\n",
      "Epoch 15, test loss: 0.165970\n",
      "Epoch 16, test loss: 0.165752\n",
      "Epoch 17, test loss: 0.165801\n",
      "Epoch 18, test loss: 0.165544\n",
      "Epoch 19, test loss: 0.165829\n",
      "Epoch 20, test loss: 0.165548\n",
      "Epoch 21, test loss: 0.166646\n",
      "Epoch 22, test loss: 0.165533\n",
      "Epoch 23, test loss: 0.165460\n",
      "Epoch 24, test loss: 0.166200\n",
      "Epoch 25, test loss: 0.166268\n",
      "Epoch 26, test loss: 0.166673\n",
      "Epoch 27, test loss: 0.165542\n",
      "Epoch 28, test loss: 0.166272\n",
      "Epoch 29, test loss: 0.167315\n",
      "Epoch 30, test loss: 0.166387\n",
      "Epoch 31, test loss: 0.165758\n",
      "Epoch 32, test loss: 0.166298\n",
      "Epoch 33, test loss: 0.167104\n",
      "Epoch 34, test loss: 0.167794\n",
      "Epoch 35, test loss: 0.165666\n",
      "Epoch 36, test loss: 0.167492\n",
      "Epoch 37, test loss: 0.166130\n",
      "Epoch 38, test loss: 0.165717\n",
      "Epoch 39, test loss: 0.166772\n",
      "Epoch 40, test loss: 0.165726\n",
      "Epoch 41, test loss: 0.173230\n",
      "Epoch 42, test loss: 0.166821\n",
      "Epoch 43, test loss: 0.165659\n",
      "Epoch 44, test loss: 0.165362\n",
      "Epoch 45, test loss: 0.166343\n",
      "Epoch 46, test loss: 0.166088\n",
      "Epoch 47, test loss: 0.165653\n",
      "Epoch 48, test loss: 0.166136\n",
      "Epoch 49, test loss: 0.166629\n",
      "Epoch 50, test loss: 0.165265\n",
      "Epoch 51, test loss: 0.165755\n",
      "Epoch 52, test loss: 0.170400\n",
      "Epoch 53, test loss: 0.168093\n",
      "Epoch 54, test loss: 0.165870\n",
      "Epoch 55, test loss: 0.165824\n",
      "Epoch 56, test loss: 0.165583\n",
      "Epoch 57, test loss: 0.167838\n",
      "Epoch 58, test loss: 0.165490\n",
      "Epoch 59, test loss: 0.166677\n",
      "Epoch 60, test loss: 0.168840\n",
      "Epoch 61, test loss: 0.168054\n",
      "Epoch 62, test loss: 0.165671\n",
      "Epoch 63, test loss: 0.165700\n",
      "Epoch 64, test loss: 0.165560\n",
      "Epoch 65, test loss: 0.165895\n",
      "Epoch 66, test loss: 0.165633\n",
      "Epoch 67, test loss: 0.166737\n",
      "Epoch 68, test loss: 0.166685\n",
      "Epoch 69, test loss: 0.165874\n",
      "Epoch 70, test loss: 0.166517\n",
      "Epoch 71, test loss: 0.167520\n",
      "Epoch 72, test loss: 0.166133\n",
      "Epoch 73, test loss: 0.166953\n",
      "Epoch 74, test loss: 0.165816\n",
      "Epoch 75, test loss: 0.165759\n",
      "Epoch 76, test loss: 0.166974\n",
      "Epoch 77, test loss: 0.165839\n",
      "Epoch 78, test loss: 0.166912\n",
      "Epoch 79, test loss: 0.165648\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A296749C00>\n",
      "Epoch 0, test loss: 0.164518\n",
      "Epoch 1, test loss: 0.165052\n",
      "Epoch 2, test loss: 0.166815\n",
      "Epoch 3, test loss: 0.165025\n",
      "Epoch 4, test loss: 0.165799\n",
      "Epoch 5, test loss: 0.165611\n",
      "Epoch 6, test loss: 0.165209\n",
      "Epoch 7, test loss: 0.165401\n",
      "Epoch 8, test loss: 0.166924\n",
      "Epoch 9, test loss: 0.165731\n",
      "Epoch 10, test loss: 0.164989\n",
      "Epoch 11, test loss: 0.165418\n",
      "Epoch 12, test loss: 0.165469\n",
      "Epoch 13, test loss: 0.167423\n",
      "Epoch 14, test loss: 0.165514\n",
      "Epoch 15, test loss: 0.166094\n",
      "Epoch 16, test loss: 0.165092\n",
      "Epoch 17, test loss: 0.166042\n",
      "Epoch 18, test loss: 0.167145\n",
      "Epoch 19, test loss: 0.165605\n",
      "Epoch 20, test loss: 0.165464\n",
      "Epoch 21, test loss: 0.169743\n",
      "Epoch 22, test loss: 0.165367\n",
      "Epoch 23, test loss: 0.166163\n",
      "Epoch 24, test loss: 0.165861\n",
      "Epoch 25, test loss: 0.165226\n",
      "Epoch 26, test loss: 0.165405\n",
      "Epoch 27, test loss: 0.166146\n",
      "Epoch 28, test loss: 0.166679\n",
      "Epoch 29, test loss: 0.166760\n",
      "Epoch 30, test loss: 0.165263\n",
      "Epoch 31, test loss: 0.166163\n",
      "Epoch 32, test loss: 0.165894\n",
      "Epoch 33, test loss: 0.165966\n",
      "Epoch 34, test loss: 0.165385\n",
      "Epoch 35, test loss: 0.165041\n",
      "Epoch 36, test loss: 0.165587\n",
      "Epoch 37, test loss: 0.166041\n",
      "Epoch 38, test loss: 0.165857\n",
      "Epoch 39, test loss: 0.165439\n",
      "Epoch 40, test loss: 0.165548\n",
      "Epoch 41, test loss: 0.165157\n",
      "Epoch 42, test loss: 0.165234\n",
      "Epoch 43, test loss: 0.165093\n",
      "Epoch 44, test loss: 0.165895\n",
      "Epoch 45, test loss: 0.166329\n",
      "Epoch 46, test loss: 0.167192\n",
      "Epoch 47, test loss: 0.165889\n",
      "Epoch 48, test loss: 0.165506\n",
      "Epoch 49, test loss: 0.166553\n",
      "Epoch 50, test loss: 0.165610\n",
      "Epoch 51, test loss: 0.165689\n",
      "Epoch 52, test loss: 0.167238\n",
      "Epoch 53, test loss: 0.166558\n",
      "Epoch 54, test loss: 0.165598\n",
      "Epoch 55, test loss: 0.165043\n",
      "Epoch 56, test loss: 0.165264\n",
      "Epoch 57, test loss: 0.165455\n",
      "Epoch 58, test loss: 0.165459\n",
      "Epoch 59, test loss: 0.165615\n",
      "Epoch 60, test loss: 0.165141\n",
      "Epoch 61, test loss: 0.168013\n",
      "Epoch 62, test loss: 0.165613\n",
      "Epoch 63, test loss: 0.167184\n",
      "Epoch 64, test loss: 0.164826\n",
      "Epoch 65, test loss: 0.165962\n",
      "Epoch 66, test loss: 0.166057\n",
      "Epoch 67, test loss: 0.165339\n",
      "Epoch 68, test loss: 0.166040\n",
      "Epoch 69, test loss: 0.165948\n",
      "Epoch 70, test loss: 0.167862\n",
      "Epoch 71, test loss: 0.167122\n",
      "Epoch 72, test loss: 0.166500\n",
      "Epoch 73, test loss: 0.164991\n",
      "Epoch 74, test loss: 0.168446\n",
      "Epoch 75, test loss: 0.167786\n",
      "Epoch 76, test loss: 0.168567\n",
      "Epoch 77, test loss: 0.165831\n",
      "Epoch 78, test loss: 0.165821\n",
      "Epoch 79, test loss: 0.165244\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A296749C00>\n",
      "Epoch 0, test loss: 0.353469\n",
      "Epoch 1, test loss: 0.189752\n",
      "Epoch 2, test loss: 0.184700\n",
      "Epoch 3, test loss: 0.181328\n",
      "Epoch 4, test loss: 0.177803\n",
      "Epoch 5, test loss: 0.175255\n",
      "Epoch 6, test loss: 0.173648\n",
      "Epoch 7, test loss: 0.172563\n",
      "Epoch 8, test loss: 0.173183\n",
      "Epoch 9, test loss: 0.174504\n",
      "Epoch 10, test loss: 0.171063\n",
      "Epoch 11, test loss: 0.170444\n",
      "Epoch 12, test loss: 0.171333\n",
      "Epoch 13, test loss: 0.169904\n",
      "Epoch 14, test loss: 0.169597\n",
      "Epoch 15, test loss: 0.171749\n",
      "Epoch 16, test loss: 0.171232\n",
      "Epoch 17, test loss: 0.170041\n",
      "Epoch 18, test loss: 0.169598\n",
      "Epoch 19, test loss: 0.169652\n",
      "Epoch 20, test loss: 0.169338\n",
      "Epoch 21, test loss: 0.171473\n",
      "Epoch 22, test loss: 0.169579\n",
      "Epoch 23, test loss: 0.170065\n",
      "Epoch 24, test loss: 0.172343\n",
      "Epoch 25, test loss: 0.168751\n",
      "Epoch 26, test loss: 0.168936\n",
      "Epoch 27, test loss: 0.171885\n",
      "Epoch 28, test loss: 0.171266\n",
      "Epoch 29, test loss: 0.169315\n",
      "Epoch 30, test loss: 0.168473\n",
      "Epoch 31, test loss: 0.172157\n",
      "Epoch 32, test loss: 0.169636\n",
      "Epoch 33, test loss: 0.168043\n",
      "Epoch 34, test loss: 0.168962\n",
      "Epoch 35, test loss: 0.168189\n",
      "Epoch 36, test loss: 0.171934\n",
      "Epoch 37, test loss: 0.169640\n",
      "Epoch 38, test loss: 0.167878\n",
      "Epoch 39, test loss: 0.168710\n",
      "Epoch 40, test loss: 0.170951\n",
      "Epoch 41, test loss: 0.167800\n",
      "Epoch 42, test loss: 0.168485\n",
      "Epoch 43, test loss: 0.172726\n",
      "Epoch 44, test loss: 0.167939\n",
      "Epoch 45, test loss: 0.167920\n",
      "Epoch 46, test loss: 0.167927\n",
      "Epoch 47, test loss: 0.167287\n",
      "Epoch 48, test loss: 0.168518\n",
      "Epoch 49, test loss: 0.168493\n",
      "Epoch 50, test loss: 0.169705\n",
      "Epoch 51, test loss: 0.169066\n",
      "Epoch 52, test loss: 0.171564\n",
      "Epoch 53, test loss: 0.167549\n",
      "Epoch 54, test loss: 0.168204\n",
      "Epoch 55, test loss: 0.167237\n",
      "Epoch 56, test loss: 0.172742\n",
      "Epoch 57, test loss: 0.169376\n",
      "Epoch 58, test loss: 0.168075\n",
      "Epoch 59, test loss: 0.167437\n",
      "Epoch 60, test loss: 0.169497\n",
      "Epoch 61, test loss: 0.167704\n",
      "Epoch 62, test loss: 0.167424\n",
      "Epoch 63, test loss: 0.166753\n",
      "Epoch 64, test loss: 0.169301\n",
      "Epoch 65, test loss: 0.167231\n",
      "Epoch 66, test loss: 0.167511\n",
      "Epoch 67, test loss: 0.167302\n",
      "Epoch 68, test loss: 0.168626\n",
      "Epoch 69, test loss: 0.166890\n",
      "Epoch 70, test loss: 0.168883\n",
      "Epoch 71, test loss: 0.167407\n",
      "Epoch 72, test loss: 0.170252\n",
      "Epoch 73, test loss: 0.168318\n",
      "Epoch 74, test loss: 0.167109\n",
      "Epoch 75, test loss: 0.171991\n",
      "Epoch 76, test loss: 0.167216\n",
      "Epoch 77, test loss: 0.167553\n",
      "Epoch 78, test loss: 0.167087\n",
      "Epoch 79, test loss: 0.167490\n",
      "Epoch 80, test loss: 0.167951\n",
      "Epoch 81, test loss: 0.167663\n",
      "Epoch 82, test loss: 0.169117\n",
      "Epoch 83, test loss: 0.166852\n",
      "Epoch 84, test loss: 0.167375\n",
      "Epoch 85, test loss: 0.166988\n",
      "Epoch 86, test loss: 0.167489\n",
      "Epoch 87, test loss: 0.170356\n",
      "Epoch 88, test loss: 0.168339\n",
      "Epoch 89, test loss: 0.166815\n",
      "Epoch 90, test loss: 0.168579\n",
      "Epoch 91, test loss: 0.168941\n",
      "Epoch 92, test loss: 0.167054\n",
      "Epoch 93, test loss: 0.168108\n",
      "Epoch 94, test loss: 0.168054\n",
      "Epoch 95, test loss: 0.167015\n",
      "Epoch 96, test loss: 0.167696\n",
      "Epoch 97, test loss: 0.169204\n",
      "Epoch 98, test loss: 0.166853\n",
      "Epoch 99, test loss: 0.169924\n",
      "Epoch 100, test loss: 0.167429\n",
      "Epoch 101, test loss: 0.167950\n",
      "Epoch 102, test loss: 0.167278\n",
      "Epoch 103, test loss: 0.166514\n",
      "Epoch 104, test loss: 0.167112\n",
      "Epoch 105, test loss: 0.169274\n",
      "Epoch 106, test loss: 0.170660\n",
      "Epoch 107, test loss: 0.168339\n",
      "Epoch 108, test loss: 0.167112\n",
      "Epoch 109, test loss: 0.167038\n",
      "Epoch 110, test loss: 0.166689\n",
      "Epoch 111, test loss: 0.167000\n",
      "Epoch 112, test loss: 0.166743\n",
      "Epoch 113, test loss: 0.166871\n",
      "Epoch 114, test loss: 0.170272\n",
      "Epoch 115, test loss: 0.168439\n",
      "Epoch 116, test loss: 0.167712\n",
      "Epoch 117, test loss: 0.167183\n",
      "Epoch 118, test loss: 0.167302\n",
      "Epoch 119, test loss: 0.169689\n",
      "Epoch 120, test loss: 0.167783\n",
      "Epoch 121, test loss: 0.169106\n",
      "Epoch 122, test loss: 0.166589\n",
      "Epoch 123, test loss: 0.166717\n",
      "Epoch 124, test loss: 0.166950\n",
      "Epoch 125, test loss: 0.167291\n",
      "Epoch 126, test loss: 0.169437\n",
      "Epoch 127, test loss: 0.166485\n",
      "Epoch 128, test loss: 0.167990\n",
      "Epoch 129, test loss: 0.166619\n",
      "Epoch 130, test loss: 0.167055\n",
      "Epoch 131, test loss: 0.166499\n",
      "Epoch 132, test loss: 0.169077\n",
      "Epoch 133, test loss: 0.168034\n",
      "Epoch 134, test loss: 0.167199\n",
      "Epoch 135, test loss: 0.167185\n",
      "Epoch 136, test loss: 0.166571\n",
      "Epoch 137, test loss: 0.167047\n",
      "Epoch 138, test loss: 0.168528\n",
      "Epoch 139, test loss: 0.167653\n",
      "Epoch 140, test loss: 0.168744\n",
      "Epoch 141, test loss: 0.166890\n",
      "Epoch 142, test loss: 0.168233\n",
      "Epoch 143, test loss: 0.166895\n",
      "Epoch 144, test loss: 0.167811\n",
      "Epoch 145, test loss: 0.166796\n",
      "Epoch 146, test loss: 0.171796\n",
      "Epoch 147, test loss: 0.167558\n",
      "Epoch 148, test loss: 0.166475\n",
      "Epoch 149, test loss: 0.169084\n",
      "Epoch 150, test loss: 0.167065\n",
      "Epoch 151, test loss: 0.167817\n",
      "Epoch 152, test loss: 0.168763\n",
      "Epoch 153, test loss: 0.169341\n",
      "Epoch 154, test loss: 0.166814\n",
      "Epoch 155, test loss: 0.167834\n",
      "Epoch 156, test loss: 0.167530\n",
      "Epoch 157, test loss: 0.167054\n",
      "Epoch 158, test loss: 0.167394\n",
      "Epoch 159, test loss: 0.167568\n",
      "Epoch 160, test loss: 0.166790\n",
      "Epoch 161, test loss: 0.167664\n",
      "Epoch 162, test loss: 0.166938\n",
      "Epoch 163, test loss: 0.168351\n",
      "Epoch 164, test loss: 0.167164\n",
      "Epoch 165, test loss: 0.167130\n",
      "Epoch 166, test loss: 0.169307\n",
      "Epoch 167, test loss: 0.166704\n",
      "Epoch 168, test loss: 0.166568\n",
      "Epoch 169, test loss: 0.166876\n",
      "Epoch 170, test loss: 0.166744\n",
      "Epoch 171, test loss: 0.167405\n",
      "Epoch 172, test loss: 0.168692\n",
      "Epoch 173, test loss: 0.168859\n",
      "Epoch 174, test loss: 0.168450\n",
      "Epoch 175, test loss: 0.167099\n",
      "Epoch 176, test loss: 0.166715\n",
      "Epoch 177, test loss: 0.167529\n",
      "Epoch 178, test loss: 0.167699\n",
      "Epoch 179, test loss: 0.166646\n",
      "Epoch 180, test loss: 0.166426\n",
      "Epoch 181, test loss: 0.168630\n",
      "Epoch 182, test loss: 0.166481\n",
      "Epoch 183, test loss: 0.166290\n",
      "Epoch 184, test loss: 0.166905\n",
      "Epoch 185, test loss: 0.166546\n",
      "Epoch 186, test loss: 0.166615\n",
      "Epoch 187, test loss: 0.166324\n",
      "Epoch 188, test loss: 0.166928\n",
      "Epoch 189, test loss: 0.167203\n",
      "Epoch 190, test loss: 0.166641\n",
      "Epoch 191, test loss: 0.169661\n",
      "Epoch 192, test loss: 0.166550\n",
      "Epoch 193, test loss: 0.166514\n",
      "Epoch 194, test loss: 0.167494\n",
      "Epoch 195, test loss: 0.166924\n",
      "Epoch 196, test loss: 0.167827\n",
      "Epoch 197, test loss: 0.166672\n",
      "Epoch 198, test loss: 0.172395\n",
      "Epoch 199, test loss: 0.167421\n",
      "Epoch 200, test loss: 0.166876\n",
      "Epoch 201, test loss: 0.168232\n",
      "Epoch 202, test loss: 0.168052\n",
      "Epoch 203, test loss: 0.167434\n",
      "Epoch 204, test loss: 0.166324\n",
      "Epoch 205, test loss: 0.166747\n",
      "Epoch 206, test loss: 0.168540\n",
      "Epoch 207, test loss: 0.166681\n",
      "Epoch 208, test loss: 0.168787\n",
      "Epoch 209, test loss: 0.166324\n",
      "Epoch 210, test loss: 0.167495\n",
      "Epoch 211, test loss: 0.166558\n",
      "Epoch 212, test loss: 0.166252\n",
      "Epoch 213, test loss: 0.167221\n",
      "Epoch 214, test loss: 0.166391\n",
      "Epoch 215, test loss: 0.166237\n",
      "Epoch 216, test loss: 0.166850\n",
      "Epoch 217, test loss: 0.167855\n",
      "Epoch 218, test loss: 0.166636\n",
      "Epoch 219, test loss: 0.166762\n",
      "Epoch 220, test loss: 0.166405\n",
      "Epoch 221, test loss: 0.167151\n",
      "Epoch 222, test loss: 0.168379\n",
      "Epoch 223, test loss: 0.166745\n",
      "Epoch 224, test loss: 0.166868\n",
      "Epoch 225, test loss: 0.170094\n",
      "Epoch 226, test loss: 0.166971\n",
      "Epoch 227, test loss: 0.166684\n",
      "Epoch 228, test loss: 0.166267\n",
      "Epoch 229, test loss: 0.167934\n",
      "Epoch 230, test loss: 0.167023\n",
      "Epoch 231, test loss: 0.166289\n",
      "Epoch 232, test loss: 0.166487\n",
      "Epoch 233, test loss: 0.166188\n",
      "Epoch 234, test loss: 0.166734\n",
      "Epoch 235, test loss: 0.166171\n",
      "Epoch 236, test loss: 0.166300\n",
      "Epoch 237, test loss: 0.166367\n",
      "Epoch 238, test loss: 0.166693\n",
      "Epoch 239, test loss: 0.167366\n",
      "Pretrain data: 19669610.0\n",
      "Building dataset, requesting data from 0 to 819\n",
      "x here is\n",
      "[[ 95.  86.  81.  81.  82.  82.]\n",
      " [ 86.  81.  81.  82.  82.  84.]\n",
      " [ 81.  81.  82.  82.  84.  88.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[ 97.  97.  97.  97.  97.  97.]\n",
      " [ 97.  97.  97.  97.  97.  97.]\n",
      " [ 95.  95.  95.  95.  95.  95.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7835/107513\n",
      "Found 819 continuous time series\n",
      "Data shape: (115350, 6), Train/test: 115348/2\n",
      "Train test ratio: 57674.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1802\n",
      "Epoch 0, train loss: 0.211520\n",
      "Epoch 1, train loss: 0.189334\n",
      "Epoch 2, train loss: 0.210281\n",
      "Epoch 3, train loss: 0.228941\n",
      "Epoch 4, train loss: 0.165271\n",
      "Epoch 5, train loss: 0.247307\n",
      "Epoch 6, train loss: 0.204178\n",
      "Epoch 7, train loss: 0.216355\n",
      "Epoch 8, train loss: 0.177955\n",
      "Epoch 9, train loss: 0.202000\n",
      "Epoch 10, train loss: 0.204432\n",
      "Epoch 11, train loss: 0.202360\n",
      "Epoch 12, train loss: 0.202418\n",
      "Epoch 13, train loss: 0.198948\n",
      "Epoch 14, train loss: 0.206333\n",
      "Epoch 15, train loss: 0.224167\n",
      "Epoch 16, train loss: 0.174208\n",
      "Epoch 17, train loss: 0.265482\n",
      "Epoch 18, train loss: 0.166363\n",
      "Epoch 19, train loss: 0.229860\n",
      "Epoch 20, train loss: 0.182633\n",
      "Epoch 21, train loss: 0.194260\n",
      "Epoch 22, train loss: 0.185375\n",
      "Epoch 23, train loss: 0.287158\n",
      "Epoch 24, train loss: 0.204044\n",
      "Epoch 25, train loss: 0.261986\n",
      "Epoch 26, train loss: 0.203983\n",
      "Epoch 27, train loss: 0.230224\n",
      "Epoch 28, train loss: 0.186466\n",
      "Epoch 29, train loss: 0.224292\n",
      "Epoch 30, train loss: 0.198060\n",
      "Epoch 31, train loss: 0.205179\n",
      "Epoch 32, train loss: 0.159171\n",
      "Epoch 33, train loss: 0.268230\n",
      "Epoch 34, train loss: 0.213641\n",
      "Epoch 35, train loss: 0.274122\n",
      "Epoch 36, train loss: 0.246950\n",
      "Epoch 37, train loss: 0.180184\n",
      "Epoch 38, train loss: 0.167523\n",
      "Epoch 39, train loss: 0.193323\n",
      "Epoch 40, train loss: 0.194393\n",
      "Epoch 41, train loss: 0.199717\n",
      "Epoch 42, train loss: 0.294044\n",
      "Epoch 43, train loss: 0.166109\n",
      "Epoch 44, train loss: 0.200211\n",
      "Epoch 45, train loss: 0.217787\n",
      "Epoch 46, train loss: 0.211901\n",
      "Epoch 47, train loss: 0.195286\n",
      "Epoch 48, train loss: 0.243270\n",
      "Epoch 49, train loss: 0.237851\n",
      "Epoch 50, train loss: 0.191922\n",
      "Epoch 51, train loss: 0.156049\n",
      "Epoch 52, train loss: 0.227059\n",
      "Epoch 53, train loss: 0.198365\n",
      "Epoch 54, train loss: 0.180300\n",
      "Epoch 55, train loss: 0.175746\n",
      "Epoch 56, train loss: 0.199639\n",
      "Epoch 57, train loss: 0.222627\n",
      "Epoch 58, train loss: 0.142926\n",
      "Epoch 59, train loss: 0.237192\n",
      "Epoch 60, train loss: 0.215549\n",
      "Epoch 61, train loss: 0.197892\n",
      "Epoch 62, train loss: 0.216452\n",
      "Epoch 63, train loss: 0.213347\n",
      "Epoch 64, train loss: 0.182266\n",
      "Epoch 65, train loss: 0.158280\n",
      "Epoch 66, train loss: 0.187428\n",
      "Epoch 67, train loss: 0.261275\n",
      "Epoch 68, train loss: 0.253647\n",
      "Epoch 69, train loss: 0.151505\n",
      "Epoch 70, train loss: 0.226813\n",
      "Epoch 71, train loss: 0.272981\n",
      "Epoch 72, train loss: 0.163274\n",
      "Epoch 73, train loss: 0.189352\n",
      "Epoch 74, train loss: 0.208028\n",
      "Epoch 75, train loss: 0.262756\n",
      "Epoch 76, train loss: 0.230868\n",
      "Epoch 77, train loss: 0.273677\n",
      "Epoch 78, train loss: 0.208663\n",
      "Epoch 79, train loss: 0.193641\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[161. 155. 150. 147. 144. 141.]\n",
      " [155. 150. 147. 144. 141. 139.]\n",
      " [150. 147. 144. 141. 139. 135.]\n",
      " ...\n",
      " [310. 301. 293. 293. 301. 307.]\n",
      " [301. 293. 293. 301. 307. 306.]\n",
      " [293. 293. 301. 307. 306. 300.]]\n",
      "y here is\n",
      "[[121. 121. 121. 121. 121. 121.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " [113. 113. 113. 113. 113. 113.]\n",
      " ...\n",
      " [284. 284. 284. 284. 284. 284.]\n",
      " [273. 273. 273. 273. 273. 273.]\n",
      " [262. 262. 262. 262. 262. 262.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2639, 6), Train/test: 1/2638\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [180. 181. 178. 176. 173. 172.]\n",
      " [181. 178. 176. 173. 172. 173.]\n",
      " [178. 176. 173. 172. 173. 173.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [164. 164. 164. 164. 164. 164.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 450/9926\n",
      "Found 23 continuous time series\n",
      "Data shape: (10378, 6), Train/test: 10376/2\n",
      "Train test ratio: 5188.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930E32E0>\n",
      "Epoch 0, test loss: 0.171160\n",
      "Epoch 1, test loss: 0.169830\n",
      "Epoch 2, test loss: 0.169489\n",
      "Epoch 3, test loss: 0.169450\n",
      "Epoch 4, test loss: 0.169531\n",
      "Epoch 5, test loss: 0.170028\n",
      "Epoch 6, test loss: 0.171888\n",
      "Epoch 7, test loss: 0.169689\n",
      "Epoch 8, test loss: 0.169704\n",
      "Epoch 9, test loss: 0.171590\n",
      "Epoch 10, test loss: 0.169680\n",
      "Epoch 11, test loss: 0.170473\n",
      "Epoch 12, test loss: 0.170181\n",
      "Epoch 13, test loss: 0.169829\n",
      "Epoch 14, test loss: 0.170220\n",
      "Epoch 15, test loss: 0.170759\n",
      "Epoch 16, test loss: 0.169718\n",
      "Epoch 17, test loss: 0.169781\n",
      "Epoch 18, test loss: 0.173229\n",
      "Epoch 19, test loss: 0.170308\n",
      "Epoch 20, test loss: 0.170417\n",
      "Epoch 21, test loss: 0.171375\n",
      "Epoch 22, test loss: 0.170205\n",
      "Epoch 23, test loss: 0.176966\n",
      "Epoch 24, test loss: 0.170332\n",
      "Epoch 25, test loss: 0.171285\n",
      "Epoch 26, test loss: 0.175773\n",
      "Epoch 27, test loss: 0.169857\n",
      "Epoch 28, test loss: 0.174951\n",
      "Epoch 29, test loss: 0.169664\n",
      "Epoch 30, test loss: 0.169417\n",
      "Epoch 31, test loss: 0.175500\n",
      "Epoch 32, test loss: 0.169927\n",
      "Epoch 33, test loss: 0.170291\n",
      "Epoch 34, test loss: 0.172326\n",
      "Epoch 35, test loss: 0.171172\n",
      "Epoch 36, test loss: 0.169080\n",
      "Epoch 37, test loss: 0.170197\n",
      "Epoch 38, test loss: 0.169532\n",
      "Epoch 39, test loss: 0.169580\n",
      "Epoch 40, test loss: 0.169545\n",
      "Epoch 41, test loss: 0.169603\n",
      "Epoch 42, test loss: 0.170092\n",
      "Epoch 43, test loss: 0.170275\n",
      "Epoch 44, test loss: 0.171369\n",
      "Epoch 45, test loss: 0.170773\n",
      "Epoch 46, test loss: 0.169125\n",
      "Epoch 47, test loss: 0.169601\n",
      "Epoch 48, test loss: 0.169080\n",
      "Epoch 49, test loss: 0.169668\n",
      "Epoch 50, test loss: 0.170302\n",
      "Epoch 51, test loss: 0.178052\n",
      "Epoch 52, test loss: 0.177688\n",
      "Epoch 53, test loss: 0.172246\n",
      "Epoch 54, test loss: 0.169934\n",
      "Epoch 55, test loss: 0.170561\n",
      "Epoch 56, test loss: 0.171980\n",
      "Epoch 57, test loss: 0.170370\n",
      "Epoch 58, test loss: 0.169173\n",
      "Epoch 59, test loss: 0.169689\n",
      "Epoch 60, test loss: 0.169192\n",
      "Epoch 61, test loss: 0.170216\n",
      "Epoch 62, test loss: 0.171054\n",
      "Epoch 63, test loss: 0.169900\n",
      "Epoch 64, test loss: 0.169897\n",
      "Epoch 65, test loss: 0.172162\n",
      "Epoch 66, test loss: 0.169705\n",
      "Epoch 67, test loss: 0.169522\n",
      "Epoch 68, test loss: 0.170138\n",
      "Epoch 69, test loss: 0.169461\n",
      "Epoch 70, test loss: 0.169361\n",
      "Epoch 71, test loss: 0.170658\n",
      "Epoch 72, test loss: 0.169274\n",
      "Epoch 73, test loss: 0.170358\n",
      "Epoch 74, test loss: 0.170673\n",
      "Epoch 75, test loss: 0.170545\n",
      "Epoch 76, test loss: 0.172058\n",
      "Epoch 77, test loss: 0.169732\n",
      "Epoch 78, test loss: 0.174408\n",
      "Epoch 79, test loss: 0.169526\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930E32E0>\n",
      "Epoch 0, test loss: 0.169217\n",
      "Epoch 1, test loss: 0.169425\n",
      "Epoch 2, test loss: 0.169429\n",
      "Epoch 3, test loss: 0.170006\n",
      "Epoch 4, test loss: 0.169913\n",
      "Epoch 5, test loss: 0.170034\n",
      "Epoch 6, test loss: 0.169861\n",
      "Epoch 7, test loss: 0.169963\n",
      "Epoch 8, test loss: 0.170282\n",
      "Epoch 9, test loss: 0.169592\n",
      "Epoch 10, test loss: 0.170943\n",
      "Epoch 11, test loss: 0.169898\n",
      "Epoch 12, test loss: 0.170820\n",
      "Epoch 13, test loss: 0.170131\n",
      "Epoch 14, test loss: 0.169686\n",
      "Epoch 15, test loss: 0.169782\n",
      "Epoch 16, test loss: 0.170247\n",
      "Epoch 17, test loss: 0.169952\n",
      "Epoch 18, test loss: 0.169922\n",
      "Epoch 19, test loss: 0.172660\n",
      "Epoch 20, test loss: 0.170875\n",
      "Epoch 21, test loss: 0.170181\n",
      "Epoch 22, test loss: 0.169753\n",
      "Epoch 23, test loss: 0.169696\n",
      "Epoch 24, test loss: 0.172086\n",
      "Epoch 25, test loss: 0.170126\n",
      "Epoch 26, test loss: 0.169630\n",
      "Epoch 27, test loss: 0.173557\n",
      "Epoch 28, test loss: 0.169858\n",
      "Epoch 29, test loss: 0.170191\n",
      "Epoch 30, test loss: 0.169830\n",
      "Epoch 31, test loss: 0.170238\n",
      "Epoch 32, test loss: 0.170046\n",
      "Epoch 33, test loss: 0.172233\n",
      "Epoch 34, test loss: 0.169669\n",
      "Epoch 35, test loss: 0.170222\n",
      "Epoch 36, test loss: 0.170312\n",
      "Epoch 37, test loss: 0.170012\n",
      "Epoch 38, test loss: 0.169586\n",
      "Epoch 39, test loss: 0.169444\n",
      "Epoch 40, test loss: 0.169571\n",
      "Epoch 41, test loss: 0.169542\n",
      "Epoch 42, test loss: 0.170940\n",
      "Epoch 43, test loss: 0.170823\n",
      "Epoch 44, test loss: 0.169436\n",
      "Epoch 45, test loss: 0.170418\n",
      "Epoch 46, test loss: 0.170618\n",
      "Epoch 47, test loss: 0.169872\n",
      "Epoch 48, test loss: 0.170152\n",
      "Epoch 49, test loss: 0.170281\n",
      "Epoch 50, test loss: 0.169919\n",
      "Epoch 51, test loss: 0.169500\n",
      "Epoch 52, test loss: 0.170319\n",
      "Epoch 53, test loss: 0.169085\n",
      "Epoch 54, test loss: 0.169776\n",
      "Epoch 55, test loss: 0.169243\n",
      "Epoch 56, test loss: 0.169433\n",
      "Epoch 57, test loss: 0.170034\n",
      "Epoch 58, test loss: 0.169398\n",
      "Epoch 59, test loss: 0.169427\n",
      "Epoch 60, test loss: 0.171130\n",
      "Epoch 61, test loss: 0.170543\n",
      "Epoch 62, test loss: 0.169566\n",
      "Epoch 63, test loss: 0.169423\n",
      "Epoch 64, test loss: 0.170217\n",
      "Epoch 65, test loss: 0.175318\n",
      "Epoch 66, test loss: 0.171049\n",
      "Epoch 67, test loss: 0.169830\n",
      "Epoch 68, test loss: 0.169585\n",
      "Epoch 69, test loss: 0.172050\n",
      "Epoch 70, test loss: 0.170757\n",
      "Epoch 71, test loss: 0.169813\n",
      "Epoch 72, test loss: 0.168997\n",
      "Epoch 73, test loss: 0.168971\n",
      "Epoch 74, test loss: 0.171090\n",
      "Epoch 75, test loss: 0.169177\n",
      "Epoch 76, test loss: 0.169525\n",
      "Epoch 77, test loss: 0.169805\n",
      "Epoch 78, test loss: 0.169517\n",
      "Epoch 79, test loss: 0.169965\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930E32E0>\n",
      "Epoch 0, test loss: 0.275292\n",
      "Epoch 1, test loss: 0.188375\n",
      "Epoch 2, test loss: 0.187484\n",
      "Epoch 3, test loss: 0.182464\n",
      "Epoch 4, test loss: 0.177926\n",
      "Epoch 5, test loss: 0.176216\n",
      "Epoch 6, test loss: 0.176379\n",
      "Epoch 7, test loss: 0.175128\n",
      "Epoch 8, test loss: 0.174710\n",
      "Epoch 9, test loss: 0.173855\n",
      "Epoch 10, test loss: 0.172344\n",
      "Epoch 11, test loss: 0.174697\n",
      "Epoch 12, test loss: 0.172162\n",
      "Epoch 13, test loss: 0.172950\n",
      "Epoch 14, test loss: 0.171281\n",
      "Epoch 15, test loss: 0.171362\n",
      "Epoch 16, test loss: 0.172857\n",
      "Epoch 17, test loss: 0.171160\n",
      "Epoch 18, test loss: 0.171182\n",
      "Epoch 19, test loss: 0.170783\n",
      "Epoch 20, test loss: 0.171254\n",
      "Epoch 21, test loss: 0.171153\n",
      "Epoch 22, test loss: 0.170857\n",
      "Epoch 23, test loss: 0.170964\n",
      "Epoch 24, test loss: 0.170642\n",
      "Epoch 25, test loss: 0.172499\n",
      "Epoch 26, test loss: 0.171888\n",
      "Epoch 27, test loss: 0.170403\n",
      "Epoch 28, test loss: 0.170889\n",
      "Epoch 29, test loss: 0.170225\n",
      "Epoch 30, test loss: 0.170753\n",
      "Epoch 31, test loss: 0.171611\n",
      "Epoch 32, test loss: 0.171096\n",
      "Epoch 33, test loss: 0.170565\n",
      "Epoch 34, test loss: 0.171146\n",
      "Epoch 35, test loss: 0.172536\n",
      "Epoch 36, test loss: 0.170426\n",
      "Epoch 37, test loss: 0.170442\n",
      "Epoch 38, test loss: 0.170481\n",
      "Epoch 39, test loss: 0.172540\n",
      "Epoch 40, test loss: 0.170650\n",
      "Epoch 41, test loss: 0.170394\n",
      "Epoch 42, test loss: 0.170332\n",
      "Epoch 43, test loss: 0.170600\n",
      "Epoch 44, test loss: 0.174815\n",
      "Epoch 45, test loss: 0.170039\n",
      "Epoch 46, test loss: 0.170643\n",
      "Epoch 47, test loss: 0.171586\n",
      "Epoch 48, test loss: 0.171267\n",
      "Epoch 49, test loss: 0.170239\n",
      "Epoch 50, test loss: 0.170405\n",
      "Epoch 51, test loss: 0.170109\n",
      "Epoch 52, test loss: 0.170352\n",
      "Epoch 53, test loss: 0.172114\n",
      "Epoch 54, test loss: 0.170980\n",
      "Epoch 55, test loss: 0.170058\n",
      "Epoch 56, test loss: 0.170178\n",
      "Epoch 57, test loss: 0.170395\n",
      "Epoch 58, test loss: 0.169767\n",
      "Epoch 59, test loss: 0.170020\n",
      "Epoch 60, test loss: 0.170587\n",
      "Epoch 61, test loss: 0.170139\n",
      "Epoch 62, test loss: 0.170158\n",
      "Epoch 63, test loss: 0.170609\n",
      "Epoch 64, test loss: 0.170400\n",
      "Epoch 65, test loss: 0.171162\n",
      "Epoch 66, test loss: 0.170102\n",
      "Epoch 67, test loss: 0.169958\n",
      "Epoch 68, test loss: 0.172235\n",
      "Epoch 69, test loss: 0.171290\n",
      "Epoch 70, test loss: 0.170229\n",
      "Epoch 71, test loss: 0.170156\n",
      "Epoch 72, test loss: 0.173067\n",
      "Epoch 73, test loss: 0.169756\n",
      "Epoch 74, test loss: 0.169712\n",
      "Epoch 75, test loss: 0.170791\n",
      "Epoch 76, test loss: 0.170910\n",
      "Epoch 77, test loss: 0.169982\n",
      "Epoch 78, test loss: 0.169936\n",
      "Epoch 79, test loss: 0.169626\n",
      "Epoch 80, test loss: 0.170210\n",
      "Epoch 81, test loss: 0.169829\n",
      "Epoch 82, test loss: 0.169854\n",
      "Epoch 83, test loss: 0.169737\n",
      "Epoch 84, test loss: 0.170097\n",
      "Epoch 85, test loss: 0.170026\n",
      "Epoch 86, test loss: 0.170738\n",
      "Epoch 87, test loss: 0.170779\n",
      "Epoch 88, test loss: 0.171124\n",
      "Epoch 89, test loss: 0.170145\n",
      "Epoch 90, test loss: 0.169640\n",
      "Epoch 91, test loss: 0.169832\n",
      "Epoch 92, test loss: 0.169721\n",
      "Epoch 93, test loss: 0.171603\n",
      "Epoch 94, test loss: 0.170061\n",
      "Epoch 95, test loss: 0.169946\n",
      "Epoch 96, test loss: 0.170262\n",
      "Epoch 97, test loss: 0.169565\n",
      "Epoch 98, test loss: 0.169991\n",
      "Epoch 99, test loss: 0.170164\n",
      "Epoch 100, test loss: 0.169862\n",
      "Epoch 101, test loss: 0.170535\n",
      "Epoch 102, test loss: 0.172319\n",
      "Epoch 103, test loss: 0.170925\n",
      "Epoch 104, test loss: 0.172181\n",
      "Epoch 105, test loss: 0.169647\n",
      "Epoch 106, test loss: 0.170457\n",
      "Epoch 107, test loss: 0.169702\n",
      "Epoch 108, test loss: 0.169588\n",
      "Epoch 109, test loss: 0.169444\n",
      "Epoch 110, test loss: 0.170820\n",
      "Epoch 111, test loss: 0.170427\n",
      "Epoch 112, test loss: 0.169455\n",
      "Epoch 113, test loss: 0.169942\n",
      "Epoch 114, test loss: 0.169710\n",
      "Epoch 115, test loss: 0.169999\n",
      "Epoch 116, test loss: 0.169725\n",
      "Epoch 117, test loss: 0.170123\n",
      "Epoch 118, test loss: 0.169845\n",
      "Epoch 119, test loss: 0.171775\n",
      "Epoch 120, test loss: 0.169638\n",
      "Epoch 121, test loss: 0.169930\n",
      "Epoch 122, test loss: 0.169411\n",
      "Epoch 123, test loss: 0.171722\n",
      "Epoch 124, test loss: 0.169332\n",
      "Epoch 125, test loss: 0.170098\n",
      "Epoch 126, test loss: 0.169459\n",
      "Epoch 127, test loss: 0.169887\n",
      "Epoch 128, test loss: 0.173293\n",
      "Epoch 129, test loss: 0.169678\n",
      "Epoch 130, test loss: 0.172518\n",
      "Epoch 131, test loss: 0.171548\n",
      "Epoch 132, test loss: 0.169628\n",
      "Epoch 133, test loss: 0.169251\n",
      "Epoch 134, test loss: 0.170315\n",
      "Epoch 135, test loss: 0.173186\n",
      "Epoch 136, test loss: 0.171925\n",
      "Epoch 137, test loss: 0.169379\n",
      "Epoch 138, test loss: 0.170011\n",
      "Epoch 139, test loss: 0.169314\n",
      "Epoch 140, test loss: 0.169856\n",
      "Epoch 141, test loss: 0.171907\n",
      "Epoch 142, test loss: 0.169360\n",
      "Epoch 143, test loss: 0.174707\n",
      "Epoch 144, test loss: 0.169593\n",
      "Epoch 145, test loss: 0.169388\n",
      "Epoch 146, test loss: 0.169805\n",
      "Epoch 147, test loss: 0.171720\n",
      "Epoch 148, test loss: 0.171580\n",
      "Epoch 149, test loss: 0.171067\n",
      "Epoch 150, test loss: 0.172114\n",
      "Epoch 151, test loss: 0.169623\n",
      "Epoch 152, test loss: 0.170408\n",
      "Epoch 153, test loss: 0.173086\n",
      "Epoch 154, test loss: 0.169538\n",
      "Epoch 155, test loss: 0.169899\n",
      "Epoch 156, test loss: 0.169473\n",
      "Epoch 157, test loss: 0.169863\n",
      "Epoch 158, test loss: 0.169550\n",
      "Epoch 159, test loss: 0.169607\n",
      "Epoch 160, test loss: 0.169312\n",
      "Epoch 161, test loss: 0.170370\n",
      "Epoch 162, test loss: 0.169596\n",
      "Epoch 163, test loss: 0.171111\n",
      "Epoch 164, test loss: 0.169565\n",
      "Epoch 165, test loss: 0.170482\n",
      "Epoch 166, test loss: 0.171816\n",
      "Epoch 167, test loss: 0.171910\n",
      "Epoch 168, test loss: 0.170262\n",
      "Epoch 169, test loss: 0.169985\n",
      "Epoch 170, test loss: 0.169906\n",
      "Epoch 171, test loss: 0.169804\n",
      "Epoch 172, test loss: 0.173395\n",
      "Epoch 173, test loss: 0.169587\n",
      "Epoch 174, test loss: 0.170157\n",
      "Epoch 175, test loss: 0.172952\n",
      "Epoch 176, test loss: 0.169226\n",
      "Epoch 177, test loss: 0.169511\n",
      "Epoch 178, test loss: 0.172571\n",
      "Epoch 179, test loss: 0.169799\n",
      "Epoch 180, test loss: 0.170588\n",
      "Epoch 181, test loss: 0.169724\n",
      "Epoch 182, test loss: 0.170343\n",
      "Epoch 183, test loss: 0.170035\n",
      "Epoch 184, test loss: 0.169761\n",
      "Epoch 185, test loss: 0.169306\n",
      "Epoch 186, test loss: 0.170365\n",
      "Epoch 187, test loss: 0.169388\n",
      "Epoch 188, test loss: 0.169755\n",
      "Epoch 189, test loss: 0.169118\n",
      "Epoch 190, test loss: 0.172955\n",
      "Epoch 191, test loss: 0.170700\n",
      "Epoch 192, test loss: 0.170157\n",
      "Epoch 193, test loss: 0.169278\n",
      "Epoch 194, test loss: 0.169608\n",
      "Epoch 195, test loss: 0.169997\n",
      "Epoch 196, test loss: 0.169798\n",
      "Epoch 197, test loss: 0.169273\n",
      "Epoch 198, test loss: 0.169813\n",
      "Epoch 199, test loss: 0.169306\n",
      "Epoch 200, test loss: 0.169409\n",
      "Epoch 201, test loss: 0.169322\n",
      "Epoch 202, test loss: 0.171573\n",
      "Epoch 203, test loss: 0.170095\n",
      "Epoch 204, test loss: 0.170316\n",
      "Epoch 205, test loss: 0.169727\n",
      "Epoch 206, test loss: 0.169578\n",
      "Epoch 207, test loss: 0.170082\n",
      "Epoch 208, test loss: 0.169891\n",
      "Epoch 209, test loss: 0.169285\n",
      "Epoch 210, test loss: 0.169112\n",
      "Epoch 211, test loss: 0.169164\n",
      "Epoch 212, test loss: 0.169355\n",
      "Epoch 213, test loss: 0.170120\n",
      "Epoch 214, test loss: 0.172251\n",
      "Epoch 215, test loss: 0.169964\n",
      "Epoch 216, test loss: 0.169308\n",
      "Epoch 217, test loss: 0.169741\n",
      "Epoch 218, test loss: 0.171038\n",
      "Epoch 219, test loss: 0.169393\n",
      "Epoch 220, test loss: 0.170529\n",
      "Epoch 221, test loss: 0.172403\n",
      "Epoch 222, test loss: 0.171403\n",
      "Epoch 223, test loss: 0.169972\n",
      "Epoch 224, test loss: 0.170074\n",
      "Epoch 225, test loss: 0.169761\n",
      "Epoch 226, test loss: 0.171061\n",
      "Epoch 227, test loss: 0.169580\n",
      "Epoch 228, test loss: 0.171278\n",
      "Epoch 229, test loss: 0.169332\n",
      "Epoch 230, test loss: 0.170259\n",
      "Epoch 231, test loss: 0.169466\n",
      "Epoch 232, test loss: 0.169315\n",
      "Epoch 233, test loss: 0.169073\n",
      "Epoch 234, test loss: 0.169391\n",
      "Epoch 235, test loss: 0.170104\n",
      "Epoch 236, test loss: 0.171731\n",
      "Epoch 237, test loss: 0.169419\n",
      "Epoch 238, test loss: 0.170890\n",
      "Epoch 239, test loss: 0.169067\n",
      "Pretrain data: 19754507.0\n",
      "Building dataset, requesting data from 0 to 655\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7434/109388\n",
      "Found 655 continuous time series\n",
      "Data shape: (116824, 6), Train/test: 116822/2\n",
      "Train test ratio: 58411.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1825\n",
      "Epoch 0, train loss: 0.209361\n",
      "Epoch 1, train loss: 0.240253\n",
      "Epoch 2, train loss: 0.186656\n",
      "Epoch 3, train loss: 0.215362\n",
      "Epoch 4, train loss: 0.259095\n",
      "Epoch 5, train loss: 0.210105\n",
      "Epoch 6, train loss: 0.194355\n",
      "Epoch 7, train loss: 0.245804\n",
      "Epoch 8, train loss: 0.180257\n",
      "Epoch 9, train loss: 0.295652\n",
      "Epoch 10, train loss: 0.200167\n",
      "Epoch 11, train loss: 0.255395\n",
      "Epoch 12, train loss: 0.244352\n",
      "Epoch 13, train loss: 0.226041\n",
      "Epoch 14, train loss: 0.242592\n",
      "Epoch 15, train loss: 0.216703\n",
      "Epoch 16, train loss: 0.179808\n",
      "Epoch 17, train loss: 0.209738\n",
      "Epoch 18, train loss: 0.168669\n",
      "Epoch 19, train loss: 0.201543\n",
      "Epoch 20, train loss: 0.207742\n",
      "Epoch 21, train loss: 0.248582\n",
      "Epoch 22, train loss: 0.210050\n",
      "Epoch 23, train loss: 0.190857\n",
      "Epoch 24, train loss: 0.198722\n",
      "Epoch 25, train loss: 0.165793\n",
      "Epoch 26, train loss: 0.177792\n",
      "Epoch 27, train loss: 0.191317\n",
      "Epoch 28, train loss: 0.188164\n",
      "Epoch 29, train loss: 0.237895\n",
      "Epoch 30, train loss: 0.209108\n",
      "Epoch 31, train loss: 0.173608\n",
      "Epoch 32, train loss: 0.202164\n",
      "Epoch 33, train loss: 0.161922\n",
      "Epoch 34, train loss: 0.264438\n",
      "Epoch 35, train loss: 0.196123\n",
      "Epoch 36, train loss: 0.226878\n",
      "Epoch 37, train loss: 0.171601\n",
      "Epoch 38, train loss: 0.268342\n",
      "Epoch 39, train loss: 0.185753\n",
      "Epoch 40, train loss: 0.193197\n",
      "Epoch 41, train loss: 0.192418\n",
      "Epoch 42, train loss: 0.176950\n",
      "Epoch 43, train loss: 0.274599\n",
      "Epoch 44, train loss: 0.180810\n",
      "Epoch 45, train loss: 0.194937\n",
      "Epoch 46, train loss: 0.278143\n",
      "Epoch 47, train loss: 0.225697\n",
      "Epoch 48, train loss: 0.232937\n",
      "Epoch 49, train loss: 0.199108\n",
      "Epoch 50, train loss: 0.201931\n",
      "Epoch 51, train loss: 0.152854\n",
      "Epoch 52, train loss: 0.215167\n",
      "Epoch 53, train loss: 0.208327\n",
      "Epoch 54, train loss: 0.231108\n",
      "Epoch 55, train loss: 0.223246\n",
      "Epoch 56, train loss: 0.178733\n",
      "Epoch 57, train loss: 0.244243\n",
      "Epoch 58, train loss: 0.217641\n",
      "Epoch 59, train loss: 0.195146\n",
      "Epoch 60, train loss: 0.174928\n",
      "Epoch 61, train loss: 0.174314\n",
      "Epoch 62, train loss: 0.148868\n",
      "Epoch 63, train loss: 0.216888\n",
      "Epoch 64, train loss: 0.196322\n",
      "Epoch 65, train loss: 0.194712\n",
      "Epoch 66, train loss: 0.188195\n",
      "Epoch 67, train loss: 0.336085\n",
      "Epoch 68, train loss: 0.233189\n",
      "Epoch 69, train loss: 0.150917\n",
      "Epoch 70, train loss: 0.192325\n",
      "Epoch 71, train loss: 0.159819\n",
      "Epoch 72, train loss: 0.195498\n",
      "Epoch 73, train loss: 0.199356\n",
      "Epoch 74, train loss: 0.134607\n",
      "Epoch 75, train loss: 0.267560\n",
      "Epoch 76, train loss: 0.170005\n",
      "Epoch 77, train loss: 0.331139\n",
      "Epoch 78, train loss: 0.226871\n",
      "Epoch 79, train loss: 0.220689\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[296. 290. 284. 279. 272. 266.]\n",
      " [290. 284. 279. 272. 266. 259.]\n",
      " [284. 279. 272. 266. 259. 253.]\n",
      " ...\n",
      " [186. 179. 174. 172. 171. 178.]\n",
      " [179. 174. 172. 171. 178. 180.]\n",
      " [174. 172. 171. 178. 180. 180.]]\n",
      "y here is\n",
      "[[234. 234. 234. 234. 234. 234.]\n",
      " [229. 229. 229. 229. 229. 229.]\n",
      " [222. 222. 222. 222. 222. 222.]\n",
      " ...\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [164. 164. 164. 164. 164. 164.]\n",
      " [157. 157. 157. 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1928, 6), Train/test: 1/1927\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 187 segments\n",
      "Building dataset, requesting data from 0 to 187\n",
      "x here is\n",
      "[[ 71.  71.  71.  71.  72.  71.]\n",
      " [ 71.  71.  71.  72.  71.  71.]\n",
      " [ 71.  71.  72.  71.  71.  71.]\n",
      " ...\n",
      " [280. 281. 279. 277. 277. 277.]\n",
      " [281. 279. 277. 277. 277. 279.]\n",
      " [279. 277. 277. 277. 279. 282.]]\n",
      "y here is\n",
      "[[ 71.  71.  71.  71.  71.  71.]\n",
      " [ 71.  71.  71.  71.  71.  71.]\n",
      " [ 71.  71.  71.  71.  71.  71.]\n",
      " ...\n",
      " [315. 315. 315. 315. 315. 315.]\n",
      " [310. 310. 310. 310. 310. 310.]\n",
      " [303. 303. 303. 303. 303. 303.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 851/8051\n",
      "Found 187 continuous time series\n",
      "Data shape: (8904, 6), Train/test: 8902/2\n",
      "Train test ratio: 4451.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47011F310>\n",
      "Epoch 0, test loss: 0.208360\n",
      "Epoch 1, test loss: 0.209480\n",
      "Epoch 2, test loss: 0.220544\n",
      "Epoch 3, test loss: 0.208534\n",
      "Epoch 4, test loss: 0.210902\n",
      "Epoch 5, test loss: 0.207570\n",
      "Epoch 6, test loss: 0.218976\n",
      "Epoch 7, test loss: 0.209549\n",
      "Epoch 8, test loss: 0.210578\n",
      "Epoch 9, test loss: 0.216335\n",
      "Epoch 10, test loss: 0.208505\n",
      "Epoch 11, test loss: 0.206801\n",
      "Epoch 12, test loss: 0.208498\n",
      "Epoch 13, test loss: 0.208046\n",
      "Epoch 14, test loss: 0.207362\n",
      "Epoch 15, test loss: 0.210594\n",
      "Epoch 16, test loss: 0.210813\n",
      "Epoch 17, test loss: 0.209675\n",
      "Epoch 18, test loss: 0.208862\n",
      "Epoch 19, test loss: 0.211572\n",
      "Epoch 20, test loss: 0.207781\n",
      "Epoch 21, test loss: 0.207732\n",
      "Epoch 22, test loss: 0.207440\n",
      "Epoch 23, test loss: 0.210651\n",
      "Epoch 24, test loss: 0.207613\n",
      "Epoch 25, test loss: 0.209241\n",
      "Epoch 26, test loss: 0.210778\n",
      "Epoch 27, test loss: 0.208029\n",
      "Epoch 28, test loss: 0.207721\n",
      "Epoch 29, test loss: 0.208684\n",
      "Epoch 30, test loss: 0.207222\n",
      "Epoch 31, test loss: 0.209943\n",
      "Epoch 32, test loss: 0.210257\n",
      "Epoch 33, test loss: 0.208276\n",
      "Epoch 34, test loss: 0.207919\n",
      "Epoch 35, test loss: 0.209326\n",
      "Epoch 36, test loss: 0.208300\n",
      "Epoch 37, test loss: 0.210005\n",
      "Epoch 38, test loss: 0.211252\n",
      "Epoch 39, test loss: 0.209174\n",
      "Epoch 40, test loss: 0.207857\n",
      "Epoch 41, test loss: 0.213127\n",
      "Epoch 42, test loss: 0.209081\n",
      "Epoch 43, test loss: 0.212242\n",
      "Epoch 44, test loss: 0.207642\n",
      "Epoch 45, test loss: 0.209513\n",
      "Epoch 46, test loss: 0.207875\n",
      "Epoch 47, test loss: 0.206958\n",
      "Epoch 48, test loss: 0.207350\n",
      "Epoch 49, test loss: 0.207976\n",
      "Epoch 50, test loss: 0.207339\n",
      "Epoch 51, test loss: 0.211820\n",
      "Epoch 52, test loss: 0.206828\n",
      "Epoch 53, test loss: 0.207500\n",
      "Epoch 54, test loss: 0.210384\n",
      "Epoch 55, test loss: 0.215298\n",
      "Epoch 56, test loss: 0.209803\n",
      "Epoch 57, test loss: 0.207082\n",
      "Epoch 58, test loss: 0.209185\n",
      "Epoch 59, test loss: 0.206199\n",
      "Epoch 60, test loss: 0.208279\n",
      "Epoch 61, test loss: 0.210232\n",
      "Epoch 62, test loss: 0.206992\n",
      "Epoch 63, test loss: 0.210169\n",
      "Epoch 64, test loss: 0.208004\n",
      "Epoch 65, test loss: 0.207141\n",
      "Epoch 66, test loss: 0.207027\n",
      "Epoch 67, test loss: 0.208495\n",
      "Epoch 68, test loss: 0.209815\n",
      "Epoch 69, test loss: 0.207528\n",
      "Epoch 70, test loss: 0.208094\n",
      "Epoch 71, test loss: 0.211334\n",
      "Epoch 72, test loss: 0.209466\n",
      "Epoch 73, test loss: 0.207650\n",
      "Epoch 74, test loss: 0.209858\n",
      "Epoch 75, test loss: 0.210366\n",
      "Epoch 76, test loss: 0.210041\n",
      "Epoch 77, test loss: 0.208582\n",
      "Epoch 78, test loss: 0.210874\n",
      "Epoch 79, test loss: 0.211089\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47011F310>\n",
      "Epoch 0, test loss: 0.209913\n",
      "Epoch 1, test loss: 0.208847\n",
      "Epoch 2, test loss: 0.208890\n",
      "Epoch 3, test loss: 0.208822\n",
      "Epoch 4, test loss: 0.208581\n",
      "Epoch 5, test loss: 0.209364\n",
      "Epoch 6, test loss: 0.208912\n",
      "Epoch 7, test loss: 0.209829\n",
      "Epoch 8, test loss: 0.208695\n",
      "Epoch 9, test loss: 0.211456\n",
      "Epoch 10, test loss: 0.208431\n",
      "Epoch 11, test loss: 0.209402\n",
      "Epoch 12, test loss: 0.209964\n",
      "Epoch 13, test loss: 0.208940\n",
      "Epoch 14, test loss: 0.208566\n",
      "Epoch 15, test loss: 0.209042\n",
      "Epoch 16, test loss: 0.210663\n",
      "Epoch 17, test loss: 0.212512\n",
      "Epoch 18, test loss: 0.208450\n",
      "Epoch 19, test loss: 0.208616\n",
      "Epoch 20, test loss: 0.211602\n",
      "Epoch 21, test loss: 0.210143\n",
      "Epoch 22, test loss: 0.208612\n",
      "Epoch 23, test loss: 0.209041\n",
      "Epoch 24, test loss: 0.208525\n",
      "Epoch 25, test loss: 0.207945\n",
      "Epoch 26, test loss: 0.208495\n",
      "Epoch 27, test loss: 0.209048\n",
      "Epoch 28, test loss: 0.210345\n",
      "Epoch 29, test loss: 0.209874\n",
      "Epoch 30, test loss: 0.214449\n",
      "Epoch 31, test loss: 0.209065\n",
      "Epoch 32, test loss: 0.208331\n",
      "Epoch 33, test loss: 0.207593\n",
      "Epoch 34, test loss: 0.210539\n",
      "Epoch 35, test loss: 0.207705\n",
      "Epoch 36, test loss: 0.211063\n",
      "Epoch 37, test loss: 0.207526\n",
      "Epoch 38, test loss: 0.212589\n",
      "Epoch 39, test loss: 0.207865\n",
      "Epoch 40, test loss: 0.209194\n",
      "Epoch 41, test loss: 0.208273\n",
      "Epoch 42, test loss: 0.208545\n",
      "Epoch 43, test loss: 0.207579\n",
      "Epoch 44, test loss: 0.208672\n",
      "Epoch 45, test loss: 0.209874\n",
      "Epoch 46, test loss: 0.209642\n",
      "Epoch 47, test loss: 0.208401\n",
      "Epoch 48, test loss: 0.208384\n",
      "Epoch 49, test loss: 0.208824\n",
      "Epoch 50, test loss: 0.208458\n",
      "Epoch 51, test loss: 0.208535\n",
      "Epoch 52, test loss: 0.208197\n",
      "Epoch 53, test loss: 0.208163\n",
      "Epoch 54, test loss: 0.210133\n",
      "Epoch 55, test loss: 0.208714\n",
      "Epoch 56, test loss: 0.208202\n",
      "Epoch 57, test loss: 0.209685\n",
      "Epoch 58, test loss: 0.207491\n",
      "Epoch 59, test loss: 0.208995\n",
      "Epoch 60, test loss: 0.209156\n",
      "Epoch 61, test loss: 0.210140\n",
      "Epoch 62, test loss: 0.208326\n",
      "Epoch 63, test loss: 0.209383\n",
      "Epoch 64, test loss: 0.208136\n",
      "Epoch 65, test loss: 0.208104\n",
      "Epoch 66, test loss: 0.207440\n",
      "Epoch 67, test loss: 0.210677\n",
      "Epoch 68, test loss: 0.210572\n",
      "Epoch 69, test loss: 0.209708\n",
      "Epoch 70, test loss: 0.208991\n",
      "Epoch 71, test loss: 0.209417\n",
      "Epoch 72, test loss: 0.209339\n",
      "Epoch 73, test loss: 0.207411\n",
      "Epoch 74, test loss: 0.208536\n",
      "Epoch 75, test loss: 0.209289\n",
      "Epoch 76, test loss: 0.211200\n",
      "Epoch 77, test loss: 0.208676\n",
      "Epoch 78, test loss: 0.209134\n",
      "Epoch 79, test loss: 0.209113\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47011F310>\n",
      "Epoch 0, test loss: 0.344170\n",
      "Epoch 1, test loss: 0.248661\n",
      "Epoch 2, test loss: 0.235471\n",
      "Epoch 3, test loss: 0.227437\n",
      "Epoch 4, test loss: 0.218306\n",
      "Epoch 5, test loss: 0.215291\n",
      "Epoch 6, test loss: 0.214241\n",
      "Epoch 7, test loss: 0.217941\n",
      "Epoch 8, test loss: 0.212232\n",
      "Epoch 9, test loss: 0.213251\n",
      "Epoch 10, test loss: 0.214997\n",
      "Epoch 11, test loss: 0.212542\n",
      "Epoch 12, test loss: 0.210957\n",
      "Epoch 13, test loss: 0.212165\n",
      "Epoch 14, test loss: 0.214206\n",
      "Epoch 15, test loss: 0.213895\n",
      "Epoch 16, test loss: 0.210073\n",
      "Epoch 17, test loss: 0.211910\n",
      "Epoch 18, test loss: 0.210533\n",
      "Epoch 19, test loss: 0.214314\n",
      "Epoch 20, test loss: 0.210814\n",
      "Epoch 21, test loss: 0.210517\n",
      "Epoch 22, test loss: 0.214319\n",
      "Epoch 23, test loss: 0.213494\n",
      "Epoch 24, test loss: 0.209724\n",
      "Epoch 25, test loss: 0.212023\n",
      "Epoch 26, test loss: 0.210804\n",
      "Epoch 27, test loss: 0.214564\n",
      "Epoch 28, test loss: 0.210627\n",
      "Epoch 29, test loss: 0.209422\n",
      "Epoch 30, test loss: 0.209679\n",
      "Epoch 31, test loss: 0.209457\n",
      "Epoch 32, test loss: 0.213257\n",
      "Epoch 33, test loss: 0.209193\n",
      "Epoch 34, test loss: 0.209060\n",
      "Epoch 35, test loss: 0.211566\n",
      "Epoch 36, test loss: 0.209319\n",
      "Epoch 37, test loss: 0.211638\n",
      "Epoch 38, test loss: 0.210117\n",
      "Epoch 39, test loss: 0.209015\n",
      "Epoch 40, test loss: 0.211889\n",
      "Epoch 41, test loss: 0.211506\n",
      "Epoch 42, test loss: 0.210169\n",
      "Epoch 43, test loss: 0.213918\n",
      "Epoch 44, test loss: 0.208295\n",
      "Epoch 45, test loss: 0.208845\n",
      "Epoch 46, test loss: 0.208787\n",
      "Epoch 47, test loss: 0.208513\n",
      "Epoch 48, test loss: 0.208979\n",
      "Epoch 49, test loss: 0.212192\n",
      "Epoch 50, test loss: 0.213588\n",
      "Epoch 51, test loss: 0.209045\n",
      "Epoch 52, test loss: 0.210789\n",
      "Epoch 53, test loss: 0.211157\n",
      "Epoch 54, test loss: 0.208413\n",
      "Epoch 55, test loss: 0.209218\n",
      "Epoch 56, test loss: 0.209908\n",
      "Epoch 57, test loss: 0.209395\n",
      "Epoch 58, test loss: 0.215000\n",
      "Epoch 59, test loss: 0.213691\n",
      "Epoch 60, test loss: 0.209581\n",
      "Epoch 61, test loss: 0.208200\n",
      "Epoch 62, test loss: 0.208135\n",
      "Epoch 63, test loss: 0.209164\n",
      "Epoch 64, test loss: 0.208304\n",
      "Epoch 65, test loss: 0.210354\n",
      "Epoch 66, test loss: 0.208631\n",
      "Epoch 67, test loss: 0.208437\n",
      "Epoch 68, test loss: 0.208310\n",
      "Epoch 69, test loss: 0.208060\n",
      "Epoch 70, test loss: 0.209212\n",
      "Epoch 71, test loss: 0.208215\n",
      "Epoch 72, test loss: 0.213413\n",
      "Epoch 73, test loss: 0.208305\n",
      "Epoch 74, test loss: 0.208137\n",
      "Epoch 75, test loss: 0.208656\n",
      "Epoch 76, test loss: 0.209843\n",
      "Epoch 77, test loss: 0.208262\n",
      "Epoch 78, test loss: 0.208436\n",
      "Epoch 79, test loss: 0.209134\n",
      "Epoch 80, test loss: 0.208535\n",
      "Epoch 81, test loss: 0.207895\n",
      "Epoch 82, test loss: 0.209681\n",
      "Epoch 83, test loss: 0.208409\n",
      "Epoch 84, test loss: 0.208126\n",
      "Epoch 85, test loss: 0.210025\n",
      "Epoch 86, test loss: 0.208812\n",
      "Epoch 87, test loss: 0.210888\n",
      "Epoch 88, test loss: 0.212392\n",
      "Epoch 89, test loss: 0.216329\n",
      "Epoch 90, test loss: 0.210931\n",
      "Epoch 91, test loss: 0.211001\n",
      "Epoch 92, test loss: 0.208388\n",
      "Epoch 93, test loss: 0.210185\n",
      "Epoch 94, test loss: 0.210350\n",
      "Epoch 95, test loss: 0.209190\n",
      "Epoch 96, test loss: 0.210209\n",
      "Epoch 97, test loss: 0.208122\n",
      "Epoch 98, test loss: 0.212380\n",
      "Epoch 99, test loss: 0.214553\n",
      "Epoch 100, test loss: 0.208145\n",
      "Epoch 101, test loss: 0.207568\n",
      "Epoch 102, test loss: 0.209188\n",
      "Epoch 103, test loss: 0.208482\n",
      "Epoch 104, test loss: 0.207600\n",
      "Epoch 105, test loss: 0.211207\n",
      "Epoch 106, test loss: 0.211707\n",
      "Epoch 107, test loss: 0.207551\n",
      "Epoch 108, test loss: 0.207716\n",
      "Epoch 109, test loss: 0.207448\n",
      "Epoch 110, test loss: 0.209674\n",
      "Epoch 111, test loss: 0.209750\n",
      "Epoch 112, test loss: 0.208567\n",
      "Epoch 113, test loss: 0.207540\n",
      "Epoch 114, test loss: 0.209747\n",
      "Epoch 115, test loss: 0.207443\n",
      "Epoch 116, test loss: 0.207386\n",
      "Epoch 117, test loss: 0.209965\n",
      "Epoch 118, test loss: 0.208203\n",
      "Epoch 119, test loss: 0.207157\n",
      "Epoch 120, test loss: 0.210925\n",
      "Epoch 121, test loss: 0.208778\n",
      "Epoch 122, test loss: 0.208543\n",
      "Epoch 123, test loss: 0.207909\n",
      "Epoch 124, test loss: 0.207106\n",
      "Epoch 125, test loss: 0.209393\n",
      "Epoch 126, test loss: 0.210778\n",
      "Epoch 127, test loss: 0.209280\n",
      "Epoch 128, test loss: 0.208428\n",
      "Epoch 129, test loss: 0.207357\n",
      "Epoch 130, test loss: 0.208793\n",
      "Epoch 131, test loss: 0.207005\n",
      "Epoch 132, test loss: 0.207653\n",
      "Epoch 133, test loss: 0.207117\n",
      "Epoch 134, test loss: 0.207512\n",
      "Epoch 135, test loss: 0.207014\n",
      "Epoch 136, test loss: 0.207888\n",
      "Epoch 137, test loss: 0.208109\n",
      "Epoch 138, test loss: 0.208303\n",
      "Epoch 139, test loss: 0.212549\n",
      "Epoch 140, test loss: 0.212308\n",
      "Epoch 141, test loss: 0.215083\n",
      "Epoch 142, test loss: 0.207105\n",
      "Epoch 143, test loss: 0.207531\n",
      "Epoch 144, test loss: 0.206939\n",
      "Epoch 145, test loss: 0.207590\n",
      "Epoch 146, test loss: 0.207459\n",
      "Epoch 147, test loss: 0.206984\n",
      "Epoch 148, test loss: 0.207038\n",
      "Epoch 149, test loss: 0.207862\n",
      "Epoch 150, test loss: 0.207257\n",
      "Epoch 151, test loss: 0.207827\n",
      "Epoch 152, test loss: 0.209785\n",
      "Epoch 153, test loss: 0.208745\n",
      "Epoch 154, test loss: 0.207217\n",
      "Epoch 155, test loss: 0.207174\n",
      "Epoch 156, test loss: 0.207175\n",
      "Epoch 157, test loss: 0.206521\n",
      "Epoch 158, test loss: 0.209644\n",
      "Epoch 159, test loss: 0.206839\n",
      "Epoch 160, test loss: 0.207518\n",
      "Epoch 161, test loss: 0.206329\n",
      "Epoch 162, test loss: 0.207175\n",
      "Epoch 163, test loss: 0.207473\n",
      "Epoch 164, test loss: 0.206906\n",
      "Epoch 165, test loss: 0.206700\n",
      "Epoch 166, test loss: 0.206816\n",
      "Epoch 167, test loss: 0.207407\n",
      "Epoch 168, test loss: 0.208013\n",
      "Epoch 169, test loss: 0.207286\n",
      "Epoch 170, test loss: 0.207715\n",
      "Epoch 171, test loss: 0.207301\n",
      "Epoch 172, test loss: 0.208440\n",
      "Epoch 173, test loss: 0.205959\n",
      "Epoch 174, test loss: 0.207449\n",
      "Epoch 175, test loss: 0.206264\n",
      "Epoch 176, test loss: 0.211548\n",
      "Epoch 177, test loss: 0.211890\n",
      "Epoch 178, test loss: 0.208570\n",
      "Epoch 179, test loss: 0.207063\n",
      "Epoch 180, test loss: 0.208140\n",
      "Epoch 181, test loss: 0.206873\n",
      "Epoch 182, test loss: 0.206673\n",
      "Epoch 183, test loss: 0.208120\n",
      "Epoch 184, test loss: 0.206353\n",
      "Epoch 185, test loss: 0.207118\n",
      "Epoch 186, test loss: 0.206819\n",
      "Epoch 187, test loss: 0.206902\n",
      "Epoch 188, test loss: 0.206179\n",
      "Epoch 189, test loss: 0.206557\n",
      "Epoch 190, test loss: 0.208230\n",
      "Epoch 191, test loss: 0.206910\n",
      "Epoch 192, test loss: 0.207428\n",
      "Epoch 193, test loss: 0.207468\n",
      "Epoch 194, test loss: 0.207646\n",
      "Epoch 195, test loss: 0.212105\n",
      "Epoch 196, test loss: 0.208785\n",
      "Epoch 197, test loss: 0.207304\n",
      "Epoch 198, test loss: 0.206796\n",
      "Epoch 199, test loss: 0.208169\n",
      "Epoch 200, test loss: 0.207110\n",
      "Epoch 201, test loss: 0.206797\n",
      "Epoch 202, test loss: 0.208314\n",
      "Epoch 203, test loss: 0.207373\n",
      "Epoch 204, test loss: 0.206891\n",
      "Epoch 205, test loss: 0.208118\n",
      "Epoch 206, test loss: 0.210485\n",
      "Epoch 207, test loss: 0.209120\n",
      "Epoch 208, test loss: 0.207417\n",
      "Epoch 209, test loss: 0.206653\n",
      "Epoch 210, test loss: 0.209921\n",
      "Epoch 211, test loss: 0.207077\n",
      "Epoch 212, test loss: 0.208913\n",
      "Epoch 213, test loss: 0.209846\n",
      "Epoch 214, test loss: 0.208191\n",
      "Epoch 215, test loss: 0.209018\n",
      "Epoch 216, test loss: 0.206679\n",
      "Epoch 217, test loss: 0.207452\n",
      "Epoch 218, test loss: 0.206570\n",
      "Epoch 219, test loss: 0.208102\n",
      "Epoch 220, test loss: 0.206982\n",
      "Epoch 221, test loss: 0.210147\n",
      "Epoch 222, test loss: 0.206604\n",
      "Epoch 223, test loss: 0.206108\n",
      "Epoch 224, test loss: 0.208538\n",
      "Epoch 225, test loss: 0.206868\n",
      "Epoch 226, test loss: 0.216222\n",
      "Epoch 227, test loss: 0.207211\n",
      "Epoch 228, test loss: 0.208427\n",
      "Epoch 229, test loss: 0.207532\n",
      "Epoch 230, test loss: 0.214804\n",
      "Epoch 231, test loss: 0.207101\n",
      "Epoch 232, test loss: 0.208079\n",
      "Epoch 233, test loss: 0.206738\n",
      "Epoch 234, test loss: 0.206724\n",
      "Epoch 235, test loss: 0.210800\n",
      "Epoch 236, test loss: 0.209311\n",
      "Epoch 237, test loss: 0.207544\n",
      "Epoch 238, test loss: 0.208721\n",
      "Epoch 239, test loss: 0.207220\n",
      "Pretrain data: 19086006.0\n",
      "Building dataset, requesting data from 0 to 782\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 8121/106107\n",
      "Found 782 continuous time series\n",
      "Data shape: (114230, 6), Train/test: 114228/2\n",
      "Train test ratio: 57114.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1784\n",
      "Epoch 0, train loss: 0.239711\n",
      "Epoch 1, train loss: 0.240734\n",
      "Epoch 2, train loss: 0.223806\n",
      "Epoch 3, train loss: 0.240199\n",
      "Epoch 4, train loss: 0.205949\n",
      "Epoch 5, train loss: 0.208705\n",
      "Epoch 6, train loss: 0.193797\n",
      "Epoch 7, train loss: 0.225980\n",
      "Epoch 8, train loss: 0.186628\n",
      "Epoch 9, train loss: 0.201385\n",
      "Epoch 10, train loss: 0.196236\n",
      "Epoch 11, train loss: 0.192322\n",
      "Epoch 12, train loss: 0.176956\n",
      "Epoch 13, train loss: 0.186095\n",
      "Epoch 14, train loss: 0.193883\n",
      "Epoch 15, train loss: 0.171356\n",
      "Epoch 16, train loss: 0.240756\n",
      "Epoch 17, train loss: 0.166128\n",
      "Epoch 18, train loss: 0.151274\n",
      "Epoch 19, train loss: 0.198265\n",
      "Epoch 20, train loss: 0.222270\n",
      "Epoch 21, train loss: 0.216624\n",
      "Epoch 22, train loss: 0.166916\n",
      "Epoch 23, train loss: 0.199483\n",
      "Epoch 24, train loss: 0.179214\n",
      "Epoch 25, train loss: 0.209204\n",
      "Epoch 26, train loss: 0.195200\n",
      "Epoch 27, train loss: 0.185983\n",
      "Epoch 28, train loss: 0.210115\n",
      "Epoch 29, train loss: 0.209309\n",
      "Epoch 30, train loss: 0.235469\n",
      "Epoch 31, train loss: 0.230736\n",
      "Epoch 32, train loss: 0.177634\n",
      "Epoch 33, train loss: 0.175381\n",
      "Epoch 34, train loss: 0.188479\n",
      "Epoch 35, train loss: 0.184283\n",
      "Epoch 36, train loss: 0.325712\n",
      "Epoch 37, train loss: 0.198316\n",
      "Epoch 38, train loss: 0.156209\n",
      "Epoch 39, train loss: 0.211705\n",
      "Epoch 40, train loss: 0.201513\n",
      "Epoch 41, train loss: 0.206687\n",
      "Epoch 42, train loss: 0.143860\n",
      "Epoch 43, train loss: 0.167540\n",
      "Epoch 44, train loss: 0.233298\n",
      "Epoch 45, train loss: 0.189099\n",
      "Epoch 46, train loss: 0.188965\n",
      "Epoch 47, train loss: 0.178759\n",
      "Epoch 48, train loss: 0.217968\n",
      "Epoch 49, train loss: 0.246603\n",
      "Epoch 50, train loss: 0.155371\n",
      "Epoch 51, train loss: 0.198634\n",
      "Epoch 52, train loss: 0.166275\n",
      "Epoch 53, train loss: 0.185070\n",
      "Epoch 54, train loss: 0.203850\n",
      "Epoch 55, train loss: 0.237878\n",
      "Epoch 56, train loss: 0.195299\n",
      "Epoch 57, train loss: 0.145547\n",
      "Epoch 58, train loss: 0.176422\n",
      "Epoch 59, train loss: 0.189788\n",
      "Epoch 60, train loss: 0.189966\n",
      "Epoch 61, train loss: 0.185862\n",
      "Epoch 62, train loss: 0.197146\n",
      "Epoch 63, train loss: 0.165742\n",
      "Epoch 64, train loss: 0.195976\n",
      "Epoch 65, train loss: 0.253069\n",
      "Epoch 66, train loss: 0.160129\n",
      "Epoch 67, train loss: 0.204530\n",
      "Epoch 68, train loss: 0.175768\n",
      "Epoch 69, train loss: 0.271339\n",
      "Epoch 70, train loss: 0.271551\n",
      "Epoch 71, train loss: 0.180965\n",
      "Epoch 72, train loss: 0.200837\n",
      "Epoch 73, train loss: 0.199617\n",
      "Epoch 74, train loss: 0.205958\n",
      "Epoch 75, train loss: 0.196498\n",
      "Epoch 76, train loss: 0.200923\n",
      "Epoch 77, train loss: 0.178405\n",
      "Epoch 78, train loss: 0.202034\n",
      "Epoch 79, train loss: 0.189801\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "x here is\n",
      "[[243. 253. 262. 269. 269. 257.]\n",
      " [253. 262. 269. 269. 257. 258.]\n",
      " [262. 269. 269. 257. 258. 267.]\n",
      " ...\n",
      " [ 41.  49.  55.  53.  66.  90.]\n",
      " [ 49.  55.  53.  66.  90.  87.]\n",
      " [ 55.  53.  66.  90.  87.  86.]]\n",
      "y here is\n",
      "[[299. 299. 299. 299. 299. 299.]\n",
      " [300. 300. 300. 300. 300. 300.]\n",
      " [309. 309. 309. 309. 309. 309.]\n",
      " ...\n",
      " [ 72.  72.  72.  72.  72.  72.]\n",
      " [ 78.  78.  78.  78.  78.  78.]\n",
      " [ 79.  79.  79.  79.  79.  79.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (2489, 6), Train/test: 1/2488\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 60 segments\n",
      "Building dataset, requesting data from 0 to 60\n",
      "x here is\n",
      "[[ 48.  48.  53.  63.  69.  74.]\n",
      " [ 48.  53.  63.  69.  74.  77.]\n",
      " [ 53.  63.  69.  74.  77.  70.]\n",
      " ...\n",
      " [173. 185. 199. 212. 223. 227.]\n",
      " [185. 199. 212. 223. 227. 223.]\n",
      " [199. 212. 223. 227. 223. 215.]]\n",
      "y here is\n",
      "[[ 44.  44.  44.  44.  44.  44.]\n",
      " [ 44.  44.  44.  44.  44.  44.]\n",
      " [ 51.  51.  51.  51.  51.  51.]\n",
      " ...\n",
      " [213. 213. 213. 213. 213. 213.]\n",
      " [224. 224. 224. 224. 224. 224.]\n",
      " [235. 235. 235. 235. 235. 235.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 164/11332\n",
      "Found 60 continuous time series\n",
      "Data shape: (11498, 6), Train/test: 11496/2\n",
      "Train test ratio: 5748.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293035D20>\n",
      "Epoch 0, test loss: 0.215748\n",
      "Epoch 1, test loss: 0.217655\n",
      "Epoch 2, test loss: 0.217875\n",
      "Epoch 3, test loss: 0.217285\n",
      "Epoch 4, test loss: 0.217734\n",
      "Epoch 5, test loss: 0.217692\n",
      "Epoch 6, test loss: 0.218187\n",
      "Epoch 7, test loss: 0.217044\n",
      "Epoch 8, test loss: 0.218139\n",
      "Epoch 9, test loss: 0.217829\n",
      "Epoch 10, test loss: 0.217695\n",
      "Epoch 11, test loss: 0.217315\n",
      "Epoch 12, test loss: 0.217805\n",
      "Epoch 13, test loss: 0.229236\n",
      "Epoch 14, test loss: 0.217431\n",
      "Epoch 15, test loss: 0.218160\n",
      "Epoch 16, test loss: 0.225041\n",
      "Epoch 17, test loss: 0.219952\n",
      "Epoch 18, test loss: 0.219168\n",
      "Epoch 19, test loss: 0.216432\n",
      "Epoch 20, test loss: 0.216712\n",
      "Epoch 21, test loss: 0.227657\n",
      "Epoch 22, test loss: 0.217873\n",
      "Epoch 23, test loss: 0.219740\n",
      "Epoch 24, test loss: 0.223504\n",
      "Epoch 25, test loss: 0.219112\n",
      "Epoch 26, test loss: 0.216919\n",
      "Epoch 27, test loss: 0.217161\n",
      "Epoch 28, test loss: 0.217306\n",
      "Epoch 29, test loss: 0.219015\n",
      "Epoch 30, test loss: 0.221038\n",
      "Epoch 31, test loss: 0.218915\n",
      "Epoch 32, test loss: 0.218100\n",
      "Epoch 33, test loss: 0.217779\n",
      "Epoch 34, test loss: 0.216785\n",
      "Epoch 35, test loss: 0.218285\n",
      "Epoch 36, test loss: 0.221208\n",
      "Epoch 37, test loss: 0.216652\n",
      "Epoch 38, test loss: 0.217064\n",
      "Epoch 39, test loss: 0.216396\n",
      "Epoch 40, test loss: 0.215952\n",
      "Epoch 41, test loss: 0.221066\n",
      "Epoch 42, test loss: 0.216379\n",
      "Epoch 43, test loss: 0.216608\n",
      "Epoch 44, test loss: 0.216148\n",
      "Epoch 45, test loss: 0.221558\n",
      "Epoch 46, test loss: 0.217429\n",
      "Epoch 47, test loss: 0.217237\n",
      "Epoch 48, test loss: 0.216627\n",
      "Epoch 49, test loss: 0.218061\n",
      "Epoch 50, test loss: 0.219245\n",
      "Epoch 51, test loss: 0.220702\n",
      "Epoch 52, test loss: 0.219660\n",
      "Epoch 53, test loss: 0.219075\n",
      "Epoch 54, test loss: 0.218232\n",
      "Epoch 55, test loss: 0.217442\n",
      "Epoch 56, test loss: 0.221630\n",
      "Epoch 57, test loss: 0.218432\n",
      "Epoch 58, test loss: 0.217409\n",
      "Epoch 59, test loss: 0.218323\n",
      "Epoch 60, test loss: 0.217069\n",
      "Epoch 61, test loss: 0.217301\n",
      "Epoch 62, test loss: 0.217086\n",
      "Epoch 63, test loss: 0.216338\n",
      "Epoch 64, test loss: 0.219232\n",
      "Epoch 65, test loss: 0.220026\n",
      "Epoch 66, test loss: 0.217381\n",
      "Epoch 67, test loss: 0.217744\n",
      "Epoch 68, test loss: 0.217214\n",
      "Epoch 69, test loss: 0.218053\n",
      "Epoch 70, test loss: 0.216637\n",
      "Epoch 71, test loss: 0.215957\n",
      "Epoch 72, test loss: 0.216271\n",
      "Epoch 73, test loss: 0.222919\n",
      "Epoch 74, test loss: 0.216242\n",
      "Epoch 75, test loss: 0.217449\n",
      "Epoch 76, test loss: 0.216264\n",
      "Epoch 77, test loss: 0.215980\n",
      "Epoch 78, test loss: 0.216209\n",
      "Epoch 79, test loss: 0.217572\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293035D20>\n",
      "Epoch 0, test loss: 0.216419\n",
      "Epoch 1, test loss: 0.220212\n",
      "Epoch 2, test loss: 0.216384\n",
      "Epoch 3, test loss: 0.220414\n",
      "Epoch 4, test loss: 0.218564\n",
      "Epoch 5, test loss: 0.216130\n",
      "Epoch 6, test loss: 0.218657\n",
      "Epoch 7, test loss: 0.218754\n",
      "Epoch 8, test loss: 0.216203\n",
      "Epoch 9, test loss: 0.216730\n",
      "Epoch 10, test loss: 0.218354\n",
      "Epoch 11, test loss: 0.217623\n",
      "Epoch 12, test loss: 0.223994\n",
      "Epoch 13, test loss: 0.217501\n",
      "Epoch 14, test loss: 0.220701\n",
      "Epoch 15, test loss: 0.215355\n",
      "Epoch 16, test loss: 0.216946\n",
      "Epoch 17, test loss: 0.216888\n",
      "Epoch 18, test loss: 0.216190\n",
      "Epoch 19, test loss: 0.218393\n",
      "Epoch 20, test loss: 0.220008\n",
      "Epoch 21, test loss: 0.216131\n",
      "Epoch 22, test loss: 0.216888\n",
      "Epoch 23, test loss: 0.228470\n",
      "Epoch 24, test loss: 0.215632\n",
      "Epoch 25, test loss: 0.215472\n",
      "Epoch 26, test loss: 0.218423\n",
      "Epoch 27, test loss: 0.215790\n",
      "Epoch 28, test loss: 0.215184\n",
      "Epoch 29, test loss: 0.215377\n",
      "Epoch 30, test loss: 0.215708\n",
      "Epoch 31, test loss: 0.216115\n",
      "Epoch 32, test loss: 0.215683\n",
      "Epoch 33, test loss: 0.216359\n",
      "Epoch 34, test loss: 0.218372\n",
      "Epoch 35, test loss: 0.215806\n",
      "Epoch 36, test loss: 0.216211\n",
      "Epoch 37, test loss: 0.216490\n",
      "Epoch 38, test loss: 0.216917\n",
      "Epoch 39, test loss: 0.215631\n",
      "Epoch 40, test loss: 0.219068\n",
      "Epoch 41, test loss: 0.217275\n",
      "Epoch 42, test loss: 0.215334\n",
      "Epoch 43, test loss: 0.215524\n",
      "Epoch 44, test loss: 0.215636\n",
      "Epoch 45, test loss: 0.215739\n",
      "Epoch 46, test loss: 0.218061\n",
      "Epoch 47, test loss: 0.216203\n",
      "Epoch 48, test loss: 0.215999\n",
      "Epoch 49, test loss: 0.215682\n",
      "Epoch 50, test loss: 0.215662\n",
      "Epoch 51, test loss: 0.215669\n",
      "Epoch 52, test loss: 0.218856\n",
      "Epoch 53, test loss: 0.215975\n",
      "Epoch 54, test loss: 0.215586\n",
      "Epoch 55, test loss: 0.215615\n",
      "Epoch 56, test loss: 0.214701\n",
      "Epoch 57, test loss: 0.215160\n",
      "Epoch 58, test loss: 0.215224\n",
      "Epoch 59, test loss: 0.216098\n",
      "Epoch 60, test loss: 0.216444\n",
      "Epoch 61, test loss: 0.215213\n",
      "Epoch 62, test loss: 0.215632\n",
      "Epoch 63, test loss: 0.217006\n",
      "Epoch 64, test loss: 0.215940\n",
      "Epoch 65, test loss: 0.215486\n",
      "Epoch 66, test loss: 0.215844\n",
      "Epoch 67, test loss: 0.219286\n",
      "Epoch 68, test loss: 0.216374\n",
      "Epoch 69, test loss: 0.215520\n",
      "Epoch 70, test loss: 0.221467\n",
      "Epoch 71, test loss: 0.216071\n",
      "Epoch 72, test loss: 0.218508\n",
      "Epoch 73, test loss: 0.217578\n",
      "Epoch 74, test loss: 0.215958\n",
      "Epoch 75, test loss: 0.215382\n",
      "Epoch 76, test loss: 0.217555\n",
      "Epoch 77, test loss: 0.214786\n",
      "Epoch 78, test loss: 0.217315\n",
      "Epoch 79, test loss: 0.216893\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293035D20>\n",
      "Epoch 0, test loss: 0.251899\n",
      "Epoch 1, test loss: 0.237584\n",
      "Epoch 2, test loss: 0.230063\n",
      "Epoch 3, test loss: 0.225172\n",
      "Epoch 4, test loss: 0.222386\n",
      "Epoch 5, test loss: 0.220361\n",
      "Epoch 6, test loss: 0.221596\n",
      "Epoch 7, test loss: 0.218979\n",
      "Epoch 8, test loss: 0.219037\n",
      "Epoch 9, test loss: 0.217841\n",
      "Epoch 10, test loss: 0.217533\n",
      "Epoch 11, test loss: 0.218798\n",
      "Epoch 12, test loss: 0.217284\n",
      "Epoch 13, test loss: 0.218268\n",
      "Epoch 14, test loss: 0.219254\n",
      "Epoch 15, test loss: 0.221319\n",
      "Epoch 16, test loss: 0.218037\n",
      "Epoch 17, test loss: 0.218823\n",
      "Epoch 18, test loss: 0.217111\n",
      "Epoch 19, test loss: 0.218511\n",
      "Epoch 20, test loss: 0.216432\n",
      "Epoch 21, test loss: 0.218603\n",
      "Epoch 22, test loss: 0.216630\n",
      "Epoch 23, test loss: 0.217860\n",
      "Epoch 24, test loss: 0.217104\n",
      "Epoch 25, test loss: 0.219145\n",
      "Epoch 26, test loss: 0.217201\n",
      "Epoch 27, test loss: 0.217444\n",
      "Epoch 28, test loss: 0.217850\n",
      "Epoch 29, test loss: 0.218465\n",
      "Epoch 30, test loss: 0.220588\n",
      "Epoch 31, test loss: 0.220956\n",
      "Epoch 32, test loss: 0.220793\n",
      "Epoch 33, test loss: 0.218182\n",
      "Epoch 34, test loss: 0.218315\n",
      "Epoch 35, test loss: 0.217702\n",
      "Epoch 36, test loss: 0.218089\n",
      "Epoch 37, test loss: 0.218845\n",
      "Epoch 38, test loss: 0.217039\n",
      "Epoch 39, test loss: 0.218368\n",
      "Epoch 40, test loss: 0.217836\n",
      "Epoch 41, test loss: 0.218372\n",
      "Epoch 42, test loss: 0.217210\n",
      "Epoch 43, test loss: 0.217909\n",
      "Epoch 44, test loss: 0.220733\n",
      "Epoch 45, test loss: 0.219789\n",
      "Epoch 46, test loss: 0.219037\n",
      "Epoch 47, test loss: 0.217744\n",
      "Epoch 48, test loss: 0.218237\n",
      "Epoch 49, test loss: 0.218513\n",
      "Epoch 50, test loss: 0.216923\n",
      "Epoch 51, test loss: 0.217136\n",
      "Epoch 52, test loss: 0.218265\n",
      "Epoch 53, test loss: 0.217001\n",
      "Epoch 54, test loss: 0.221878\n",
      "Epoch 55, test loss: 0.220250\n",
      "Epoch 56, test loss: 0.218086\n",
      "Epoch 57, test loss: 0.217741\n",
      "Epoch 58, test loss: 0.221012\n",
      "Epoch 59, test loss: 0.217110\n",
      "Epoch 60, test loss: 0.217408\n",
      "Epoch 61, test loss: 0.218067\n",
      "Epoch 62, test loss: 0.218377\n",
      "Epoch 63, test loss: 0.218497\n",
      "Epoch 64, test loss: 0.225490\n",
      "Epoch 65, test loss: 0.217667\n",
      "Epoch 66, test loss: 0.217347\n",
      "Epoch 67, test loss: 0.218179\n",
      "Epoch 68, test loss: 0.219038\n",
      "Epoch 69, test loss: 0.216265\n",
      "Epoch 70, test loss: 0.217475\n",
      "Epoch 71, test loss: 0.218997\n",
      "Epoch 72, test loss: 0.219477\n",
      "Epoch 73, test loss: 0.216347\n",
      "Epoch 74, test loss: 0.216948\n",
      "Epoch 75, test loss: 0.217449\n",
      "Epoch 76, test loss: 0.222007\n",
      "Epoch 77, test loss: 0.217487\n",
      "Epoch 78, test loss: 0.218859\n",
      "Epoch 79, test loss: 0.216824\n",
      "Epoch 80, test loss: 0.217045\n",
      "Epoch 81, test loss: 0.220893\n",
      "Epoch 82, test loss: 0.217086\n",
      "Epoch 83, test loss: 0.218000\n",
      "Epoch 84, test loss: 0.217614\n",
      "Epoch 85, test loss: 0.218138\n",
      "Epoch 86, test loss: 0.227803\n",
      "Epoch 87, test loss: 0.217424\n",
      "Epoch 88, test loss: 0.217956\n",
      "Epoch 89, test loss: 0.218042\n",
      "Epoch 90, test loss: 0.224433\n",
      "Epoch 91, test loss: 0.216976\n",
      "Epoch 92, test loss: 0.217692\n",
      "Epoch 93, test loss: 0.217685\n",
      "Epoch 94, test loss: 0.216361\n",
      "Epoch 95, test loss: 0.218008\n",
      "Epoch 96, test loss: 0.219288\n",
      "Epoch 97, test loss: 0.218957\n",
      "Epoch 98, test loss: 0.218331\n",
      "Epoch 99, test loss: 0.218218\n",
      "Epoch 100, test loss: 0.217258\n",
      "Epoch 101, test loss: 0.218736\n",
      "Epoch 102, test loss: 0.220039\n",
      "Epoch 103, test loss: 0.217299\n",
      "Epoch 104, test loss: 0.216395\n",
      "Epoch 105, test loss: 0.217389\n",
      "Epoch 106, test loss: 0.217670\n",
      "Epoch 107, test loss: 0.216792\n",
      "Epoch 108, test loss: 0.217245\n",
      "Epoch 109, test loss: 0.217680\n",
      "Epoch 110, test loss: 0.218632\n",
      "Epoch 111, test loss: 0.217670\n",
      "Epoch 112, test loss: 0.216222\n",
      "Epoch 113, test loss: 0.216221\n",
      "Epoch 114, test loss: 0.221647\n",
      "Epoch 115, test loss: 0.216932\n",
      "Epoch 116, test loss: 0.217516\n",
      "Epoch 117, test loss: 0.216065\n",
      "Epoch 118, test loss: 0.216290\n",
      "Epoch 119, test loss: 0.216347\n",
      "Epoch 120, test loss: 0.217850\n",
      "Epoch 121, test loss: 0.223642\n",
      "Epoch 122, test loss: 0.216719\n",
      "Epoch 123, test loss: 0.216792\n",
      "Epoch 124, test loss: 0.218249\n",
      "Epoch 125, test loss: 0.217465\n",
      "Epoch 126, test loss: 0.218113\n",
      "Epoch 127, test loss: 0.221042\n",
      "Epoch 128, test loss: 0.217198\n",
      "Epoch 129, test loss: 0.216797\n",
      "Epoch 130, test loss: 0.218020\n",
      "Epoch 131, test loss: 0.217035\n",
      "Epoch 132, test loss: 0.218215\n",
      "Epoch 133, test loss: 0.218463\n",
      "Epoch 134, test loss: 0.218009\n",
      "Epoch 135, test loss: 0.218332\n",
      "Epoch 136, test loss: 0.219152\n",
      "Epoch 137, test loss: 0.216503\n",
      "Epoch 138, test loss: 0.217196\n",
      "Epoch 139, test loss: 0.220240\n",
      "Epoch 140, test loss: 0.218329\n",
      "Epoch 141, test loss: 0.224042\n",
      "Epoch 142, test loss: 0.216850\n",
      "Epoch 143, test loss: 0.216640\n",
      "Epoch 144, test loss: 0.217572\n",
      "Epoch 145, test loss: 0.215915\n",
      "Epoch 146, test loss: 0.218887\n",
      "Epoch 147, test loss: 0.217817\n",
      "Epoch 148, test loss: 0.217003\n",
      "Epoch 149, test loss: 0.219180\n",
      "Epoch 150, test loss: 0.215846\n",
      "Epoch 151, test loss: 0.215969\n",
      "Epoch 152, test loss: 0.217520\n",
      "Epoch 153, test loss: 0.218089\n",
      "Epoch 154, test loss: 0.218916\n",
      "Epoch 155, test loss: 0.217964\n",
      "Epoch 156, test loss: 0.221275\n",
      "Epoch 157, test loss: 0.217252\n",
      "Epoch 158, test loss: 0.216753\n",
      "Epoch 159, test loss: 0.220798\n",
      "Epoch 160, test loss: 0.216321\n",
      "Epoch 161, test loss: 0.216231\n",
      "Epoch 162, test loss: 0.216353\n",
      "Epoch 163, test loss: 0.216217\n",
      "Epoch 164, test loss: 0.220553\n",
      "Epoch 165, test loss: 0.217659\n",
      "Epoch 166, test loss: 0.216759\n",
      "Epoch 167, test loss: 0.217747\n",
      "Epoch 168, test loss: 0.218004\n",
      "Epoch 169, test loss: 0.215913\n",
      "Epoch 170, test loss: 0.216345\n",
      "Epoch 171, test loss: 0.217476\n",
      "Epoch 172, test loss: 0.216607\n",
      "Epoch 173, test loss: 0.216699\n",
      "Epoch 174, test loss: 0.218869\n",
      "Epoch 175, test loss: 0.216429\n",
      "Epoch 176, test loss: 0.218456\n",
      "Epoch 177, test loss: 0.216800\n",
      "Epoch 178, test loss: 0.217961\n",
      "Epoch 179, test loss: 0.217167\n",
      "Epoch 180, test loss: 0.217463\n",
      "Epoch 181, test loss: 0.218469\n",
      "Epoch 182, test loss: 0.217524\n",
      "Epoch 183, test loss: 0.218780\n",
      "Epoch 184, test loss: 0.217010\n",
      "Epoch 185, test loss: 0.216430\n",
      "Epoch 186, test loss: 0.216857\n",
      "Epoch 187, test loss: 0.216582\n",
      "Epoch 188, test loss: 0.218365\n",
      "Epoch 189, test loss: 0.216814\n",
      "Epoch 190, test loss: 0.216774\n",
      "Epoch 191, test loss: 0.217208\n",
      "Epoch 192, test loss: 0.216732\n",
      "Epoch 193, test loss: 0.216769\n",
      "Epoch 194, test loss: 0.221064\n",
      "Epoch 195, test loss: 0.219539\n",
      "Epoch 196, test loss: 0.217203\n",
      "Epoch 197, test loss: 0.219832\n",
      "Epoch 198, test loss: 0.217881\n",
      "Epoch 199, test loss: 0.217002\n",
      "Epoch 200, test loss: 0.218247\n",
      "Epoch 201, test loss: 0.219278\n",
      "Epoch 202, test loss: 0.218370\n",
      "Epoch 203, test loss: 0.221008\n",
      "Epoch 204, test loss: 0.216908\n",
      "Epoch 205, test loss: 0.216722\n",
      "Epoch 206, test loss: 0.220146\n",
      "Epoch 207, test loss: 0.217254\n",
      "Epoch 208, test loss: 0.216415\n",
      "Epoch 209, test loss: 0.218083\n",
      "Epoch 210, test loss: 0.216387\n",
      "Epoch 211, test loss: 0.217325\n",
      "Epoch 212, test loss: 0.217666\n",
      "Epoch 213, test loss: 0.218178\n",
      "Epoch 214, test loss: 0.217702\n",
      "Epoch 215, test loss: 0.216647\n",
      "Epoch 216, test loss: 0.218783\n",
      "Epoch 217, test loss: 0.216914\n",
      "Epoch 218, test loss: 0.221506\n",
      "Epoch 219, test loss: 0.219882\n",
      "Epoch 220, test loss: 0.218202\n",
      "Epoch 221, test loss: 0.217059\n",
      "Epoch 222, test loss: 0.217383\n",
      "Epoch 223, test loss: 0.217082\n",
      "Epoch 224, test loss: 0.217552\n",
      "Epoch 225, test loss: 0.220652\n",
      "Epoch 226, test loss: 0.218191\n",
      "Epoch 227, test loss: 0.220392\n",
      "Epoch 228, test loss: 0.220409\n",
      "Epoch 229, test loss: 0.216045\n",
      "Epoch 230, test loss: 0.216650\n",
      "Epoch 231, test loss: 0.217019\n",
      "Epoch 232, test loss: 0.221790\n",
      "Epoch 233, test loss: 0.217590\n",
      "Epoch 234, test loss: 0.216778\n",
      "Epoch 235, test loss: 0.217572\n",
      "Epoch 236, test loss: 0.217239\n",
      "Epoch 237, test loss: 0.216565\n",
      "Epoch 238, test loss: 0.219572\n",
      "Epoch 239, test loss: 0.218296\n",
      "Pretrain data: 19823659.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7717/107424\n",
      "Found 815 continuous time series\n",
      "Data shape: (115143, 6), Train/test: 115141/2\n",
      "Train test ratio: 57570.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1799\n",
      "Epoch 0, train loss: 0.218111\n",
      "Epoch 1, train loss: 0.250035\n",
      "Epoch 2, train loss: 0.178407\n",
      "Epoch 3, train loss: 0.250443\n",
      "Epoch 4, train loss: 0.252769\n",
      "Epoch 5, train loss: 0.190039\n",
      "Epoch 6, train loss: 0.276961\n",
      "Epoch 7, train loss: 0.193784\n",
      "Epoch 8, train loss: 0.234303\n",
      "Epoch 9, train loss: 0.174959\n",
      "Epoch 10, train loss: 0.250378\n",
      "Epoch 11, train loss: 0.197274\n",
      "Epoch 12, train loss: 0.149643\n",
      "Epoch 13, train loss: 0.225849\n",
      "Epoch 14, train loss: 0.247691\n",
      "Epoch 15, train loss: 0.192273\n",
      "Epoch 16, train loss: 0.206463\n",
      "Epoch 17, train loss: 0.272451\n",
      "Epoch 18, train loss: 0.197231\n",
      "Epoch 19, train loss: 0.256405\n",
      "Epoch 20, train loss: 0.230954\n",
      "Epoch 21, train loss: 0.208072\n",
      "Epoch 22, train loss: 0.213157\n",
      "Epoch 23, train loss: 0.379565\n",
      "Epoch 24, train loss: 0.213685\n",
      "Epoch 25, train loss: 0.192824\n",
      "Epoch 26, train loss: 0.239681\n",
      "Epoch 27, train loss: 0.204343\n",
      "Epoch 28, train loss: 0.217575\n",
      "Epoch 29, train loss: 0.284518\n",
      "Epoch 30, train loss: 0.262241\n",
      "Epoch 31, train loss: 0.149677\n",
      "Epoch 32, train loss: 0.161031\n",
      "Epoch 33, train loss: 0.268261\n",
      "Epoch 34, train loss: 0.160707\n",
      "Epoch 35, train loss: 0.294821\n",
      "Epoch 36, train loss: 0.212135\n",
      "Epoch 37, train loss: 0.230808\n",
      "Epoch 38, train loss: 0.203994\n",
      "Epoch 39, train loss: 0.236281\n",
      "Epoch 40, train loss: 0.248629\n",
      "Epoch 41, train loss: 0.210260\n",
      "Epoch 42, train loss: 0.212213\n",
      "Epoch 43, train loss: 0.187008\n",
      "Epoch 44, train loss: 0.201186\n",
      "Epoch 45, train loss: 0.248454\n",
      "Epoch 46, train loss: 0.166798\n",
      "Epoch 47, train loss: 0.217353\n",
      "Epoch 48, train loss: 0.243703\n",
      "Epoch 49, train loss: 0.386654\n",
      "Epoch 50, train loss: 0.278291\n",
      "Epoch 51, train loss: 0.238272\n",
      "Epoch 52, train loss: 0.238156\n",
      "Epoch 53, train loss: 0.206043\n",
      "Epoch 54, train loss: 0.291863\n",
      "Epoch 55, train loss: 0.178035\n",
      "Epoch 56, train loss: 0.223536\n",
      "Epoch 57, train loss: 0.193832\n",
      "Epoch 58, train loss: 0.171560\n",
      "Epoch 59, train loss: 0.174529\n",
      "Epoch 60, train loss: 0.259362\n",
      "Epoch 61, train loss: 0.184069\n",
      "Epoch 62, train loss: 0.191441\n",
      "Epoch 63, train loss: 0.202251\n",
      "Epoch 64, train loss: 0.182924\n",
      "Epoch 65, train loss: 0.180212\n",
      "Epoch 66, train loss: 0.299713\n",
      "Epoch 67, train loss: 0.199281\n",
      "Epoch 68, train loss: 0.179073\n",
      "Epoch 69, train loss: 0.209928\n",
      "Epoch 70, train loss: 0.202699\n",
      "Epoch 71, train loss: 0.206855\n",
      "Epoch 72, train loss: 0.188776\n",
      "Epoch 73, train loss: 0.251144\n",
      "Epoch 74, train loss: 0.208965\n",
      "Epoch 75, train loss: 0.187403\n",
      "Epoch 76, train loss: 0.181858\n",
      "Epoch 77, train loss: 0.183790\n",
      "Epoch 78, train loss: 0.166777\n",
      "Epoch 79, train loss: 0.245479\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[174. 173. 173. 172. 170. 169.]\n",
      " [173. 173. 172. 170. 169. 169.]\n",
      " [173. 172. 170. 169. 169. 168.]\n",
      " ...\n",
      " [103.  99.  95.  92.  88.  85.]\n",
      " [ 99.  95.  92.  88.  85.  84.]\n",
      " [ 95.  92.  88.  85.  84.  86.]]\n",
      "y here is\n",
      "[[165. 165. 165. 165. 165. 165.]\n",
      " [165. 165. 165. 165. 165. 165.]\n",
      " [165. 165. 165. 165. 165. 165.]\n",
      " ...\n",
      " [ 90.  90.  90.  90.  90.  90.]\n",
      " [ 86.  86.  86.  86.  86.  86.]\n",
      " [ 87.  87.  87.  87.  87.  87.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2666, 6), Train/test: 1/2665\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[142. 142. 142. 141. 139. 137.]\n",
      " [142. 142. 141. 139. 137. 136.]\n",
      " [142. 141. 139. 137. 136. 137.]\n",
      " ...\n",
      " [158. 159. 161. 163. 166. 168.]\n",
      " [159. 161. 163. 166. 168. 167.]\n",
      " [161. 163. 166. 168. 167. 168.]]\n",
      "y here is\n",
      "[[127. 127. 127. 127. 127. 127.]\n",
      " [127. 127. 127. 127. 127. 127.]\n",
      " [126. 126. 126. 126. 126. 126.]\n",
      " ...\n",
      " [173. 173. 173. 173. 173. 173.]\n",
      " [174. 174. 174. 174. 174. 174.]\n",
      " [174. 174. 174. 174. 174. 174.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 568/10015\n",
      "Found 27 continuous time series\n",
      "Data shape: (10585, 6), Train/test: 10583/2\n",
      "Train test ratio: 5291.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930FB100>\n",
      "Epoch 0, test loss: 0.171937\n",
      "Epoch 1, test loss: 0.172095\n",
      "Epoch 2, test loss: 0.171482\n",
      "Epoch 3, test loss: 0.172500\n",
      "Epoch 4, test loss: 0.171612\n",
      "Epoch 5, test loss: 0.171470\n",
      "Epoch 6, test loss: 0.172492\n",
      "Epoch 7, test loss: 0.171329\n",
      "Epoch 8, test loss: 0.174493\n",
      "Epoch 9, test loss: 0.172173\n",
      "Epoch 10, test loss: 0.171320\n",
      "Epoch 11, test loss: 0.172912\n",
      "Epoch 12, test loss: 0.172502\n",
      "Epoch 13, test loss: 0.171290\n",
      "Epoch 14, test loss: 0.172160\n",
      "Epoch 15, test loss: 0.174432\n",
      "Epoch 16, test loss: 0.170848\n",
      "Epoch 17, test loss: 0.171065\n",
      "Epoch 18, test loss: 0.170963\n",
      "Epoch 19, test loss: 0.174238\n",
      "Epoch 20, test loss: 0.171110\n",
      "Epoch 21, test loss: 0.171048\n",
      "Epoch 22, test loss: 0.174261\n",
      "Epoch 23, test loss: 0.172472\n",
      "Epoch 24, test loss: 0.172273\n",
      "Epoch 25, test loss: 0.171234\n",
      "Epoch 26, test loss: 0.171196\n",
      "Epoch 27, test loss: 0.171731\n",
      "Epoch 28, test loss: 0.179043\n",
      "Epoch 29, test loss: 0.172062\n",
      "Epoch 30, test loss: 0.170771\n",
      "Epoch 31, test loss: 0.173281\n",
      "Epoch 32, test loss: 0.171064\n",
      "Epoch 33, test loss: 0.171113\n",
      "Epoch 34, test loss: 0.170832\n",
      "Epoch 35, test loss: 0.172653\n",
      "Epoch 36, test loss: 0.170687\n",
      "Epoch 37, test loss: 0.171548\n",
      "Epoch 38, test loss: 0.171391\n",
      "Epoch 39, test loss: 0.171262\n",
      "Epoch 40, test loss: 0.174862\n",
      "Epoch 41, test loss: 0.175502\n",
      "Epoch 42, test loss: 0.173593\n",
      "Epoch 43, test loss: 0.173659\n",
      "Epoch 44, test loss: 0.174791\n",
      "Epoch 45, test loss: 0.172342\n",
      "Epoch 46, test loss: 0.172269\n",
      "Epoch 47, test loss: 0.171880\n",
      "Epoch 48, test loss: 0.172525\n",
      "Epoch 49, test loss: 0.173069\n",
      "Epoch 50, test loss: 0.170813\n",
      "Epoch 51, test loss: 0.171961\n",
      "Epoch 52, test loss: 0.178988\n",
      "Epoch 53, test loss: 0.172348\n",
      "Epoch 54, test loss: 0.172581\n",
      "Epoch 55, test loss: 0.171431\n",
      "Epoch 56, test loss: 0.172542\n",
      "Epoch 57, test loss: 0.171598\n",
      "Epoch 58, test loss: 0.171544\n",
      "Epoch 59, test loss: 0.172752\n",
      "Epoch 60, test loss: 0.171563\n",
      "Epoch 61, test loss: 0.171235\n",
      "Epoch 62, test loss: 0.170707\n",
      "Epoch 63, test loss: 0.171325\n",
      "Epoch 64, test loss: 0.170835\n",
      "Epoch 65, test loss: 0.173365\n",
      "Epoch 66, test loss: 0.172607\n",
      "Epoch 67, test loss: 0.172860\n",
      "Epoch 68, test loss: 0.171952\n",
      "Epoch 69, test loss: 0.170757\n",
      "Epoch 70, test loss: 0.172181\n",
      "Epoch 71, test loss: 0.173085\n",
      "Epoch 72, test loss: 0.174772\n",
      "Epoch 73, test loss: 0.171643\n",
      "Epoch 74, test loss: 0.172977\n",
      "Epoch 75, test loss: 0.172546\n",
      "Epoch 76, test loss: 0.170972\n",
      "Epoch 77, test loss: 0.173485\n",
      "Epoch 78, test loss: 0.172345\n",
      "Epoch 79, test loss: 0.173243\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930FB100>\n",
      "Epoch 0, test loss: 0.171177\n",
      "Epoch 1, test loss: 0.171000\n",
      "Epoch 2, test loss: 0.171046\n",
      "Epoch 3, test loss: 0.174963\n",
      "Epoch 4, test loss: 0.171809\n",
      "Epoch 5, test loss: 0.170985\n",
      "Epoch 6, test loss: 0.172288\n",
      "Epoch 7, test loss: 0.171354\n",
      "Epoch 8, test loss: 0.171586\n",
      "Epoch 9, test loss: 0.171270\n",
      "Epoch 10, test loss: 0.171064\n",
      "Epoch 11, test loss: 0.172518\n",
      "Epoch 12, test loss: 0.170755\n",
      "Epoch 13, test loss: 0.170828\n",
      "Epoch 14, test loss: 0.171629\n",
      "Epoch 15, test loss: 0.170761\n",
      "Epoch 16, test loss: 0.171082\n",
      "Epoch 17, test loss: 0.171392\n",
      "Epoch 18, test loss: 0.171185\n",
      "Epoch 19, test loss: 0.170952\n",
      "Epoch 20, test loss: 0.171393\n",
      "Epoch 21, test loss: 0.170845\n",
      "Epoch 22, test loss: 0.171280\n",
      "Epoch 23, test loss: 0.171728\n",
      "Epoch 24, test loss: 0.171085\n",
      "Epoch 25, test loss: 0.171137\n",
      "Epoch 26, test loss: 0.171132\n",
      "Epoch 27, test loss: 0.171166\n",
      "Epoch 28, test loss: 0.172040\n",
      "Epoch 29, test loss: 0.171463\n",
      "Epoch 30, test loss: 0.172566\n",
      "Epoch 31, test loss: 0.171216\n",
      "Epoch 32, test loss: 0.172827\n",
      "Epoch 33, test loss: 0.171049\n",
      "Epoch 34, test loss: 0.172211\n",
      "Epoch 35, test loss: 0.171264\n",
      "Epoch 36, test loss: 0.171302\n",
      "Epoch 37, test loss: 0.171672\n",
      "Epoch 38, test loss: 0.171274\n",
      "Epoch 39, test loss: 0.171296\n",
      "Epoch 40, test loss: 0.170492\n",
      "Epoch 41, test loss: 0.171588\n",
      "Epoch 42, test loss: 0.170843\n",
      "Epoch 43, test loss: 0.171640\n",
      "Epoch 44, test loss: 0.172009\n",
      "Epoch 45, test loss: 0.171353\n",
      "Epoch 46, test loss: 0.171269\n",
      "Epoch 47, test loss: 0.171633\n",
      "Epoch 48, test loss: 0.171321\n",
      "Epoch 49, test loss: 0.171421\n",
      "Epoch 50, test loss: 0.170855\n",
      "Epoch 51, test loss: 0.171045\n",
      "Epoch 52, test loss: 0.170826\n",
      "Epoch 53, test loss: 0.172635\n",
      "Epoch 54, test loss: 0.172523\n",
      "Epoch 55, test loss: 0.170959\n",
      "Epoch 56, test loss: 0.171661\n",
      "Epoch 57, test loss: 0.170757\n",
      "Epoch 58, test loss: 0.171728\n",
      "Epoch 59, test loss: 0.171658\n",
      "Epoch 60, test loss: 0.171115\n",
      "Epoch 61, test loss: 0.171017\n",
      "Epoch 62, test loss: 0.171214\n",
      "Epoch 63, test loss: 0.171742\n",
      "Epoch 64, test loss: 0.173696\n",
      "Epoch 65, test loss: 0.170993\n",
      "Epoch 66, test loss: 0.172226\n",
      "Epoch 67, test loss: 0.171687\n",
      "Epoch 68, test loss: 0.172203\n",
      "Epoch 69, test loss: 0.172230\n",
      "Epoch 70, test loss: 0.171042\n",
      "Epoch 71, test loss: 0.171146\n",
      "Epoch 72, test loss: 0.172728\n",
      "Epoch 73, test loss: 0.171503\n",
      "Epoch 74, test loss: 0.172115\n",
      "Epoch 75, test loss: 0.171891\n",
      "Epoch 76, test loss: 0.172357\n",
      "Epoch 77, test loss: 0.171024\n",
      "Epoch 78, test loss: 0.171415\n",
      "Epoch 79, test loss: 0.171549\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2930FB100>\n",
      "Epoch 0, test loss: 0.210870\n",
      "Epoch 1, test loss: 0.190456\n",
      "Epoch 2, test loss: 0.187898\n",
      "Epoch 3, test loss: 0.184068\n",
      "Epoch 4, test loss: 0.182301\n",
      "Epoch 5, test loss: 0.181225\n",
      "Epoch 6, test loss: 0.179597\n",
      "Epoch 7, test loss: 0.178963\n",
      "Epoch 8, test loss: 0.178989\n",
      "Epoch 9, test loss: 0.179460\n",
      "Epoch 10, test loss: 0.179320\n",
      "Epoch 11, test loss: 0.177948\n",
      "Epoch 12, test loss: 0.179699\n",
      "Epoch 13, test loss: 0.178313\n",
      "Epoch 14, test loss: 0.177822\n",
      "Epoch 15, test loss: 0.176943\n",
      "Epoch 16, test loss: 0.179876\n",
      "Epoch 17, test loss: 0.177401\n",
      "Epoch 18, test loss: 0.175973\n",
      "Epoch 19, test loss: 0.175865\n",
      "Epoch 20, test loss: 0.175521\n",
      "Epoch 21, test loss: 0.177244\n",
      "Epoch 22, test loss: 0.175315\n",
      "Epoch 23, test loss: 0.174936\n",
      "Epoch 24, test loss: 0.174407\n",
      "Epoch 25, test loss: 0.174294\n",
      "Epoch 26, test loss: 0.175059\n",
      "Epoch 27, test loss: 0.174601\n",
      "Epoch 28, test loss: 0.174254\n",
      "Epoch 29, test loss: 0.173921\n",
      "Epoch 30, test loss: 0.177718\n",
      "Epoch 31, test loss: 0.173721\n",
      "Epoch 32, test loss: 0.173861\n",
      "Epoch 33, test loss: 0.173297\n",
      "Epoch 34, test loss: 0.173660\n",
      "Epoch 35, test loss: 0.173222\n",
      "Epoch 36, test loss: 0.174267\n",
      "Epoch 37, test loss: 0.172919\n",
      "Epoch 38, test loss: 0.173158\n",
      "Epoch 39, test loss: 0.174381\n",
      "Epoch 40, test loss: 0.172437\n",
      "Epoch 41, test loss: 0.174283\n",
      "Epoch 42, test loss: 0.174233\n",
      "Epoch 43, test loss: 0.173045\n",
      "Epoch 44, test loss: 0.172757\n",
      "Epoch 45, test loss: 0.174206\n",
      "Epoch 46, test loss: 0.173097\n",
      "Epoch 47, test loss: 0.174896\n",
      "Epoch 48, test loss: 0.172483\n",
      "Epoch 49, test loss: 0.172606\n",
      "Epoch 50, test loss: 0.172333\n",
      "Epoch 51, test loss: 0.172464\n",
      "Epoch 52, test loss: 0.172441\n",
      "Epoch 53, test loss: 0.173478\n",
      "Epoch 54, test loss: 0.172348\n",
      "Epoch 55, test loss: 0.172998\n",
      "Epoch 56, test loss: 0.173186\n",
      "Epoch 57, test loss: 0.173169\n",
      "Epoch 58, test loss: 0.172813\n",
      "Epoch 59, test loss: 0.172934\n",
      "Epoch 60, test loss: 0.172831\n",
      "Epoch 61, test loss: 0.172579\n",
      "Epoch 62, test loss: 0.172393\n",
      "Epoch 63, test loss: 0.174599\n",
      "Epoch 64, test loss: 0.172494\n",
      "Epoch 65, test loss: 0.172483\n",
      "Epoch 66, test loss: 0.173205\n",
      "Epoch 67, test loss: 0.172771\n",
      "Epoch 68, test loss: 0.172468\n",
      "Epoch 69, test loss: 0.172126\n",
      "Epoch 70, test loss: 0.172935\n",
      "Epoch 71, test loss: 0.172536\n",
      "Epoch 72, test loss: 0.172622\n",
      "Epoch 73, test loss: 0.172027\n",
      "Epoch 74, test loss: 0.175134\n",
      "Epoch 75, test loss: 0.173412\n",
      "Epoch 76, test loss: 0.172579\n",
      "Epoch 77, test loss: 0.172595\n",
      "Epoch 78, test loss: 0.173653\n",
      "Epoch 79, test loss: 0.173238\n",
      "Epoch 80, test loss: 0.173183\n",
      "Epoch 81, test loss: 0.172487\n",
      "Epoch 82, test loss: 0.172647\n",
      "Epoch 83, test loss: 0.172975\n",
      "Epoch 84, test loss: 0.171993\n",
      "Epoch 85, test loss: 0.173609\n",
      "Epoch 86, test loss: 0.172356\n",
      "Epoch 87, test loss: 0.172182\n",
      "Epoch 88, test loss: 0.174431\n",
      "Epoch 89, test loss: 0.172462\n",
      "Epoch 90, test loss: 0.173727\n",
      "Epoch 91, test loss: 0.173752\n",
      "Epoch 92, test loss: 0.173767\n",
      "Epoch 93, test loss: 0.172267\n",
      "Epoch 94, test loss: 0.172468\n",
      "Epoch 95, test loss: 0.172104\n",
      "Epoch 96, test loss: 0.172448\n",
      "Epoch 97, test loss: 0.172336\n",
      "Epoch 98, test loss: 0.172279\n",
      "Epoch 99, test loss: 0.172205\n",
      "Epoch 100, test loss: 0.173222\n",
      "Epoch 101, test loss: 0.172011\n",
      "Epoch 102, test loss: 0.172743\n",
      "Epoch 103, test loss: 0.172509\n",
      "Epoch 104, test loss: 0.172975\n",
      "Epoch 105, test loss: 0.173120\n",
      "Epoch 106, test loss: 0.172264\n",
      "Epoch 107, test loss: 0.173320\n",
      "Epoch 108, test loss: 0.173751\n",
      "Epoch 109, test loss: 0.173003\n",
      "Epoch 110, test loss: 0.175252\n",
      "Epoch 111, test loss: 0.173050\n",
      "Epoch 112, test loss: 0.172483\n",
      "Epoch 113, test loss: 0.172672\n",
      "Epoch 114, test loss: 0.172848\n",
      "Epoch 115, test loss: 0.171950\n",
      "Epoch 116, test loss: 0.171692\n",
      "Epoch 117, test loss: 0.172568\n",
      "Epoch 118, test loss: 0.172306\n",
      "Epoch 119, test loss: 0.171586\n",
      "Epoch 120, test loss: 0.174663\n",
      "Epoch 121, test loss: 0.172819\n",
      "Epoch 122, test loss: 0.172538\n",
      "Epoch 123, test loss: 0.173427\n",
      "Epoch 124, test loss: 0.173126\n",
      "Epoch 125, test loss: 0.172859\n",
      "Epoch 126, test loss: 0.172334\n",
      "Epoch 127, test loss: 0.171604\n",
      "Epoch 128, test loss: 0.172403\n",
      "Epoch 129, test loss: 0.172785\n",
      "Epoch 130, test loss: 0.174937\n",
      "Epoch 131, test loss: 0.172044\n",
      "Epoch 132, test loss: 0.171973\n",
      "Epoch 133, test loss: 0.172621\n",
      "Epoch 134, test loss: 0.172446\n",
      "Epoch 135, test loss: 0.172274\n",
      "Epoch 136, test loss: 0.172224\n",
      "Epoch 137, test loss: 0.174404\n",
      "Epoch 138, test loss: 0.172009\n",
      "Epoch 139, test loss: 0.172561\n",
      "Epoch 140, test loss: 0.171542\n",
      "Epoch 141, test loss: 0.172025\n",
      "Epoch 142, test loss: 0.174158\n",
      "Epoch 143, test loss: 0.172292\n",
      "Epoch 144, test loss: 0.171674\n",
      "Epoch 145, test loss: 0.172192\n",
      "Epoch 146, test loss: 0.174607\n",
      "Epoch 147, test loss: 0.171529\n",
      "Epoch 148, test loss: 0.173293\n",
      "Epoch 149, test loss: 0.171819\n",
      "Epoch 150, test loss: 0.171843\n",
      "Epoch 151, test loss: 0.173945\n",
      "Epoch 152, test loss: 0.171914\n",
      "Epoch 153, test loss: 0.173964\n",
      "Epoch 154, test loss: 0.172043\n",
      "Epoch 155, test loss: 0.172964\n",
      "Epoch 156, test loss: 0.172129\n",
      "Epoch 157, test loss: 0.171927\n",
      "Epoch 158, test loss: 0.171487\n",
      "Epoch 159, test loss: 0.172744\n",
      "Epoch 160, test loss: 0.172945\n",
      "Epoch 161, test loss: 0.171522\n",
      "Epoch 162, test loss: 0.173057\n",
      "Epoch 163, test loss: 0.171665\n",
      "Epoch 164, test loss: 0.171585\n",
      "Epoch 165, test loss: 0.172031\n",
      "Epoch 166, test loss: 0.171962\n",
      "Epoch 167, test loss: 0.172267\n",
      "Epoch 168, test loss: 0.172108\n",
      "Epoch 169, test loss: 0.171334\n",
      "Epoch 170, test loss: 0.172157\n",
      "Epoch 171, test loss: 0.172037\n",
      "Epoch 172, test loss: 0.171513\n",
      "Epoch 173, test loss: 0.172090\n",
      "Epoch 174, test loss: 0.171556\n",
      "Epoch 175, test loss: 0.171468\n",
      "Epoch 176, test loss: 0.172430\n",
      "Epoch 177, test loss: 0.171447\n",
      "Epoch 178, test loss: 0.171696\n",
      "Epoch 179, test loss: 0.173233\n",
      "Epoch 180, test loss: 0.171765\n",
      "Epoch 181, test loss: 0.173210\n",
      "Epoch 182, test loss: 0.171586\n",
      "Epoch 183, test loss: 0.171931\n",
      "Epoch 184, test loss: 0.171918\n",
      "Epoch 185, test loss: 0.172707\n",
      "Epoch 186, test loss: 0.172019\n",
      "Epoch 187, test loss: 0.173168\n",
      "Epoch 188, test loss: 0.171478\n",
      "Epoch 189, test loss: 0.173852\n",
      "Epoch 190, test loss: 0.171872\n",
      "Epoch 191, test loss: 0.172892\n",
      "Epoch 192, test loss: 0.171679\n",
      "Epoch 193, test loss: 0.172739\n",
      "Epoch 194, test loss: 0.173097\n",
      "Epoch 195, test loss: 0.171993\n",
      "Epoch 196, test loss: 0.171531\n",
      "Epoch 197, test loss: 0.171686\n",
      "Epoch 198, test loss: 0.172030\n",
      "Epoch 199, test loss: 0.172770\n",
      "Epoch 200, test loss: 0.173442\n",
      "Epoch 201, test loss: 0.171371\n",
      "Epoch 202, test loss: 0.171770\n",
      "Epoch 203, test loss: 0.171142\n",
      "Epoch 204, test loss: 0.172211\n",
      "Epoch 205, test loss: 0.171443\n",
      "Epoch 206, test loss: 0.172986\n",
      "Epoch 207, test loss: 0.174136\n",
      "Epoch 208, test loss: 0.171550\n",
      "Epoch 209, test loss: 0.171530\n",
      "Epoch 210, test loss: 0.172059\n",
      "Epoch 211, test loss: 0.172124\n",
      "Epoch 212, test loss: 0.171614\n",
      "Epoch 213, test loss: 0.171696\n",
      "Epoch 214, test loss: 0.172587\n",
      "Epoch 215, test loss: 0.172487\n",
      "Epoch 216, test loss: 0.171793\n",
      "Epoch 217, test loss: 0.171368\n",
      "Epoch 218, test loss: 0.171299\n",
      "Epoch 219, test loss: 0.171335\n",
      "Epoch 220, test loss: 0.171713\n",
      "Epoch 221, test loss: 0.171487\n",
      "Epoch 222, test loss: 0.172154\n",
      "Epoch 223, test loss: 0.171762\n",
      "Epoch 224, test loss: 0.174418\n",
      "Epoch 225, test loss: 0.171921\n",
      "Epoch 226, test loss: 0.171514\n",
      "Epoch 227, test loss: 0.172694\n",
      "Epoch 228, test loss: 0.173867\n",
      "Epoch 229, test loss: 0.171173\n",
      "Epoch 230, test loss: 0.171787\n",
      "Epoch 231, test loss: 0.171449\n",
      "Epoch 232, test loss: 0.172877\n",
      "Epoch 233, test loss: 0.173576\n",
      "Epoch 234, test loss: 0.171597\n",
      "Epoch 235, test loss: 0.171720\n",
      "Epoch 236, test loss: 0.172246\n",
      "Epoch 237, test loss: 0.171501\n",
      "Epoch 238, test loss: 0.174550\n",
      "Epoch 239, test loss: 0.172673\n",
      "Reading 44 segments\n",
      "Pretrain data: 19625082.0\n",
      "Building dataset, requesting data from 0 to 798\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7135/103752\n",
      "Found 798 continuous time series\n",
      "Data shape: (110889, 12), Train/test: 110887/2\n",
      "Train test ratio: 55443.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1732\n",
      "Epoch 0, train loss: 0.245792\n",
      "Epoch 1, train loss: 0.216620\n",
      "Epoch 2, train loss: 0.225514\n",
      "Epoch 3, train loss: 0.231546\n",
      "Epoch 4, train loss: 0.206300\n",
      "Epoch 5, train loss: 0.234024\n",
      "Epoch 6, train loss: 0.212472\n",
      "Epoch 7, train loss: 0.217756\n",
      "Epoch 8, train loss: 0.235992\n",
      "Epoch 9, train loss: 0.196717\n",
      "Epoch 10, train loss: 0.210196\n",
      "Epoch 11, train loss: 0.234287\n",
      "Epoch 12, train loss: 0.198875\n",
      "Epoch 13, train loss: 0.208161\n",
      "Epoch 14, train loss: 0.184288\n",
      "Epoch 15, train loss: 0.253083\n",
      "Epoch 16, train loss: 0.176124\n",
      "Epoch 17, train loss: 0.215491\n",
      "Epoch 18, train loss: 0.179268\n",
      "Epoch 19, train loss: 0.250669\n",
      "Epoch 20, train loss: 0.196009\n",
      "Epoch 21, train loss: 0.177046\n",
      "Epoch 22, train loss: 0.202515\n",
      "Epoch 23, train loss: 0.216505\n",
      "Epoch 24, train loss: 0.227074\n",
      "Epoch 25, train loss: 0.216237\n",
      "Epoch 26, train loss: 0.151203\n",
      "Epoch 27, train loss: 0.207399\n",
      "Epoch 28, train loss: 0.193896\n",
      "Epoch 29, train loss: 0.178988\n",
      "Epoch 30, train loss: 0.238381\n",
      "Epoch 31, train loss: 0.233060\n",
      "Epoch 32, train loss: 0.259060\n",
      "Epoch 33, train loss: 0.229091\n",
      "Epoch 34, train loss: 0.229994\n",
      "Epoch 35, train loss: 0.189749\n",
      "Epoch 36, train loss: 0.203608\n",
      "Epoch 37, train loss: 0.277005\n",
      "Epoch 38, train loss: 0.234589\n",
      "Epoch 39, train loss: 0.228677\n",
      "Epoch 40, train loss: 0.246965\n",
      "Epoch 41, train loss: 0.199405\n",
      "Epoch 42, train loss: 0.206198\n",
      "Epoch 43, train loss: 0.169501\n",
      "Epoch 44, train loss: 0.267758\n",
      "Epoch 45, train loss: 0.191659\n",
      "Epoch 46, train loss: 0.186493\n",
      "Epoch 47, train loss: 0.200589\n",
      "Epoch 48, train loss: 0.207890\n",
      "Epoch 49, train loss: 0.220926\n",
      "Epoch 50, train loss: 0.174204\n",
      "Epoch 51, train loss: 0.227773\n",
      "Epoch 52, train loss: 0.184981\n",
      "Epoch 53, train loss: 0.206859\n",
      "Epoch 54, train loss: 0.202787\n",
      "Epoch 55, train loss: 0.189391\n",
      "Epoch 56, train loss: 0.205430\n",
      "Epoch 57, train loss: 0.224189\n",
      "Epoch 58, train loss: 0.188072\n",
      "Epoch 59, train loss: 0.189040\n",
      "Epoch 60, train loss: 0.209161\n",
      "Epoch 61, train loss: 0.184897\n",
      "Epoch 62, train loss: 0.195703\n",
      "Epoch 63, train loss: 0.200500\n",
      "Epoch 64, train loss: 0.179624\n",
      "Epoch 65, train loss: 0.202944\n",
      "Epoch 66, train loss: 0.109913\n",
      "Epoch 67, train loss: 0.255907\n",
      "Epoch 68, train loss: 0.323004\n",
      "Epoch 69, train loss: 0.153260\n",
      "Epoch 70, train loss: 0.200201\n",
      "Epoch 71, train loss: 0.228145\n",
      "Epoch 72, train loss: 0.175456\n",
      "Epoch 73, train loss: 0.178454\n",
      "Epoch 74, train loss: 0.187310\n",
      "Epoch 75, train loss: 0.154139\n",
      "Epoch 76, train loss: 0.185617\n",
      "Epoch 77, train loss: 0.196083\n",
      "Epoch 78, train loss: 0.202018\n",
      "Epoch 79, train loss: 0.207283\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "x here is\n",
      "[[179. 183. 187. ... 211. 211. 215.]\n",
      " [183. 187. 191. ... 211. 215. 225.]\n",
      " [187. 191. 195. ... 215. 225. 233.]\n",
      " ...\n",
      " [179. 181. 182. ... 186. 186. 187.]\n",
      " [181. 182. 184. ... 186. 187. 188.]\n",
      " [182. 184. 187. ... 187. 188. 187.]]\n",
      "y here is\n",
      "[[248. 248. 248. ... 248. 248. 248.]\n",
      " [250. 250. 250. ... 250. 250. 250.]\n",
      " [252. 252. 252. ... 252. 252. 252.]\n",
      " ...\n",
      " [182. 182. 182. ... 182. 182. 182.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " [177. 177. 177. ... 177. 177. 177.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (2310, 12), Train/test: 1/2309\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "x here is\n",
      "[[101.  98. 104. ... 145. 148. 151.]\n",
      " [ 98. 104. 112. ... 148. 151. 150.]\n",
      " [104. 112. 120. ... 151. 150. 124.]\n",
      " ...\n",
      " [143. 144. 145. ... 149. 150. 152.]\n",
      " [144. 145. 147. ... 150. 152. 155.]\n",
      " [145. 147. 148. ... 152. 155. 156.]]\n",
      "y here is\n",
      "[[115. 115. 115. ... 115. 115. 115.]\n",
      " [111. 111. 111. ... 111. 111. 111.]\n",
      " [109. 109. 109. ... 109. 109. 109.]\n",
      " ...\n",
      " [168. 168. 168. ... 168. 168. 168.]\n",
      " [172. 172. 172. ... 172. 172. 172.]\n",
      " [176. 176. 176. ... 176. 176. 176.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 773/9273\n",
      "Found 44 continuous time series\n",
      "Data shape: (10048, 12), Train/test: 10046/2\n",
      "Train test ratio: 5023.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701569E0>\n",
      "Epoch 0, test loss: 0.202661\n",
      "Epoch 1, test loss: 0.193324\n",
      "Epoch 2, test loss: 0.190962\n",
      "Epoch 3, test loss: 0.192725\n",
      "Epoch 4, test loss: 0.191943\n",
      "Epoch 5, test loss: 0.190581\n",
      "Epoch 6, test loss: 0.193985\n",
      "Epoch 7, test loss: 0.191008\n",
      "Epoch 8, test loss: 0.197346\n",
      "Epoch 9, test loss: 0.196799\n",
      "Epoch 10, test loss: 0.190446\n",
      "Epoch 11, test loss: 0.193684\n",
      "Epoch 12, test loss: 0.189924\n",
      "Epoch 13, test loss: 0.190661\n",
      "Epoch 14, test loss: 0.190196\n",
      "Epoch 15, test loss: 0.192499\n",
      "Epoch 16, test loss: 0.192697\n",
      "Epoch 17, test loss: 0.189531\n",
      "Epoch 18, test loss: 0.190083\n",
      "Epoch 19, test loss: 0.196932\n",
      "Epoch 20, test loss: 0.201839\n",
      "Epoch 21, test loss: 0.194732\n",
      "Epoch 22, test loss: 0.191243\n",
      "Epoch 23, test loss: 0.197035\n",
      "Epoch 24, test loss: 0.193027\n",
      "Epoch 25, test loss: 0.197737\n",
      "Epoch 26, test loss: 0.190478\n",
      "Epoch 27, test loss: 0.190488\n",
      "Epoch 28, test loss: 0.197747\n",
      "Epoch 29, test loss: 0.193534\n",
      "Epoch 30, test loss: 0.190436\n",
      "Epoch 31, test loss: 0.191477\n",
      "Epoch 32, test loss: 0.190895\n",
      "Epoch 33, test loss: 0.209360\n",
      "Epoch 34, test loss: 0.194071\n",
      "Epoch 35, test loss: 0.190743\n",
      "Epoch 36, test loss: 0.197209\n",
      "Epoch 37, test loss: 0.196270\n",
      "Epoch 38, test loss: 0.189913\n",
      "Epoch 39, test loss: 0.193143\n",
      "Epoch 40, test loss: 0.190774\n",
      "Epoch 41, test loss: 0.190869\n",
      "Epoch 42, test loss: 0.192671\n",
      "Epoch 43, test loss: 0.201223\n",
      "Epoch 44, test loss: 0.194459\n",
      "Epoch 45, test loss: 0.194172\n",
      "Epoch 46, test loss: 0.190874\n",
      "Epoch 47, test loss: 0.190251\n",
      "Epoch 48, test loss: 0.202135\n",
      "Epoch 49, test loss: 0.198018\n",
      "Epoch 50, test loss: 0.193883\n",
      "Epoch 51, test loss: 0.193969\n",
      "Epoch 52, test loss: 0.189823\n",
      "Epoch 53, test loss: 0.190387\n",
      "Epoch 54, test loss: 0.190813\n",
      "Epoch 55, test loss: 0.190340\n",
      "Epoch 56, test loss: 0.189734\n",
      "Epoch 57, test loss: 0.190146\n",
      "Epoch 58, test loss: 0.193469\n",
      "Epoch 59, test loss: 0.190299\n",
      "Epoch 60, test loss: 0.190053\n",
      "Epoch 61, test loss: 0.191652\n",
      "Epoch 62, test loss: 0.191473\n",
      "Epoch 63, test loss: 0.190103\n",
      "Epoch 64, test loss: 0.190488\n",
      "Epoch 65, test loss: 0.202469\n",
      "Epoch 66, test loss: 0.197457\n",
      "Epoch 67, test loss: 0.195475\n",
      "Epoch 68, test loss: 0.193259\n",
      "Epoch 69, test loss: 0.191155\n",
      "Epoch 70, test loss: 0.200523\n",
      "Epoch 71, test loss: 0.192205\n",
      "Epoch 72, test loss: 0.190644\n",
      "Epoch 73, test loss: 0.190351\n",
      "Epoch 74, test loss: 0.190706\n",
      "Epoch 75, test loss: 0.195931\n",
      "Epoch 76, test loss: 0.192076\n",
      "Epoch 77, test loss: 0.190173\n",
      "Epoch 78, test loss: 0.197317\n",
      "Epoch 79, test loss: 0.197802\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701569E0>\n",
      "Epoch 0, test loss: 0.195074\n",
      "Epoch 1, test loss: 0.190018\n",
      "Epoch 2, test loss: 0.193287\n",
      "Epoch 3, test loss: 0.190070\n",
      "Epoch 4, test loss: 0.189938\n",
      "Epoch 5, test loss: 0.190901\n",
      "Epoch 6, test loss: 0.190180\n",
      "Epoch 7, test loss: 0.190117\n",
      "Epoch 8, test loss: 0.189415\n",
      "Epoch 9, test loss: 0.189872\n",
      "Epoch 10, test loss: 0.194035\n",
      "Epoch 11, test loss: 0.190163\n",
      "Epoch 12, test loss: 0.190028\n",
      "Epoch 13, test loss: 0.190602\n",
      "Epoch 14, test loss: 0.192240\n",
      "Epoch 15, test loss: 0.195100\n",
      "Epoch 16, test loss: 0.192351\n",
      "Epoch 17, test loss: 0.194042\n",
      "Epoch 18, test loss: 0.191889\n",
      "Epoch 19, test loss: 0.206740\n",
      "Epoch 20, test loss: 0.191271\n",
      "Epoch 21, test loss: 0.190457\n",
      "Epoch 22, test loss: 0.191105\n",
      "Epoch 23, test loss: 0.195550\n",
      "Epoch 24, test loss: 0.195261\n",
      "Epoch 25, test loss: 0.191070\n",
      "Epoch 26, test loss: 0.198334\n",
      "Epoch 27, test loss: 0.191214\n",
      "Epoch 28, test loss: 0.192245\n",
      "Epoch 29, test loss: 0.196031\n",
      "Epoch 30, test loss: 0.192465\n",
      "Epoch 31, test loss: 0.191597\n",
      "Epoch 32, test loss: 0.191252\n",
      "Epoch 33, test loss: 0.192557\n",
      "Epoch 34, test loss: 0.190974\n",
      "Epoch 35, test loss: 0.192158\n",
      "Epoch 36, test loss: 0.190310\n",
      "Epoch 37, test loss: 0.193279\n",
      "Epoch 38, test loss: 0.191955\n",
      "Epoch 39, test loss: 0.196846\n",
      "Epoch 40, test loss: 0.190322\n",
      "Epoch 41, test loss: 0.192331\n",
      "Epoch 42, test loss: 0.193840\n",
      "Epoch 43, test loss: 0.190757\n",
      "Epoch 44, test loss: 0.197966\n",
      "Epoch 45, test loss: 0.191393\n",
      "Epoch 46, test loss: 0.192453\n",
      "Epoch 47, test loss: 0.191286\n",
      "Epoch 48, test loss: 0.191944\n",
      "Epoch 49, test loss: 0.190583\n",
      "Epoch 50, test loss: 0.190930\n",
      "Epoch 51, test loss: 0.190609\n",
      "Epoch 52, test loss: 0.193077\n",
      "Epoch 53, test loss: 0.190995\n",
      "Epoch 54, test loss: 0.191671\n",
      "Epoch 55, test loss: 0.193058\n",
      "Epoch 56, test loss: 0.191189\n",
      "Epoch 57, test loss: 0.191606\n",
      "Epoch 58, test loss: 0.194938\n",
      "Epoch 59, test loss: 0.193319\n",
      "Epoch 60, test loss: 0.193172\n",
      "Epoch 61, test loss: 0.197023\n",
      "Epoch 62, test loss: 0.195866\n",
      "Epoch 63, test loss: 0.191975\n",
      "Epoch 64, test loss: 0.194032\n",
      "Epoch 65, test loss: 0.193567\n",
      "Epoch 66, test loss: 0.191758\n",
      "Epoch 67, test loss: 0.190533\n",
      "Epoch 68, test loss: 0.193329\n",
      "Epoch 69, test loss: 0.193685\n",
      "Epoch 70, test loss: 0.192423\n",
      "Epoch 71, test loss: 0.199667\n",
      "Epoch 72, test loss: 0.191154\n",
      "Epoch 73, test loss: 0.190500\n",
      "Epoch 74, test loss: 0.191233\n",
      "Epoch 75, test loss: 0.190727\n",
      "Epoch 76, test loss: 0.193321\n",
      "Epoch 77, test loss: 0.192944\n",
      "Epoch 78, test loss: 0.191229\n",
      "Epoch 79, test loss: 0.190810\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701569E0>\n",
      "Epoch 0, test loss: 0.267841\n",
      "Epoch 1, test loss: 0.235011\n",
      "Epoch 2, test loss: 0.219214\n",
      "Epoch 3, test loss: 0.206924\n",
      "Epoch 4, test loss: 0.199197\n",
      "Epoch 5, test loss: 0.198777\n",
      "Epoch 6, test loss: 0.197537\n",
      "Epoch 7, test loss: 0.197205\n",
      "Epoch 8, test loss: 0.200005\n",
      "Epoch 9, test loss: 0.198099\n",
      "Epoch 10, test loss: 0.203306\n",
      "Epoch 11, test loss: 0.196362\n",
      "Epoch 12, test loss: 0.196790\n",
      "Epoch 13, test loss: 0.201388\n",
      "Epoch 14, test loss: 0.197973\n",
      "Epoch 15, test loss: 0.196005\n",
      "Epoch 16, test loss: 0.196145\n",
      "Epoch 17, test loss: 0.197955\n",
      "Epoch 18, test loss: 0.196384\n",
      "Epoch 19, test loss: 0.196206\n",
      "Epoch 20, test loss: 0.198829\n",
      "Epoch 21, test loss: 0.196420\n",
      "Epoch 22, test loss: 0.197489\n",
      "Epoch 23, test loss: 0.205030\n",
      "Epoch 24, test loss: 0.196520\n",
      "Epoch 25, test loss: 0.196467\n",
      "Epoch 26, test loss: 0.196703\n",
      "Epoch 27, test loss: 0.197417\n",
      "Epoch 28, test loss: 0.197664\n",
      "Epoch 29, test loss: 0.199063\n",
      "Epoch 30, test loss: 0.197712\n",
      "Epoch 31, test loss: 0.197173\n",
      "Epoch 32, test loss: 0.196692\n",
      "Epoch 33, test loss: 0.204721\n",
      "Epoch 34, test loss: 0.198303\n",
      "Epoch 35, test loss: 0.198641\n",
      "Epoch 36, test loss: 0.196008\n",
      "Epoch 37, test loss: 0.198453\n",
      "Epoch 38, test loss: 0.200320\n",
      "Epoch 39, test loss: 0.195803\n",
      "Epoch 40, test loss: 0.197727\n",
      "Epoch 41, test loss: 0.201047\n",
      "Epoch 42, test loss: 0.197932\n",
      "Epoch 43, test loss: 0.196001\n",
      "Epoch 44, test loss: 0.196769\n",
      "Epoch 45, test loss: 0.200503\n",
      "Epoch 46, test loss: 0.198088\n",
      "Epoch 47, test loss: 0.202751\n",
      "Epoch 48, test loss: 0.201572\n",
      "Epoch 49, test loss: 0.197636\n",
      "Epoch 50, test loss: 0.199071\n",
      "Epoch 51, test loss: 0.196120\n",
      "Epoch 52, test loss: 0.199938\n",
      "Epoch 53, test loss: 0.197160\n",
      "Epoch 54, test loss: 0.197840\n",
      "Epoch 55, test loss: 0.201991\n",
      "Epoch 56, test loss: 0.198575\n",
      "Epoch 57, test loss: 0.200214\n",
      "Epoch 58, test loss: 0.196688\n",
      "Epoch 59, test loss: 0.199949\n",
      "Epoch 60, test loss: 0.201848\n",
      "Epoch 61, test loss: 0.202743\n",
      "Epoch 62, test loss: 0.199017\n",
      "Epoch 63, test loss: 0.197526\n",
      "Epoch 64, test loss: 0.196958\n",
      "Epoch 65, test loss: 0.195751\n",
      "Epoch 66, test loss: 0.197402\n",
      "Epoch 67, test loss: 0.197640\n",
      "Epoch 68, test loss: 0.205751\n",
      "Epoch 69, test loss: 0.198961\n",
      "Epoch 70, test loss: 0.196489\n",
      "Epoch 71, test loss: 0.197940\n",
      "Epoch 72, test loss: 0.196750\n",
      "Epoch 73, test loss: 0.197676\n",
      "Epoch 74, test loss: 0.199431\n",
      "Epoch 75, test loss: 0.196367\n",
      "Epoch 76, test loss: 0.196171\n",
      "Epoch 77, test loss: 0.198275\n",
      "Epoch 78, test loss: 0.196975\n",
      "Epoch 79, test loss: 0.203424\n",
      "Epoch 80, test loss: 0.195930\n",
      "Epoch 81, test loss: 0.196690\n",
      "Epoch 82, test loss: 0.198521\n",
      "Epoch 83, test loss: 0.196034\n",
      "Epoch 84, test loss: 0.203024\n",
      "Epoch 85, test loss: 0.199658\n",
      "Epoch 86, test loss: 0.196666\n",
      "Epoch 87, test loss: 0.197726\n",
      "Epoch 88, test loss: 0.197144\n",
      "Epoch 89, test loss: 0.200946\n",
      "Epoch 90, test loss: 0.197331\n",
      "Epoch 91, test loss: 0.198486\n",
      "Epoch 92, test loss: 0.197342\n",
      "Epoch 93, test loss: 0.199401\n",
      "Epoch 94, test loss: 0.199691\n",
      "Epoch 95, test loss: 0.202402\n",
      "Epoch 96, test loss: 0.198673\n",
      "Epoch 97, test loss: 0.197689\n",
      "Epoch 98, test loss: 0.199798\n",
      "Epoch 99, test loss: 0.197653\n",
      "Epoch 100, test loss: 0.200795\n",
      "Epoch 101, test loss: 0.198445\n",
      "Epoch 102, test loss: 0.199510\n",
      "Epoch 103, test loss: 0.201122\n",
      "Epoch 104, test loss: 0.202409\n",
      "Epoch 105, test loss: 0.196316\n",
      "Epoch 106, test loss: 0.196371\n",
      "Epoch 107, test loss: 0.205301\n",
      "Epoch 108, test loss: 0.197539\n",
      "Epoch 109, test loss: 0.196851\n",
      "Epoch 110, test loss: 0.195921\n",
      "Epoch 111, test loss: 0.196724\n",
      "Epoch 112, test loss: 0.197980\n",
      "Epoch 113, test loss: 0.195925\n",
      "Epoch 114, test loss: 0.199430\n",
      "Epoch 115, test loss: 0.202322\n",
      "Epoch 116, test loss: 0.200564\n",
      "Epoch 117, test loss: 0.201147\n",
      "Epoch 118, test loss: 0.203866\n",
      "Epoch 119, test loss: 0.198873\n",
      "Epoch 120, test loss: 0.198643\n",
      "Epoch 121, test loss: 0.195931\n",
      "Epoch 122, test loss: 0.198522\n",
      "Epoch 123, test loss: 0.203686\n",
      "Epoch 124, test loss: 0.200008\n",
      "Epoch 125, test loss: 0.197567\n",
      "Epoch 126, test loss: 0.199993\n",
      "Epoch 127, test loss: 0.196288\n",
      "Epoch 128, test loss: 0.196435\n",
      "Epoch 129, test loss: 0.197630\n",
      "Epoch 130, test loss: 0.196328\n",
      "Epoch 131, test loss: 0.196800\n",
      "Epoch 132, test loss: 0.196657\n",
      "Epoch 133, test loss: 0.195812\n",
      "Epoch 134, test loss: 0.195478\n",
      "Epoch 135, test loss: 0.197266\n",
      "Epoch 136, test loss: 0.200264\n",
      "Epoch 137, test loss: 0.196467\n",
      "Epoch 138, test loss: 0.199081\n",
      "Epoch 139, test loss: 0.201180\n",
      "Epoch 140, test loss: 0.202901\n",
      "Epoch 141, test loss: 0.200517\n",
      "Epoch 142, test loss: 0.198701\n",
      "Epoch 143, test loss: 0.196488\n",
      "Epoch 144, test loss: 0.195976\n",
      "Epoch 145, test loss: 0.196917\n",
      "Epoch 146, test loss: 0.199523\n",
      "Epoch 147, test loss: 0.202117\n",
      "Epoch 148, test loss: 0.196817\n",
      "Epoch 149, test loss: 0.196732\n",
      "Epoch 150, test loss: 0.196995\n",
      "Epoch 151, test loss: 0.196292\n",
      "Epoch 152, test loss: 0.197850\n",
      "Epoch 153, test loss: 0.195584\n",
      "Epoch 154, test loss: 0.196355\n",
      "Epoch 155, test loss: 0.196082\n",
      "Epoch 156, test loss: 0.195589\n",
      "Epoch 157, test loss: 0.196726\n",
      "Epoch 158, test loss: 0.196185\n",
      "Epoch 159, test loss: 0.195387\n",
      "Epoch 160, test loss: 0.202051\n",
      "Epoch 161, test loss: 0.200244\n",
      "Epoch 162, test loss: 0.196964\n",
      "Epoch 163, test loss: 0.197948\n",
      "Epoch 164, test loss: 0.196140\n",
      "Epoch 165, test loss: 0.200104\n",
      "Epoch 166, test loss: 0.202070\n",
      "Epoch 167, test loss: 0.200591\n",
      "Epoch 168, test loss: 0.195621\n",
      "Epoch 169, test loss: 0.196306\n",
      "Epoch 170, test loss: 0.197017\n",
      "Epoch 171, test loss: 0.195525\n",
      "Epoch 172, test loss: 0.196326\n",
      "Epoch 173, test loss: 0.196626\n",
      "Epoch 174, test loss: 0.195615\n",
      "Epoch 175, test loss: 0.195905\n",
      "Epoch 176, test loss: 0.196078\n",
      "Epoch 177, test loss: 0.199143\n",
      "Epoch 178, test loss: 0.208575\n",
      "Epoch 179, test loss: 0.199381\n",
      "Epoch 180, test loss: 0.196107\n",
      "Epoch 181, test loss: 0.201287\n",
      "Epoch 182, test loss: 0.195681\n",
      "Epoch 183, test loss: 0.197821\n",
      "Epoch 184, test loss: 0.201581\n",
      "Epoch 185, test loss: 0.195594\n",
      "Epoch 186, test loss: 0.197005\n",
      "Epoch 187, test loss: 0.196101\n",
      "Epoch 188, test loss: 0.196183\n",
      "Epoch 189, test loss: 0.198810\n",
      "Epoch 190, test loss: 0.201557\n",
      "Epoch 191, test loss: 0.197993\n",
      "Epoch 192, test loss: 0.197054\n",
      "Epoch 193, test loss: 0.197786\n",
      "Epoch 194, test loss: 0.198523\n",
      "Epoch 195, test loss: 0.200281\n",
      "Epoch 196, test loss: 0.196479\n",
      "Epoch 197, test loss: 0.198467\n",
      "Epoch 198, test loss: 0.197873\n",
      "Epoch 199, test loss: 0.195695\n",
      "Epoch 200, test loss: 0.199046\n",
      "Epoch 201, test loss: 0.196537\n",
      "Epoch 202, test loss: 0.200974\n",
      "Epoch 203, test loss: 0.196041\n",
      "Epoch 204, test loss: 0.197678\n",
      "Epoch 205, test loss: 0.196731\n",
      "Epoch 206, test loss: 0.203685\n",
      "Epoch 207, test loss: 0.204063\n",
      "Epoch 208, test loss: 0.199648\n",
      "Epoch 209, test loss: 0.196754\n",
      "Epoch 210, test loss: 0.196751\n",
      "Epoch 211, test loss: 0.197358\n",
      "Epoch 212, test loss: 0.197538\n",
      "Epoch 213, test loss: 0.204760\n",
      "Epoch 214, test loss: 0.201976\n",
      "Epoch 215, test loss: 0.198463\n",
      "Epoch 216, test loss: 0.199234\n",
      "Epoch 217, test loss: 0.196554\n",
      "Epoch 218, test loss: 0.195799\n",
      "Epoch 219, test loss: 0.195369\n",
      "Epoch 220, test loss: 0.196418\n",
      "Epoch 221, test loss: 0.196983\n",
      "Epoch 222, test loss: 0.200009\n",
      "Epoch 223, test loss: 0.198777\n",
      "Epoch 224, test loss: 0.200441\n",
      "Epoch 225, test loss: 0.196364\n",
      "Epoch 226, test loss: 0.197907\n",
      "Epoch 227, test loss: 0.199621\n",
      "Epoch 228, test loss: 0.196965\n",
      "Epoch 229, test loss: 0.195753\n",
      "Epoch 230, test loss: 0.195848\n",
      "Epoch 231, test loss: 0.196566\n",
      "Epoch 232, test loss: 0.201054\n",
      "Epoch 233, test loss: 0.196000\n",
      "Epoch 234, test loss: 0.199183\n",
      "Epoch 235, test loss: 0.199795\n",
      "Epoch 236, test loss: 0.197036\n",
      "Epoch 237, test loss: 0.195852\n",
      "Epoch 238, test loss: 0.201767\n",
      "Epoch 239, test loss: 0.196620\n",
      "Pretrain data: 19653653.0\n",
      "Building dataset, requesting data from 0 to 820\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7225/101960\n",
      "Found 820 continuous time series\n",
      "Data shape: (109187, 12), Train/test: 109185/2\n",
      "Train test ratio: 54592.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1706\n",
      "Epoch 0, train loss: 0.237963\n",
      "Epoch 1, train loss: 0.264666\n",
      "Epoch 2, train loss: 0.214403\n",
      "Epoch 3, train loss: 0.240573\n",
      "Epoch 4, train loss: 0.248481\n",
      "Epoch 5, train loss: 0.238496\n",
      "Epoch 6, train loss: 0.214737\n",
      "Epoch 7, train loss: 0.235362\n",
      "Epoch 8, train loss: 0.201598\n",
      "Epoch 9, train loss: 0.230820\n",
      "Epoch 10, train loss: 0.242290\n",
      "Epoch 11, train loss: 0.255199\n",
      "Epoch 12, train loss: 0.184320\n",
      "Epoch 13, train loss: 0.236176\n",
      "Epoch 14, train loss: 0.214555\n",
      "Epoch 15, train loss: 0.190640\n",
      "Epoch 16, train loss: 0.184224\n",
      "Epoch 17, train loss: 0.250411\n",
      "Epoch 18, train loss: 0.142145\n",
      "Epoch 19, train loss: 0.269928\n",
      "Epoch 20, train loss: 0.319818\n",
      "Epoch 21, train loss: 0.195868\n",
      "Epoch 22, train loss: 0.232870\n",
      "Epoch 23, train loss: 0.215577\n",
      "Epoch 24, train loss: 0.183342\n",
      "Epoch 25, train loss: 0.183146\n",
      "Epoch 26, train loss: 0.196364\n",
      "Epoch 27, train loss: 0.202418\n",
      "Epoch 28, train loss: 0.230953\n",
      "Epoch 29, train loss: 0.182224\n",
      "Epoch 30, train loss: 0.212040\n",
      "Epoch 31, train loss: 0.197754\n",
      "Epoch 32, train loss: 0.194932\n",
      "Epoch 33, train loss: 0.242781\n",
      "Epoch 34, train loss: 0.214756\n",
      "Epoch 35, train loss: 0.173966\n",
      "Epoch 36, train loss: 0.240261\n",
      "Epoch 37, train loss: 0.227021\n",
      "Epoch 38, train loss: 0.225014\n",
      "Epoch 39, train loss: 0.284014\n",
      "Epoch 40, train loss: 0.232944\n",
      "Epoch 41, train loss: 0.215541\n",
      "Epoch 42, train loss: 0.189373\n",
      "Epoch 43, train loss: 0.224592\n",
      "Epoch 44, train loss: 0.235946\n",
      "Epoch 45, train loss: 0.245746\n",
      "Epoch 46, train loss: 0.220867\n",
      "Epoch 47, train loss: 0.192286\n",
      "Epoch 48, train loss: 0.246903\n",
      "Epoch 49, train loss: 0.192745\n",
      "Epoch 50, train loss: 0.225412\n",
      "Epoch 51, train loss: 0.278807\n",
      "Epoch 52, train loss: 0.209060\n",
      "Epoch 53, train loss: 0.181717\n",
      "Epoch 54, train loss: 0.195974\n",
      "Epoch 55, train loss: 0.192439\n",
      "Epoch 56, train loss: 0.240828\n",
      "Epoch 57, train loss: 0.239404\n",
      "Epoch 58, train loss: 0.180159\n",
      "Epoch 59, train loss: 0.263394\n",
      "Epoch 60, train loss: 0.195512\n",
      "Epoch 61, train loss: 0.243108\n",
      "Epoch 62, train loss: 0.209503\n",
      "Epoch 63, train loss: 0.248052\n",
      "Epoch 64, train loss: 0.192548\n",
      "Epoch 65, train loss: 0.200645\n",
      "Epoch 66, train loss: 0.166125\n",
      "Epoch 67, train loss: 0.213725\n",
      "Epoch 68, train loss: 0.247252\n",
      "Epoch 69, train loss: 0.212775\n",
      "Epoch 70, train loss: 0.203826\n",
      "Epoch 71, train loss: 0.152455\n",
      "Epoch 72, train loss: 0.370722\n",
      "Epoch 73, train loss: 0.220751\n",
      "Epoch 74, train loss: 0.203249\n",
      "Epoch 75, train loss: 0.236903\n",
      "Epoch 76, train loss: 0.216856\n",
      "Epoch 77, train loss: 0.207683\n",
      "Epoch 78, train loss: 0.257531\n",
      "Epoch 79, train loss: 0.227656\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "x here is\n",
      "[[239. 238. 235. ... 216. 210. 208.]\n",
      " [238. 235. 233. ... 210. 208. 205.]\n",
      " [235. 233. 231. ... 208. 205. 204.]\n",
      " ...\n",
      " [144. 145. 145. ... 152. 151. 149.]\n",
      " [145. 145. 145. ... 151. 149. 149.]\n",
      " [145. 145. 143. ... 149. 149. 145.]]\n",
      "y here is\n",
      "[[202. 202. 202. ... 202. 202. 202.]\n",
      " [203. 203. 203. ... 203. 203. 203.]\n",
      " [203. 203. 203. ... 203. 203. 203.]\n",
      " ...\n",
      " [144. 144. 144. ... 144. 144. 144.]\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [145. 145. 145. ... 145. 145. 145.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (2502, 12), Train/test: 1/2501\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "x here is\n",
      "[[219. 229. 224. ... 196. 208. 202.]\n",
      " [229. 224. 221. ... 208. 202. 192.]\n",
      " [224. 221. 215. ... 202. 192. 194.]\n",
      " ...\n",
      " [201. 204. 205. ... 237. 241. 243.]\n",
      " [204. 205. 209. ... 241. 243. 251.]\n",
      " [205. 209. 216. ... 243. 251. 257.]]\n",
      "y here is\n",
      "[[183. 183. 183. ... 183. 183. 183.]\n",
      " [172. 172. 172. ... 172. 172. 172.]\n",
      " [166. 166. 166. ... 166. 166. 166.]\n",
      " ...\n",
      " [250. 250. 250. ... 250. 250. 250.]\n",
      " [246. 246. 246. ... 246. 246. 246.]\n",
      " [240. 240. 240. ... 240. 240. 240.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 683/11065\n",
      "Found 22 continuous time series\n",
      "Data shape: (11750, 12), Train/test: 11748/2\n",
      "Train test ratio: 5874.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294D174C0>\n",
      "Epoch 0, test loss: 0.181542\n",
      "Epoch 1, test loss: 0.181982\n",
      "Epoch 2, test loss: 0.183072\n",
      "Epoch 3, test loss: 0.181155\n",
      "Epoch 4, test loss: 0.181041\n",
      "Epoch 5, test loss: 0.187836\n",
      "Epoch 6, test loss: 0.180017\n",
      "Epoch 7, test loss: 0.180218\n",
      "Epoch 8, test loss: 0.181244\n",
      "Epoch 9, test loss: 0.182370\n",
      "Epoch 10, test loss: 0.181240\n",
      "Epoch 11, test loss: 0.182196\n",
      "Epoch 12, test loss: 0.185311\n",
      "Epoch 13, test loss: 0.180298\n",
      "Epoch 14, test loss: 0.181436\n",
      "Epoch 15, test loss: 0.183849\n",
      "Epoch 16, test loss: 0.186187\n",
      "Epoch 17, test loss: 0.187728\n",
      "Epoch 18, test loss: 0.180764\n",
      "Epoch 19, test loss: 0.184047\n",
      "Epoch 20, test loss: 0.181488\n",
      "Epoch 21, test loss: 0.181017\n",
      "Epoch 22, test loss: 0.182236\n",
      "Epoch 23, test loss: 0.181526\n",
      "Epoch 24, test loss: 0.184190\n",
      "Epoch 25, test loss: 0.181494\n",
      "Epoch 26, test loss: 0.181833\n",
      "Epoch 27, test loss: 0.181733\n",
      "Epoch 28, test loss: 0.182713\n",
      "Epoch 29, test loss: 0.182494\n",
      "Epoch 30, test loss: 0.187326\n",
      "Epoch 31, test loss: 0.181179\n",
      "Epoch 32, test loss: 0.192300\n",
      "Epoch 33, test loss: 0.182700\n",
      "Epoch 34, test loss: 0.181719\n",
      "Epoch 35, test loss: 0.184678\n",
      "Epoch 36, test loss: 0.188732\n",
      "Epoch 37, test loss: 0.182544\n",
      "Epoch 38, test loss: 0.183568\n",
      "Epoch 39, test loss: 0.181159\n",
      "Epoch 40, test loss: 0.182473\n",
      "Epoch 41, test loss: 0.184382\n",
      "Epoch 42, test loss: 0.181626\n",
      "Epoch 43, test loss: 0.181063\n",
      "Epoch 44, test loss: 0.181674\n",
      "Epoch 45, test loss: 0.181917\n",
      "Epoch 46, test loss: 0.183924\n",
      "Epoch 47, test loss: 0.181559\n",
      "Epoch 48, test loss: 0.183226\n",
      "Epoch 49, test loss: 0.187206\n",
      "Epoch 50, test loss: 0.181786\n",
      "Epoch 51, test loss: 0.182695\n",
      "Epoch 52, test loss: 0.184310\n",
      "Epoch 53, test loss: 0.182485\n",
      "Epoch 54, test loss: 0.184107\n",
      "Epoch 55, test loss: 0.182185\n",
      "Epoch 56, test loss: 0.181472\n",
      "Epoch 57, test loss: 0.184166\n",
      "Epoch 58, test loss: 0.185324\n",
      "Epoch 59, test loss: 0.188857\n",
      "Epoch 60, test loss: 0.181931\n",
      "Epoch 61, test loss: 0.184657\n",
      "Epoch 62, test loss: 0.182905\n",
      "Epoch 63, test loss: 0.181811\n",
      "Epoch 64, test loss: 0.184380\n",
      "Epoch 65, test loss: 0.190596\n",
      "Epoch 66, test loss: 0.182306\n",
      "Epoch 67, test loss: 0.184405\n",
      "Epoch 68, test loss: 0.182879\n",
      "Epoch 69, test loss: 0.184315\n",
      "Epoch 70, test loss: 0.182794\n",
      "Epoch 71, test loss: 0.181920\n",
      "Epoch 72, test loss: 0.183828\n",
      "Epoch 73, test loss: 0.183923\n",
      "Epoch 74, test loss: 0.180782\n",
      "Epoch 75, test loss: 0.186371\n",
      "Epoch 76, test loss: 0.193772\n",
      "Epoch 77, test loss: 0.181529\n",
      "Epoch 78, test loss: 0.181666\n",
      "Epoch 79, test loss: 0.189013\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294D174C0>\n",
      "Epoch 0, test loss: 0.181321\n",
      "Epoch 1, test loss: 0.181351\n",
      "Epoch 2, test loss: 0.185134\n",
      "Epoch 3, test loss: 0.182510\n",
      "Epoch 4, test loss: 0.181679\n",
      "Epoch 5, test loss: 0.185813\n",
      "Epoch 6, test loss: 0.181398\n",
      "Epoch 7, test loss: 0.182159\n",
      "Epoch 8, test loss: 0.181327\n",
      "Epoch 9, test loss: 0.181430\n",
      "Epoch 10, test loss: 0.182710\n",
      "Epoch 11, test loss: 0.182745\n",
      "Epoch 12, test loss: 0.181832\n",
      "Epoch 13, test loss: 0.186563\n",
      "Epoch 14, test loss: 0.181897\n",
      "Epoch 15, test loss: 0.181492\n",
      "Epoch 16, test loss: 0.182054\n",
      "Epoch 17, test loss: 0.184058\n",
      "Epoch 18, test loss: 0.183925\n",
      "Epoch 19, test loss: 0.182458\n",
      "Epoch 20, test loss: 0.183283\n",
      "Epoch 21, test loss: 0.181405\n",
      "Epoch 22, test loss: 0.182577\n",
      "Epoch 23, test loss: 0.182109\n",
      "Epoch 24, test loss: 0.184099\n",
      "Epoch 25, test loss: 0.183004\n",
      "Epoch 26, test loss: 0.182728\n",
      "Epoch 27, test loss: 0.182128\n",
      "Epoch 28, test loss: 0.181472\n",
      "Epoch 29, test loss: 0.182089\n",
      "Epoch 30, test loss: 0.183769\n",
      "Epoch 31, test loss: 0.184780\n",
      "Epoch 32, test loss: 0.184363\n",
      "Epoch 33, test loss: 0.182677\n",
      "Epoch 34, test loss: 0.183617\n",
      "Epoch 35, test loss: 0.183125\n",
      "Epoch 36, test loss: 0.183551\n",
      "Epoch 37, test loss: 0.183693\n",
      "Epoch 38, test loss: 0.190850\n",
      "Epoch 39, test loss: 0.183250\n",
      "Epoch 40, test loss: 0.182182\n",
      "Epoch 41, test loss: 0.185581\n",
      "Epoch 42, test loss: 0.183232\n",
      "Epoch 43, test loss: 0.183086\n",
      "Epoch 44, test loss: 0.182464\n",
      "Epoch 45, test loss: 0.186546\n",
      "Epoch 46, test loss: 0.184271\n",
      "Epoch 47, test loss: 0.181576\n",
      "Epoch 48, test loss: 0.183958\n",
      "Epoch 49, test loss: 0.182997\n",
      "Epoch 50, test loss: 0.183678\n",
      "Epoch 51, test loss: 0.183400\n",
      "Epoch 52, test loss: 0.182207\n",
      "Epoch 53, test loss: 0.183745\n",
      "Epoch 54, test loss: 0.184456\n",
      "Epoch 55, test loss: 0.183096\n",
      "Epoch 56, test loss: 0.188248\n",
      "Epoch 57, test loss: 0.183776\n",
      "Epoch 58, test loss: 0.184772\n",
      "Epoch 59, test loss: 0.184834\n",
      "Epoch 60, test loss: 0.183257\n",
      "Epoch 61, test loss: 0.182912\n",
      "Epoch 62, test loss: 0.183511\n",
      "Epoch 63, test loss: 0.183750\n",
      "Epoch 64, test loss: 0.187292\n",
      "Epoch 65, test loss: 0.183602\n",
      "Epoch 66, test loss: 0.184965\n",
      "Epoch 67, test loss: 0.183210\n",
      "Epoch 68, test loss: 0.183559\n",
      "Epoch 69, test loss: 0.184536\n",
      "Epoch 70, test loss: 0.184119\n",
      "Epoch 71, test loss: 0.183267\n",
      "Epoch 72, test loss: 0.183949\n",
      "Epoch 73, test loss: 0.183114\n",
      "Epoch 74, test loss: 0.182872\n",
      "Epoch 75, test loss: 0.183819\n",
      "Epoch 76, test loss: 0.182912\n",
      "Epoch 77, test loss: 0.183176\n",
      "Epoch 78, test loss: 0.183827\n",
      "Epoch 79, test loss: 0.182875\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294D174C0>\n",
      "Epoch 0, test loss: 0.222584\n",
      "Epoch 1, test loss: 0.205892\n",
      "Epoch 2, test loss: 0.194250\n",
      "Epoch 3, test loss: 0.191243\n",
      "Epoch 4, test loss: 0.191308\n",
      "Epoch 5, test loss: 0.191106\n",
      "Epoch 6, test loss: 0.193453\n",
      "Epoch 7, test loss: 0.191075\n",
      "Epoch 8, test loss: 0.194585\n",
      "Epoch 9, test loss: 0.191894\n",
      "Epoch 10, test loss: 0.192890\n",
      "Epoch 11, test loss: 0.191952\n",
      "Epoch 12, test loss: 0.192363\n",
      "Epoch 13, test loss: 0.192402\n",
      "Epoch 14, test loss: 0.192530\n",
      "Epoch 15, test loss: 0.192567\n",
      "Epoch 16, test loss: 0.194026\n",
      "Epoch 17, test loss: 0.195071\n",
      "Epoch 18, test loss: 0.195458\n",
      "Epoch 19, test loss: 0.192272\n",
      "Epoch 20, test loss: 0.193298\n",
      "Epoch 21, test loss: 0.197017\n",
      "Epoch 22, test loss: 0.192318\n",
      "Epoch 23, test loss: 0.192458\n",
      "Epoch 24, test loss: 0.191599\n",
      "Epoch 25, test loss: 0.194708\n",
      "Epoch 26, test loss: 0.192246\n",
      "Epoch 27, test loss: 0.192648\n",
      "Epoch 28, test loss: 0.193900\n",
      "Epoch 29, test loss: 0.193757\n",
      "Epoch 30, test loss: 0.192824\n",
      "Epoch 31, test loss: 0.193522\n",
      "Epoch 32, test loss: 0.193274\n",
      "Epoch 33, test loss: 0.193062\n",
      "Epoch 34, test loss: 0.195287\n",
      "Epoch 35, test loss: 0.191782\n",
      "Epoch 36, test loss: 0.193355\n",
      "Epoch 37, test loss: 0.193441\n",
      "Epoch 38, test loss: 0.196446\n",
      "Epoch 39, test loss: 0.194955\n",
      "Epoch 40, test loss: 0.196870\n",
      "Epoch 41, test loss: 0.193010\n",
      "Epoch 42, test loss: 0.194346\n",
      "Epoch 43, test loss: 0.192116\n",
      "Epoch 44, test loss: 0.194338\n",
      "Epoch 45, test loss: 0.194590\n",
      "Epoch 46, test loss: 0.196624\n",
      "Epoch 47, test loss: 0.191789\n",
      "Epoch 48, test loss: 0.191653\n",
      "Epoch 49, test loss: 0.192117\n",
      "Epoch 50, test loss: 0.192251\n",
      "Epoch 51, test loss: 0.192970\n",
      "Epoch 52, test loss: 0.195733\n",
      "Epoch 53, test loss: 0.193535\n",
      "Epoch 54, test loss: 0.191463\n",
      "Epoch 55, test loss: 0.192087\n",
      "Epoch 56, test loss: 0.193555\n",
      "Epoch 57, test loss: 0.191202\n",
      "Epoch 58, test loss: 0.191125\n",
      "Epoch 59, test loss: 0.191142\n",
      "Epoch 60, test loss: 0.191116\n",
      "Epoch 61, test loss: 0.190703\n",
      "Epoch 62, test loss: 0.192335\n",
      "Epoch 63, test loss: 0.191791\n",
      "Epoch 64, test loss: 0.191418\n",
      "Epoch 65, test loss: 0.192036\n",
      "Epoch 66, test loss: 0.191073\n",
      "Epoch 67, test loss: 0.192611\n",
      "Epoch 68, test loss: 0.190955\n",
      "Epoch 69, test loss: 0.190991\n",
      "Epoch 70, test loss: 0.192327\n",
      "Epoch 71, test loss: 0.191262\n",
      "Epoch 72, test loss: 0.192614\n",
      "Epoch 73, test loss: 0.192398\n",
      "Epoch 74, test loss: 0.191735\n",
      "Epoch 75, test loss: 0.192879\n",
      "Epoch 76, test loss: 0.192886\n",
      "Epoch 77, test loss: 0.191515\n",
      "Epoch 78, test loss: 0.192238\n",
      "Epoch 79, test loss: 0.190769\n",
      "Epoch 80, test loss: 0.190983\n",
      "Epoch 81, test loss: 0.191577\n",
      "Epoch 82, test loss: 0.192176\n",
      "Epoch 83, test loss: 0.191775\n",
      "Epoch 84, test loss: 0.191506\n",
      "Epoch 85, test loss: 0.191437\n",
      "Epoch 86, test loss: 0.192656\n",
      "Epoch 87, test loss: 0.191733\n",
      "Epoch 88, test loss: 0.191227\n",
      "Epoch 89, test loss: 0.191680\n",
      "Epoch 90, test loss: 0.192920\n",
      "Epoch 91, test loss: 0.191023\n",
      "Epoch 92, test loss: 0.191038\n",
      "Epoch 93, test loss: 0.191310\n",
      "Epoch 94, test loss: 0.192776\n",
      "Epoch 95, test loss: 0.199308\n",
      "Epoch 96, test loss: 0.191494\n",
      "Epoch 97, test loss: 0.190686\n",
      "Epoch 98, test loss: 0.191321\n",
      "Epoch 99, test loss: 0.190437\n",
      "Epoch 100, test loss: 0.191738\n",
      "Epoch 101, test loss: 0.191615\n",
      "Epoch 102, test loss: 0.192315\n",
      "Epoch 103, test loss: 0.191014\n",
      "Epoch 104, test loss: 0.192900\n",
      "Epoch 105, test loss: 0.196437\n",
      "Epoch 106, test loss: 0.202015\n",
      "Epoch 107, test loss: 0.190460\n",
      "Epoch 108, test loss: 0.193919\n",
      "Epoch 109, test loss: 0.195049\n",
      "Epoch 110, test loss: 0.190229\n",
      "Epoch 111, test loss: 0.190467\n",
      "Epoch 112, test loss: 0.191768\n",
      "Epoch 113, test loss: 0.191861\n",
      "Epoch 114, test loss: 0.193340\n",
      "Epoch 115, test loss: 0.190624\n",
      "Epoch 116, test loss: 0.191030\n",
      "Epoch 117, test loss: 0.191861\n",
      "Epoch 118, test loss: 0.190625\n",
      "Epoch 119, test loss: 0.192632\n",
      "Epoch 120, test loss: 0.192045\n",
      "Epoch 121, test loss: 0.195966\n",
      "Epoch 122, test loss: 0.190826\n",
      "Epoch 123, test loss: 0.189876\n",
      "Epoch 124, test loss: 0.191015\n",
      "Epoch 125, test loss: 0.194295\n",
      "Epoch 126, test loss: 0.194582\n",
      "Epoch 127, test loss: 0.193237\n",
      "Epoch 128, test loss: 0.191007\n",
      "Epoch 129, test loss: 0.190504\n",
      "Epoch 130, test loss: 0.191759\n",
      "Epoch 131, test loss: 0.190882\n",
      "Epoch 132, test loss: 0.190735\n",
      "Epoch 133, test loss: 0.190243\n",
      "Epoch 134, test loss: 0.191772\n",
      "Epoch 135, test loss: 0.190569\n",
      "Epoch 136, test loss: 0.190018\n",
      "Epoch 137, test loss: 0.189490\n",
      "Epoch 138, test loss: 0.193933\n",
      "Epoch 139, test loss: 0.190253\n",
      "Epoch 140, test loss: 0.190701\n",
      "Epoch 141, test loss: 0.189967\n",
      "Epoch 142, test loss: 0.189294\n",
      "Epoch 143, test loss: 0.189563\n",
      "Epoch 144, test loss: 0.192999\n",
      "Epoch 145, test loss: 0.189122\n",
      "Epoch 146, test loss: 0.190582\n",
      "Epoch 147, test loss: 0.191908\n",
      "Epoch 148, test loss: 0.194980\n",
      "Epoch 149, test loss: 0.189514\n",
      "Epoch 150, test loss: 0.190631\n",
      "Epoch 151, test loss: 0.190297\n",
      "Epoch 152, test loss: 0.189048\n",
      "Epoch 153, test loss: 0.190439\n",
      "Epoch 154, test loss: 0.191094\n",
      "Epoch 155, test loss: 0.189093\n",
      "Epoch 156, test loss: 0.191543\n",
      "Epoch 157, test loss: 0.190617\n",
      "Epoch 158, test loss: 0.189971\n",
      "Epoch 159, test loss: 0.191616\n",
      "Epoch 160, test loss: 0.192112\n",
      "Epoch 161, test loss: 0.190009\n",
      "Epoch 162, test loss: 0.190444\n",
      "Epoch 163, test loss: 0.192892\n",
      "Epoch 164, test loss: 0.193185\n",
      "Epoch 165, test loss: 0.190373\n",
      "Epoch 166, test loss: 0.190159\n",
      "Epoch 167, test loss: 0.192568\n",
      "Epoch 168, test loss: 0.190653\n",
      "Epoch 169, test loss: 0.193612\n",
      "Epoch 170, test loss: 0.194289\n",
      "Epoch 171, test loss: 0.190153\n",
      "Epoch 172, test loss: 0.191547\n",
      "Epoch 173, test loss: 0.190092\n",
      "Epoch 174, test loss: 0.191017\n",
      "Epoch 175, test loss: 0.194700\n",
      "Epoch 176, test loss: 0.189960\n",
      "Epoch 177, test loss: 0.190093\n",
      "Epoch 178, test loss: 0.197096\n",
      "Epoch 179, test loss: 0.190660\n",
      "Epoch 180, test loss: 0.191220\n",
      "Epoch 181, test loss: 0.192421\n",
      "Epoch 182, test loss: 0.190115\n",
      "Epoch 183, test loss: 0.193447\n",
      "Epoch 184, test loss: 0.190721\n",
      "Epoch 185, test loss: 0.190148\n",
      "Epoch 186, test loss: 0.190389\n",
      "Epoch 187, test loss: 0.193991\n",
      "Epoch 188, test loss: 0.192248\n",
      "Epoch 189, test loss: 0.190750\n",
      "Epoch 190, test loss: 0.191178\n",
      "Epoch 191, test loss: 0.192575\n",
      "Epoch 192, test loss: 0.191390\n",
      "Epoch 193, test loss: 0.190817\n",
      "Epoch 194, test loss: 0.195001\n",
      "Epoch 195, test loss: 0.190535\n",
      "Epoch 196, test loss: 0.191237\n",
      "Epoch 197, test loss: 0.191387\n",
      "Epoch 198, test loss: 0.192162\n",
      "Epoch 199, test loss: 0.190572\n",
      "Epoch 200, test loss: 0.190371\n",
      "Epoch 201, test loss: 0.191080\n",
      "Epoch 202, test loss: 0.191357\n",
      "Epoch 203, test loss: 0.190321\n",
      "Epoch 204, test loss: 0.189804\n",
      "Epoch 205, test loss: 0.195031\n",
      "Epoch 206, test loss: 0.194586\n",
      "Epoch 207, test loss: 0.191515\n",
      "Epoch 208, test loss: 0.189365\n",
      "Epoch 209, test loss: 0.195004\n",
      "Epoch 210, test loss: 0.189981\n",
      "Epoch 211, test loss: 0.189683\n",
      "Epoch 212, test loss: 0.191056\n",
      "Epoch 213, test loss: 0.190886\n",
      "Epoch 214, test loss: 0.190821\n",
      "Epoch 215, test loss: 0.190431\n",
      "Epoch 216, test loss: 0.189745\n",
      "Epoch 217, test loss: 0.189914\n",
      "Epoch 218, test loss: 0.191264\n",
      "Epoch 219, test loss: 0.189498\n",
      "Epoch 220, test loss: 0.192733\n",
      "Epoch 221, test loss: 0.188879\n",
      "Epoch 222, test loss: 0.189904\n",
      "Epoch 223, test loss: 0.189868\n",
      "Epoch 224, test loss: 0.189283\n",
      "Epoch 225, test loss: 0.189118\n",
      "Epoch 226, test loss: 0.189339\n",
      "Epoch 227, test loss: 0.190276\n",
      "Epoch 228, test loss: 0.191234\n",
      "Epoch 229, test loss: 0.188902\n",
      "Epoch 230, test loss: 0.189805\n",
      "Epoch 231, test loss: 0.192522\n",
      "Epoch 232, test loss: 0.190329\n",
      "Epoch 233, test loss: 0.188886\n",
      "Epoch 234, test loss: 0.190519\n",
      "Epoch 235, test loss: 0.189993\n",
      "Epoch 236, test loss: 0.195881\n",
      "Epoch 237, test loss: 0.194836\n",
      "Epoch 238, test loss: 0.189362\n",
      "Epoch 239, test loss: 0.193514\n",
      "Pretrain data: 19365644.0\n",
      "Building dataset, requesting data from 0 to 821\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7488/102822\n",
      "Found 821 continuous time series\n",
      "Data shape: (110312, 12), Train/test: 110310/2\n",
      "Train test ratio: 55155.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1723\n",
      "Epoch 0, train loss: 0.226845\n",
      "Epoch 1, train loss: 0.198214\n",
      "Epoch 2, train loss: 0.298340\n",
      "Epoch 3, train loss: 0.261036\n",
      "Epoch 4, train loss: 0.210926\n",
      "Epoch 5, train loss: 0.221138\n",
      "Epoch 6, train loss: 0.213913\n",
      "Epoch 7, train loss: 0.193112\n",
      "Epoch 8, train loss: 0.219566\n",
      "Epoch 9, train loss: 0.177651\n",
      "Epoch 10, train loss: 0.215173\n",
      "Epoch 11, train loss: 0.204827\n",
      "Epoch 12, train loss: 0.196673\n",
      "Epoch 13, train loss: 0.190790\n",
      "Epoch 14, train loss: 0.197974\n",
      "Epoch 15, train loss: 0.350842\n",
      "Epoch 16, train loss: 0.182747\n",
      "Epoch 17, train loss: 0.171167\n",
      "Epoch 18, train loss: 0.188780\n",
      "Epoch 19, train loss: 0.173286\n",
      "Epoch 20, train loss: 0.233362\n",
      "Epoch 21, train loss: 0.204874\n",
      "Epoch 22, train loss: 0.184743\n",
      "Epoch 23, train loss: 0.194619\n",
      "Epoch 24, train loss: 0.205645\n",
      "Epoch 25, train loss: 0.253671\n",
      "Epoch 26, train loss: 0.210616\n",
      "Epoch 27, train loss: 0.183375\n",
      "Epoch 28, train loss: 0.213334\n",
      "Epoch 29, train loss: 0.179432\n",
      "Epoch 30, train loss: 0.193466\n",
      "Epoch 31, train loss: 0.202361\n",
      "Epoch 32, train loss: 0.229400\n",
      "Epoch 33, train loss: 0.215960\n",
      "Epoch 34, train loss: 0.169182\n",
      "Epoch 35, train loss: 0.230159\n",
      "Epoch 36, train loss: 0.196430\n",
      "Epoch 37, train loss: 0.173061\n",
      "Epoch 38, train loss: 0.184829\n",
      "Epoch 39, train loss: 0.223351\n",
      "Epoch 40, train loss: 0.233283\n",
      "Epoch 41, train loss: 0.192721\n",
      "Epoch 42, train loss: 0.210205\n",
      "Epoch 43, train loss: 0.208777\n",
      "Epoch 44, train loss: 0.211058\n",
      "Epoch 45, train loss: 0.179301\n",
      "Epoch 46, train loss: 0.195107\n",
      "Epoch 47, train loss: 0.197952\n",
      "Epoch 48, train loss: 0.226091\n",
      "Epoch 49, train loss: 0.202465\n",
      "Epoch 50, train loss: 0.230059\n",
      "Epoch 51, train loss: 0.197145\n",
      "Epoch 52, train loss: 0.219868\n",
      "Epoch 53, train loss: 0.191534\n",
      "Epoch 54, train loss: 0.158858\n",
      "Epoch 55, train loss: 0.215941\n",
      "Epoch 56, train loss: 0.252490\n",
      "Epoch 57, train loss: 0.197011\n",
      "Epoch 58, train loss: 0.155992\n",
      "Epoch 59, train loss: 0.273006\n",
      "Epoch 60, train loss: 0.236558\n",
      "Epoch 61, train loss: 0.250440\n",
      "Epoch 62, train loss: 0.167325\n",
      "Epoch 63, train loss: 0.191922\n",
      "Epoch 64, train loss: 0.187061\n",
      "Epoch 65, train loss: 0.184755\n",
      "Epoch 66, train loss: 0.205716\n",
      "Epoch 67, train loss: 0.262949\n",
      "Epoch 68, train loss: 0.186699\n",
      "Epoch 69, train loss: 0.225375\n",
      "Epoch 70, train loss: 0.185409\n",
      "Epoch 71, train loss: 0.168544\n",
      "Epoch 72, train loss: 0.291066\n",
      "Epoch 73, train loss: 0.188637\n",
      "Epoch 74, train loss: 0.152612\n",
      "Epoch 75, train loss: 0.215887\n",
      "Epoch 76, train loss: 0.199948\n",
      "Epoch 77, train loss: 0.185398\n",
      "Epoch 78, train loss: 0.210519\n",
      "Epoch 79, train loss: 0.274639\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "x here is\n",
      "[[135. 143. 152. ... 195. 199. 203.]\n",
      " [143. 152. 159. ... 199. 203. 204.]\n",
      " [152. 159. 166. ... 203. 204. 205.]\n",
      " ...\n",
      " [187. 190. 192. ... 204. 202. 201.]\n",
      " [190. 192. 194. ... 202. 201. 201.]\n",
      " [192. 194. 197. ... 201. 201. 201.]]\n",
      "y here is\n",
      "[[223. 223. 223. ... 223. 223. 223.]\n",
      " [222. 222. 222. ... 222. 222. 222.]\n",
      " [220. 220. 220. ... 220. 220. 220.]\n",
      " ...\n",
      " [212. 212. 212. ... 212. 212. 212.]\n",
      " [218. 218. 218. ... 218. 218. 218.]\n",
      " [224. 224. 224. ... 224. 224. 224.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (2575, 12), Train/test: 1/2574\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "x here is\n",
      "[[101. 100. 100. ...  90.  87.  86.]\n",
      " [100. 100.  99. ...  87.  86.  85.]\n",
      " [100.  99.  98. ...  86.  85.  85.]\n",
      " ...\n",
      " [ 68.  68.  69. ...  74.  76.  79.]\n",
      " [ 68.  69.  69. ...  76.  79.  87.]\n",
      " [ 69.  69.  69. ...  79.  87.  95.]]\n",
      "y here is\n",
      "[[ 86.  86.  86. ...  86.  86.  86.]\n",
      " [ 85.  85.  85. ...  85.  85.  85.]\n",
      " [ 83.  83.  83. ...  83.  83.  83.]\n",
      " ...\n",
      " [120. 120. 120. ... 120. 120. 120.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [128. 128. 128. ... 128. 128. 128.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 420/10203\n",
      "Found 21 continuous time series\n",
      "Data shape: (10625, 12), Train/test: 10623/2\n",
      "Train test ratio: 5311.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47015F160>\n",
      "Epoch 0, test loss: 0.156273\n",
      "Epoch 1, test loss: 0.156946\n",
      "Epoch 2, test loss: 0.156798\n",
      "Epoch 3, test loss: 0.156420\n",
      "Epoch 4, test loss: 0.156341\n",
      "Epoch 5, test loss: 0.156482\n",
      "Epoch 6, test loss: 0.156830\n",
      "Epoch 7, test loss: 0.169914\n",
      "Epoch 8, test loss: 0.160724\n",
      "Epoch 9, test loss: 0.161304\n",
      "Epoch 10, test loss: 0.156597\n",
      "Epoch 11, test loss: 0.157814\n",
      "Epoch 12, test loss: 0.158881\n",
      "Epoch 13, test loss: 0.161793\n",
      "Epoch 14, test loss: 0.164874\n",
      "Epoch 15, test loss: 0.156735\n",
      "Epoch 16, test loss: 0.157824\n",
      "Epoch 17, test loss: 0.160958\n",
      "Epoch 18, test loss: 0.162411\n",
      "Epoch 19, test loss: 0.160003\n",
      "Epoch 20, test loss: 0.158714\n",
      "Epoch 21, test loss: 0.159172\n",
      "Epoch 22, test loss: 0.161019\n",
      "Epoch 23, test loss: 0.157667\n",
      "Epoch 24, test loss: 0.159791\n",
      "Epoch 25, test loss: 0.158278\n",
      "Epoch 26, test loss: 0.156874\n",
      "Epoch 27, test loss: 0.158353\n",
      "Epoch 28, test loss: 0.157139\n",
      "Epoch 29, test loss: 0.158020\n",
      "Epoch 30, test loss: 0.158644\n",
      "Epoch 31, test loss: 0.158982\n",
      "Epoch 32, test loss: 0.156662\n",
      "Epoch 33, test loss: 0.158954\n",
      "Epoch 34, test loss: 0.157912\n",
      "Epoch 35, test loss: 0.156839\n",
      "Epoch 36, test loss: 0.157734\n",
      "Epoch 37, test loss: 0.161135\n",
      "Epoch 38, test loss: 0.156597\n",
      "Epoch 39, test loss: 0.158326\n",
      "Epoch 40, test loss: 0.156678\n",
      "Epoch 41, test loss: 0.159355\n",
      "Epoch 42, test loss: 0.158351\n",
      "Epoch 43, test loss: 0.156497\n",
      "Epoch 44, test loss: 0.161318\n",
      "Epoch 45, test loss: 0.163442\n",
      "Epoch 46, test loss: 0.161672\n",
      "Epoch 47, test loss: 0.158251\n",
      "Epoch 48, test loss: 0.163143\n",
      "Epoch 49, test loss: 0.158499\n",
      "Epoch 50, test loss: 0.157720\n",
      "Epoch 51, test loss: 0.161937\n",
      "Epoch 52, test loss: 0.164749\n",
      "Epoch 53, test loss: 0.159325\n",
      "Epoch 54, test loss: 0.158468\n",
      "Epoch 55, test loss: 0.156721\n",
      "Epoch 56, test loss: 0.158918\n",
      "Epoch 57, test loss: 0.158766\n",
      "Epoch 58, test loss: 0.156800\n",
      "Epoch 59, test loss: 0.158542\n",
      "Epoch 60, test loss: 0.157140\n",
      "Epoch 61, test loss: 0.156995\n",
      "Epoch 62, test loss: 0.157162\n",
      "Epoch 63, test loss: 0.156620\n",
      "Epoch 64, test loss: 0.157730\n",
      "Epoch 65, test loss: 0.159862\n",
      "Epoch 66, test loss: 0.160871\n",
      "Epoch 67, test loss: 0.156724\n",
      "Epoch 68, test loss: 0.157944\n",
      "Epoch 69, test loss: 0.164717\n",
      "Epoch 70, test loss: 0.162308\n",
      "Epoch 71, test loss: 0.157724\n",
      "Epoch 72, test loss: 0.161343\n",
      "Epoch 73, test loss: 0.160558\n",
      "Epoch 74, test loss: 0.162634\n",
      "Epoch 75, test loss: 0.168843\n",
      "Epoch 76, test loss: 0.160081\n",
      "Epoch 77, test loss: 0.157254\n",
      "Epoch 78, test loss: 0.164239\n",
      "Epoch 79, test loss: 0.158162\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47015F160>\n",
      "Epoch 0, test loss: 0.157370\n",
      "Epoch 1, test loss: 0.157310\n",
      "Epoch 2, test loss: 0.158779\n",
      "Epoch 3, test loss: 0.157116\n",
      "Epoch 4, test loss: 0.157313\n",
      "Epoch 5, test loss: 0.157553\n",
      "Epoch 6, test loss: 0.157440\n",
      "Epoch 7, test loss: 0.157972\n",
      "Epoch 8, test loss: 0.157554\n",
      "Epoch 9, test loss: 0.157990\n",
      "Epoch 10, test loss: 0.157778\n",
      "Epoch 11, test loss: 0.157582\n",
      "Epoch 12, test loss: 0.157565\n",
      "Epoch 13, test loss: 0.158947\n",
      "Epoch 14, test loss: 0.158017\n",
      "Epoch 15, test loss: 0.162872\n",
      "Epoch 16, test loss: 0.160499\n",
      "Epoch 17, test loss: 0.158276\n",
      "Epoch 18, test loss: 0.160225\n",
      "Epoch 19, test loss: 0.158543\n",
      "Epoch 20, test loss: 0.157913\n",
      "Epoch 21, test loss: 0.166277\n",
      "Epoch 22, test loss: 0.157793\n",
      "Epoch 23, test loss: 0.158771\n",
      "Epoch 24, test loss: 0.158229\n",
      "Epoch 25, test loss: 0.158570\n",
      "Epoch 26, test loss: 0.158525\n",
      "Epoch 27, test loss: 0.157726\n",
      "Epoch 28, test loss: 0.157937\n",
      "Epoch 29, test loss: 0.157746\n",
      "Epoch 30, test loss: 0.158510\n",
      "Epoch 31, test loss: 0.163785\n",
      "Epoch 32, test loss: 0.158233\n",
      "Epoch 33, test loss: 0.164947\n",
      "Epoch 34, test loss: 0.157917\n",
      "Epoch 35, test loss: 0.158200\n",
      "Epoch 36, test loss: 0.166671\n",
      "Epoch 37, test loss: 0.163474\n",
      "Epoch 38, test loss: 0.159454\n",
      "Epoch 39, test loss: 0.160378\n",
      "Epoch 40, test loss: 0.162261\n",
      "Epoch 41, test loss: 0.162888\n",
      "Epoch 42, test loss: 0.158567\n",
      "Epoch 43, test loss: 0.158201\n",
      "Epoch 44, test loss: 0.158395\n",
      "Epoch 45, test loss: 0.159076\n",
      "Epoch 46, test loss: 0.167569\n",
      "Epoch 47, test loss: 0.157820\n",
      "Epoch 48, test loss: 0.159107\n",
      "Epoch 49, test loss: 0.159234\n",
      "Epoch 50, test loss: 0.162623\n",
      "Epoch 51, test loss: 0.158310\n",
      "Epoch 52, test loss: 0.158060\n",
      "Epoch 53, test loss: 0.159668\n",
      "Epoch 54, test loss: 0.158974\n",
      "Epoch 55, test loss: 0.158436\n",
      "Epoch 56, test loss: 0.159910\n",
      "Epoch 57, test loss: 0.158744\n",
      "Epoch 58, test loss: 0.158017\n",
      "Epoch 59, test loss: 0.159059\n",
      "Epoch 60, test loss: 0.157870\n",
      "Epoch 61, test loss: 0.159436\n",
      "Epoch 62, test loss: 0.159115\n",
      "Epoch 63, test loss: 0.159175\n",
      "Epoch 64, test loss: 0.159720\n",
      "Epoch 65, test loss: 0.162664\n",
      "Epoch 66, test loss: 0.158583\n",
      "Epoch 67, test loss: 0.160048\n",
      "Epoch 68, test loss: 0.157687\n",
      "Epoch 69, test loss: 0.157629\n",
      "Epoch 70, test loss: 0.160747\n",
      "Epoch 71, test loss: 0.157965\n",
      "Epoch 72, test loss: 0.161322\n",
      "Epoch 73, test loss: 0.157824\n",
      "Epoch 74, test loss: 0.158146\n",
      "Epoch 75, test loss: 0.158219\n",
      "Epoch 76, test loss: 0.166349\n",
      "Epoch 77, test loss: 0.157711\n",
      "Epoch 78, test loss: 0.157958\n",
      "Epoch 79, test loss: 0.157551\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47015F160>\n",
      "Epoch 0, test loss: 0.200041\n",
      "Epoch 1, test loss: 0.184329\n",
      "Epoch 2, test loss: 0.176998\n",
      "Epoch 3, test loss: 0.172404\n",
      "Epoch 4, test loss: 0.168855\n",
      "Epoch 5, test loss: 0.166586\n",
      "Epoch 6, test loss: 0.165719\n",
      "Epoch 7, test loss: 0.164286\n",
      "Epoch 8, test loss: 0.164625\n",
      "Epoch 9, test loss: 0.163851\n",
      "Epoch 10, test loss: 0.163370\n",
      "Epoch 11, test loss: 0.166148\n",
      "Epoch 12, test loss: 0.162890\n",
      "Epoch 13, test loss: 0.165129\n",
      "Epoch 14, test loss: 0.162037\n",
      "Epoch 15, test loss: 0.161512\n",
      "Epoch 16, test loss: 0.165880\n",
      "Epoch 17, test loss: 0.162469\n",
      "Epoch 18, test loss: 0.167858\n",
      "Epoch 19, test loss: 0.163983\n",
      "Epoch 20, test loss: 0.160879\n",
      "Epoch 21, test loss: 0.162719\n",
      "Epoch 22, test loss: 0.164801\n",
      "Epoch 23, test loss: 0.160773\n",
      "Epoch 24, test loss: 0.165373\n",
      "Epoch 25, test loss: 0.161385\n",
      "Epoch 26, test loss: 0.163716\n",
      "Epoch 27, test loss: 0.165144\n",
      "Epoch 28, test loss: 0.171814\n",
      "Epoch 29, test loss: 0.162164\n",
      "Epoch 30, test loss: 0.165856\n",
      "Epoch 31, test loss: 0.163336\n",
      "Epoch 32, test loss: 0.165978\n",
      "Epoch 33, test loss: 0.160307\n",
      "Epoch 34, test loss: 0.159660\n",
      "Epoch 35, test loss: 0.160394\n",
      "Epoch 36, test loss: 0.160826\n",
      "Epoch 37, test loss: 0.160416\n",
      "Epoch 38, test loss: 0.159583\n",
      "Epoch 39, test loss: 0.159654\n",
      "Epoch 40, test loss: 0.160827\n",
      "Epoch 41, test loss: 0.160963\n",
      "Epoch 42, test loss: 0.160549\n",
      "Epoch 43, test loss: 0.168419\n",
      "Epoch 44, test loss: 0.159795\n",
      "Epoch 45, test loss: 0.162380\n",
      "Epoch 46, test loss: 0.159748\n",
      "Epoch 47, test loss: 0.159541\n",
      "Epoch 48, test loss: 0.160395\n",
      "Epoch 49, test loss: 0.160290\n",
      "Epoch 50, test loss: 0.161614\n",
      "Epoch 51, test loss: 0.160094\n",
      "Epoch 52, test loss: 0.160744\n",
      "Epoch 53, test loss: 0.160324\n",
      "Epoch 54, test loss: 0.160118\n",
      "Epoch 55, test loss: 0.159713\n",
      "Epoch 56, test loss: 0.159790\n",
      "Epoch 57, test loss: 0.162671\n",
      "Epoch 58, test loss: 0.161879\n",
      "Epoch 59, test loss: 0.164052\n",
      "Epoch 60, test loss: 0.161464\n",
      "Epoch 61, test loss: 0.160615\n",
      "Epoch 62, test loss: 0.159570\n",
      "Epoch 63, test loss: 0.159918\n",
      "Epoch 64, test loss: 0.160700\n",
      "Epoch 65, test loss: 0.160132\n",
      "Epoch 66, test loss: 0.163852\n",
      "Epoch 67, test loss: 0.161247\n",
      "Epoch 68, test loss: 0.159870\n",
      "Epoch 69, test loss: 0.161135\n",
      "Epoch 70, test loss: 0.161888\n",
      "Epoch 71, test loss: 0.160673\n",
      "Epoch 72, test loss: 0.160436\n",
      "Epoch 73, test loss: 0.159785\n",
      "Epoch 74, test loss: 0.162061\n",
      "Epoch 75, test loss: 0.161677\n",
      "Epoch 76, test loss: 0.163701\n",
      "Epoch 77, test loss: 0.160927\n",
      "Epoch 78, test loss: 0.164761\n",
      "Epoch 79, test loss: 0.163551\n",
      "Epoch 80, test loss: 0.160279\n",
      "Epoch 81, test loss: 0.163944\n",
      "Epoch 82, test loss: 0.161159\n",
      "Epoch 83, test loss: 0.159858\n",
      "Epoch 84, test loss: 0.160569\n",
      "Epoch 85, test loss: 0.163989\n",
      "Epoch 86, test loss: 0.159963\n",
      "Epoch 87, test loss: 0.161164\n",
      "Epoch 88, test loss: 0.162642\n",
      "Epoch 89, test loss: 0.162327\n",
      "Epoch 90, test loss: 0.160708\n",
      "Epoch 91, test loss: 0.159831\n",
      "Epoch 92, test loss: 0.163903\n",
      "Epoch 93, test loss: 0.160931\n",
      "Epoch 94, test loss: 0.160014\n",
      "Epoch 95, test loss: 0.160978\n",
      "Epoch 96, test loss: 0.160730\n",
      "Epoch 97, test loss: 0.161960\n",
      "Epoch 98, test loss: 0.159977\n",
      "Epoch 99, test loss: 0.160487\n",
      "Epoch 100, test loss: 0.161105\n",
      "Epoch 101, test loss: 0.160899\n",
      "Epoch 102, test loss: 0.167790\n",
      "Epoch 103, test loss: 0.160259\n",
      "Epoch 104, test loss: 0.167809\n",
      "Epoch 105, test loss: 0.161721\n",
      "Epoch 106, test loss: 0.159823\n",
      "Epoch 107, test loss: 0.163000\n",
      "Epoch 108, test loss: 0.163577\n",
      "Epoch 109, test loss: 0.159913\n",
      "Epoch 110, test loss: 0.160901\n",
      "Epoch 111, test loss: 0.160189\n",
      "Epoch 112, test loss: 0.160487\n",
      "Epoch 113, test loss: 0.162576\n",
      "Epoch 114, test loss: 0.162028\n",
      "Epoch 115, test loss: 0.169831\n",
      "Epoch 116, test loss: 0.160042\n",
      "Epoch 117, test loss: 0.160962\n",
      "Epoch 118, test loss: 0.161729\n",
      "Epoch 119, test loss: 0.162183\n",
      "Epoch 120, test loss: 0.165464\n",
      "Epoch 121, test loss: 0.162106\n",
      "Epoch 122, test loss: 0.162397\n",
      "Epoch 123, test loss: 0.161660\n",
      "Epoch 124, test loss: 0.164212\n",
      "Epoch 125, test loss: 0.161668\n",
      "Epoch 126, test loss: 0.159844\n",
      "Epoch 127, test loss: 0.161731\n",
      "Epoch 128, test loss: 0.165052\n",
      "Epoch 129, test loss: 0.171259\n",
      "Epoch 130, test loss: 0.162644\n",
      "Epoch 131, test loss: 0.162233\n",
      "Epoch 132, test loss: 0.161508\n",
      "Epoch 133, test loss: 0.165018\n",
      "Epoch 134, test loss: 0.165067\n",
      "Epoch 135, test loss: 0.160243\n",
      "Epoch 136, test loss: 0.163989\n",
      "Epoch 137, test loss: 0.165495\n",
      "Epoch 138, test loss: 0.161334\n",
      "Epoch 139, test loss: 0.161154\n",
      "Epoch 140, test loss: 0.159400\n",
      "Epoch 141, test loss: 0.161720\n",
      "Epoch 142, test loss: 0.161463\n",
      "Epoch 143, test loss: 0.160872\n",
      "Epoch 144, test loss: 0.166778\n",
      "Epoch 145, test loss: 0.160542\n",
      "Epoch 146, test loss: 0.161443\n",
      "Epoch 147, test loss: 0.159392\n",
      "Epoch 148, test loss: 0.164159\n",
      "Epoch 149, test loss: 0.160770\n",
      "Epoch 150, test loss: 0.163026\n",
      "Epoch 151, test loss: 0.161380\n",
      "Epoch 152, test loss: 0.160511\n",
      "Epoch 153, test loss: 0.163177\n",
      "Epoch 154, test loss: 0.160780\n",
      "Epoch 155, test loss: 0.165383\n",
      "Epoch 156, test loss: 0.162623\n",
      "Epoch 157, test loss: 0.159920\n",
      "Epoch 158, test loss: 0.160764\n",
      "Epoch 159, test loss: 0.160970\n",
      "Epoch 160, test loss: 0.160020\n",
      "Epoch 161, test loss: 0.160648\n",
      "Epoch 162, test loss: 0.159877\n",
      "Epoch 163, test loss: 0.162872\n",
      "Epoch 164, test loss: 0.160531\n",
      "Epoch 165, test loss: 0.160577\n",
      "Epoch 166, test loss: 0.159759\n",
      "Epoch 167, test loss: 0.161984\n",
      "Epoch 168, test loss: 0.161565\n",
      "Epoch 169, test loss: 0.160849\n",
      "Epoch 170, test loss: 0.160084\n",
      "Epoch 171, test loss: 0.159552\n",
      "Epoch 172, test loss: 0.162572\n",
      "Epoch 173, test loss: 0.161199\n",
      "Epoch 174, test loss: 0.161867\n",
      "Epoch 175, test loss: 0.163449\n",
      "Epoch 176, test loss: 0.159946\n",
      "Epoch 177, test loss: 0.159690\n",
      "Epoch 178, test loss: 0.162086\n",
      "Epoch 179, test loss: 0.159659\n",
      "Epoch 180, test loss: 0.163443\n",
      "Epoch 181, test loss: 0.161991\n",
      "Epoch 182, test loss: 0.159862\n",
      "Epoch 183, test loss: 0.160195\n",
      "Epoch 184, test loss: 0.161032\n",
      "Epoch 185, test loss: 0.159178\n",
      "Epoch 186, test loss: 0.160119\n",
      "Epoch 187, test loss: 0.159058\n",
      "Epoch 188, test loss: 0.159884\n",
      "Epoch 189, test loss: 0.160550\n",
      "Epoch 190, test loss: 0.159494\n",
      "Epoch 191, test loss: 0.160130\n",
      "Epoch 192, test loss: 0.159516\n",
      "Epoch 193, test loss: 0.158617\n",
      "Epoch 194, test loss: 0.158467\n",
      "Epoch 195, test loss: 0.162489\n",
      "Epoch 196, test loss: 0.164963\n",
      "Epoch 197, test loss: 0.158518\n",
      "Epoch 198, test loss: 0.160924\n",
      "Epoch 199, test loss: 0.159176\n",
      "Epoch 200, test loss: 0.167244\n",
      "Epoch 201, test loss: 0.159073\n",
      "Epoch 202, test loss: 0.160866\n",
      "Epoch 203, test loss: 0.158581\n",
      "Epoch 204, test loss: 0.160111\n",
      "Epoch 205, test loss: 0.158546\n",
      "Epoch 206, test loss: 0.160110\n",
      "Epoch 207, test loss: 0.158309\n",
      "Epoch 208, test loss: 0.159516\n",
      "Epoch 209, test loss: 0.158829\n",
      "Epoch 210, test loss: 0.158341\n",
      "Epoch 211, test loss: 0.159934\n",
      "Epoch 212, test loss: 0.161897\n",
      "Epoch 213, test loss: 0.169264\n",
      "Epoch 214, test loss: 0.161256\n",
      "Epoch 215, test loss: 0.161863\n",
      "Epoch 216, test loss: 0.159099\n",
      "Epoch 217, test loss: 0.160057\n",
      "Epoch 218, test loss: 0.161127\n",
      "Epoch 219, test loss: 0.158010\n",
      "Epoch 220, test loss: 0.159830\n",
      "Epoch 221, test loss: 0.162261\n",
      "Epoch 222, test loss: 0.171242\n",
      "Epoch 223, test loss: 0.160998\n",
      "Epoch 224, test loss: 0.160112\n",
      "Epoch 225, test loss: 0.158525\n",
      "Epoch 226, test loss: 0.162419\n",
      "Epoch 227, test loss: 0.158656\n",
      "Epoch 228, test loss: 0.160139\n",
      "Epoch 229, test loss: 0.159493\n",
      "Epoch 230, test loss: 0.160919\n",
      "Epoch 231, test loss: 0.157794\n",
      "Epoch 232, test loss: 0.162070\n",
      "Epoch 233, test loss: 0.157783\n",
      "Epoch 234, test loss: 0.158188\n",
      "Epoch 235, test loss: 0.163657\n",
      "Epoch 236, test loss: 0.161369\n",
      "Epoch 237, test loss: 0.159264\n",
      "Epoch 238, test loss: 0.159049\n",
      "Epoch 239, test loss: 0.159064\n",
      "Pretrain data: 19339240.0\n",
      "Building dataset, requesting data from 0 to 831\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7624/100849\n",
      "Found 831 continuous time series\n",
      "Data shape: (108475, 12), Train/test: 108473/2\n",
      "Train test ratio: 54236.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1694\n",
      "Epoch 0, train loss: 0.256066\n",
      "Epoch 1, train loss: 0.266333\n",
      "Epoch 2, train loss: 0.224322\n",
      "Epoch 3, train loss: 0.255640\n",
      "Epoch 4, train loss: 0.204605\n",
      "Epoch 5, train loss: 0.195938\n",
      "Epoch 6, train loss: 0.199326\n",
      "Epoch 7, train loss: 0.235762\n",
      "Epoch 8, train loss: 0.152673\n",
      "Epoch 9, train loss: 0.221185\n",
      "Epoch 10, train loss: 0.206740\n",
      "Epoch 11, train loss: 0.194017\n",
      "Epoch 12, train loss: 0.236683\n",
      "Epoch 13, train loss: 0.206091\n",
      "Epoch 14, train loss: 0.193434\n",
      "Epoch 15, train loss: 0.177877\n",
      "Epoch 16, train loss: 0.173528\n",
      "Epoch 17, train loss: 0.191210\n",
      "Epoch 18, train loss: 0.216039\n",
      "Epoch 19, train loss: 0.175628\n",
      "Epoch 20, train loss: 0.240814\n",
      "Epoch 21, train loss: 0.195563\n",
      "Epoch 22, train loss: 0.173326\n",
      "Epoch 23, train loss: 0.198549\n",
      "Epoch 24, train loss: 0.200809\n",
      "Epoch 25, train loss: 0.192878\n",
      "Epoch 26, train loss: 0.210535\n",
      "Epoch 27, train loss: 0.182101\n",
      "Epoch 28, train loss: 0.197967\n",
      "Epoch 29, train loss: 0.196290\n",
      "Epoch 30, train loss: 0.233785\n",
      "Epoch 31, train loss: 0.160146\n",
      "Epoch 32, train loss: 0.177322\n",
      "Epoch 33, train loss: 0.205520\n",
      "Epoch 34, train loss: 0.209826\n",
      "Epoch 35, train loss: 0.255598\n",
      "Epoch 36, train loss: 0.188156\n",
      "Epoch 37, train loss: 0.214247\n",
      "Epoch 38, train loss: 0.230236\n",
      "Epoch 39, train loss: 0.275582\n",
      "Epoch 40, train loss: 0.216986\n",
      "Epoch 41, train loss: 0.216705\n",
      "Epoch 42, train loss: 0.202009\n",
      "Epoch 43, train loss: 0.169851\n",
      "Epoch 44, train loss: 0.185073\n",
      "Epoch 45, train loss: 0.241288\n",
      "Epoch 46, train loss: 0.205973\n",
      "Epoch 47, train loss: 0.201372\n",
      "Epoch 48, train loss: 0.195388\n",
      "Epoch 49, train loss: 0.213037\n",
      "Epoch 50, train loss: 0.237255\n",
      "Epoch 51, train loss: 0.196179\n",
      "Epoch 52, train loss: 0.249535\n",
      "Epoch 53, train loss: 0.233392\n",
      "Epoch 54, train loss: 0.304618\n",
      "Epoch 55, train loss: 0.204297\n",
      "Epoch 56, train loss: 0.203783\n",
      "Epoch 57, train loss: 0.218677\n",
      "Epoch 58, train loss: 0.208056\n",
      "Epoch 59, train loss: 0.205178\n",
      "Epoch 60, train loss: 0.178073\n",
      "Epoch 61, train loss: 0.199187\n",
      "Epoch 62, train loss: 0.179932\n",
      "Epoch 63, train loss: 0.162013\n",
      "Epoch 64, train loss: 0.213104\n",
      "Epoch 65, train loss: 0.244219\n",
      "Epoch 66, train loss: 0.189034\n",
      "Epoch 67, train loss: 0.192950\n",
      "Epoch 68, train loss: 0.228903\n",
      "Epoch 69, train loss: 0.218268\n",
      "Epoch 70, train loss: 0.230078\n",
      "Epoch 71, train loss: 0.207829\n",
      "Epoch 72, train loss: 0.207719\n",
      "Epoch 73, train loss: 0.238593\n",
      "Epoch 74, train loss: 0.184158\n",
      "Epoch 75, train loss: 0.255918\n",
      "Epoch 76, train loss: 0.197839\n",
      "Epoch 77, train loss: 0.179088\n",
      "Epoch 78, train loss: 0.260788\n",
      "Epoch 79, train loss: 0.177721\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "x here is\n",
      "[[127. 123. 118. ...  93.  93.  89.]\n",
      " [123. 118. 112. ...  93.  89.  87.]\n",
      " [118. 112. 108. ...  89.  87.  85.]\n",
      " ...\n",
      " [182. 191. 203. ... 254. 263. 280.]\n",
      " [191. 203. 215. ... 263. 280. 288.]\n",
      " [203. 215. 223. ... 280. 288. 301.]]\n",
      "y here is\n",
      "[[ 76.  76.  76. ...  76.  76.  76.]\n",
      " [ 74.  74.  74. ...  74.  74.  74.]\n",
      " [ 83.  83.  83. ...  83.  83.  83.]\n",
      " ...\n",
      " [307. 307. 307. ... 307. 307. 307.]\n",
      " [311. 311. 311. ... 311. 311. 311.]\n",
      " [321. 321. 321. ... 321. 321. 321.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 3 continuous time series\n",
      "Data shape: (2740, 12), Train/test: 1/2739\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[116. 117. 119. ... 117. 121. 126.]\n",
      " [117. 119. 116. ... 121. 126. 131.]\n",
      " [119. 116. 111. ... 126. 131. 136.]\n",
      " ...\n",
      " [218. 214. 212. ... 180. 175. 171.]\n",
      " [214. 212. 211. ... 175. 171. 168.]\n",
      " [212. 211. 207. ... 171. 168. 162.]]\n",
      "y here is\n",
      "[[149. 149. 149. ... 149. 149. 149.]\n",
      " [149. 149. 149. ... 149. 149. 149.]\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " ...\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [137. 137. 137. ... 137. 137. 137.]\n",
      " [132. 132. 132. ... 132. 132. 132.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 284/12176\n",
      "Found 11 continuous time series\n",
      "Data shape: (12462, 12), Train/test: 12460/2\n",
      "Train test ratio: 6230.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29775DE10>\n",
      "Epoch 0, test loss: 0.187985\n",
      "Epoch 1, test loss: 0.186035\n",
      "Epoch 2, test loss: 0.187547\n",
      "Epoch 3, test loss: 0.185365\n",
      "Epoch 4, test loss: 0.185209\n",
      "Epoch 5, test loss: 0.185237\n",
      "Epoch 6, test loss: 0.185908\n",
      "Epoch 7, test loss: 0.186383\n",
      "Epoch 8, test loss: 0.185606\n",
      "Epoch 9, test loss: 0.185759\n",
      "Epoch 10, test loss: 0.194115\n",
      "Epoch 11, test loss: 0.186232\n",
      "Epoch 12, test loss: 0.186519\n",
      "Epoch 13, test loss: 0.184998\n",
      "Epoch 14, test loss: 0.186515\n",
      "Epoch 15, test loss: 0.186319\n",
      "Epoch 16, test loss: 0.191869\n",
      "Epoch 17, test loss: 0.186611\n",
      "Epoch 18, test loss: 0.190811\n",
      "Epoch 19, test loss: 0.186035\n",
      "Epoch 20, test loss: 0.187899\n",
      "Epoch 21, test loss: 0.188575\n",
      "Epoch 22, test loss: 0.185470\n",
      "Epoch 23, test loss: 0.196167\n",
      "Epoch 24, test loss: 0.186580\n",
      "Epoch 25, test loss: 0.187544\n",
      "Epoch 26, test loss: 0.187101\n",
      "Epoch 27, test loss: 0.190131\n",
      "Epoch 28, test loss: 0.192003\n",
      "Epoch 29, test loss: 0.188498\n",
      "Epoch 30, test loss: 0.191362\n",
      "Epoch 31, test loss: 0.188406\n",
      "Epoch 32, test loss: 0.187977\n",
      "Epoch 33, test loss: 0.185846\n",
      "Epoch 34, test loss: 0.186677\n",
      "Epoch 35, test loss: 0.187156\n",
      "Epoch 36, test loss: 0.186904\n",
      "Epoch 37, test loss: 0.188722\n",
      "Epoch 38, test loss: 0.188218\n",
      "Epoch 39, test loss: 0.192515\n",
      "Epoch 40, test loss: 0.185963\n",
      "Epoch 41, test loss: 0.185649\n",
      "Epoch 42, test loss: 0.190574\n",
      "Epoch 43, test loss: 0.186558\n",
      "Epoch 44, test loss: 0.186223\n",
      "Epoch 45, test loss: 0.191895\n",
      "Epoch 46, test loss: 0.189127\n",
      "Epoch 47, test loss: 0.201983\n",
      "Epoch 48, test loss: 0.190767\n",
      "Epoch 49, test loss: 0.189885\n",
      "Epoch 50, test loss: 0.188226\n",
      "Epoch 51, test loss: 0.190824\n",
      "Epoch 52, test loss: 0.185560\n",
      "Epoch 53, test loss: 0.193487\n",
      "Epoch 54, test loss: 0.187197\n",
      "Epoch 55, test loss: 0.186432\n",
      "Epoch 56, test loss: 0.185990\n",
      "Epoch 57, test loss: 0.190778\n",
      "Epoch 58, test loss: 0.191524\n",
      "Epoch 59, test loss: 0.186060\n",
      "Epoch 60, test loss: 0.197142\n",
      "Epoch 61, test loss: 0.187271\n",
      "Epoch 62, test loss: 0.188113\n",
      "Epoch 63, test loss: 0.192825\n",
      "Epoch 64, test loss: 0.191561\n",
      "Epoch 65, test loss: 0.186076\n",
      "Epoch 66, test loss: 0.189529\n",
      "Epoch 67, test loss: 0.186628\n",
      "Epoch 68, test loss: 0.194210\n",
      "Epoch 69, test loss: 0.186832\n",
      "Epoch 70, test loss: 0.193174\n",
      "Epoch 71, test loss: 0.187859\n",
      "Epoch 72, test loss: 0.187470\n",
      "Epoch 73, test loss: 0.188467\n",
      "Epoch 74, test loss: 0.188637\n",
      "Epoch 75, test loss: 0.187762\n",
      "Epoch 76, test loss: 0.187654\n",
      "Epoch 77, test loss: 0.190908\n",
      "Epoch 78, test loss: 0.190649\n",
      "Epoch 79, test loss: 0.192055\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29775DE10>\n",
      "Epoch 0, test loss: 0.187375\n",
      "Epoch 1, test loss: 0.186365\n",
      "Epoch 2, test loss: 0.189832\n",
      "Epoch 3, test loss: 0.188712\n",
      "Epoch 4, test loss: 0.186317\n",
      "Epoch 5, test loss: 0.185926\n",
      "Epoch 6, test loss: 0.187166\n",
      "Epoch 7, test loss: 0.187955\n",
      "Epoch 8, test loss: 0.185702\n",
      "Epoch 9, test loss: 0.186607\n",
      "Epoch 10, test loss: 0.188485\n",
      "Epoch 11, test loss: 0.186330\n",
      "Epoch 12, test loss: 0.193301\n",
      "Epoch 13, test loss: 0.190839\n",
      "Epoch 14, test loss: 0.187162\n",
      "Epoch 15, test loss: 0.185376\n",
      "Epoch 16, test loss: 0.188342\n",
      "Epoch 17, test loss: 0.185869\n",
      "Epoch 18, test loss: 0.188881\n",
      "Epoch 19, test loss: 0.185065\n",
      "Epoch 20, test loss: 0.185676\n",
      "Epoch 21, test loss: 0.187208\n",
      "Epoch 22, test loss: 0.185130\n",
      "Epoch 23, test loss: 0.187277\n",
      "Epoch 24, test loss: 0.186212\n",
      "Epoch 25, test loss: 0.187158\n",
      "Epoch 26, test loss: 0.188611\n",
      "Epoch 27, test loss: 0.186610\n",
      "Epoch 28, test loss: 0.185519\n",
      "Epoch 29, test loss: 0.194437\n",
      "Epoch 30, test loss: 0.188703\n",
      "Epoch 31, test loss: 0.187916\n",
      "Epoch 32, test loss: 0.186887\n",
      "Epoch 33, test loss: 0.185497\n",
      "Epoch 34, test loss: 0.190576\n",
      "Epoch 35, test loss: 0.186857\n",
      "Epoch 36, test loss: 0.189082\n",
      "Epoch 37, test loss: 0.187434\n",
      "Epoch 38, test loss: 0.189878\n",
      "Epoch 39, test loss: 0.186437\n",
      "Epoch 40, test loss: 0.186857\n",
      "Epoch 41, test loss: 0.185854\n",
      "Epoch 42, test loss: 0.187820\n",
      "Epoch 43, test loss: 0.187165\n",
      "Epoch 44, test loss: 0.187746\n",
      "Epoch 45, test loss: 0.186323\n",
      "Epoch 46, test loss: 0.188028\n",
      "Epoch 47, test loss: 0.186637\n",
      "Epoch 48, test loss: 0.185776\n",
      "Epoch 49, test loss: 0.186260\n",
      "Epoch 50, test loss: 0.186691\n",
      "Epoch 51, test loss: 0.186803\n",
      "Epoch 52, test loss: 0.190137\n",
      "Epoch 53, test loss: 0.186730\n",
      "Epoch 54, test loss: 0.186276\n",
      "Epoch 55, test loss: 0.186302\n",
      "Epoch 56, test loss: 0.188713\n",
      "Epoch 57, test loss: 0.187597\n",
      "Epoch 58, test loss: 0.188604\n",
      "Epoch 59, test loss: 0.187091\n",
      "Epoch 60, test loss: 0.186931\n",
      "Epoch 61, test loss: 0.188456\n",
      "Epoch 62, test loss: 0.188663\n",
      "Epoch 63, test loss: 0.189262\n",
      "Epoch 64, test loss: 0.189228\n",
      "Epoch 65, test loss: 0.189535\n",
      "Epoch 66, test loss: 0.186551\n",
      "Epoch 67, test loss: 0.185335\n",
      "Epoch 68, test loss: 0.186776\n",
      "Epoch 69, test loss: 0.187171\n",
      "Epoch 70, test loss: 0.189089\n",
      "Epoch 71, test loss: 0.193808\n",
      "Epoch 72, test loss: 0.186865\n",
      "Epoch 73, test loss: 0.187210\n",
      "Epoch 74, test loss: 0.187685\n",
      "Epoch 75, test loss: 0.190711\n",
      "Epoch 76, test loss: 0.186660\n",
      "Epoch 77, test loss: 0.187017\n",
      "Epoch 78, test loss: 0.187112\n",
      "Epoch 79, test loss: 0.186668\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29775DE10>\n",
      "Epoch 0, test loss: 0.226646\n",
      "Epoch 1, test loss: 0.207350\n",
      "Epoch 2, test loss: 0.198313\n",
      "Epoch 3, test loss: 0.193204\n",
      "Epoch 4, test loss: 0.189894\n",
      "Epoch 5, test loss: 0.191847\n",
      "Epoch 6, test loss: 0.196351\n",
      "Epoch 7, test loss: 0.188634\n",
      "Epoch 8, test loss: 0.188208\n",
      "Epoch 9, test loss: 0.190669\n",
      "Epoch 10, test loss: 0.194712\n",
      "Epoch 11, test loss: 0.189661\n",
      "Epoch 12, test loss: 0.191041\n",
      "Epoch 13, test loss: 0.196051\n",
      "Epoch 14, test loss: 0.188138\n",
      "Epoch 15, test loss: 0.190986\n",
      "Epoch 16, test loss: 0.188727\n",
      "Epoch 17, test loss: 0.188422\n",
      "Epoch 18, test loss: 0.198318\n",
      "Epoch 19, test loss: 0.189168\n",
      "Epoch 20, test loss: 0.193556\n",
      "Epoch 21, test loss: 0.188366\n",
      "Epoch 22, test loss: 0.188114\n",
      "Epoch 23, test loss: 0.187999\n",
      "Epoch 24, test loss: 0.192147\n",
      "Epoch 25, test loss: 0.192526\n",
      "Epoch 26, test loss: 0.196381\n",
      "Epoch 27, test loss: 0.189977\n",
      "Epoch 28, test loss: 0.191133\n",
      "Epoch 29, test loss: 0.187902\n",
      "Epoch 30, test loss: 0.189003\n",
      "Epoch 31, test loss: 0.188449\n",
      "Epoch 32, test loss: 0.191986\n",
      "Epoch 33, test loss: 0.187951\n",
      "Epoch 34, test loss: 0.188972\n",
      "Epoch 35, test loss: 0.189224\n",
      "Epoch 36, test loss: 0.188137\n",
      "Epoch 37, test loss: 0.188044\n",
      "Epoch 38, test loss: 0.186921\n",
      "Epoch 39, test loss: 0.187957\n",
      "Epoch 40, test loss: 0.187682\n",
      "Epoch 41, test loss: 0.189137\n",
      "Epoch 42, test loss: 0.186700\n",
      "Epoch 43, test loss: 0.194107\n",
      "Epoch 44, test loss: 0.190402\n",
      "Epoch 45, test loss: 0.187387\n",
      "Epoch 46, test loss: 0.187129\n",
      "Epoch 47, test loss: 0.188513\n",
      "Epoch 48, test loss: 0.186856\n",
      "Epoch 49, test loss: 0.187852\n",
      "Epoch 50, test loss: 0.188920\n",
      "Epoch 51, test loss: 0.187595\n",
      "Epoch 52, test loss: 0.188995\n",
      "Epoch 53, test loss: 0.185726\n",
      "Epoch 54, test loss: 0.188970\n",
      "Epoch 55, test loss: 0.186910\n",
      "Epoch 56, test loss: 0.185883\n",
      "Epoch 57, test loss: 0.188369\n",
      "Epoch 58, test loss: 0.187115\n",
      "Epoch 59, test loss: 0.189632\n",
      "Epoch 60, test loss: 0.185826\n",
      "Epoch 61, test loss: 0.186786\n",
      "Epoch 62, test loss: 0.186980\n",
      "Epoch 63, test loss: 0.189638\n",
      "Epoch 64, test loss: 0.186023\n",
      "Epoch 65, test loss: 0.186409\n",
      "Epoch 66, test loss: 0.186211\n",
      "Epoch 67, test loss: 0.190569\n",
      "Epoch 68, test loss: 0.188075\n",
      "Epoch 69, test loss: 0.189213\n",
      "Epoch 70, test loss: 0.187473\n",
      "Epoch 71, test loss: 0.187822\n",
      "Epoch 72, test loss: 0.190084\n",
      "Epoch 73, test loss: 0.185977\n",
      "Epoch 74, test loss: 0.187012\n",
      "Epoch 75, test loss: 0.187255\n",
      "Epoch 76, test loss: 0.186051\n",
      "Epoch 77, test loss: 0.187394\n",
      "Epoch 78, test loss: 0.187022\n",
      "Epoch 79, test loss: 0.188715\n",
      "Epoch 80, test loss: 0.196276\n",
      "Epoch 81, test loss: 0.187199\n",
      "Epoch 82, test loss: 0.188472\n",
      "Epoch 83, test loss: 0.187750\n",
      "Epoch 84, test loss: 0.186875\n",
      "Epoch 85, test loss: 0.193783\n",
      "Epoch 86, test loss: 0.187888\n",
      "Epoch 87, test loss: 0.187307\n",
      "Epoch 88, test loss: 0.186916\n",
      "Epoch 89, test loss: 0.186144\n",
      "Epoch 90, test loss: 0.192138\n",
      "Epoch 91, test loss: 0.188480\n",
      "Epoch 92, test loss: 0.186312\n",
      "Epoch 93, test loss: 0.190095\n",
      "Epoch 94, test loss: 0.189891\n",
      "Epoch 95, test loss: 0.190159\n",
      "Epoch 96, test loss: 0.186342\n",
      "Epoch 97, test loss: 0.186514\n",
      "Epoch 98, test loss: 0.186261\n",
      "Epoch 99, test loss: 0.185329\n",
      "Epoch 100, test loss: 0.186101\n",
      "Epoch 101, test loss: 0.186324\n",
      "Epoch 102, test loss: 0.188221\n",
      "Epoch 103, test loss: 0.191803\n",
      "Epoch 104, test loss: 0.185853\n",
      "Epoch 105, test loss: 0.191510\n",
      "Epoch 106, test loss: 0.185863\n",
      "Epoch 107, test loss: 0.187652\n",
      "Epoch 108, test loss: 0.198375\n",
      "Epoch 109, test loss: 0.187760\n",
      "Epoch 110, test loss: 0.187070\n",
      "Epoch 111, test loss: 0.190252\n",
      "Epoch 112, test loss: 0.190147\n",
      "Epoch 113, test loss: 0.186626\n",
      "Epoch 114, test loss: 0.189434\n",
      "Epoch 115, test loss: 0.195260\n",
      "Epoch 116, test loss: 0.190091\n",
      "Epoch 117, test loss: 0.185764\n",
      "Epoch 118, test loss: 0.187091\n",
      "Epoch 119, test loss: 0.188813\n",
      "Epoch 120, test loss: 0.186491\n",
      "Epoch 121, test loss: 0.186556\n",
      "Epoch 122, test loss: 0.185710\n",
      "Epoch 123, test loss: 0.187106\n",
      "Epoch 124, test loss: 0.196244\n",
      "Epoch 125, test loss: 0.185636\n",
      "Epoch 126, test loss: 0.188691\n",
      "Epoch 127, test loss: 0.189865\n",
      "Epoch 128, test loss: 0.186756\n",
      "Epoch 129, test loss: 0.186941\n",
      "Epoch 130, test loss: 0.191080\n",
      "Epoch 131, test loss: 0.191631\n",
      "Epoch 132, test loss: 0.186307\n",
      "Epoch 133, test loss: 0.188861\n",
      "Epoch 134, test loss: 0.186979\n",
      "Epoch 135, test loss: 0.186814\n",
      "Epoch 136, test loss: 0.188719\n",
      "Epoch 137, test loss: 0.187109\n",
      "Epoch 138, test loss: 0.185849\n",
      "Epoch 139, test loss: 0.187103\n",
      "Epoch 140, test loss: 0.185690\n",
      "Epoch 141, test loss: 0.186146\n",
      "Epoch 142, test loss: 0.190288\n",
      "Epoch 143, test loss: 0.187766\n",
      "Epoch 144, test loss: 0.186213\n",
      "Epoch 145, test loss: 0.187035\n",
      "Epoch 146, test loss: 0.189571\n",
      "Epoch 147, test loss: 0.190970\n",
      "Epoch 148, test loss: 0.187427\n",
      "Epoch 149, test loss: 0.185668\n",
      "Epoch 150, test loss: 0.186320\n",
      "Epoch 151, test loss: 0.188764\n",
      "Epoch 152, test loss: 0.186122\n",
      "Epoch 153, test loss: 0.187437\n",
      "Epoch 154, test loss: 0.190015\n",
      "Epoch 155, test loss: 0.188016\n",
      "Epoch 156, test loss: 0.188722\n",
      "Epoch 157, test loss: 0.185872\n",
      "Epoch 158, test loss: 0.185922\n",
      "Epoch 159, test loss: 0.189268\n",
      "Epoch 160, test loss: 0.190330\n",
      "Epoch 161, test loss: 0.187000\n",
      "Epoch 162, test loss: 0.190141\n",
      "Epoch 163, test loss: 0.186571\n",
      "Epoch 164, test loss: 0.185751\n",
      "Epoch 165, test loss: 0.185786\n",
      "Epoch 166, test loss: 0.186643\n",
      "Epoch 167, test loss: 0.187202\n",
      "Epoch 168, test loss: 0.186374\n",
      "Epoch 169, test loss: 0.185973\n",
      "Epoch 170, test loss: 0.186066\n",
      "Epoch 171, test loss: 0.186488\n",
      "Epoch 172, test loss: 0.191553\n",
      "Epoch 173, test loss: 0.189064\n",
      "Epoch 174, test loss: 0.186655\n",
      "Epoch 175, test loss: 0.190329\n",
      "Epoch 176, test loss: 0.188955\n",
      "Epoch 177, test loss: 0.186157\n",
      "Epoch 178, test loss: 0.189139\n",
      "Epoch 179, test loss: 0.190768\n",
      "Epoch 180, test loss: 0.186235\n",
      "Epoch 181, test loss: 0.189122\n",
      "Epoch 182, test loss: 0.192390\n",
      "Epoch 183, test loss: 0.186069\n",
      "Epoch 184, test loss: 0.188235\n",
      "Epoch 185, test loss: 0.186888\n",
      "Epoch 186, test loss: 0.186981\n",
      "Epoch 187, test loss: 0.185843\n",
      "Epoch 188, test loss: 0.189769\n",
      "Epoch 189, test loss: 0.194853\n",
      "Epoch 190, test loss: 0.188329\n",
      "Epoch 191, test loss: 0.187298\n",
      "Epoch 192, test loss: 0.185835\n",
      "Epoch 193, test loss: 0.187407\n",
      "Epoch 194, test loss: 0.185914\n",
      "Epoch 195, test loss: 0.194496\n",
      "Epoch 196, test loss: 0.186677\n",
      "Epoch 197, test loss: 0.185772\n",
      "Epoch 198, test loss: 0.186466\n",
      "Epoch 199, test loss: 0.188837\n",
      "Epoch 200, test loss: 0.188651\n",
      "Epoch 201, test loss: 0.187749\n",
      "Epoch 202, test loss: 0.186795\n",
      "Epoch 203, test loss: 0.186354\n",
      "Epoch 204, test loss: 0.191340\n",
      "Epoch 205, test loss: 0.188375\n",
      "Epoch 206, test loss: 0.185860\n",
      "Epoch 207, test loss: 0.197030\n",
      "Epoch 208, test loss: 0.190412\n",
      "Epoch 209, test loss: 0.186271\n",
      "Epoch 210, test loss: 0.187512\n",
      "Epoch 211, test loss: 0.186599\n",
      "Epoch 212, test loss: 0.186255\n",
      "Epoch 213, test loss: 0.187264\n",
      "Epoch 214, test loss: 0.191269\n",
      "Epoch 215, test loss: 0.187480\n",
      "Epoch 216, test loss: 0.188128\n",
      "Epoch 217, test loss: 0.186758\n",
      "Epoch 218, test loss: 0.187514\n",
      "Epoch 219, test loss: 0.186788\n",
      "Epoch 220, test loss: 0.186238\n",
      "Epoch 221, test loss: 0.187157\n",
      "Epoch 222, test loss: 0.185967\n",
      "Epoch 223, test loss: 0.189370\n",
      "Epoch 224, test loss: 0.190360\n",
      "Epoch 225, test loss: 0.186659\n",
      "Epoch 226, test loss: 0.190201\n",
      "Epoch 227, test loss: 0.188292\n",
      "Epoch 228, test loss: 0.188122\n",
      "Epoch 229, test loss: 0.186648\n",
      "Epoch 230, test loss: 0.197400\n",
      "Epoch 231, test loss: 0.186102\n",
      "Epoch 232, test loss: 0.187648\n",
      "Epoch 233, test loss: 0.188432\n",
      "Epoch 234, test loss: 0.186362\n",
      "Epoch 235, test loss: 0.187597\n",
      "Epoch 236, test loss: 0.188273\n",
      "Epoch 237, test loss: 0.189695\n",
      "Epoch 238, test loss: 0.185988\n",
      "Epoch 239, test loss: 0.190327\n",
      "Pretrain data: 19742408.0\n",
      "Building dataset, requesting data from 0 to 769\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [255. 246. 235. ... 232. 236. 237.]\n",
      " [246. 235. 231. ... 236. 237. 247.]\n",
      " [235. 231. 224. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6494/103808\n",
      "Found 769 continuous time series\n",
      "Data shape: (110304, 12), Train/test: 110302/2\n",
      "Train test ratio: 55151.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1723\n",
      "Epoch 0, train loss: 0.261140\n",
      "Epoch 1, train loss: 0.221029\n",
      "Epoch 2, train loss: 0.145732\n",
      "Epoch 3, train loss: 0.216289\n",
      "Epoch 4, train loss: 0.222159\n",
      "Epoch 5, train loss: 0.231164\n",
      "Epoch 6, train loss: 0.194432\n",
      "Epoch 7, train loss: 0.222121\n",
      "Epoch 8, train loss: 0.203982\n",
      "Epoch 9, train loss: 0.209573\n",
      "Epoch 10, train loss: 0.137283\n",
      "Epoch 11, train loss: 0.178918\n",
      "Epoch 12, train loss: 0.155805\n",
      "Epoch 13, train loss: 0.184170\n",
      "Epoch 14, train loss: 0.245925\n",
      "Epoch 15, train loss: 0.190060\n",
      "Epoch 16, train loss: 0.249882\n",
      "Epoch 17, train loss: 0.184405\n",
      "Epoch 18, train loss: 0.184130\n",
      "Epoch 19, train loss: 0.216081\n",
      "Epoch 20, train loss: 0.173084\n",
      "Epoch 21, train loss: 0.262384\n",
      "Epoch 22, train loss: 0.206972\n",
      "Epoch 23, train loss: 0.228580\n",
      "Epoch 24, train loss: 0.197195\n",
      "Epoch 25, train loss: 0.213005\n",
      "Epoch 26, train loss: 0.225876\n",
      "Epoch 27, train loss: 0.211326\n",
      "Epoch 28, train loss: 0.175434\n",
      "Epoch 29, train loss: 0.191694\n",
      "Epoch 30, train loss: 0.252671\n",
      "Epoch 31, train loss: 0.252689\n",
      "Epoch 32, train loss: 0.176848\n",
      "Epoch 33, train loss: 0.218472\n",
      "Epoch 34, train loss: 0.229746\n",
      "Epoch 35, train loss: 0.216824\n",
      "Epoch 36, train loss: 0.162266\n",
      "Epoch 37, train loss: 0.184879\n",
      "Epoch 38, train loss: 0.271419\n",
      "Epoch 39, train loss: 0.180740\n",
      "Epoch 40, train loss: 0.220748\n",
      "Epoch 41, train loss: 0.232207\n",
      "Epoch 42, train loss: 0.235370\n",
      "Epoch 43, train loss: 0.150376\n",
      "Epoch 44, train loss: 0.209465\n",
      "Epoch 45, train loss: 0.206997\n",
      "Epoch 46, train loss: 0.162994\n",
      "Epoch 47, train loss: 0.216233\n",
      "Epoch 48, train loss: 0.183845\n",
      "Epoch 49, train loss: 0.183637\n",
      "Epoch 50, train loss: 0.165648\n",
      "Epoch 51, train loss: 0.266561\n",
      "Epoch 52, train loss: 0.235067\n",
      "Epoch 53, train loss: 0.304934\n",
      "Epoch 54, train loss: 0.204881\n",
      "Epoch 55, train loss: 0.246639\n",
      "Epoch 56, train loss: 0.209981\n",
      "Epoch 57, train loss: 0.189778\n",
      "Epoch 58, train loss: 0.253033\n",
      "Epoch 59, train loss: 0.175076\n",
      "Epoch 60, train loss: 0.211252\n",
      "Epoch 61, train loss: 0.202346\n",
      "Epoch 62, train loss: 0.231513\n",
      "Epoch 63, train loss: 0.277432\n",
      "Epoch 64, train loss: 0.210331\n",
      "Epoch 65, train loss: 0.228615\n",
      "Epoch 66, train loss: 0.223373\n",
      "Epoch 67, train loss: 0.213695\n",
      "Epoch 68, train loss: 0.211723\n",
      "Epoch 69, train loss: 0.243491\n",
      "Epoch 70, train loss: 0.208976\n",
      "Epoch 71, train loss: 0.228901\n",
      "Epoch 72, train loss: 0.187068\n",
      "Epoch 73, train loss: 0.223241\n",
      "Epoch 74, train loss: 0.221676\n",
      "Epoch 75, train loss: 0.222095\n",
      "Epoch 76, train loss: 0.168419\n",
      "Epoch 77, train loss: 0.183726\n",
      "Epoch 78, train loss: 0.192180\n",
      "Epoch 79, train loss: 0.158929\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[214. 217. 217. ... 207. 201. 190.]\n",
      " [217. 217. 212. ... 201. 190. 183.]\n",
      " [217. 212. 209. ... 190. 183. 179.]\n",
      " ...\n",
      " [115. 107. 101. ... 128. 130. 124.]\n",
      " [107. 101. 110. ... 130. 124. 120.]\n",
      " [101. 110. 123. ... 124. 120. 117.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [173. 173. 173. ... 173. 173. 173.]\n",
      " [171. 171. 171. ... 171. 171. 171.]\n",
      " ...\n",
      " [129. 129. 129. ... 129. 129. 129.]\n",
      " [139. 139. 139. ... 139. 139. 139.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (2406, 12), Train/test: 1/2405\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 73 segments\n",
      "Building dataset, requesting data from 0 to 73\n",
      "x here is\n",
      "[[128. 123. 120. ... 119. 119. 118.]\n",
      " [123. 120. 124. ... 119. 118. 117.]\n",
      " [120. 124. 121. ... 118. 117. 117.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[115. 115. 115. ... 115. 115. 115.]\n",
      " [115. 115. 115. ... 115. 115. 115.]\n",
      " [114. 114. 114. ... 114. 114. 114.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1414/9217\n",
      "Found 73 continuous time series\n",
      "Data shape: (10633, 12), Train/test: 10631/2\n",
      "Train test ratio: 5315.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B45030>\n",
      "Epoch 0, test loss: 0.227720\n",
      "Epoch 1, test loss: 0.225360\n",
      "Epoch 2, test loss: 0.224200\n",
      "Epoch 3, test loss: 0.225808\n",
      "Epoch 4, test loss: 0.225602\n",
      "Epoch 5, test loss: 0.223193\n",
      "Epoch 6, test loss: 0.222459\n",
      "Epoch 7, test loss: 0.232466\n",
      "Epoch 8, test loss: 0.222527\n",
      "Epoch 9, test loss: 0.223314\n",
      "Epoch 10, test loss: 0.225988\n",
      "Epoch 11, test loss: 0.227294\n",
      "Epoch 12, test loss: 0.225925\n",
      "Epoch 13, test loss: 0.223760\n",
      "Epoch 14, test loss: 0.222898\n",
      "Epoch 15, test loss: 0.222696\n",
      "Epoch 16, test loss: 0.221976\n",
      "Epoch 17, test loss: 0.222262\n",
      "Epoch 18, test loss: 0.221976\n",
      "Epoch 19, test loss: 0.227900\n",
      "Epoch 20, test loss: 0.224426\n",
      "Epoch 21, test loss: 0.222344\n",
      "Epoch 22, test loss: 0.222721\n",
      "Epoch 23, test loss: 0.224453\n",
      "Epoch 24, test loss: 0.222816\n",
      "Epoch 25, test loss: 0.222078\n",
      "Epoch 26, test loss: 0.225362\n",
      "Epoch 27, test loss: 0.222827\n",
      "Epoch 28, test loss: 0.222610\n",
      "Epoch 29, test loss: 0.221981\n",
      "Epoch 30, test loss: 0.220547\n",
      "Epoch 31, test loss: 0.231413\n",
      "Epoch 32, test loss: 0.224866\n",
      "Epoch 33, test loss: 0.224200\n",
      "Epoch 34, test loss: 0.223028\n",
      "Epoch 35, test loss: 0.223725\n",
      "Epoch 36, test loss: 0.230872\n",
      "Epoch 37, test loss: 0.223885\n",
      "Epoch 38, test loss: 0.223007\n",
      "Epoch 39, test loss: 0.226070\n",
      "Epoch 40, test loss: 0.226857\n",
      "Epoch 41, test loss: 0.224231\n",
      "Epoch 42, test loss: 0.220928\n",
      "Epoch 43, test loss: 0.223173\n",
      "Epoch 44, test loss: 0.222446\n",
      "Epoch 45, test loss: 0.223807\n",
      "Epoch 46, test loss: 0.228216\n",
      "Epoch 47, test loss: 0.224002\n",
      "Epoch 48, test loss: 0.223946\n",
      "Epoch 49, test loss: 0.224739\n",
      "Epoch 50, test loss: 0.225115\n",
      "Epoch 51, test loss: 0.223237\n",
      "Epoch 52, test loss: 0.223484\n",
      "Epoch 53, test loss: 0.228056\n",
      "Epoch 54, test loss: 0.229178\n",
      "Epoch 55, test loss: 0.225196\n",
      "Epoch 56, test loss: 0.223796\n",
      "Epoch 57, test loss: 0.222199\n",
      "Epoch 58, test loss: 0.224631\n",
      "Epoch 59, test loss: 0.223014\n",
      "Epoch 60, test loss: 0.222824\n",
      "Epoch 61, test loss: 0.226191\n",
      "Epoch 62, test loss: 0.225902\n",
      "Epoch 63, test loss: 0.225509\n",
      "Epoch 64, test loss: 0.222977\n",
      "Epoch 65, test loss: 0.223942\n",
      "Epoch 66, test loss: 0.234097\n",
      "Epoch 67, test loss: 0.223854\n",
      "Epoch 68, test loss: 0.225735\n",
      "Epoch 69, test loss: 0.230641\n",
      "Epoch 70, test loss: 0.225245\n",
      "Epoch 71, test loss: 0.225212\n",
      "Epoch 72, test loss: 0.224631\n",
      "Epoch 73, test loss: 0.229503\n",
      "Epoch 74, test loss: 0.222518\n",
      "Epoch 75, test loss: 0.222466\n",
      "Epoch 76, test loss: 0.225165\n",
      "Epoch 77, test loss: 0.225444\n",
      "Epoch 78, test loss: 0.226761\n",
      "Epoch 79, test loss: 0.225165\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B45030>\n",
      "Epoch 0, test loss: 0.223417\n",
      "Epoch 1, test loss: 0.225372\n",
      "Epoch 2, test loss: 0.225083\n",
      "Epoch 3, test loss: 0.223885\n",
      "Epoch 4, test loss: 0.225153\n",
      "Epoch 5, test loss: 0.223196\n",
      "Epoch 6, test loss: 0.230257\n",
      "Epoch 7, test loss: 0.225823\n",
      "Epoch 8, test loss: 0.223237\n",
      "Epoch 9, test loss: 0.221884\n",
      "Epoch 10, test loss: 0.227915\n",
      "Epoch 11, test loss: 0.222546\n",
      "Epoch 12, test loss: 0.222831\n",
      "Epoch 13, test loss: 0.223617\n",
      "Epoch 14, test loss: 0.222437\n",
      "Epoch 15, test loss: 0.222388\n",
      "Epoch 16, test loss: 0.223188\n",
      "Epoch 17, test loss: 0.225234\n",
      "Epoch 18, test loss: 0.227149\n",
      "Epoch 19, test loss: 0.223653\n",
      "Epoch 20, test loss: 0.222990\n",
      "Epoch 21, test loss: 0.222708\n",
      "Epoch 22, test loss: 0.221815\n",
      "Epoch 23, test loss: 0.222563\n",
      "Epoch 24, test loss: 0.228372\n",
      "Epoch 25, test loss: 0.223637\n",
      "Epoch 26, test loss: 0.224620\n",
      "Epoch 27, test loss: 0.222434\n",
      "Epoch 28, test loss: 0.222632\n",
      "Epoch 29, test loss: 0.222756\n",
      "Epoch 30, test loss: 0.222599\n",
      "Epoch 31, test loss: 0.226921\n",
      "Epoch 32, test loss: 0.223132\n",
      "Epoch 33, test loss: 0.223798\n",
      "Epoch 34, test loss: 0.224763\n",
      "Epoch 35, test loss: 0.222365\n",
      "Epoch 36, test loss: 0.228001\n",
      "Epoch 37, test loss: 0.224681\n",
      "Epoch 38, test loss: 0.223084\n",
      "Epoch 39, test loss: 0.222273\n",
      "Epoch 40, test loss: 0.222954\n",
      "Epoch 41, test loss: 0.231088\n",
      "Epoch 42, test loss: 0.223894\n",
      "Epoch 43, test loss: 0.223700\n",
      "Epoch 44, test loss: 0.223824\n",
      "Epoch 45, test loss: 0.223829\n",
      "Epoch 46, test loss: 0.223966\n",
      "Epoch 47, test loss: 0.225446\n",
      "Epoch 48, test loss: 0.222875\n",
      "Epoch 49, test loss: 0.224331\n",
      "Epoch 50, test loss: 0.226170\n",
      "Epoch 51, test loss: 0.221354\n",
      "Epoch 52, test loss: 0.225616\n",
      "Epoch 53, test loss: 0.223375\n",
      "Epoch 54, test loss: 0.222979\n",
      "Epoch 55, test loss: 0.223290\n",
      "Epoch 56, test loss: 0.223303\n",
      "Epoch 57, test loss: 0.224887\n",
      "Epoch 58, test loss: 0.222309\n",
      "Epoch 59, test loss: 0.223525\n",
      "Epoch 60, test loss: 0.222484\n",
      "Epoch 61, test loss: 0.223093\n",
      "Epoch 62, test loss: 0.223618\n",
      "Epoch 63, test loss: 0.226046\n",
      "Epoch 64, test loss: 0.223145\n",
      "Epoch 65, test loss: 0.223767\n",
      "Epoch 66, test loss: 0.224443\n",
      "Epoch 67, test loss: 0.221439\n",
      "Epoch 68, test loss: 0.221759\n",
      "Epoch 69, test loss: 0.224462\n",
      "Epoch 70, test loss: 0.222162\n",
      "Epoch 71, test loss: 0.222295\n",
      "Epoch 72, test loss: 0.226747\n",
      "Epoch 73, test loss: 0.223444\n",
      "Epoch 74, test loss: 0.221558\n",
      "Epoch 75, test loss: 0.223212\n",
      "Epoch 76, test loss: 0.225697\n",
      "Epoch 77, test loss: 0.222925\n",
      "Epoch 78, test loss: 0.221851\n",
      "Epoch 79, test loss: 0.221690\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B45030>\n",
      "Epoch 0, test loss: 0.263057\n",
      "Epoch 1, test loss: 0.241811\n",
      "Epoch 2, test loss: 0.244998\n",
      "Epoch 3, test loss: 0.248003\n",
      "Epoch 4, test loss: 0.246959\n",
      "Epoch 5, test loss: 0.248736\n",
      "Epoch 6, test loss: 0.248875\n",
      "Epoch 7, test loss: 0.243949\n",
      "Epoch 8, test loss: 0.244934\n",
      "Epoch 9, test loss: 0.244342\n",
      "Epoch 10, test loss: 0.244734\n",
      "Epoch 11, test loss: 0.244797\n",
      "Epoch 12, test loss: 0.242635\n",
      "Epoch 13, test loss: 0.244161\n",
      "Epoch 14, test loss: 0.242288\n",
      "Epoch 15, test loss: 0.243055\n",
      "Epoch 16, test loss: 0.242438\n",
      "Epoch 17, test loss: 0.241435\n",
      "Epoch 18, test loss: 0.240818\n",
      "Epoch 19, test loss: 0.240572\n",
      "Epoch 20, test loss: 0.243000\n",
      "Epoch 21, test loss: 0.240632\n",
      "Epoch 22, test loss: 0.243161\n",
      "Epoch 23, test loss: 0.238946\n",
      "Epoch 24, test loss: 0.237686\n",
      "Epoch 25, test loss: 0.236956\n",
      "Epoch 26, test loss: 0.236721\n",
      "Epoch 27, test loss: 0.236705\n",
      "Epoch 28, test loss: 0.236665\n",
      "Epoch 29, test loss: 0.236382\n",
      "Epoch 30, test loss: 0.235442\n",
      "Epoch 31, test loss: 0.236672\n",
      "Epoch 32, test loss: 0.234386\n",
      "Epoch 33, test loss: 0.234208\n",
      "Epoch 34, test loss: 0.233016\n",
      "Epoch 35, test loss: 0.233231\n",
      "Epoch 36, test loss: 0.232553\n",
      "Epoch 37, test loss: 0.233902\n",
      "Epoch 38, test loss: 0.233993\n",
      "Epoch 39, test loss: 0.233132\n",
      "Epoch 40, test loss: 0.230430\n",
      "Epoch 41, test loss: 0.231620\n",
      "Epoch 42, test loss: 0.236494\n",
      "Epoch 43, test loss: 0.231852\n",
      "Epoch 44, test loss: 0.229345\n",
      "Epoch 45, test loss: 0.230479\n",
      "Epoch 46, test loss: 0.230566\n",
      "Epoch 47, test loss: 0.231227\n",
      "Epoch 48, test loss: 0.227987\n",
      "Epoch 49, test loss: 0.228978\n",
      "Epoch 50, test loss: 0.229456\n",
      "Epoch 51, test loss: 0.227495\n",
      "Epoch 52, test loss: 0.231663\n",
      "Epoch 53, test loss: 0.228387\n",
      "Epoch 54, test loss: 0.229395\n",
      "Epoch 55, test loss: 0.226987\n",
      "Epoch 56, test loss: 0.226569\n",
      "Epoch 57, test loss: 0.227422\n",
      "Epoch 58, test loss: 0.227147\n",
      "Epoch 59, test loss: 0.230307\n",
      "Epoch 60, test loss: 0.225831\n",
      "Epoch 61, test loss: 0.226659\n",
      "Epoch 62, test loss: 0.229233\n",
      "Epoch 63, test loss: 0.225360\n",
      "Epoch 64, test loss: 0.226673\n",
      "Epoch 65, test loss: 0.228014\n",
      "Epoch 66, test loss: 0.224774\n",
      "Epoch 67, test loss: 0.228684\n",
      "Epoch 68, test loss: 0.225611\n",
      "Epoch 69, test loss: 0.224750\n",
      "Epoch 70, test loss: 0.225156\n",
      "Epoch 71, test loss: 0.225061\n",
      "Epoch 72, test loss: 0.226558\n",
      "Epoch 73, test loss: 0.225315\n",
      "Epoch 74, test loss: 0.229240\n",
      "Epoch 75, test loss: 0.228010\n",
      "Epoch 76, test loss: 0.225334\n",
      "Epoch 77, test loss: 0.225124\n",
      "Epoch 78, test loss: 0.225122\n",
      "Epoch 79, test loss: 0.229155\n",
      "Epoch 80, test loss: 0.225460\n",
      "Epoch 81, test loss: 0.225592\n",
      "Epoch 82, test loss: 0.225890\n",
      "Epoch 83, test loss: 0.225109\n",
      "Epoch 84, test loss: 0.226273\n",
      "Epoch 85, test loss: 0.227488\n",
      "Epoch 86, test loss: 0.225292\n",
      "Epoch 87, test loss: 0.226520\n",
      "Epoch 88, test loss: 0.226200\n",
      "Epoch 89, test loss: 0.225457\n",
      "Epoch 90, test loss: 0.226547\n",
      "Epoch 91, test loss: 0.225560\n",
      "Epoch 92, test loss: 0.226590\n",
      "Epoch 93, test loss: 0.227818\n",
      "Epoch 94, test loss: 0.225219\n",
      "Epoch 95, test loss: 0.227821\n",
      "Epoch 96, test loss: 0.225131\n",
      "Epoch 97, test loss: 0.226763\n",
      "Epoch 98, test loss: 0.226274\n",
      "Epoch 99, test loss: 0.230998\n",
      "Epoch 100, test loss: 0.224960\n",
      "Epoch 101, test loss: 0.226611\n",
      "Epoch 102, test loss: 0.226674\n",
      "Epoch 103, test loss: 0.230050\n",
      "Epoch 104, test loss: 0.226425\n",
      "Epoch 105, test loss: 0.228616\n",
      "Epoch 106, test loss: 0.226187\n",
      "Epoch 107, test loss: 0.231205\n",
      "Epoch 108, test loss: 0.227948\n",
      "Epoch 109, test loss: 0.225673\n",
      "Epoch 110, test loss: 0.225587\n",
      "Epoch 111, test loss: 0.225259\n",
      "Epoch 112, test loss: 0.227065\n",
      "Epoch 113, test loss: 0.228023\n",
      "Epoch 114, test loss: 0.229440\n",
      "Epoch 115, test loss: 0.224907\n",
      "Epoch 116, test loss: 0.229584\n",
      "Epoch 117, test loss: 0.225052\n",
      "Epoch 118, test loss: 0.225742\n",
      "Epoch 119, test loss: 0.225285\n",
      "Epoch 120, test loss: 0.225842\n",
      "Epoch 121, test loss: 0.226262\n",
      "Epoch 122, test loss: 0.231363\n",
      "Epoch 123, test loss: 0.225785\n",
      "Epoch 124, test loss: 0.228767\n",
      "Epoch 125, test loss: 0.225131\n",
      "Epoch 126, test loss: 0.229474\n",
      "Epoch 127, test loss: 0.230774\n",
      "Epoch 128, test loss: 0.229481\n",
      "Epoch 129, test loss: 0.226218\n",
      "Epoch 130, test loss: 0.226548\n",
      "Epoch 131, test loss: 0.224817\n",
      "Epoch 132, test loss: 0.225437\n",
      "Epoch 133, test loss: 0.224978\n",
      "Epoch 134, test loss: 0.226146\n",
      "Epoch 135, test loss: 0.224691\n",
      "Epoch 136, test loss: 0.225557\n",
      "Epoch 137, test loss: 0.225302\n",
      "Epoch 138, test loss: 0.226050\n",
      "Epoch 139, test loss: 0.225595\n",
      "Epoch 140, test loss: 0.225373\n",
      "Epoch 141, test loss: 0.225582\n",
      "Epoch 142, test loss: 0.228422\n",
      "Epoch 143, test loss: 0.231045\n",
      "Epoch 144, test loss: 0.224719\n",
      "Epoch 145, test loss: 0.227435\n",
      "Epoch 146, test loss: 0.226186\n",
      "Epoch 147, test loss: 0.231236\n",
      "Epoch 148, test loss: 0.228431\n",
      "Epoch 149, test loss: 0.225279\n",
      "Epoch 150, test loss: 0.229316\n",
      "Epoch 151, test loss: 0.226492\n",
      "Epoch 152, test loss: 0.225413\n",
      "Epoch 153, test loss: 0.228920\n",
      "Epoch 154, test loss: 0.224590\n",
      "Epoch 155, test loss: 0.225155\n",
      "Epoch 156, test loss: 0.224955\n",
      "Epoch 157, test loss: 0.225132\n",
      "Epoch 158, test loss: 0.227994\n",
      "Epoch 159, test loss: 0.225623\n",
      "Epoch 160, test loss: 0.226453\n",
      "Epoch 161, test loss: 0.225603\n",
      "Epoch 162, test loss: 0.226468\n",
      "Epoch 163, test loss: 0.225877\n",
      "Epoch 164, test loss: 0.228465\n",
      "Epoch 165, test loss: 0.225697\n",
      "Epoch 166, test loss: 0.225803\n",
      "Epoch 167, test loss: 0.226123\n",
      "Epoch 168, test loss: 0.227276\n",
      "Epoch 169, test loss: 0.225306\n",
      "Epoch 170, test loss: 0.225446\n",
      "Epoch 171, test loss: 0.227940\n",
      "Epoch 172, test loss: 0.225572\n",
      "Epoch 173, test loss: 0.225320\n",
      "Epoch 174, test loss: 0.226106\n",
      "Epoch 175, test loss: 0.225044\n",
      "Epoch 176, test loss: 0.225479\n",
      "Epoch 177, test loss: 0.225051\n",
      "Epoch 178, test loss: 0.229966\n",
      "Epoch 179, test loss: 0.224994\n",
      "Epoch 180, test loss: 0.226247\n",
      "Epoch 181, test loss: 0.226750\n",
      "Epoch 182, test loss: 0.230051\n",
      "Epoch 183, test loss: 0.226103\n",
      "Epoch 184, test loss: 0.225958\n",
      "Epoch 185, test loss: 0.225821\n",
      "Epoch 186, test loss: 0.227511\n",
      "Epoch 187, test loss: 0.226226\n",
      "Epoch 188, test loss: 0.226535\n",
      "Epoch 189, test loss: 0.226281\n",
      "Epoch 190, test loss: 0.225539\n",
      "Epoch 191, test loss: 0.226834\n",
      "Epoch 192, test loss: 0.225946\n",
      "Epoch 193, test loss: 0.225953\n",
      "Epoch 194, test loss: 0.226314\n",
      "Epoch 195, test loss: 0.225349\n",
      "Epoch 196, test loss: 0.226769\n",
      "Epoch 197, test loss: 0.226972\n",
      "Epoch 198, test loss: 0.234694\n",
      "Epoch 199, test loss: 0.228315\n",
      "Epoch 200, test loss: 0.225863\n",
      "Epoch 201, test loss: 0.227624\n",
      "Epoch 202, test loss: 0.226161\n",
      "Epoch 203, test loss: 0.226941\n",
      "Epoch 204, test loss: 0.225888\n",
      "Epoch 205, test loss: 0.226183\n",
      "Epoch 206, test loss: 0.229041\n",
      "Epoch 207, test loss: 0.225407\n",
      "Epoch 208, test loss: 0.230056\n",
      "Epoch 209, test loss: 0.225552\n",
      "Epoch 210, test loss: 0.225735\n",
      "Epoch 211, test loss: 0.225911\n",
      "Epoch 212, test loss: 0.226748\n",
      "Epoch 213, test loss: 0.227846\n",
      "Epoch 214, test loss: 0.225246\n",
      "Epoch 215, test loss: 0.227430\n",
      "Epoch 216, test loss: 0.225439\n",
      "Epoch 217, test loss: 0.225238\n",
      "Epoch 218, test loss: 0.227018\n",
      "Epoch 219, test loss: 0.226559\n",
      "Epoch 220, test loss: 0.225862\n",
      "Epoch 221, test loss: 0.227352\n",
      "Epoch 222, test loss: 0.225953\n",
      "Epoch 223, test loss: 0.225450\n",
      "Epoch 224, test loss: 0.227531\n",
      "Epoch 225, test loss: 0.225541\n",
      "Epoch 226, test loss: 0.237471\n",
      "Epoch 227, test loss: 0.225886\n",
      "Epoch 228, test loss: 0.225329\n",
      "Epoch 229, test loss: 0.225640\n",
      "Epoch 230, test loss: 0.226394\n",
      "Epoch 231, test loss: 0.226854\n",
      "Epoch 232, test loss: 0.226094\n",
      "Epoch 233, test loss: 0.228233\n",
      "Epoch 234, test loss: 0.228462\n",
      "Epoch 235, test loss: 0.226437\n",
      "Epoch 236, test loss: 0.225448\n",
      "Epoch 237, test loss: 0.226071\n",
      "Epoch 238, test loss: 0.225574\n",
      "Epoch 239, test loss: 0.226656\n",
      "Pretrain data: 19732321.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7179/103358\n",
      "Found 815 continuous time series\n",
      "Data shape: (110539, 12), Train/test: 110537/2\n",
      "Train test ratio: 55268.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1727\n",
      "Epoch 0, train loss: 0.252092\n",
      "Epoch 1, train loss: 0.223300\n",
      "Epoch 2, train loss: 0.231758\n",
      "Epoch 3, train loss: 0.157468\n",
      "Epoch 4, train loss: 0.278874\n",
      "Epoch 5, train loss: 0.178519\n",
      "Epoch 6, train loss: 0.228824\n",
      "Epoch 7, train loss: 0.165702\n",
      "Epoch 8, train loss: 0.180147\n",
      "Epoch 9, train loss: 0.142522\n",
      "Epoch 10, train loss: 0.217479\n",
      "Epoch 11, train loss: 0.178170\n",
      "Epoch 12, train loss: 0.229155\n",
      "Epoch 13, train loss: 0.278092\n",
      "Epoch 14, train loss: 0.175808\n",
      "Epoch 15, train loss: 0.280613\n",
      "Epoch 16, train loss: 0.268382\n",
      "Epoch 17, train loss: 0.218330\n",
      "Epoch 18, train loss: 0.192193\n",
      "Epoch 19, train loss: 0.244301\n",
      "Epoch 20, train loss: 0.194065\n",
      "Epoch 21, train loss: 0.218777\n",
      "Epoch 22, train loss: 0.229681\n",
      "Epoch 23, train loss: 0.218628\n",
      "Epoch 24, train loss: 0.190709\n",
      "Epoch 25, train loss: 0.210353\n",
      "Epoch 26, train loss: 0.200811\n",
      "Epoch 27, train loss: 0.213728\n",
      "Epoch 28, train loss: 0.308901\n",
      "Epoch 29, train loss: 0.244868\n",
      "Epoch 30, train loss: 0.184356\n",
      "Epoch 31, train loss: 0.211047\n",
      "Epoch 32, train loss: 0.164751\n",
      "Epoch 33, train loss: 0.236807\n",
      "Epoch 34, train loss: 0.205227\n",
      "Epoch 35, train loss: 0.171860\n",
      "Epoch 36, train loss: 0.187688\n",
      "Epoch 37, train loss: 0.206309\n",
      "Epoch 38, train loss: 0.163731\n",
      "Epoch 39, train loss: 0.264567\n",
      "Epoch 40, train loss: 0.225686\n",
      "Epoch 41, train loss: 0.201703\n",
      "Epoch 42, train loss: 0.208170\n",
      "Epoch 43, train loss: 0.197949\n",
      "Epoch 44, train loss: 0.196951\n",
      "Epoch 45, train loss: 0.227253\n",
      "Epoch 46, train loss: 0.171718\n",
      "Epoch 47, train loss: 0.288675\n",
      "Epoch 48, train loss: 0.231348\n",
      "Epoch 49, train loss: 0.280673\n",
      "Epoch 50, train loss: 0.185466\n",
      "Epoch 51, train loss: 0.192713\n",
      "Epoch 52, train loss: 0.181636\n",
      "Epoch 53, train loss: 0.188832\n",
      "Epoch 54, train loss: 0.268465\n",
      "Epoch 55, train loss: 0.160002\n",
      "Epoch 56, train loss: 0.263054\n",
      "Epoch 57, train loss: 0.240479\n",
      "Epoch 58, train loss: 0.182506\n",
      "Epoch 59, train loss: 0.203331\n",
      "Epoch 60, train loss: 0.160453\n",
      "Epoch 61, train loss: 0.252183\n",
      "Epoch 62, train loss: 0.218375\n",
      "Epoch 63, train loss: 0.246272\n",
      "Epoch 64, train loss: 0.219984\n",
      "Epoch 65, train loss: 0.225626\n",
      "Epoch 66, train loss: 0.210844\n",
      "Epoch 67, train loss: 0.181126\n",
      "Epoch 68, train loss: 0.153107\n",
      "Epoch 69, train loss: 0.192515\n",
      "Epoch 70, train loss: 0.184367\n",
      "Epoch 71, train loss: 0.151745\n",
      "Epoch 72, train loss: 0.155683\n",
      "Epoch 73, train loss: 0.201929\n",
      "Epoch 74, train loss: 0.218972\n",
      "Epoch 75, train loss: 0.175987\n",
      "Epoch 76, train loss: 0.192695\n",
      "Epoch 77, train loss: 0.213345\n",
      "Epoch 78, train loss: 0.181143\n",
      "Epoch 79, train loss: 0.195705\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "x here is\n",
      "[[283. 282. 281. ... 216. 208. 201.]\n",
      " [282. 281. 277. ... 208. 201. 195.]\n",
      " [281. 277. 267. ... 201. 195. 189.]\n",
      " ...\n",
      " [184. 182. 178. ... 159. 159. 156.]\n",
      " [182. 178. 176. ... 159. 156. 154.]\n",
      " [178. 176. 171. ... 156. 154. 152.]]\n",
      "y here is\n",
      "[[138. 138. 138. ... 138. 138. 138.]\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " [146. 146. 146. ... 146. 146. 146.]\n",
      " ...\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " [149. 149. 149. ... 149. 149. 149.]\n",
      " [144. 144. 144. ... 144. 144. 144.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (2675, 12), Train/test: 1/2674\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[160. 158. 160. ... 192. 201. 218.]\n",
      " [158. 160. 166. ... 201. 218. 226.]\n",
      " [160. 166. 175. ... 218. 226. 231.]\n",
      " ...\n",
      " [253. 257. 258. ... 211. 202. 196.]\n",
      " [257. 258. 258. ... 202. 196. 199.]\n",
      " [258. 258. 249. ... 196. 199. 224.]]\n",
      "y here is\n",
      "[[246. 246. 246. ... 246. 246. 246.]\n",
      " [259. 259. 259. ... 259. 259. 259.]\n",
      " [269. 269. 269. ... 269. 269. 269.]\n",
      " ...\n",
      " [268. 268. 268. ... 268. 268. 268.]\n",
      " [301. 301. 301. ... 301. 301. 301.]\n",
      " [290. 290. 290. ... 290. 290. 290.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 729/9667\n",
      "Found 27 continuous time series\n",
      "Data shape: (10398, 12), Train/test: 10396/2\n",
      "Train test ratio: 5198.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292DE68C0>\n",
      "Epoch 0, test loss: 0.207613\n",
      "Epoch 1, test loss: 0.210491\n",
      "Epoch 2, test loss: 0.210329\n",
      "Epoch 3, test loss: 0.207966\n",
      "Epoch 4, test loss: 0.208731\n",
      "Epoch 5, test loss: 0.208070\n",
      "Epoch 6, test loss: 0.209217\n",
      "Epoch 7, test loss: 0.208462\n",
      "Epoch 8, test loss: 0.208132\n",
      "Epoch 9, test loss: 0.207913\n",
      "Epoch 10, test loss: 0.207918\n",
      "Epoch 11, test loss: 0.208590\n",
      "Epoch 12, test loss: 0.208156\n",
      "Epoch 13, test loss: 0.208969\n",
      "Epoch 14, test loss: 0.208191\n",
      "Epoch 15, test loss: 0.210453\n",
      "Epoch 16, test loss: 0.208496\n",
      "Epoch 17, test loss: 0.208719\n",
      "Epoch 18, test loss: 0.209471\n",
      "Epoch 19, test loss: 0.209218\n",
      "Epoch 20, test loss: 0.208653\n",
      "Epoch 21, test loss: 0.210576\n",
      "Epoch 22, test loss: 0.209608\n",
      "Epoch 23, test loss: 0.208667\n",
      "Epoch 24, test loss: 0.209691\n",
      "Epoch 25, test loss: 0.214327\n",
      "Epoch 26, test loss: 0.209816\n",
      "Epoch 27, test loss: 0.208744\n",
      "Epoch 28, test loss: 0.209089\n",
      "Epoch 29, test loss: 0.209161\n",
      "Epoch 30, test loss: 0.209877\n",
      "Epoch 31, test loss: 0.209239\n",
      "Epoch 32, test loss: 0.209920\n",
      "Epoch 33, test loss: 0.209020\n",
      "Epoch 34, test loss: 0.209119\n",
      "Epoch 35, test loss: 0.209267\n",
      "Epoch 36, test loss: 0.209687\n",
      "Epoch 37, test loss: 0.209686\n",
      "Epoch 38, test loss: 0.209996\n",
      "Epoch 39, test loss: 0.211995\n",
      "Epoch 40, test loss: 0.212852\n",
      "Epoch 41, test loss: 0.209809\n",
      "Epoch 42, test loss: 0.210113\n",
      "Epoch 43, test loss: 0.209388\n",
      "Epoch 44, test loss: 0.210203\n",
      "Epoch 45, test loss: 0.210145\n",
      "Epoch 46, test loss: 0.211774\n",
      "Epoch 47, test loss: 0.209476\n",
      "Epoch 48, test loss: 0.210004\n",
      "Epoch 49, test loss: 0.209994\n",
      "Epoch 50, test loss: 0.211461\n",
      "Epoch 51, test loss: 0.210827\n",
      "Epoch 52, test loss: 0.209896\n",
      "Epoch 53, test loss: 0.210166\n",
      "Epoch 54, test loss: 0.211747\n",
      "Epoch 55, test loss: 0.210143\n",
      "Epoch 56, test loss: 0.210023\n",
      "Epoch 57, test loss: 0.211108\n",
      "Epoch 58, test loss: 0.210292\n",
      "Epoch 59, test loss: 0.210612\n",
      "Epoch 60, test loss: 0.210121\n",
      "Epoch 61, test loss: 0.210363\n",
      "Epoch 62, test loss: 0.211657\n",
      "Epoch 63, test loss: 0.210896\n",
      "Epoch 64, test loss: 0.210743\n",
      "Epoch 65, test loss: 0.210701\n",
      "Epoch 66, test loss: 0.212614\n",
      "Epoch 67, test loss: 0.210604\n",
      "Epoch 68, test loss: 0.214352\n",
      "Epoch 69, test loss: 0.211627\n",
      "Epoch 70, test loss: 0.210208\n",
      "Epoch 71, test loss: 0.210800\n",
      "Epoch 72, test loss: 0.211266\n",
      "Epoch 73, test loss: 0.211415\n",
      "Epoch 74, test loss: 0.210642\n",
      "Epoch 75, test loss: 0.210845\n",
      "Epoch 76, test loss: 0.211218\n",
      "Epoch 77, test loss: 0.212536\n",
      "Epoch 78, test loss: 0.210989\n",
      "Epoch 79, test loss: 0.210634\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292DE68C0>\n",
      "Epoch 0, test loss: 0.208110\n",
      "Epoch 1, test loss: 0.208268\n",
      "Epoch 2, test loss: 0.208004\n",
      "Epoch 3, test loss: 0.207702\n",
      "Epoch 4, test loss: 0.208060\n",
      "Epoch 5, test loss: 0.207779\n",
      "Epoch 6, test loss: 0.207952\n",
      "Epoch 7, test loss: 0.208368\n",
      "Epoch 8, test loss: 0.208255\n",
      "Epoch 9, test loss: 0.207962\n",
      "Epoch 10, test loss: 0.207842\n",
      "Epoch 11, test loss: 0.209586\n",
      "Epoch 12, test loss: 0.208563\n",
      "Epoch 13, test loss: 0.208322\n",
      "Epoch 14, test loss: 0.209387\n",
      "Epoch 15, test loss: 0.208601\n",
      "Epoch 16, test loss: 0.209007\n",
      "Epoch 17, test loss: 0.208560\n",
      "Epoch 18, test loss: 0.209069\n",
      "Epoch 19, test loss: 0.208852\n",
      "Epoch 20, test loss: 0.209023\n",
      "Epoch 21, test loss: 0.209143\n",
      "Epoch 22, test loss: 0.208697\n",
      "Epoch 23, test loss: 0.211321\n",
      "Epoch 24, test loss: 0.209179\n",
      "Epoch 25, test loss: 0.209692\n",
      "Epoch 26, test loss: 0.209215\n",
      "Epoch 27, test loss: 0.209386\n",
      "Epoch 28, test loss: 0.209629\n",
      "Epoch 29, test loss: 0.209235\n",
      "Epoch 30, test loss: 0.209677\n",
      "Epoch 31, test loss: 0.210342\n",
      "Epoch 32, test loss: 0.210040\n",
      "Epoch 33, test loss: 0.209739\n",
      "Epoch 34, test loss: 0.210940\n",
      "Epoch 35, test loss: 0.209582\n",
      "Epoch 36, test loss: 0.209033\n",
      "Epoch 37, test loss: 0.209706\n",
      "Epoch 38, test loss: 0.209617\n",
      "Epoch 39, test loss: 0.210047\n",
      "Epoch 40, test loss: 0.211746\n",
      "Epoch 41, test loss: 0.209605\n",
      "Epoch 42, test loss: 0.209624\n",
      "Epoch 43, test loss: 0.210654\n",
      "Epoch 44, test loss: 0.209591\n",
      "Epoch 45, test loss: 0.209607\n",
      "Epoch 46, test loss: 0.209711\n",
      "Epoch 47, test loss: 0.212113\n",
      "Epoch 48, test loss: 0.211950\n",
      "Epoch 49, test loss: 0.209531\n",
      "Epoch 50, test loss: 0.210000\n",
      "Epoch 51, test loss: 0.209856\n",
      "Epoch 52, test loss: 0.210302\n",
      "Epoch 53, test loss: 0.210434\n",
      "Epoch 54, test loss: 0.211217\n",
      "Epoch 55, test loss: 0.211994\n",
      "Epoch 56, test loss: 0.210549\n",
      "Epoch 57, test loss: 0.212148\n",
      "Epoch 58, test loss: 0.210409\n",
      "Epoch 59, test loss: 0.210013\n",
      "Epoch 60, test loss: 0.210549\n",
      "Epoch 61, test loss: 0.212455\n",
      "Epoch 62, test loss: 0.210665\n",
      "Epoch 63, test loss: 0.210628\n",
      "Epoch 64, test loss: 0.209967\n",
      "Epoch 65, test loss: 0.209851\n",
      "Epoch 66, test loss: 0.209871\n",
      "Epoch 67, test loss: 0.210254\n",
      "Epoch 68, test loss: 0.211224\n",
      "Epoch 69, test loss: 0.211779\n",
      "Epoch 70, test loss: 0.210277\n",
      "Epoch 71, test loss: 0.212516\n",
      "Epoch 72, test loss: 0.211302\n",
      "Epoch 73, test loss: 0.213131\n",
      "Epoch 74, test loss: 0.210031\n",
      "Epoch 75, test loss: 0.210237\n",
      "Epoch 76, test loss: 0.210724\n",
      "Epoch 77, test loss: 0.210619\n",
      "Epoch 78, test loss: 0.210389\n",
      "Epoch 79, test loss: 0.210285\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292DE68C0>\n",
      "Epoch 0, test loss: 0.248891\n",
      "Epoch 1, test loss: 0.223841\n",
      "Epoch 2, test loss: 0.221333\n",
      "Epoch 3, test loss: 0.217520\n",
      "Epoch 4, test loss: 0.214839\n",
      "Epoch 5, test loss: 0.214440\n",
      "Epoch 6, test loss: 0.213930\n",
      "Epoch 7, test loss: 0.216336\n",
      "Epoch 8, test loss: 0.214353\n",
      "Epoch 9, test loss: 0.214804\n",
      "Epoch 10, test loss: 0.213125\n",
      "Epoch 11, test loss: 0.214632\n",
      "Epoch 12, test loss: 0.216302\n",
      "Epoch 13, test loss: 0.212640\n",
      "Epoch 14, test loss: 0.211018\n",
      "Epoch 15, test loss: 0.211191\n",
      "Epoch 16, test loss: 0.211203\n",
      "Epoch 17, test loss: 0.213181\n",
      "Epoch 18, test loss: 0.210260\n",
      "Epoch 19, test loss: 0.209397\n",
      "Epoch 20, test loss: 0.210294\n",
      "Epoch 21, test loss: 0.210120\n",
      "Epoch 22, test loss: 0.210483\n",
      "Epoch 23, test loss: 0.209087\n",
      "Epoch 24, test loss: 0.210733\n",
      "Epoch 25, test loss: 0.209001\n",
      "Epoch 26, test loss: 0.210897\n",
      "Epoch 27, test loss: 0.212531\n",
      "Epoch 28, test loss: 0.208662\n",
      "Epoch 29, test loss: 0.208732\n",
      "Epoch 30, test loss: 0.208935\n",
      "Epoch 31, test loss: 0.209344\n",
      "Epoch 32, test loss: 0.213343\n",
      "Epoch 33, test loss: 0.209880\n",
      "Epoch 34, test loss: 0.209916\n",
      "Epoch 35, test loss: 0.210808\n",
      "Epoch 36, test loss: 0.209623\n",
      "Epoch 37, test loss: 0.211904\n",
      "Epoch 38, test loss: 0.212174\n",
      "Epoch 39, test loss: 0.210458\n",
      "Epoch 40, test loss: 0.210199\n",
      "Epoch 41, test loss: 0.209288\n",
      "Epoch 42, test loss: 0.208667\n",
      "Epoch 43, test loss: 0.208852\n",
      "Epoch 44, test loss: 0.211701\n",
      "Epoch 45, test loss: 0.209603\n",
      "Epoch 46, test loss: 0.211290\n",
      "Epoch 47, test loss: 0.208501\n",
      "Epoch 48, test loss: 0.208958\n",
      "Epoch 49, test loss: 0.209324\n",
      "Epoch 50, test loss: 0.210241\n",
      "Epoch 51, test loss: 0.213370\n",
      "Epoch 52, test loss: 0.208937\n",
      "Epoch 53, test loss: 0.210388\n",
      "Epoch 54, test loss: 0.209333\n",
      "Epoch 55, test loss: 0.209187\n",
      "Epoch 56, test loss: 0.209709\n",
      "Epoch 57, test loss: 0.209326\n",
      "Epoch 58, test loss: 0.210387\n",
      "Epoch 59, test loss: 0.212148\n",
      "Epoch 60, test loss: 0.210073\n",
      "Epoch 61, test loss: 0.212948\n",
      "Epoch 62, test loss: 0.210925\n",
      "Epoch 63, test loss: 0.209458\n",
      "Epoch 64, test loss: 0.209472\n",
      "Epoch 65, test loss: 0.211485\n",
      "Epoch 66, test loss: 0.211676\n",
      "Epoch 67, test loss: 0.209972\n",
      "Epoch 68, test loss: 0.210169\n",
      "Epoch 69, test loss: 0.211143\n",
      "Epoch 70, test loss: 0.211033\n",
      "Epoch 71, test loss: 0.210285\n",
      "Epoch 72, test loss: 0.210796\n",
      "Epoch 73, test loss: 0.210801\n",
      "Epoch 74, test loss: 0.209891\n",
      "Epoch 75, test loss: 0.210474\n",
      "Epoch 76, test loss: 0.211286\n",
      "Epoch 77, test loss: 0.209989\n",
      "Epoch 78, test loss: 0.210147\n",
      "Epoch 79, test loss: 0.210547\n",
      "Epoch 80, test loss: 0.210875\n",
      "Epoch 81, test loss: 0.210218\n",
      "Epoch 82, test loss: 0.210663\n",
      "Epoch 83, test loss: 0.210126\n",
      "Epoch 84, test loss: 0.210094\n",
      "Epoch 85, test loss: 0.210020\n",
      "Epoch 86, test loss: 0.211896\n",
      "Epoch 87, test loss: 0.210098\n",
      "Epoch 88, test loss: 0.211029\n",
      "Epoch 89, test loss: 0.216508\n",
      "Epoch 90, test loss: 0.210936\n",
      "Epoch 91, test loss: 0.210320\n",
      "Epoch 92, test loss: 0.214028\n",
      "Epoch 93, test loss: 0.209840\n",
      "Epoch 94, test loss: 0.210284\n",
      "Epoch 95, test loss: 0.211596\n",
      "Epoch 96, test loss: 0.209880\n",
      "Epoch 97, test loss: 0.212429\n",
      "Epoch 98, test loss: 0.210098\n",
      "Epoch 99, test loss: 0.210322\n",
      "Epoch 100, test loss: 0.211510\n",
      "Epoch 101, test loss: 0.210721\n",
      "Epoch 102, test loss: 0.210223\n",
      "Epoch 103, test loss: 0.210785\n",
      "Epoch 104, test loss: 0.210650\n",
      "Epoch 105, test loss: 0.214549\n",
      "Epoch 106, test loss: 0.210884\n",
      "Epoch 107, test loss: 0.210813\n",
      "Epoch 108, test loss: 0.210213\n",
      "Epoch 109, test loss: 0.211007\n",
      "Epoch 110, test loss: 0.210507\n",
      "Epoch 111, test loss: 0.211460\n",
      "Epoch 112, test loss: 0.212336\n",
      "Epoch 113, test loss: 0.212987\n",
      "Epoch 114, test loss: 0.210347\n",
      "Epoch 115, test loss: 0.211947\n",
      "Epoch 116, test loss: 0.211226\n",
      "Epoch 117, test loss: 0.217163\n",
      "Epoch 118, test loss: 0.211056\n",
      "Epoch 119, test loss: 0.211348\n",
      "Epoch 120, test loss: 0.211058\n",
      "Epoch 121, test loss: 0.210566\n",
      "Epoch 122, test loss: 0.210444\n",
      "Epoch 123, test loss: 0.211852\n",
      "Epoch 124, test loss: 0.210908\n",
      "Epoch 125, test loss: 0.211195\n",
      "Epoch 126, test loss: 0.210597\n",
      "Epoch 127, test loss: 0.211800\n",
      "Epoch 128, test loss: 0.213158\n",
      "Epoch 129, test loss: 0.211295\n",
      "Epoch 130, test loss: 0.210943\n",
      "Epoch 131, test loss: 0.210895\n",
      "Epoch 132, test loss: 0.216962\n",
      "Epoch 133, test loss: 0.211169\n",
      "Epoch 134, test loss: 0.214597\n",
      "Epoch 135, test loss: 0.210717\n",
      "Epoch 136, test loss: 0.210880\n",
      "Epoch 137, test loss: 0.211025\n",
      "Epoch 138, test loss: 0.219223\n",
      "Epoch 139, test loss: 0.211604\n",
      "Epoch 140, test loss: 0.211748\n",
      "Epoch 141, test loss: 0.214026\n",
      "Epoch 142, test loss: 0.211745\n",
      "Epoch 143, test loss: 0.212154\n",
      "Epoch 144, test loss: 0.212037\n",
      "Epoch 145, test loss: 0.211344\n",
      "Epoch 146, test loss: 0.211084\n",
      "Epoch 147, test loss: 0.210942\n",
      "Epoch 148, test loss: 0.210654\n",
      "Epoch 149, test loss: 0.211384\n",
      "Epoch 150, test loss: 0.211018\n",
      "Epoch 151, test loss: 0.211124\n",
      "Epoch 152, test loss: 0.212362\n",
      "Epoch 153, test loss: 0.210898\n",
      "Epoch 154, test loss: 0.211261\n",
      "Epoch 155, test loss: 0.211489\n",
      "Epoch 156, test loss: 0.211163\n",
      "Epoch 157, test loss: 0.212958\n",
      "Epoch 158, test loss: 0.211736\n",
      "Epoch 159, test loss: 0.211223\n",
      "Epoch 160, test loss: 0.214503\n",
      "Epoch 161, test loss: 0.211841\n",
      "Epoch 162, test loss: 0.211314\n",
      "Epoch 163, test loss: 0.211846\n",
      "Epoch 164, test loss: 0.215429\n",
      "Epoch 165, test loss: 0.211325\n",
      "Epoch 166, test loss: 0.213238\n",
      "Epoch 167, test loss: 0.211121\n",
      "Epoch 168, test loss: 0.211571\n",
      "Epoch 169, test loss: 0.212317\n",
      "Epoch 170, test loss: 0.212682\n",
      "Epoch 171, test loss: 0.211713\n",
      "Epoch 172, test loss: 0.211386\n",
      "Epoch 173, test loss: 0.211361\n",
      "Epoch 174, test loss: 0.211659\n",
      "Epoch 175, test loss: 0.211166\n",
      "Epoch 176, test loss: 0.212256\n",
      "Epoch 177, test loss: 0.210799\n",
      "Epoch 178, test loss: 0.211439\n",
      "Epoch 179, test loss: 0.211095\n",
      "Epoch 180, test loss: 0.211485\n",
      "Epoch 181, test loss: 0.211032\n",
      "Epoch 182, test loss: 0.211307\n",
      "Epoch 183, test loss: 0.211364\n",
      "Epoch 184, test loss: 0.211936\n",
      "Epoch 185, test loss: 0.211060\n",
      "Epoch 186, test loss: 0.211536\n",
      "Epoch 187, test loss: 0.211444\n",
      "Epoch 188, test loss: 0.213557\n",
      "Epoch 189, test loss: 0.211196\n",
      "Epoch 190, test loss: 0.212853\n",
      "Epoch 191, test loss: 0.211338\n",
      "Epoch 192, test loss: 0.211933\n",
      "Epoch 193, test loss: 0.212290\n",
      "Epoch 194, test loss: 0.211447\n",
      "Epoch 195, test loss: 0.214026\n",
      "Epoch 196, test loss: 0.213647\n",
      "Epoch 197, test loss: 0.211894\n",
      "Epoch 198, test loss: 0.211342\n",
      "Epoch 199, test loss: 0.212363\n",
      "Epoch 200, test loss: 0.211765\n",
      "Epoch 201, test loss: 0.211791\n",
      "Epoch 202, test loss: 0.211330\n",
      "Epoch 203, test loss: 0.213024\n",
      "Epoch 204, test loss: 0.211623\n",
      "Epoch 205, test loss: 0.211790\n",
      "Epoch 206, test loss: 0.211603\n",
      "Epoch 207, test loss: 0.211735\n",
      "Epoch 208, test loss: 0.211554\n",
      "Epoch 209, test loss: 0.211926\n",
      "Epoch 210, test loss: 0.211785\n",
      "Epoch 211, test loss: 0.211335\n",
      "Epoch 212, test loss: 0.212399\n",
      "Epoch 213, test loss: 0.213112\n",
      "Epoch 214, test loss: 0.212065\n",
      "Epoch 215, test loss: 0.211617\n",
      "Epoch 216, test loss: 0.212981\n",
      "Epoch 217, test loss: 0.211361\n",
      "Epoch 218, test loss: 0.213558\n",
      "Epoch 219, test loss: 0.211548\n",
      "Epoch 220, test loss: 0.214394\n",
      "Epoch 221, test loss: 0.212137\n",
      "Epoch 222, test loss: 0.211873\n",
      "Epoch 223, test loss: 0.211656\n",
      "Epoch 224, test loss: 0.211902\n",
      "Epoch 225, test loss: 0.212223\n",
      "Epoch 226, test loss: 0.213102\n",
      "Epoch 227, test loss: 0.211753\n",
      "Epoch 228, test loss: 0.211987\n",
      "Epoch 229, test loss: 0.212898\n",
      "Epoch 230, test loss: 0.211805\n",
      "Epoch 231, test loss: 0.218756\n",
      "Epoch 232, test loss: 0.211665\n",
      "Epoch 233, test loss: 0.212485\n",
      "Epoch 234, test loss: 0.211456\n",
      "Epoch 235, test loss: 0.211890\n",
      "Epoch 236, test loss: 0.211910\n",
      "Epoch 237, test loss: 0.214045\n",
      "Epoch 238, test loss: 0.212320\n",
      "Epoch 239, test loss: 0.212600\n",
      "Pretrain data: 19786775.0\n",
      "Building dataset, requesting data from 0 to 665\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6679/105209\n",
      "Found 665 continuous time series\n",
      "Data shape: (111890, 12), Train/test: 111888/2\n",
      "Train test ratio: 55944.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1748\n",
      "Epoch 0, train loss: 0.190619\n",
      "Epoch 1, train loss: 0.220741\n",
      "Epoch 2, train loss: 0.210943\n",
      "Epoch 3, train loss: 0.188606\n",
      "Epoch 4, train loss: 0.166454\n",
      "Epoch 5, train loss: 0.235582\n",
      "Epoch 6, train loss: 0.142776\n",
      "Epoch 7, train loss: 0.226980\n",
      "Epoch 8, train loss: 0.220381\n",
      "Epoch 9, train loss: 0.209045\n",
      "Epoch 10, train loss: 0.258669\n",
      "Epoch 11, train loss: 0.208452\n",
      "Epoch 12, train loss: 0.160652\n",
      "Epoch 13, train loss: 0.190932\n",
      "Epoch 14, train loss: 0.180668\n",
      "Epoch 15, train loss: 0.184299\n",
      "Epoch 16, train loss: 0.170866\n",
      "Epoch 17, train loss: 0.206497\n",
      "Epoch 18, train loss: 0.251675\n",
      "Epoch 19, train loss: 0.171666\n",
      "Epoch 20, train loss: 0.205506\n",
      "Epoch 21, train loss: 0.244874\n",
      "Epoch 22, train loss: 0.155131\n",
      "Epoch 23, train loss: 0.171299\n",
      "Epoch 24, train loss: 0.271662\n",
      "Epoch 25, train loss: 0.174551\n",
      "Epoch 26, train loss: 0.203462\n",
      "Epoch 27, train loss: 0.199281\n",
      "Epoch 28, train loss: 0.211139\n",
      "Epoch 29, train loss: 0.232201\n",
      "Epoch 30, train loss: 0.188259\n",
      "Epoch 31, train loss: 0.135115\n",
      "Epoch 32, train loss: 0.207509\n",
      "Epoch 33, train loss: 0.266897\n",
      "Epoch 34, train loss: 0.249789\n",
      "Epoch 35, train loss: 0.191564\n",
      "Epoch 36, train loss: 0.186594\n",
      "Epoch 37, train loss: 0.200082\n",
      "Epoch 38, train loss: 0.203551\n",
      "Epoch 39, train loss: 0.174046\n",
      "Epoch 40, train loss: 0.322400\n",
      "Epoch 41, train loss: 0.239546\n",
      "Epoch 42, train loss: 0.247306\n",
      "Epoch 43, train loss: 0.204416\n",
      "Epoch 44, train loss: 0.175445\n",
      "Epoch 45, train loss: 0.191653\n",
      "Epoch 46, train loss: 0.282673\n",
      "Epoch 47, train loss: 0.301144\n",
      "Epoch 48, train loss: 0.164228\n",
      "Epoch 49, train loss: 0.224330\n",
      "Epoch 50, train loss: 0.187216\n",
      "Epoch 51, train loss: 0.213842\n",
      "Epoch 52, train loss: 0.237055\n",
      "Epoch 53, train loss: 0.144294\n",
      "Epoch 54, train loss: 0.245568\n",
      "Epoch 55, train loss: 0.240050\n",
      "Epoch 56, train loss: 0.223257\n",
      "Epoch 57, train loss: 0.215407\n",
      "Epoch 58, train loss: 0.204857\n",
      "Epoch 59, train loss: 0.202658\n",
      "Epoch 60, train loss: 0.178859\n",
      "Epoch 61, train loss: 0.187273\n",
      "Epoch 62, train loss: 0.209591\n",
      "Epoch 63, train loss: 0.233829\n",
      "Epoch 64, train loss: 0.179853\n",
      "Epoch 65, train loss: 0.224333\n",
      "Epoch 66, train loss: 0.264961\n",
      "Epoch 67, train loss: 0.195809\n",
      "Epoch 68, train loss: 0.233946\n",
      "Epoch 69, train loss: 0.237055\n",
      "Epoch 70, train loss: 0.201095\n",
      "Epoch 71, train loss: 0.252475\n",
      "Epoch 72, train loss: 0.175162\n",
      "Epoch 73, train loss: 0.206548\n",
      "Epoch 74, train loss: 0.161952\n",
      "Epoch 75, train loss: 0.173914\n",
      "Epoch 76, train loss: 0.199691\n",
      "Epoch 77, train loss: 0.189044\n",
      "Epoch 78, train loss: 0.179418\n",
      "Epoch 79, train loss: 0.185943\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "x here is\n",
      "[[254. 250. 249. ... 217. 215. 212.]\n",
      " [250. 249. 247. ... 215. 212. 209.]\n",
      " [249. 247. 242. ... 212. 209. 205.]\n",
      " ...\n",
      " [111. 112. 114. ... 143. 159. 177.]\n",
      " [112. 114. 116. ... 159. 177. 181.]\n",
      " [114. 116. 115. ... 177. 181. 205.]]\n",
      "y here is\n",
      "[[201. 201. 201. ... 201. 201. 201.]\n",
      " [198. 198. 198. ... 198. 198. 198.]\n",
      " [195. 195. 195. ... 195. 195. 195.]\n",
      " ...\n",
      " [254. 254. 254. ... 254. 254. 254.]\n",
      " [248. 248. 248. ... 248. 248. 248.]\n",
      " [242. 242. 242. ... 242. 242. 242.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 45 continuous time series\n",
      "Data shape: (2139, 12), Train/test: 1/2138\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "x here is\n",
      "[[ 76.  72.  68. ...  99. 110. 121.]\n",
      " [ 72.  68.  65. ... 110. 121. 131.]\n",
      " [ 68.  65.  63. ... 121. 131. 137.]\n",
      " ...\n",
      " [255. 246. 235. ... 232. 236. 237.]\n",
      " [246. 235. 231. ... 236. 237. 247.]\n",
      " [235. 231. 224. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[142. 142. 142. ... 142. 142. 142.]\n",
      " [136. 136. 136. ... 136. 136. 136.]\n",
      " [125. 125. 125. ... 125. 125. 125.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1229/7816\n",
      "Found 177 continuous time series\n",
      "Data shape: (9047, 12), Train/test: 9045/2\n",
      "Train test ratio: 4522.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B8ED40>\n",
      "Epoch 0, test loss: 0.219670\n",
      "Epoch 1, test loss: 0.220214\n",
      "Epoch 2, test loss: 0.217333\n",
      "Epoch 3, test loss: 0.218145\n",
      "Epoch 4, test loss: 0.220802\n",
      "Epoch 5, test loss: 0.216316\n",
      "Epoch 6, test loss: 0.217457\n",
      "Epoch 7, test loss: 0.217848\n",
      "Epoch 8, test loss: 0.219215\n",
      "Epoch 9, test loss: 0.218953\n",
      "Epoch 10, test loss: 0.223265\n",
      "Epoch 11, test loss: 0.229108\n",
      "Epoch 12, test loss: 0.218661\n",
      "Epoch 13, test loss: 0.217928\n",
      "Epoch 14, test loss: 0.219145\n",
      "Epoch 15, test loss: 0.226718\n",
      "Epoch 16, test loss: 0.217990\n",
      "Epoch 17, test loss: 0.219419\n",
      "Epoch 18, test loss: 0.216295\n",
      "Epoch 19, test loss: 0.216231\n",
      "Epoch 20, test loss: 0.221172\n",
      "Epoch 21, test loss: 0.217036\n",
      "Epoch 22, test loss: 0.220591\n",
      "Epoch 23, test loss: 0.216942\n",
      "Epoch 24, test loss: 0.216590\n",
      "Epoch 25, test loss: 0.218004\n",
      "Epoch 26, test loss: 0.218446\n",
      "Epoch 27, test loss: 0.224065\n",
      "Epoch 28, test loss: 0.217234\n",
      "Epoch 29, test loss: 0.218473\n",
      "Epoch 30, test loss: 0.217313\n",
      "Epoch 31, test loss: 0.220093\n",
      "Epoch 32, test loss: 0.221867\n",
      "Epoch 33, test loss: 0.215932\n",
      "Epoch 34, test loss: 0.220106\n",
      "Epoch 35, test loss: 0.221938\n",
      "Epoch 36, test loss: 0.219102\n",
      "Epoch 37, test loss: 0.222296\n",
      "Epoch 38, test loss: 0.220491\n",
      "Epoch 39, test loss: 0.219320\n",
      "Epoch 40, test loss: 0.217044\n",
      "Epoch 41, test loss: 0.215675\n",
      "Epoch 42, test loss: 0.220963\n",
      "Epoch 43, test loss: 0.225122\n",
      "Epoch 44, test loss: 0.216262\n",
      "Epoch 45, test loss: 0.219133\n",
      "Epoch 46, test loss: 0.217441\n",
      "Epoch 47, test loss: 0.216732\n",
      "Epoch 48, test loss: 0.215882\n",
      "Epoch 49, test loss: 0.217173\n",
      "Epoch 50, test loss: 0.217827\n",
      "Epoch 51, test loss: 0.220658\n",
      "Epoch 52, test loss: 0.218078\n",
      "Epoch 53, test loss: 0.216275\n",
      "Epoch 54, test loss: 0.218049\n",
      "Epoch 55, test loss: 0.217536\n",
      "Epoch 56, test loss: 0.216482\n",
      "Epoch 57, test loss: 0.215923\n",
      "Epoch 58, test loss: 0.226004\n",
      "Epoch 59, test loss: 0.216106\n",
      "Epoch 60, test loss: 0.220072\n",
      "Epoch 61, test loss: 0.215932\n",
      "Epoch 62, test loss: 0.224424\n",
      "Epoch 63, test loss: 0.216155\n",
      "Epoch 64, test loss: 0.221241\n",
      "Epoch 65, test loss: 0.219385\n",
      "Epoch 66, test loss: 0.216683\n",
      "Epoch 67, test loss: 0.217823\n",
      "Epoch 68, test loss: 0.221324\n",
      "Epoch 69, test loss: 0.215957\n",
      "Epoch 70, test loss: 0.216672\n",
      "Epoch 71, test loss: 0.218183\n",
      "Epoch 72, test loss: 0.219928\n",
      "Epoch 73, test loss: 0.215906\n",
      "Epoch 74, test loss: 0.218924\n",
      "Epoch 75, test loss: 0.218517\n",
      "Epoch 76, test loss: 0.217643\n",
      "Epoch 77, test loss: 0.218780\n",
      "Epoch 78, test loss: 0.217703\n",
      "Epoch 79, test loss: 0.218920\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B8ED40>\n",
      "Epoch 0, test loss: 0.219423\n",
      "Epoch 1, test loss: 0.220314\n",
      "Epoch 2, test loss: 0.217503\n",
      "Epoch 3, test loss: 0.224556\n",
      "Epoch 4, test loss: 0.218601\n",
      "Epoch 5, test loss: 0.220132\n",
      "Epoch 6, test loss: 0.229187\n",
      "Epoch 7, test loss: 0.216946\n",
      "Epoch 8, test loss: 0.217028\n",
      "Epoch 9, test loss: 0.219162\n",
      "Epoch 10, test loss: 0.219960\n",
      "Epoch 11, test loss: 0.216460\n",
      "Epoch 12, test loss: 0.216652\n",
      "Epoch 13, test loss: 0.215895\n",
      "Epoch 14, test loss: 0.216806\n",
      "Epoch 15, test loss: 0.218844\n",
      "Epoch 16, test loss: 0.217130\n",
      "Epoch 17, test loss: 0.217686\n",
      "Epoch 18, test loss: 0.219759\n",
      "Epoch 19, test loss: 0.218196\n",
      "Epoch 20, test loss: 0.220025\n",
      "Epoch 21, test loss: 0.216987\n",
      "Epoch 22, test loss: 0.216766\n",
      "Epoch 23, test loss: 0.219061\n",
      "Epoch 24, test loss: 0.216697\n",
      "Epoch 25, test loss: 0.222759\n",
      "Epoch 26, test loss: 0.219986\n",
      "Epoch 27, test loss: 0.217710\n",
      "Epoch 28, test loss: 0.216571\n",
      "Epoch 29, test loss: 0.216063\n",
      "Epoch 30, test loss: 0.216310\n",
      "Epoch 31, test loss: 0.217706\n",
      "Epoch 32, test loss: 0.217762\n",
      "Epoch 33, test loss: 0.216314\n",
      "Epoch 34, test loss: 0.217207\n",
      "Epoch 35, test loss: 0.217694\n",
      "Epoch 36, test loss: 0.216463\n",
      "Epoch 37, test loss: 0.216669\n",
      "Epoch 38, test loss: 0.223087\n",
      "Epoch 39, test loss: 0.216639\n",
      "Epoch 40, test loss: 0.216919\n",
      "Epoch 41, test loss: 0.217345\n",
      "Epoch 42, test loss: 0.216121\n",
      "Epoch 43, test loss: 0.228187\n",
      "Epoch 44, test loss: 0.218361\n",
      "Epoch 45, test loss: 0.222941\n",
      "Epoch 46, test loss: 0.216698\n",
      "Epoch 47, test loss: 0.216393\n",
      "Epoch 48, test loss: 0.216913\n",
      "Epoch 49, test loss: 0.216206\n",
      "Epoch 50, test loss: 0.216269\n",
      "Epoch 51, test loss: 0.217044\n",
      "Epoch 52, test loss: 0.217828\n",
      "Epoch 53, test loss: 0.216885\n",
      "Epoch 54, test loss: 0.217621\n",
      "Epoch 55, test loss: 0.219659\n",
      "Epoch 56, test loss: 0.217259\n",
      "Epoch 57, test loss: 0.215952\n",
      "Epoch 58, test loss: 0.223017\n",
      "Epoch 59, test loss: 0.216849\n",
      "Epoch 60, test loss: 0.216545\n",
      "Epoch 61, test loss: 0.216794\n",
      "Epoch 62, test loss: 0.215780\n",
      "Epoch 63, test loss: 0.222231\n",
      "Epoch 64, test loss: 0.220346\n",
      "Epoch 65, test loss: 0.215929\n",
      "Epoch 66, test loss: 0.219975\n",
      "Epoch 67, test loss: 0.220661\n",
      "Epoch 68, test loss: 0.217091\n",
      "Epoch 69, test loss: 0.218702\n",
      "Epoch 70, test loss: 0.217008\n",
      "Epoch 71, test loss: 0.220813\n",
      "Epoch 72, test loss: 0.223690\n",
      "Epoch 73, test loss: 0.220000\n",
      "Epoch 74, test loss: 0.217828\n",
      "Epoch 75, test loss: 0.218867\n",
      "Epoch 76, test loss: 0.215650\n",
      "Epoch 77, test loss: 0.226821\n",
      "Epoch 78, test loss: 0.216193\n",
      "Epoch 79, test loss: 0.215534\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294B8ED40>\n",
      "Epoch 0, test loss: 0.292072\n",
      "Epoch 1, test loss: 0.246050\n",
      "Epoch 2, test loss: 0.233242\n",
      "Epoch 3, test loss: 0.231200\n",
      "Epoch 4, test loss: 0.229097\n",
      "Epoch 5, test loss: 0.225641\n",
      "Epoch 6, test loss: 0.226394\n",
      "Epoch 7, test loss: 0.226486\n",
      "Epoch 8, test loss: 0.227485\n",
      "Epoch 9, test loss: 0.223884\n",
      "Epoch 10, test loss: 0.225295\n",
      "Epoch 11, test loss: 0.223388\n",
      "Epoch 12, test loss: 0.223056\n",
      "Epoch 13, test loss: 0.224334\n",
      "Epoch 14, test loss: 0.223107\n",
      "Epoch 15, test loss: 0.225287\n",
      "Epoch 16, test loss: 0.223579\n",
      "Epoch 17, test loss: 0.225928\n",
      "Epoch 18, test loss: 0.226886\n",
      "Epoch 19, test loss: 0.223949\n",
      "Epoch 20, test loss: 0.223914\n",
      "Epoch 21, test loss: 0.225207\n",
      "Epoch 22, test loss: 0.224080\n",
      "Epoch 23, test loss: 0.223482\n",
      "Epoch 24, test loss: 0.223651\n",
      "Epoch 25, test loss: 0.223491\n",
      "Epoch 26, test loss: 0.223316\n",
      "Epoch 27, test loss: 0.222739\n",
      "Epoch 28, test loss: 0.223627\n",
      "Epoch 29, test loss: 0.225075\n",
      "Epoch 30, test loss: 0.223817\n",
      "Epoch 31, test loss: 0.223405\n",
      "Epoch 32, test loss: 0.224604\n",
      "Epoch 33, test loss: 0.224635\n",
      "Epoch 34, test loss: 0.223556\n",
      "Epoch 35, test loss: 0.222840\n",
      "Epoch 36, test loss: 0.224256\n",
      "Epoch 37, test loss: 0.222981\n",
      "Epoch 38, test loss: 0.223760\n",
      "Epoch 39, test loss: 0.222851\n",
      "Epoch 40, test loss: 0.226847\n",
      "Epoch 41, test loss: 0.226720\n",
      "Epoch 42, test loss: 0.227209\n",
      "Epoch 43, test loss: 0.222986\n",
      "Epoch 44, test loss: 0.227882\n",
      "Epoch 45, test loss: 0.222578\n",
      "Epoch 46, test loss: 0.228045\n",
      "Epoch 47, test loss: 0.225621\n",
      "Epoch 48, test loss: 0.223854\n",
      "Epoch 49, test loss: 0.222870\n",
      "Epoch 50, test loss: 0.225868\n",
      "Epoch 51, test loss: 0.224098\n",
      "Epoch 52, test loss: 0.227406\n",
      "Epoch 53, test loss: 0.223466\n",
      "Epoch 54, test loss: 0.222630\n",
      "Epoch 55, test loss: 0.224510\n",
      "Epoch 56, test loss: 0.225719\n",
      "Epoch 57, test loss: 0.222060\n",
      "Epoch 58, test loss: 0.222741\n",
      "Epoch 59, test loss: 0.222042\n",
      "Epoch 60, test loss: 0.222913\n",
      "Epoch 61, test loss: 0.222881\n",
      "Epoch 62, test loss: 0.223004\n",
      "Epoch 63, test loss: 0.223291\n",
      "Epoch 64, test loss: 0.224242\n",
      "Epoch 65, test loss: 0.224621\n",
      "Epoch 66, test loss: 0.224429\n",
      "Epoch 67, test loss: 0.224344\n",
      "Epoch 68, test loss: 0.226823\n",
      "Epoch 69, test loss: 0.222669\n",
      "Epoch 70, test loss: 0.221587\n",
      "Epoch 71, test loss: 0.224605\n",
      "Epoch 72, test loss: 0.223525\n",
      "Epoch 73, test loss: 0.224298\n",
      "Epoch 74, test loss: 0.222364\n",
      "Epoch 75, test loss: 0.223965\n",
      "Epoch 76, test loss: 0.225575\n",
      "Epoch 77, test loss: 0.223095\n",
      "Epoch 78, test loss: 0.229917\n",
      "Epoch 79, test loss: 0.223526\n",
      "Epoch 80, test loss: 0.222830\n",
      "Epoch 81, test loss: 0.222193\n",
      "Epoch 82, test loss: 0.223984\n",
      "Epoch 83, test loss: 0.221767\n",
      "Epoch 84, test loss: 0.222127\n",
      "Epoch 85, test loss: 0.222945\n",
      "Epoch 86, test loss: 0.222890\n",
      "Epoch 87, test loss: 0.223705\n",
      "Epoch 88, test loss: 0.223659\n",
      "Epoch 89, test loss: 0.225615\n",
      "Epoch 90, test loss: 0.225278\n",
      "Epoch 91, test loss: 0.230213\n",
      "Epoch 92, test loss: 0.222593\n",
      "Epoch 93, test loss: 0.222236\n",
      "Epoch 94, test loss: 0.221673\n",
      "Epoch 95, test loss: 0.223198\n",
      "Epoch 96, test loss: 0.221717\n",
      "Epoch 97, test loss: 0.224465\n",
      "Epoch 98, test loss: 0.225242\n",
      "Epoch 99, test loss: 0.223512\n",
      "Epoch 100, test loss: 0.223875\n",
      "Epoch 101, test loss: 0.222091\n",
      "Epoch 102, test loss: 0.221345\n",
      "Epoch 103, test loss: 0.222136\n",
      "Epoch 104, test loss: 0.224859\n",
      "Epoch 105, test loss: 0.223301\n",
      "Epoch 106, test loss: 0.221414\n",
      "Epoch 107, test loss: 0.221658\n",
      "Epoch 108, test loss: 0.222845\n",
      "Epoch 109, test loss: 0.222942\n",
      "Epoch 110, test loss: 0.223721\n",
      "Epoch 111, test loss: 0.221367\n",
      "Epoch 112, test loss: 0.221196\n",
      "Epoch 113, test loss: 0.228522\n",
      "Epoch 114, test loss: 0.222613\n",
      "Epoch 115, test loss: 0.221784\n",
      "Epoch 116, test loss: 0.223455\n",
      "Epoch 117, test loss: 0.226109\n",
      "Epoch 118, test loss: 0.222067\n",
      "Epoch 119, test loss: 0.223829\n",
      "Epoch 120, test loss: 0.222501\n",
      "Epoch 121, test loss: 0.223008\n",
      "Epoch 122, test loss: 0.224975\n",
      "Epoch 123, test loss: 0.222667\n",
      "Epoch 124, test loss: 0.222925\n",
      "Epoch 125, test loss: 0.221610\n",
      "Epoch 126, test loss: 0.221022\n",
      "Epoch 127, test loss: 0.222476\n",
      "Epoch 128, test loss: 0.222395\n",
      "Epoch 129, test loss: 0.221454\n",
      "Epoch 130, test loss: 0.221038\n",
      "Epoch 131, test loss: 0.225168\n",
      "Epoch 132, test loss: 0.221523\n",
      "Epoch 133, test loss: 0.222466\n",
      "Epoch 134, test loss: 0.222680\n",
      "Epoch 135, test loss: 0.221395\n",
      "Epoch 136, test loss: 0.221602\n",
      "Epoch 137, test loss: 0.221499\n",
      "Epoch 138, test loss: 0.223253\n",
      "Epoch 139, test loss: 0.221759\n",
      "Epoch 140, test loss: 0.225272\n",
      "Epoch 141, test loss: 0.221194\n",
      "Epoch 142, test loss: 0.221622\n",
      "Epoch 143, test loss: 0.220808\n",
      "Epoch 144, test loss: 0.221162\n",
      "Epoch 145, test loss: 0.221522\n",
      "Epoch 146, test loss: 0.231158\n",
      "Epoch 147, test loss: 0.222490\n",
      "Epoch 148, test loss: 0.226148\n",
      "Epoch 149, test loss: 0.220789\n",
      "Epoch 150, test loss: 0.221944\n",
      "Epoch 151, test loss: 0.221067\n",
      "Epoch 152, test loss: 0.221644\n",
      "Epoch 153, test loss: 0.222440\n",
      "Epoch 154, test loss: 0.221987\n",
      "Epoch 155, test loss: 0.221985\n",
      "Epoch 156, test loss: 0.222836\n",
      "Epoch 157, test loss: 0.223092\n",
      "Epoch 158, test loss: 0.224317\n",
      "Epoch 159, test loss: 0.220627\n",
      "Epoch 160, test loss: 0.225177\n",
      "Epoch 161, test loss: 0.223135\n",
      "Epoch 162, test loss: 0.226490\n",
      "Epoch 163, test loss: 0.221682\n",
      "Epoch 164, test loss: 0.222257\n",
      "Epoch 165, test loss: 0.222442\n",
      "Epoch 166, test loss: 0.221136\n",
      "Epoch 167, test loss: 0.221730\n",
      "Epoch 168, test loss: 0.221527\n",
      "Epoch 169, test loss: 0.223239\n",
      "Epoch 170, test loss: 0.222080\n",
      "Epoch 171, test loss: 0.230190\n",
      "Epoch 172, test loss: 0.223223\n",
      "Epoch 173, test loss: 0.222728\n",
      "Epoch 174, test loss: 0.223435\n",
      "Epoch 175, test loss: 0.220497\n",
      "Epoch 176, test loss: 0.221920\n",
      "Epoch 177, test loss: 0.222484\n",
      "Epoch 178, test loss: 0.220842\n",
      "Epoch 179, test loss: 0.221117\n",
      "Epoch 180, test loss: 0.220417\n",
      "Epoch 181, test loss: 0.232028\n",
      "Epoch 182, test loss: 0.220640\n",
      "Epoch 183, test loss: 0.221598\n",
      "Epoch 184, test loss: 0.220321\n",
      "Epoch 185, test loss: 0.222345\n",
      "Epoch 186, test loss: 0.225861\n",
      "Epoch 187, test loss: 0.223371\n",
      "Epoch 188, test loss: 0.221985\n",
      "Epoch 189, test loss: 0.221310\n",
      "Epoch 190, test loss: 0.224555\n",
      "Epoch 191, test loss: 0.219748\n",
      "Epoch 192, test loss: 0.220085\n",
      "Epoch 193, test loss: 0.222558\n",
      "Epoch 194, test loss: 0.221931\n",
      "Epoch 195, test loss: 0.221332\n",
      "Epoch 196, test loss: 0.219911\n",
      "Epoch 197, test loss: 0.219799\n",
      "Epoch 198, test loss: 0.220695\n",
      "Epoch 199, test loss: 0.221995\n",
      "Epoch 200, test loss: 0.221653\n",
      "Epoch 201, test loss: 0.219878\n",
      "Epoch 202, test loss: 0.220979\n",
      "Epoch 203, test loss: 0.220617\n",
      "Epoch 204, test loss: 0.222082\n",
      "Epoch 205, test loss: 0.232793\n",
      "Epoch 206, test loss: 0.221505\n",
      "Epoch 207, test loss: 0.222708\n",
      "Epoch 208, test loss: 0.220930\n",
      "Epoch 209, test loss: 0.225032\n",
      "Epoch 210, test loss: 0.226452\n",
      "Epoch 211, test loss: 0.222215\n",
      "Epoch 212, test loss: 0.222205\n",
      "Epoch 213, test loss: 0.222905\n",
      "Epoch 214, test loss: 0.223091\n",
      "Epoch 215, test loss: 0.221610\n",
      "Epoch 216, test loss: 0.220445\n",
      "Epoch 217, test loss: 0.220860\n",
      "Epoch 218, test loss: 0.221609\n",
      "Epoch 219, test loss: 0.223114\n",
      "Epoch 220, test loss: 0.230902\n",
      "Epoch 221, test loss: 0.221249\n",
      "Epoch 222, test loss: 0.221107\n",
      "Epoch 223, test loss: 0.223787\n",
      "Epoch 224, test loss: 0.221859\n",
      "Epoch 225, test loss: 0.232112\n",
      "Epoch 226, test loss: 0.223970\n",
      "Epoch 227, test loss: 0.221422\n",
      "Epoch 228, test loss: 0.222873\n",
      "Epoch 229, test loss: 0.220633\n",
      "Epoch 230, test loss: 0.223005\n",
      "Epoch 231, test loss: 0.222841\n",
      "Epoch 232, test loss: 0.222441\n",
      "Epoch 233, test loss: 0.222331\n",
      "Epoch 234, test loss: 0.223593\n",
      "Epoch 235, test loss: 0.221438\n",
      "Epoch 236, test loss: 0.222246\n",
      "Epoch 237, test loss: 0.220544\n",
      "Epoch 238, test loss: 0.221009\n",
      "Epoch 239, test loss: 0.220605\n",
      "Pretrain data: 20092707.0\n",
      "Building dataset, requesting data from 0 to 672\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7424/107273\n",
      "Found 672 continuous time series\n",
      "Data shape: (114699, 12), Train/test: 114697/2\n",
      "Train test ratio: 57348.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1792\n",
      "Epoch 0, train loss: 0.234049\n",
      "Epoch 1, train loss: 0.245469\n",
      "Epoch 2, train loss: 0.225802\n",
      "Epoch 3, train loss: 0.188400\n",
      "Epoch 4, train loss: 0.191272\n",
      "Epoch 5, train loss: 0.225948\n",
      "Epoch 6, train loss: 0.201001\n",
      "Epoch 7, train loss: 0.219685\n",
      "Epoch 8, train loss: 0.192156\n",
      "Epoch 9, train loss: 0.189103\n",
      "Epoch 10, train loss: 0.204176\n",
      "Epoch 11, train loss: 0.226574\n",
      "Epoch 12, train loss: 0.237063\n",
      "Epoch 13, train loss: 0.221262\n",
      "Epoch 14, train loss: 0.175160\n",
      "Epoch 15, train loss: 0.157883\n",
      "Epoch 16, train loss: 0.319571\n",
      "Epoch 17, train loss: 0.164596\n",
      "Epoch 18, train loss: 0.203451\n",
      "Epoch 19, train loss: 0.178391\n",
      "Epoch 20, train loss: 0.215902\n",
      "Epoch 21, train loss: 0.196376\n",
      "Epoch 22, train loss: 0.194026\n",
      "Epoch 23, train loss: 0.295369\n",
      "Epoch 24, train loss: 0.186650\n",
      "Epoch 25, train loss: 0.180605\n",
      "Epoch 26, train loss: 0.177913\n",
      "Epoch 27, train loss: 0.158461\n",
      "Epoch 28, train loss: 0.198292\n",
      "Epoch 29, train loss: 0.199864\n",
      "Epoch 30, train loss: 0.215434\n",
      "Epoch 31, train loss: 0.162516\n",
      "Epoch 32, train loss: 0.214877\n",
      "Epoch 33, train loss: 0.154614\n",
      "Epoch 34, train loss: 0.209307\n",
      "Epoch 35, train loss: 0.224336\n",
      "Epoch 36, train loss: 0.224940\n",
      "Epoch 37, train loss: 0.214930\n",
      "Epoch 38, train loss: 0.198960\n",
      "Epoch 39, train loss: 0.203950\n",
      "Epoch 40, train loss: 0.198102\n",
      "Epoch 41, train loss: 0.294311\n",
      "Epoch 42, train loss: 0.161434\n",
      "Epoch 43, train loss: 0.161158\n",
      "Epoch 44, train loss: 0.162100\n",
      "Epoch 45, train loss: 0.194280\n",
      "Epoch 46, train loss: 0.255610\n",
      "Epoch 47, train loss: 0.173282\n",
      "Epoch 48, train loss: 0.168315\n",
      "Epoch 49, train loss: 0.248933\n",
      "Epoch 50, train loss: 0.229479\n",
      "Epoch 51, train loss: 0.234562\n",
      "Epoch 52, train loss: 0.177594\n",
      "Epoch 53, train loss: 0.199585\n",
      "Epoch 54, train loss: 0.286635\n",
      "Epoch 55, train loss: 0.189324\n",
      "Epoch 56, train loss: 0.197764\n",
      "Epoch 57, train loss: 0.137537\n",
      "Epoch 58, train loss: 0.187001\n",
      "Epoch 59, train loss: 0.167921\n",
      "Epoch 60, train loss: 0.221083\n",
      "Epoch 61, train loss: 0.197229\n",
      "Epoch 62, train loss: 0.181190\n",
      "Epoch 63, train loss: 0.206449\n",
      "Epoch 64, train loss: 0.200381\n",
      "Epoch 65, train loss: 0.174361\n",
      "Epoch 66, train loss: 0.195138\n",
      "Epoch 67, train loss: 0.237110\n",
      "Epoch 68, train loss: 0.180659\n",
      "Epoch 69, train loss: 0.189926\n",
      "Epoch 70, train loss: 0.197205\n",
      "Epoch 71, train loss: 0.206245\n",
      "Epoch 72, train loss: 0.229918\n",
      "Epoch 73, train loss: 0.271751\n",
      "Epoch 74, train loss: 0.192476\n",
      "Epoch 75, train loss: 0.227102\n",
      "Epoch 76, train loss: 0.184660\n",
      "Epoch 77, train loss: 0.197127\n",
      "Epoch 78, train loss: 0.241783\n",
      "Epoch 79, train loss: 0.212577\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[181. 180. 178. ... 157. 155. 154.]\n",
      " [180. 178. 178. ... 155. 154. 153.]\n",
      " [178. 178. 176. ... 154. 153. 151.]\n",
      " ...\n",
      " [300. 307. 304. ... 283. 276. 269.]\n",
      " [307. 304. 307. ... 276. 269. 261.]\n",
      " [304. 307. 307. ... 269. 261. 252.]]\n",
      "y here is\n",
      "[[133. 133. 133. ... 133. 133. 133.]\n",
      " [129. 129. 129. ... 129. 129. 129.]\n",
      " [125. 125. 125. ... 125. 125. 125.]\n",
      " ...\n",
      " [229. 229. 229. ... 229. 229. 229.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [215. 215. 215. ... 215. 215. 215.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1639, 12), Train/test: 1/1638\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 170 segments\n",
      "Building dataset, requesting data from 0 to 170\n",
      "x here is\n",
      "[[ 95.  86.  81. ... 110.  96.  97.]\n",
      " [ 86.  81.  81. ...  96.  97.  97.]\n",
      " [ 81.  81.  82. ...  97.  97.  95.]\n",
      " ...\n",
      " [316. 314. 309. ... 232. 222. 221.]\n",
      " [314. 309. 303. ... 222. 221. 222.]\n",
      " [309. 303. 294. ... 221. 222. 223.]]\n",
      "y here is\n",
      "[[ 99.  99.  99. ...  99.  99.  99.]\n",
      " [ 98.  98.  98. ...  98.  98.  98.]\n",
      " [100. 100. 100. ... 100. 100. 100.]\n",
      " ...\n",
      " [225. 225. 225. ... 225. 225. 225.]\n",
      " [227. 227. 227. ... 227. 227. 227.]\n",
      " [226. 226. 226. ... 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 484/5752\n",
      "Found 170 continuous time series\n",
      "Data shape: (6238, 12), Train/test: 6236/2\n",
      "Train test ratio: 3118.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701450F0>\n",
      "Epoch 0, test loss: 0.166548\n",
      "Epoch 1, test loss: 0.171194\n",
      "Epoch 2, test loss: 0.173965\n",
      "Epoch 3, test loss: 0.169526\n",
      "Epoch 4, test loss: 0.168976\n",
      "Epoch 5, test loss: 0.167069\n",
      "Epoch 6, test loss: 0.166812\n",
      "Epoch 7, test loss: 0.166675\n",
      "Epoch 8, test loss: 0.167588\n",
      "Epoch 9, test loss: 0.167608\n",
      "Epoch 10, test loss: 0.166668\n",
      "Epoch 11, test loss: 0.168053\n",
      "Epoch 12, test loss: 0.172017\n",
      "Epoch 13, test loss: 0.167653\n",
      "Epoch 14, test loss: 0.168116\n",
      "Epoch 15, test loss: 0.172928\n",
      "Epoch 16, test loss: 0.167496\n",
      "Epoch 17, test loss: 0.170854\n",
      "Epoch 18, test loss: 0.168068\n",
      "Epoch 19, test loss: 0.171400\n",
      "Epoch 20, test loss: 0.170525\n",
      "Epoch 21, test loss: 0.169937\n",
      "Epoch 22, test loss: 0.169258\n",
      "Epoch 23, test loss: 0.168178\n",
      "Epoch 24, test loss: 0.168655\n",
      "Epoch 25, test loss: 0.171017\n",
      "Epoch 26, test loss: 0.169218\n",
      "Epoch 27, test loss: 0.167637\n",
      "Epoch 28, test loss: 0.167126\n",
      "Epoch 29, test loss: 0.170722\n",
      "Epoch 30, test loss: 0.168585\n",
      "Epoch 31, test loss: 0.168268\n",
      "Epoch 32, test loss: 0.167528\n",
      "Epoch 33, test loss: 0.177577\n",
      "Epoch 34, test loss: 0.171162\n",
      "Epoch 35, test loss: 0.167860\n",
      "Epoch 36, test loss: 0.170546\n",
      "Epoch 37, test loss: 0.170516\n",
      "Epoch 38, test loss: 0.167715\n",
      "Epoch 39, test loss: 0.168963\n",
      "Epoch 40, test loss: 0.168444\n",
      "Epoch 41, test loss: 0.168591\n",
      "Epoch 42, test loss: 0.168267\n",
      "Epoch 43, test loss: 0.168320\n",
      "Epoch 44, test loss: 0.167416\n",
      "Epoch 45, test loss: 0.172739\n",
      "Epoch 46, test loss: 0.167885\n",
      "Epoch 47, test loss: 0.167390\n",
      "Epoch 48, test loss: 0.167465\n",
      "Epoch 49, test loss: 0.177534\n",
      "Epoch 50, test loss: 0.175375\n",
      "Epoch 51, test loss: 0.172105\n",
      "Epoch 52, test loss: 0.168439\n",
      "Epoch 53, test loss: 0.175753\n",
      "Epoch 54, test loss: 0.168195\n",
      "Epoch 55, test loss: 0.167796\n",
      "Epoch 56, test loss: 0.167457\n",
      "Epoch 57, test loss: 0.168318\n",
      "Epoch 58, test loss: 0.168322\n",
      "Epoch 59, test loss: 0.169427\n",
      "Epoch 60, test loss: 0.168612\n",
      "Epoch 61, test loss: 0.169719\n",
      "Epoch 62, test loss: 0.168842\n",
      "Epoch 63, test loss: 0.168141\n",
      "Epoch 64, test loss: 0.169675\n",
      "Epoch 65, test loss: 0.168375\n",
      "Epoch 66, test loss: 0.168716\n",
      "Epoch 67, test loss: 0.168784\n",
      "Epoch 68, test loss: 0.168120\n",
      "Epoch 69, test loss: 0.170282\n",
      "Epoch 70, test loss: 0.170521\n",
      "Epoch 71, test loss: 0.168749\n",
      "Epoch 72, test loss: 0.169135\n",
      "Epoch 73, test loss: 0.167533\n",
      "Epoch 74, test loss: 0.168191\n",
      "Epoch 75, test loss: 0.169034\n",
      "Epoch 76, test loss: 0.169027\n",
      "Epoch 77, test loss: 0.168361\n",
      "Epoch 78, test loss: 0.172572\n",
      "Epoch 79, test loss: 0.167665\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701450F0>\n",
      "Epoch 0, test loss: 0.168070\n",
      "Epoch 1, test loss: 0.166397\n",
      "Epoch 2, test loss: 0.166318\n",
      "Epoch 3, test loss: 0.167110\n",
      "Epoch 4, test loss: 0.169243\n",
      "Epoch 5, test loss: 0.168563\n",
      "Epoch 6, test loss: 0.166381\n",
      "Epoch 7, test loss: 0.166484\n",
      "Epoch 8, test loss: 0.171693\n",
      "Epoch 9, test loss: 0.166526\n",
      "Epoch 10, test loss: 0.166854\n",
      "Epoch 11, test loss: 0.166501\n",
      "Epoch 12, test loss: 0.167758\n",
      "Epoch 13, test loss: 0.166468\n",
      "Epoch 14, test loss: 0.168497\n",
      "Epoch 15, test loss: 0.170419\n",
      "Epoch 16, test loss: 0.166891\n",
      "Epoch 17, test loss: 0.166905\n",
      "Epoch 18, test loss: 0.168760\n",
      "Epoch 19, test loss: 0.168111\n",
      "Epoch 20, test loss: 0.167961\n",
      "Epoch 21, test loss: 0.169667\n",
      "Epoch 22, test loss: 0.167321\n",
      "Epoch 23, test loss: 0.166820\n",
      "Epoch 24, test loss: 0.170165\n",
      "Epoch 25, test loss: 0.169970\n",
      "Epoch 26, test loss: 0.166195\n",
      "Epoch 27, test loss: 0.166307\n",
      "Epoch 28, test loss: 0.167138\n",
      "Epoch 29, test loss: 0.166535\n",
      "Epoch 30, test loss: 0.166174\n",
      "Epoch 31, test loss: 0.167605\n",
      "Epoch 32, test loss: 0.167131\n",
      "Epoch 33, test loss: 0.166661\n",
      "Epoch 34, test loss: 0.169876\n",
      "Epoch 35, test loss: 0.167747\n",
      "Epoch 36, test loss: 0.166805\n",
      "Epoch 37, test loss: 0.167485\n",
      "Epoch 38, test loss: 0.167252\n",
      "Epoch 39, test loss: 0.168424\n",
      "Epoch 40, test loss: 0.167663\n",
      "Epoch 41, test loss: 0.170144\n",
      "Epoch 42, test loss: 0.166699\n",
      "Epoch 43, test loss: 0.166683\n",
      "Epoch 44, test loss: 0.166547\n",
      "Epoch 45, test loss: 0.168680\n",
      "Epoch 46, test loss: 0.166671\n",
      "Epoch 47, test loss: 0.166344\n",
      "Epoch 48, test loss: 0.166604\n",
      "Epoch 49, test loss: 0.166371\n",
      "Epoch 50, test loss: 0.167125\n",
      "Epoch 51, test loss: 0.170393\n",
      "Epoch 52, test loss: 0.169743\n",
      "Epoch 53, test loss: 0.167564\n",
      "Epoch 54, test loss: 0.166856\n",
      "Epoch 55, test loss: 0.166485\n",
      "Epoch 56, test loss: 0.166628\n",
      "Epoch 57, test loss: 0.169463\n",
      "Epoch 58, test loss: 0.166915\n",
      "Epoch 59, test loss: 0.166160\n",
      "Epoch 60, test loss: 0.167512\n",
      "Epoch 61, test loss: 0.166731\n",
      "Epoch 62, test loss: 0.166707\n",
      "Epoch 63, test loss: 0.166698\n",
      "Epoch 64, test loss: 0.170113\n",
      "Epoch 65, test loss: 0.166542\n",
      "Epoch 66, test loss: 0.167630\n",
      "Epoch 67, test loss: 0.167691\n",
      "Epoch 68, test loss: 0.168770\n",
      "Epoch 69, test loss: 0.169147\n",
      "Epoch 70, test loss: 0.168002\n",
      "Epoch 71, test loss: 0.167023\n",
      "Epoch 72, test loss: 0.168081\n",
      "Epoch 73, test loss: 0.166126\n",
      "Epoch 74, test loss: 0.169511\n",
      "Epoch 75, test loss: 0.166181\n",
      "Epoch 76, test loss: 0.166965\n",
      "Epoch 77, test loss: 0.166216\n",
      "Epoch 78, test loss: 0.167387\n",
      "Epoch 79, test loss: 0.167330\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4701450F0>\n",
      "Epoch 0, test loss: 0.742323\n",
      "Epoch 1, test loss: 0.203613\n",
      "Epoch 2, test loss: 0.189288\n",
      "Epoch 3, test loss: 0.175488\n",
      "Epoch 4, test loss: 0.172566\n",
      "Epoch 5, test loss: 0.174277\n",
      "Epoch 6, test loss: 0.170606\n",
      "Epoch 7, test loss: 0.172640\n",
      "Epoch 8, test loss: 0.171056\n",
      "Epoch 9, test loss: 0.171705\n",
      "Epoch 10, test loss: 0.170294\n",
      "Epoch 11, test loss: 0.170596\n",
      "Epoch 12, test loss: 0.170524\n",
      "Epoch 13, test loss: 0.171224\n",
      "Epoch 14, test loss: 0.172230\n",
      "Epoch 15, test loss: 0.171695\n",
      "Epoch 16, test loss: 0.174055\n",
      "Epoch 17, test loss: 0.173706\n",
      "Epoch 18, test loss: 0.178625\n",
      "Epoch 19, test loss: 0.170600\n",
      "Epoch 20, test loss: 0.171605\n",
      "Epoch 21, test loss: 0.170654\n",
      "Epoch 22, test loss: 0.172767\n",
      "Epoch 23, test loss: 0.170253\n",
      "Epoch 24, test loss: 0.175684\n",
      "Epoch 25, test loss: 0.170717\n",
      "Epoch 26, test loss: 0.171612\n",
      "Epoch 27, test loss: 0.170467\n",
      "Epoch 28, test loss: 0.169874\n",
      "Epoch 29, test loss: 0.174626\n",
      "Epoch 30, test loss: 0.172341\n",
      "Epoch 31, test loss: 0.170175\n",
      "Epoch 32, test loss: 0.169928\n",
      "Epoch 33, test loss: 0.169935\n",
      "Epoch 34, test loss: 0.171480\n",
      "Epoch 35, test loss: 0.170346\n",
      "Epoch 36, test loss: 0.172335\n",
      "Epoch 37, test loss: 0.170142\n",
      "Epoch 38, test loss: 0.171522\n",
      "Epoch 39, test loss: 0.170637\n",
      "Epoch 40, test loss: 0.170195\n",
      "Epoch 41, test loss: 0.172957\n",
      "Epoch 42, test loss: 0.172239\n",
      "Epoch 43, test loss: 0.171150\n",
      "Epoch 44, test loss: 0.170852\n",
      "Epoch 45, test loss: 0.170289\n",
      "Epoch 46, test loss: 0.169889\n",
      "Epoch 47, test loss: 0.170149\n",
      "Epoch 48, test loss: 0.171360\n",
      "Epoch 49, test loss: 0.173714\n",
      "Epoch 50, test loss: 0.170049\n",
      "Epoch 51, test loss: 0.169926\n",
      "Epoch 52, test loss: 0.173286\n",
      "Epoch 53, test loss: 0.170803\n",
      "Epoch 54, test loss: 0.174342\n",
      "Epoch 55, test loss: 0.169634\n",
      "Epoch 56, test loss: 0.172228\n",
      "Epoch 57, test loss: 0.170140\n",
      "Epoch 58, test loss: 0.170090\n",
      "Epoch 59, test loss: 0.170781\n",
      "Epoch 60, test loss: 0.172682\n",
      "Epoch 61, test loss: 0.174186\n",
      "Epoch 62, test loss: 0.170206\n",
      "Epoch 63, test loss: 0.174061\n",
      "Epoch 64, test loss: 0.172376\n",
      "Epoch 65, test loss: 0.170387\n",
      "Epoch 66, test loss: 0.172383\n",
      "Epoch 67, test loss: 0.169498\n",
      "Epoch 68, test loss: 0.171753\n",
      "Epoch 69, test loss: 0.169852\n",
      "Epoch 70, test loss: 0.170521\n",
      "Epoch 71, test loss: 0.169509\n",
      "Epoch 72, test loss: 0.169996\n",
      "Epoch 73, test loss: 0.173241\n",
      "Epoch 74, test loss: 0.169653\n",
      "Epoch 75, test loss: 0.170224\n",
      "Epoch 76, test loss: 0.173153\n",
      "Epoch 77, test loss: 0.170434\n",
      "Epoch 78, test loss: 0.169605\n",
      "Epoch 79, test loss: 0.169630\n",
      "Epoch 80, test loss: 0.169353\n",
      "Epoch 81, test loss: 0.169416\n",
      "Epoch 82, test loss: 0.169972\n",
      "Epoch 83, test loss: 0.172341\n",
      "Epoch 84, test loss: 0.169785\n",
      "Epoch 85, test loss: 0.170094\n",
      "Epoch 86, test loss: 0.169964\n",
      "Epoch 87, test loss: 0.169202\n",
      "Epoch 88, test loss: 0.169291\n",
      "Epoch 89, test loss: 0.168956\n",
      "Epoch 90, test loss: 0.169847\n",
      "Epoch 91, test loss: 0.170440\n",
      "Epoch 92, test loss: 0.170675\n",
      "Epoch 93, test loss: 0.169427\n",
      "Epoch 94, test loss: 0.170047\n",
      "Epoch 95, test loss: 0.169625\n",
      "Epoch 96, test loss: 0.169981\n",
      "Epoch 97, test loss: 0.169480\n",
      "Epoch 98, test loss: 0.169093\n",
      "Epoch 99, test loss: 0.170453\n",
      "Epoch 100, test loss: 0.169733\n",
      "Epoch 101, test loss: 0.170862\n",
      "Epoch 102, test loss: 0.169325\n",
      "Epoch 103, test loss: 0.169389\n",
      "Epoch 104, test loss: 0.170533\n",
      "Epoch 105, test loss: 0.169608\n",
      "Epoch 106, test loss: 0.170349\n",
      "Epoch 107, test loss: 0.169864\n",
      "Epoch 108, test loss: 0.168791\n",
      "Epoch 109, test loss: 0.174756\n",
      "Epoch 110, test loss: 0.169122\n",
      "Epoch 111, test loss: 0.170058\n",
      "Epoch 112, test loss: 0.168285\n",
      "Epoch 113, test loss: 0.170522\n",
      "Epoch 114, test loss: 0.169897\n",
      "Epoch 115, test loss: 0.169302\n",
      "Epoch 116, test loss: 0.173070\n",
      "Epoch 117, test loss: 0.168698\n",
      "Epoch 118, test loss: 0.169616\n",
      "Epoch 119, test loss: 0.172269\n",
      "Epoch 120, test loss: 0.170750\n",
      "Epoch 121, test loss: 0.169717\n",
      "Epoch 122, test loss: 0.169307\n",
      "Epoch 123, test loss: 0.172948\n",
      "Epoch 124, test loss: 0.170347\n",
      "Epoch 125, test loss: 0.168871\n",
      "Epoch 126, test loss: 0.169831\n",
      "Epoch 127, test loss: 0.168464\n",
      "Epoch 128, test loss: 0.168834\n",
      "Epoch 129, test loss: 0.169739\n",
      "Epoch 130, test loss: 0.168575\n",
      "Epoch 131, test loss: 0.169440\n",
      "Epoch 132, test loss: 0.169407\n",
      "Epoch 133, test loss: 0.172455\n",
      "Epoch 134, test loss: 0.168610\n",
      "Epoch 135, test loss: 0.174554\n",
      "Epoch 136, test loss: 0.171856\n",
      "Epoch 137, test loss: 0.176274\n",
      "Epoch 138, test loss: 0.169988\n",
      "Epoch 139, test loss: 0.169891\n",
      "Epoch 140, test loss: 0.169713\n",
      "Epoch 141, test loss: 0.168542\n",
      "Epoch 142, test loss: 0.172784\n",
      "Epoch 143, test loss: 0.168936\n",
      "Epoch 144, test loss: 0.169865\n",
      "Epoch 145, test loss: 0.171600\n",
      "Epoch 146, test loss: 0.168142\n",
      "Epoch 147, test loss: 0.169555\n",
      "Epoch 148, test loss: 0.170713\n",
      "Epoch 149, test loss: 0.169414\n",
      "Epoch 150, test loss: 0.170021\n",
      "Epoch 151, test loss: 0.169764\n",
      "Epoch 152, test loss: 0.168779\n",
      "Epoch 153, test loss: 0.169008\n",
      "Epoch 154, test loss: 0.173154\n",
      "Epoch 155, test loss: 0.173684\n",
      "Epoch 156, test loss: 0.168452\n",
      "Epoch 157, test loss: 0.170007\n",
      "Epoch 158, test loss: 0.168857\n",
      "Epoch 159, test loss: 0.168132\n",
      "Epoch 160, test loss: 0.169400\n",
      "Epoch 161, test loss: 0.169651\n",
      "Epoch 162, test loss: 0.170053\n",
      "Epoch 163, test loss: 0.168529\n",
      "Epoch 164, test loss: 0.173446\n",
      "Epoch 165, test loss: 0.168348\n",
      "Epoch 166, test loss: 0.168414\n",
      "Epoch 167, test loss: 0.173538\n",
      "Epoch 168, test loss: 0.169549\n",
      "Epoch 169, test loss: 0.171995\n",
      "Epoch 170, test loss: 0.169363\n",
      "Epoch 171, test loss: 0.173854\n",
      "Epoch 172, test loss: 0.168351\n",
      "Epoch 173, test loss: 0.171039\n",
      "Epoch 174, test loss: 0.169814\n",
      "Epoch 175, test loss: 0.173777\n",
      "Epoch 176, test loss: 0.170150\n",
      "Epoch 177, test loss: 0.168489\n",
      "Epoch 178, test loss: 0.170291\n",
      "Epoch 179, test loss: 0.168055\n",
      "Epoch 180, test loss: 0.169073\n",
      "Epoch 181, test loss: 0.168852\n",
      "Epoch 182, test loss: 0.168383\n",
      "Epoch 183, test loss: 0.170267\n",
      "Epoch 184, test loss: 0.168439\n",
      "Epoch 185, test loss: 0.168718\n",
      "Epoch 186, test loss: 0.167962\n",
      "Epoch 187, test loss: 0.168256\n",
      "Epoch 188, test loss: 0.169070\n",
      "Epoch 189, test loss: 0.169325\n",
      "Epoch 190, test loss: 0.168159\n",
      "Epoch 191, test loss: 0.168421\n",
      "Epoch 192, test loss: 0.169755\n",
      "Epoch 193, test loss: 0.169515\n",
      "Epoch 194, test loss: 0.170174\n",
      "Epoch 195, test loss: 0.173929\n",
      "Epoch 196, test loss: 0.167893\n",
      "Epoch 197, test loss: 0.168982\n",
      "Epoch 198, test loss: 0.169573\n",
      "Epoch 199, test loss: 0.167712\n",
      "Epoch 200, test loss: 0.167944\n",
      "Epoch 201, test loss: 0.168123\n",
      "Epoch 202, test loss: 0.171734\n",
      "Epoch 203, test loss: 0.167381\n",
      "Epoch 204, test loss: 0.167551\n",
      "Epoch 205, test loss: 0.169242\n",
      "Epoch 206, test loss: 0.171003\n",
      "Epoch 207, test loss: 0.167651\n",
      "Epoch 208, test loss: 0.167997\n",
      "Epoch 209, test loss: 0.175153\n",
      "Epoch 210, test loss: 0.168775\n",
      "Epoch 211, test loss: 0.168233\n",
      "Epoch 212, test loss: 0.174239\n",
      "Epoch 213, test loss: 0.169140\n",
      "Epoch 214, test loss: 0.170248\n",
      "Epoch 215, test loss: 0.167685\n",
      "Epoch 216, test loss: 0.167733\n",
      "Epoch 217, test loss: 0.174166\n",
      "Epoch 218, test loss: 0.174897\n",
      "Epoch 219, test loss: 0.167838\n",
      "Epoch 220, test loss: 0.167801\n",
      "Epoch 221, test loss: 0.167399\n",
      "Epoch 222, test loss: 0.167715\n",
      "Epoch 223, test loss: 0.172396\n",
      "Epoch 224, test loss: 0.173359\n",
      "Epoch 225, test loss: 0.169284\n",
      "Epoch 226, test loss: 0.167384\n",
      "Epoch 227, test loss: 0.167327\n",
      "Epoch 228, test loss: 0.167553\n",
      "Epoch 229, test loss: 0.167214\n",
      "Epoch 230, test loss: 0.170346\n",
      "Epoch 231, test loss: 0.167956\n",
      "Epoch 232, test loss: 0.169208\n",
      "Epoch 233, test loss: 0.168065\n",
      "Epoch 234, test loss: 0.167862\n",
      "Epoch 235, test loss: 0.167292\n",
      "Epoch 236, test loss: 0.169965\n",
      "Epoch 237, test loss: 0.169804\n",
      "Epoch 238, test loss: 0.167777\n",
      "Epoch 239, test loss: 0.167616\n",
      "Pretrain data: 19669610.0\n",
      "Building dataset, requesting data from 0 to 819\n",
      "x here is\n",
      "[[ 95.  86.  81. ... 110.  96.  97.]\n",
      " [ 86.  81.  81. ...  96.  97.  97.]\n",
      " [ 81.  81.  82. ...  97.  97.  95.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[ 99.  99.  99. ...  99.  99.  99.]\n",
      " [ 98.  98.  98. ...  98.  98.  98.]\n",
      " [100. 100. 100. ... 100. 100. 100.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7463/103226\n",
      "Found 819 continuous time series\n",
      "Data shape: (110691, 12), Train/test: 110689/2\n",
      "Train test ratio: 55344.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1729\n",
      "Epoch 0, train loss: 0.261244\n",
      "Epoch 1, train loss: 0.246346\n",
      "Epoch 2, train loss: 0.281792\n",
      "Epoch 3, train loss: 0.195608\n",
      "Epoch 4, train loss: 0.200426\n",
      "Epoch 5, train loss: 0.224222\n",
      "Epoch 6, train loss: 0.186125\n",
      "Epoch 7, train loss: 0.193374\n",
      "Epoch 8, train loss: 0.287173\n",
      "Epoch 9, train loss: 0.250932\n",
      "Epoch 10, train loss: 0.186104\n",
      "Epoch 11, train loss: 0.191318\n",
      "Epoch 12, train loss: 0.216746\n",
      "Epoch 13, train loss: 0.197987\n",
      "Epoch 14, train loss: 0.231033\n",
      "Epoch 15, train loss: 0.205105\n",
      "Epoch 16, train loss: 0.259666\n",
      "Epoch 17, train loss: 0.211851\n",
      "Epoch 18, train loss: 0.238841\n",
      "Epoch 19, train loss: 0.188254\n",
      "Epoch 20, train loss: 0.229134\n",
      "Epoch 21, train loss: 0.214190\n",
      "Epoch 22, train loss: 0.236264\n",
      "Epoch 23, train loss: 0.163829\n",
      "Epoch 24, train loss: 0.280485\n",
      "Epoch 25, train loss: 0.182754\n",
      "Epoch 26, train loss: 0.173982\n",
      "Epoch 27, train loss: 0.242833\n",
      "Epoch 28, train loss: 0.216977\n",
      "Epoch 29, train loss: 0.179950\n",
      "Epoch 30, train loss: 0.344532\n",
      "Epoch 31, train loss: 0.184674\n",
      "Epoch 32, train loss: 0.151222\n",
      "Epoch 33, train loss: 0.220377\n",
      "Epoch 34, train loss: 0.163057\n",
      "Epoch 35, train loss: 0.182075\n",
      "Epoch 36, train loss: 0.234246\n",
      "Epoch 37, train loss: 0.186796\n",
      "Epoch 38, train loss: 0.193344\n",
      "Epoch 39, train loss: 0.213821\n",
      "Epoch 40, train loss: 0.211309\n",
      "Epoch 41, train loss: 0.218166\n",
      "Epoch 42, train loss: 0.184292\n",
      "Epoch 43, train loss: 0.208406\n",
      "Epoch 44, train loss: 0.194886\n",
      "Epoch 45, train loss: 0.238747\n",
      "Epoch 46, train loss: 0.288428\n",
      "Epoch 47, train loss: 0.205025\n",
      "Epoch 48, train loss: 0.205173\n",
      "Epoch 49, train loss: 0.236780\n",
      "Epoch 50, train loss: 0.144316\n",
      "Epoch 51, train loss: 0.182388\n",
      "Epoch 52, train loss: 0.212465\n",
      "Epoch 53, train loss: 0.172870\n",
      "Epoch 54, train loss: 0.170196\n",
      "Epoch 55, train loss: 0.192655\n",
      "Epoch 56, train loss: 0.242581\n",
      "Epoch 57, train loss: 0.190617\n",
      "Epoch 58, train loss: 0.217581\n",
      "Epoch 59, train loss: 0.229988\n",
      "Epoch 60, train loss: 0.194641\n",
      "Epoch 61, train loss: 0.211081\n",
      "Epoch 62, train loss: 0.167846\n",
      "Epoch 63, train loss: 0.195480\n",
      "Epoch 64, train loss: 0.184625\n",
      "Epoch 65, train loss: 0.201573\n",
      "Epoch 66, train loss: 0.256772\n",
      "Epoch 67, train loss: 0.196329\n",
      "Epoch 68, train loss: 0.225840\n",
      "Epoch 69, train loss: 0.236858\n",
      "Epoch 70, train loss: 0.181105\n",
      "Epoch 71, train loss: 0.238727\n",
      "Epoch 72, train loss: 0.181195\n",
      "Epoch 73, train loss: 0.186129\n",
      "Epoch 74, train loss: 0.168745\n",
      "Epoch 75, train loss: 0.252982\n",
      "Epoch 76, train loss: 0.236189\n",
      "Epoch 77, train loss: 0.173988\n",
      "Epoch 78, train loss: 0.181186\n",
      "Epoch 79, train loss: 0.165192\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[161. 155. 150. ... 130. 125. 121.]\n",
      " [155. 150. 147. ... 125. 121. 117.]\n",
      " [150. 147. 144. ... 121. 117. 113.]\n",
      " ...\n",
      " [311. 313. 320. ... 293. 301. 307.]\n",
      " [313. 320. 321. ... 301. 307. 306.]\n",
      " [320. 321. 314. ... 307. 306. 300.]]\n",
      "y here is\n",
      "[[103. 103. 103. ... 103. 103. 103.]\n",
      " [100. 100. 100. ... 100. 100. 100.]\n",
      " [ 98.  98.  98. ...  98.  98.  98.]\n",
      " ...\n",
      " [284. 284. 284. ... 284. 284. 284.]\n",
      " [273. 273. 273. ... 273. 273. 273.]\n",
      " [262. 262. 262. ... 262. 262. 262.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2597, 12), Train/test: 1/2596\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [175. 177. 178. ... 176. 173. 172.]\n",
      " [177. 178. 176. ... 173. 172. 173.]\n",
      " [178. 176. 175. ... 172. 173. 173.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 445/9799\n",
      "Found 23 continuous time series\n",
      "Data shape: (10246, 12), Train/test: 10244/2\n",
      "Train test ratio: 5122.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2977633A0>\n",
      "Epoch 0, test loss: 0.171598\n",
      "Epoch 1, test loss: 0.177460\n",
      "Epoch 2, test loss: 0.174882\n",
      "Epoch 3, test loss: 0.171593\n",
      "Epoch 4, test loss: 0.171057\n",
      "Epoch 5, test loss: 0.175972\n",
      "Epoch 6, test loss: 0.172896\n",
      "Epoch 7, test loss: 0.171625\n",
      "Epoch 8, test loss: 0.172351\n",
      "Epoch 9, test loss: 0.171317\n",
      "Epoch 10, test loss: 0.172419\n",
      "Epoch 11, test loss: 0.171591\n",
      "Epoch 12, test loss: 0.172576\n",
      "Epoch 13, test loss: 0.172955\n",
      "Epoch 14, test loss: 0.172376\n",
      "Epoch 15, test loss: 0.171624\n",
      "Epoch 16, test loss: 0.175407\n",
      "Epoch 17, test loss: 0.171675\n",
      "Epoch 18, test loss: 0.172024\n",
      "Epoch 19, test loss: 0.172122\n",
      "Epoch 20, test loss: 0.171682\n",
      "Epoch 21, test loss: 0.178529\n",
      "Epoch 22, test loss: 0.171887\n",
      "Epoch 23, test loss: 0.171848\n",
      "Epoch 24, test loss: 0.171952\n",
      "Epoch 25, test loss: 0.175810\n",
      "Epoch 26, test loss: 0.172052\n",
      "Epoch 27, test loss: 0.171258\n",
      "Epoch 28, test loss: 0.171416\n",
      "Epoch 29, test loss: 0.171349\n",
      "Epoch 30, test loss: 0.174328\n",
      "Epoch 31, test loss: 0.171418\n",
      "Epoch 32, test loss: 0.171092\n",
      "Epoch 33, test loss: 0.171741\n",
      "Epoch 34, test loss: 0.175873\n",
      "Epoch 35, test loss: 0.172386\n",
      "Epoch 36, test loss: 0.171481\n",
      "Epoch 37, test loss: 0.172176\n",
      "Epoch 38, test loss: 0.171538\n",
      "Epoch 39, test loss: 0.175279\n",
      "Epoch 40, test loss: 0.171370\n",
      "Epoch 41, test loss: 0.172378\n",
      "Epoch 42, test loss: 0.171740\n",
      "Epoch 43, test loss: 0.171636\n",
      "Epoch 44, test loss: 0.170908\n",
      "Epoch 45, test loss: 0.173009\n",
      "Epoch 46, test loss: 0.172283\n",
      "Epoch 47, test loss: 0.172846\n",
      "Epoch 48, test loss: 0.171922\n",
      "Epoch 49, test loss: 0.171969\n",
      "Epoch 50, test loss: 0.171448\n",
      "Epoch 51, test loss: 0.172422\n",
      "Epoch 52, test loss: 0.171248\n",
      "Epoch 53, test loss: 0.171552\n",
      "Epoch 54, test loss: 0.174514\n",
      "Epoch 55, test loss: 0.171945\n",
      "Epoch 56, test loss: 0.173413\n",
      "Epoch 57, test loss: 0.171518\n",
      "Epoch 58, test loss: 0.171130\n",
      "Epoch 59, test loss: 0.171049\n",
      "Epoch 60, test loss: 0.171926\n",
      "Epoch 61, test loss: 0.171386\n",
      "Epoch 62, test loss: 0.177623\n",
      "Epoch 63, test loss: 0.173230\n",
      "Epoch 64, test loss: 0.171469\n",
      "Epoch 65, test loss: 0.171455\n",
      "Epoch 66, test loss: 0.172383\n",
      "Epoch 67, test loss: 0.171131\n",
      "Epoch 68, test loss: 0.174376\n",
      "Epoch 69, test loss: 0.177576\n",
      "Epoch 70, test loss: 0.175050\n",
      "Epoch 71, test loss: 0.171381\n",
      "Epoch 72, test loss: 0.171723\n",
      "Epoch 73, test loss: 0.172943\n",
      "Epoch 74, test loss: 0.171359\n",
      "Epoch 75, test loss: 0.171180\n",
      "Epoch 76, test loss: 0.177816\n",
      "Epoch 77, test loss: 0.171292\n",
      "Epoch 78, test loss: 0.173506\n",
      "Epoch 79, test loss: 0.175612\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2977633A0>\n",
      "Epoch 0, test loss: 0.170730\n",
      "Epoch 1, test loss: 0.173330\n",
      "Epoch 2, test loss: 0.173574\n",
      "Epoch 3, test loss: 0.170843\n",
      "Epoch 4, test loss: 0.171797\n",
      "Epoch 5, test loss: 0.171418\n",
      "Epoch 6, test loss: 0.171475\n",
      "Epoch 7, test loss: 0.171652\n",
      "Epoch 8, test loss: 0.171727\n",
      "Epoch 9, test loss: 0.173035\n",
      "Epoch 10, test loss: 0.171505\n",
      "Epoch 11, test loss: 0.171930\n",
      "Epoch 12, test loss: 0.172957\n",
      "Epoch 13, test loss: 0.171921\n",
      "Epoch 14, test loss: 0.172906\n",
      "Epoch 15, test loss: 0.172272\n",
      "Epoch 16, test loss: 0.173222\n",
      "Epoch 17, test loss: 0.173158\n",
      "Epoch 18, test loss: 0.173109\n",
      "Epoch 19, test loss: 0.173399\n",
      "Epoch 20, test loss: 0.171656\n",
      "Epoch 21, test loss: 0.174565\n",
      "Epoch 22, test loss: 0.171688\n",
      "Epoch 23, test loss: 0.171917\n",
      "Epoch 24, test loss: 0.172105\n",
      "Epoch 25, test loss: 0.171499\n",
      "Epoch 26, test loss: 0.172698\n",
      "Epoch 27, test loss: 0.172278\n",
      "Epoch 28, test loss: 0.173865\n",
      "Epoch 29, test loss: 0.171808\n",
      "Epoch 30, test loss: 0.171358\n",
      "Epoch 31, test loss: 0.171564\n",
      "Epoch 32, test loss: 0.172437\n",
      "Epoch 33, test loss: 0.171527\n",
      "Epoch 34, test loss: 0.171700\n",
      "Epoch 35, test loss: 0.172195\n",
      "Epoch 36, test loss: 0.171563\n",
      "Epoch 37, test loss: 0.171154\n",
      "Epoch 38, test loss: 0.172228\n",
      "Epoch 39, test loss: 0.171084\n",
      "Epoch 40, test loss: 0.171375\n",
      "Epoch 41, test loss: 0.173103\n",
      "Epoch 42, test loss: 0.171378\n",
      "Epoch 43, test loss: 0.171194\n",
      "Epoch 44, test loss: 0.173375\n",
      "Epoch 45, test loss: 0.171561\n",
      "Epoch 46, test loss: 0.172494\n",
      "Epoch 47, test loss: 0.170868\n",
      "Epoch 48, test loss: 0.171097\n",
      "Epoch 49, test loss: 0.171622\n",
      "Epoch 50, test loss: 0.173041\n",
      "Epoch 51, test loss: 0.173254\n",
      "Epoch 52, test loss: 0.171459\n",
      "Epoch 53, test loss: 0.171593\n",
      "Epoch 54, test loss: 0.172440\n",
      "Epoch 55, test loss: 0.171324\n",
      "Epoch 56, test loss: 0.172651\n",
      "Epoch 57, test loss: 0.173261\n",
      "Epoch 58, test loss: 0.172274\n",
      "Epoch 59, test loss: 0.172583\n",
      "Epoch 60, test loss: 0.171691\n",
      "Epoch 61, test loss: 0.172194\n",
      "Epoch 62, test loss: 0.171670\n",
      "Epoch 63, test loss: 0.171807\n",
      "Epoch 64, test loss: 0.171850\n",
      "Epoch 65, test loss: 0.171953\n",
      "Epoch 66, test loss: 0.171701\n",
      "Epoch 67, test loss: 0.171689\n",
      "Epoch 68, test loss: 0.171358\n",
      "Epoch 69, test loss: 0.171427\n",
      "Epoch 70, test loss: 0.171519\n",
      "Epoch 71, test loss: 0.171761\n",
      "Epoch 72, test loss: 0.171794\n",
      "Epoch 73, test loss: 0.171478\n",
      "Epoch 74, test loss: 0.170969\n",
      "Epoch 75, test loss: 0.174307\n",
      "Epoch 76, test loss: 0.172957\n",
      "Epoch 77, test loss: 0.171529\n",
      "Epoch 78, test loss: 0.172511\n",
      "Epoch 79, test loss: 0.176218\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2977633A0>\n",
      "Epoch 0, test loss: 0.237142\n",
      "Epoch 1, test loss: 0.207087\n",
      "Epoch 2, test loss: 0.187890\n",
      "Epoch 3, test loss: 0.181566\n",
      "Epoch 4, test loss: 0.178868\n",
      "Epoch 5, test loss: 0.177506\n",
      "Epoch 6, test loss: 0.177008\n",
      "Epoch 7, test loss: 0.177854\n",
      "Epoch 8, test loss: 0.176067\n",
      "Epoch 9, test loss: 0.178519\n",
      "Epoch 10, test loss: 0.176493\n",
      "Epoch 11, test loss: 0.177973\n",
      "Epoch 12, test loss: 0.175684\n",
      "Epoch 13, test loss: 0.175799\n",
      "Epoch 14, test loss: 0.178417\n",
      "Epoch 15, test loss: 0.175506\n",
      "Epoch 16, test loss: 0.177837\n",
      "Epoch 17, test loss: 0.177773\n",
      "Epoch 18, test loss: 0.176222\n",
      "Epoch 19, test loss: 0.175156\n",
      "Epoch 20, test loss: 0.175522\n",
      "Epoch 21, test loss: 0.176186\n",
      "Epoch 22, test loss: 0.176942\n",
      "Epoch 23, test loss: 0.181058\n",
      "Epoch 24, test loss: 0.178841\n",
      "Epoch 25, test loss: 0.175586\n",
      "Epoch 26, test loss: 0.175299\n",
      "Epoch 27, test loss: 0.174931\n",
      "Epoch 28, test loss: 0.176761\n",
      "Epoch 29, test loss: 0.175395\n",
      "Epoch 30, test loss: 0.177496\n",
      "Epoch 31, test loss: 0.179732\n",
      "Epoch 32, test loss: 0.174741\n",
      "Epoch 33, test loss: 0.176420\n",
      "Epoch 34, test loss: 0.175554\n",
      "Epoch 35, test loss: 0.175698\n",
      "Epoch 36, test loss: 0.175002\n",
      "Epoch 37, test loss: 0.175402\n",
      "Epoch 38, test loss: 0.175031\n",
      "Epoch 39, test loss: 0.175241\n",
      "Epoch 40, test loss: 0.175132\n",
      "Epoch 41, test loss: 0.176156\n",
      "Epoch 42, test loss: 0.174689\n",
      "Epoch 43, test loss: 0.176831\n",
      "Epoch 44, test loss: 0.175988\n",
      "Epoch 45, test loss: 0.175185\n",
      "Epoch 46, test loss: 0.174541\n",
      "Epoch 47, test loss: 0.175055\n",
      "Epoch 48, test loss: 0.175964\n",
      "Epoch 49, test loss: 0.174875\n",
      "Epoch 50, test loss: 0.175819\n",
      "Epoch 51, test loss: 0.176071\n",
      "Epoch 52, test loss: 0.175629\n",
      "Epoch 53, test loss: 0.175944\n",
      "Epoch 54, test loss: 0.174727\n",
      "Epoch 55, test loss: 0.174720\n",
      "Epoch 56, test loss: 0.176241\n",
      "Epoch 57, test loss: 0.178923\n",
      "Epoch 58, test loss: 0.175124\n",
      "Epoch 59, test loss: 0.174670\n",
      "Epoch 60, test loss: 0.176424\n",
      "Epoch 61, test loss: 0.175419\n",
      "Epoch 62, test loss: 0.174280\n",
      "Epoch 63, test loss: 0.174704\n",
      "Epoch 64, test loss: 0.176318\n",
      "Epoch 65, test loss: 0.175154\n",
      "Epoch 66, test loss: 0.174897\n",
      "Epoch 67, test loss: 0.174287\n",
      "Epoch 68, test loss: 0.175759\n",
      "Epoch 69, test loss: 0.174812\n",
      "Epoch 70, test loss: 0.176134\n",
      "Epoch 71, test loss: 0.176861\n",
      "Epoch 72, test loss: 0.174277\n",
      "Epoch 73, test loss: 0.174405\n",
      "Epoch 74, test loss: 0.176205\n",
      "Epoch 75, test loss: 0.175802\n",
      "Epoch 76, test loss: 0.174651\n",
      "Epoch 77, test loss: 0.177744\n",
      "Epoch 78, test loss: 0.174888\n",
      "Epoch 79, test loss: 0.174499\n",
      "Epoch 80, test loss: 0.177097\n",
      "Epoch 81, test loss: 0.174391\n",
      "Epoch 82, test loss: 0.174206\n",
      "Epoch 83, test loss: 0.174475\n",
      "Epoch 84, test loss: 0.174688\n",
      "Epoch 85, test loss: 0.174337\n",
      "Epoch 86, test loss: 0.179515\n",
      "Epoch 87, test loss: 0.174553\n",
      "Epoch 88, test loss: 0.174888\n",
      "Epoch 89, test loss: 0.175884\n",
      "Epoch 90, test loss: 0.176414\n",
      "Epoch 91, test loss: 0.174592\n",
      "Epoch 92, test loss: 0.187419\n",
      "Epoch 93, test loss: 0.175623\n",
      "Epoch 94, test loss: 0.174161\n",
      "Epoch 95, test loss: 0.173826\n",
      "Epoch 96, test loss: 0.174179\n",
      "Epoch 97, test loss: 0.174927\n",
      "Epoch 98, test loss: 0.176321\n",
      "Epoch 99, test loss: 0.181590\n",
      "Epoch 100, test loss: 0.174135\n",
      "Epoch 101, test loss: 0.174082\n",
      "Epoch 102, test loss: 0.174337\n",
      "Epoch 103, test loss: 0.174090\n",
      "Epoch 104, test loss: 0.174018\n",
      "Epoch 105, test loss: 0.176795\n",
      "Epoch 106, test loss: 0.175906\n",
      "Epoch 107, test loss: 0.175916\n",
      "Epoch 108, test loss: 0.174077\n",
      "Epoch 109, test loss: 0.174278\n",
      "Epoch 110, test loss: 0.174101\n",
      "Epoch 111, test loss: 0.176031\n",
      "Epoch 112, test loss: 0.174729\n",
      "Epoch 113, test loss: 0.174140\n",
      "Epoch 114, test loss: 0.175159\n",
      "Epoch 115, test loss: 0.175597\n",
      "Epoch 116, test loss: 0.173999\n",
      "Epoch 117, test loss: 0.175152\n",
      "Epoch 118, test loss: 0.175165\n",
      "Epoch 119, test loss: 0.176845\n",
      "Epoch 120, test loss: 0.173819\n",
      "Epoch 121, test loss: 0.174230\n",
      "Epoch 122, test loss: 0.173780\n",
      "Epoch 123, test loss: 0.174321\n",
      "Epoch 124, test loss: 0.179423\n",
      "Epoch 125, test loss: 0.173725\n",
      "Epoch 126, test loss: 0.173543\n",
      "Epoch 127, test loss: 0.175626\n",
      "Epoch 128, test loss: 0.174858\n",
      "Epoch 129, test loss: 0.174261\n",
      "Epoch 130, test loss: 0.173868\n",
      "Epoch 131, test loss: 0.174307\n",
      "Epoch 132, test loss: 0.174785\n",
      "Epoch 133, test loss: 0.176036\n",
      "Epoch 134, test loss: 0.175132\n",
      "Epoch 135, test loss: 0.174596\n",
      "Epoch 136, test loss: 0.175609\n",
      "Epoch 137, test loss: 0.178372\n",
      "Epoch 138, test loss: 0.173899\n",
      "Epoch 139, test loss: 0.173482\n",
      "Epoch 140, test loss: 0.176044\n",
      "Epoch 141, test loss: 0.175450\n",
      "Epoch 142, test loss: 0.174205\n",
      "Epoch 143, test loss: 0.175659\n",
      "Epoch 144, test loss: 0.174988\n",
      "Epoch 145, test loss: 0.174593\n",
      "Epoch 146, test loss: 0.173633\n",
      "Epoch 147, test loss: 0.174547\n",
      "Epoch 148, test loss: 0.175209\n",
      "Epoch 149, test loss: 0.175572\n",
      "Epoch 150, test loss: 0.174286\n",
      "Epoch 151, test loss: 0.173957\n",
      "Epoch 152, test loss: 0.173801\n",
      "Epoch 153, test loss: 0.174842\n",
      "Epoch 154, test loss: 0.175070\n",
      "Epoch 155, test loss: 0.173528\n",
      "Epoch 156, test loss: 0.173393\n",
      "Epoch 157, test loss: 0.173110\n",
      "Epoch 158, test loss: 0.173378\n",
      "Epoch 159, test loss: 0.174986\n",
      "Epoch 160, test loss: 0.174484\n",
      "Epoch 161, test loss: 0.173712\n",
      "Epoch 162, test loss: 0.173613\n",
      "Epoch 163, test loss: 0.174261\n",
      "Epoch 164, test loss: 0.173694\n",
      "Epoch 165, test loss: 0.173504\n",
      "Epoch 166, test loss: 0.174672\n",
      "Epoch 167, test loss: 0.174309\n",
      "Epoch 168, test loss: 0.173771\n",
      "Epoch 169, test loss: 0.174199\n",
      "Epoch 170, test loss: 0.174962\n",
      "Epoch 171, test loss: 0.173269\n",
      "Epoch 172, test loss: 0.173732\n",
      "Epoch 173, test loss: 0.175116\n",
      "Epoch 174, test loss: 0.173928\n",
      "Epoch 175, test loss: 0.174156\n",
      "Epoch 176, test loss: 0.173483\n",
      "Epoch 177, test loss: 0.173431\n",
      "Epoch 178, test loss: 0.173297\n",
      "Epoch 179, test loss: 0.174247\n",
      "Epoch 180, test loss: 0.174453\n",
      "Epoch 181, test loss: 0.173241\n",
      "Epoch 182, test loss: 0.173478\n",
      "Epoch 183, test loss: 0.174813\n",
      "Epoch 184, test loss: 0.173224\n",
      "Epoch 185, test loss: 0.173520\n",
      "Epoch 186, test loss: 0.174182\n",
      "Epoch 187, test loss: 0.173283\n",
      "Epoch 188, test loss: 0.173189\n",
      "Epoch 189, test loss: 0.176142\n",
      "Epoch 190, test loss: 0.173312\n",
      "Epoch 191, test loss: 0.173782\n",
      "Epoch 192, test loss: 0.174102\n",
      "Epoch 193, test loss: 0.178189\n",
      "Epoch 194, test loss: 0.174233\n",
      "Epoch 195, test loss: 0.173618\n",
      "Epoch 196, test loss: 0.173584\n",
      "Epoch 197, test loss: 0.173426\n",
      "Epoch 198, test loss: 0.173416\n",
      "Epoch 199, test loss: 0.172893\n",
      "Epoch 200, test loss: 0.173386\n",
      "Epoch 201, test loss: 0.173360\n",
      "Epoch 202, test loss: 0.173381\n",
      "Epoch 203, test loss: 0.174968\n",
      "Epoch 204, test loss: 0.173104\n",
      "Epoch 205, test loss: 0.172819\n",
      "Epoch 206, test loss: 0.174168\n",
      "Epoch 207, test loss: 0.173118\n",
      "Epoch 208, test loss: 0.175174\n",
      "Epoch 209, test loss: 0.173291\n",
      "Epoch 210, test loss: 0.173015\n",
      "Epoch 211, test loss: 0.173128\n",
      "Epoch 212, test loss: 0.174563\n",
      "Epoch 213, test loss: 0.173234\n",
      "Epoch 214, test loss: 0.173216\n",
      "Epoch 215, test loss: 0.174030\n",
      "Epoch 216, test loss: 0.173323\n",
      "Epoch 217, test loss: 0.175795\n",
      "Epoch 218, test loss: 0.174098\n",
      "Epoch 219, test loss: 0.173131\n",
      "Epoch 220, test loss: 0.172867\n",
      "Epoch 221, test loss: 0.172724\n",
      "Epoch 222, test loss: 0.172841\n",
      "Epoch 223, test loss: 0.175693\n",
      "Epoch 224, test loss: 0.172699\n",
      "Epoch 225, test loss: 0.173948\n",
      "Epoch 226, test loss: 0.175896\n",
      "Epoch 227, test loss: 0.175749\n",
      "Epoch 228, test loss: 0.172830\n",
      "Epoch 229, test loss: 0.172779\n",
      "Epoch 230, test loss: 0.174933\n",
      "Epoch 231, test loss: 0.173717\n",
      "Epoch 232, test loss: 0.173104\n",
      "Epoch 233, test loss: 0.173188\n",
      "Epoch 234, test loss: 0.173127\n",
      "Epoch 235, test loss: 0.173470\n",
      "Epoch 236, test loss: 0.173459\n",
      "Epoch 237, test loss: 0.174523\n",
      "Epoch 238, test loss: 0.173221\n",
      "Epoch 239, test loss: 0.172959\n",
      "Pretrain data: 19754507.0\n",
      "Building dataset, requesting data from 0 to 655\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7165/105865\n",
      "Found 655 continuous time series\n",
      "Data shape: (113032, 12), Train/test: 113030/2\n",
      "Train test ratio: 56515.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1766\n",
      "Epoch 0, train loss: 0.210145\n",
      "Epoch 1, train loss: 0.232827\n",
      "Epoch 2, train loss: 0.232688\n",
      "Epoch 3, train loss: 0.240059\n",
      "Epoch 4, train loss: 0.203391\n",
      "Epoch 5, train loss: 0.227219\n",
      "Epoch 6, train loss: 0.269299\n",
      "Epoch 7, train loss: 0.205481\n",
      "Epoch 8, train loss: 0.223001\n",
      "Epoch 9, train loss: 0.248953\n",
      "Epoch 10, train loss: 0.162716\n",
      "Epoch 11, train loss: 0.206119\n",
      "Epoch 12, train loss: 0.230483\n",
      "Epoch 13, train loss: 0.228063\n",
      "Epoch 14, train loss: 0.241329\n",
      "Epoch 15, train loss: 0.175316\n",
      "Epoch 16, train loss: 0.162006\n",
      "Epoch 17, train loss: 0.209231\n",
      "Epoch 18, train loss: 0.180751\n",
      "Epoch 19, train loss: 0.242436\n",
      "Epoch 20, train loss: 0.219863\n",
      "Epoch 21, train loss: 0.254935\n",
      "Epoch 22, train loss: 0.198439\n",
      "Epoch 23, train loss: 0.197592\n",
      "Epoch 24, train loss: 0.201552\n",
      "Epoch 25, train loss: 0.139195\n",
      "Epoch 26, train loss: 0.157753\n",
      "Epoch 27, train loss: 0.246173\n",
      "Epoch 28, train loss: 0.198723\n",
      "Epoch 29, train loss: 0.206109\n",
      "Epoch 30, train loss: 0.250749\n",
      "Epoch 31, train loss: 0.205574\n",
      "Epoch 32, train loss: 0.168428\n",
      "Epoch 33, train loss: 0.208141\n",
      "Epoch 34, train loss: 0.155156\n",
      "Epoch 35, train loss: 0.244279\n",
      "Epoch 36, train loss: 0.220531\n",
      "Epoch 37, train loss: 0.209638\n",
      "Epoch 38, train loss: 0.227762\n",
      "Epoch 39, train loss: 0.218099\n",
      "Epoch 40, train loss: 0.165883\n",
      "Epoch 41, train loss: 0.185288\n",
      "Epoch 42, train loss: 0.228784\n",
      "Epoch 43, train loss: 0.236605\n",
      "Epoch 44, train loss: 0.235881\n",
      "Epoch 45, train loss: 0.206042\n",
      "Epoch 46, train loss: 0.186481\n",
      "Epoch 47, train loss: 0.191858\n",
      "Epoch 48, train loss: 0.199530\n",
      "Epoch 49, train loss: 0.237769\n",
      "Epoch 50, train loss: 0.204902\n",
      "Epoch 51, train loss: 0.223466\n",
      "Epoch 52, train loss: 0.176868\n",
      "Epoch 53, train loss: 0.159841\n",
      "Epoch 54, train loss: 0.158891\n",
      "Epoch 55, train loss: 0.252971\n",
      "Epoch 56, train loss: 0.187980\n",
      "Epoch 57, train loss: 0.192699\n",
      "Epoch 58, train loss: 0.200630\n",
      "Epoch 59, train loss: 0.171454\n",
      "Epoch 60, train loss: 0.177938\n",
      "Epoch 61, train loss: 0.259353\n",
      "Epoch 62, train loss: 0.191480\n",
      "Epoch 63, train loss: 0.207664\n",
      "Epoch 64, train loss: 0.177761\n",
      "Epoch 65, train loss: 0.201841\n",
      "Epoch 66, train loss: 0.279806\n",
      "Epoch 67, train loss: 0.230005\n",
      "Epoch 68, train loss: 0.186621\n",
      "Epoch 69, train loss: 0.200944\n",
      "Epoch 70, train loss: 0.194130\n",
      "Epoch 71, train loss: 0.215730\n",
      "Epoch 72, train loss: 0.233244\n",
      "Epoch 73, train loss: 0.200766\n",
      "Epoch 74, train loss: 0.219322\n",
      "Epoch 75, train loss: 0.204467\n",
      "Epoch 76, train loss: 0.228436\n",
      "Epoch 77, train loss: 0.225281\n",
      "Epoch 78, train loss: 0.183983\n",
      "Epoch 79, train loss: 0.215737\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[296. 290. 284. ... 244. 239. 234.]\n",
      " [290. 284. 279. ... 239. 234. 229.]\n",
      " [284. 279. 272. ... 234. 229. 222.]\n",
      " ...\n",
      " [224. 219. 211. ... 172. 171. 178.]\n",
      " [219. 211. 204. ... 171. 178. 180.]\n",
      " [211. 204. 197. ... 178. 180. 180.]]\n",
      "y here is\n",
      "[[198. 198. 198. ... 198. 198. 198.]\n",
      " [192. 192. 192. ... 192. 192. 192.]\n",
      " [186. 186. 186. ... 186. 186. 186.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1683, 12), Train/test: 1/1682\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 187 segments\n",
      "Building dataset, requesting data from 0 to 187\n",
      "x here is\n",
      "[[ 71.  71.  71. ...  71.  71.  71.]\n",
      " [ 71.  71.  71. ...  71.  71.  71.]\n",
      " [ 71.  71.  72. ...  71.  71.  71.]\n",
      " ...\n",
      " [266. 268. 272. ... 277. 277. 277.]\n",
      " [268. 272. 280. ... 277. 277. 279.]\n",
      " [272. 280. 284. ... 277. 279. 282.]]\n",
      "y here is\n",
      "[[ 76.  76.  76. ...  76.  76.  76.]\n",
      " [ 77.  77.  77. ...  77.  77.  77.]\n",
      " [ 79.  79.  79. ...  79.  79.  79.]\n",
      " ...\n",
      " [315. 315. 315. ... 315. 315. 315.]\n",
      " [310. 310. 310. ... 310. 310. 310.]\n",
      " [303. 303. 303. ... 303. 303. 303.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 743/7160\n",
      "Found 187 continuous time series\n",
      "Data shape: (7905, 12), Train/test: 7903/2\n",
      "Train test ratio: 3951.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2967CF850>\n",
      "Epoch 0, test loss: 0.209380\n",
      "Epoch 1, test loss: 0.208572\n",
      "Epoch 2, test loss: 0.209544\n",
      "Epoch 3, test loss: 0.208673\n",
      "Epoch 4, test loss: 0.209455\n",
      "Epoch 5, test loss: 0.209746\n",
      "Epoch 6, test loss: 0.209179\n",
      "Epoch 7, test loss: 0.210217\n",
      "Epoch 8, test loss: 0.211031\n",
      "Epoch 9, test loss: 0.209259\n",
      "Epoch 10, test loss: 0.213009\n",
      "Epoch 11, test loss: 0.209425\n",
      "Epoch 12, test loss: 0.209056\n",
      "Epoch 13, test loss: 0.208929\n",
      "Epoch 14, test loss: 0.209179\n",
      "Epoch 15, test loss: 0.209115\n",
      "Epoch 16, test loss: 0.213524\n",
      "Epoch 17, test loss: 0.214051\n",
      "Epoch 18, test loss: 0.209830\n",
      "Epoch 19, test loss: 0.213217\n",
      "Epoch 20, test loss: 0.210026\n",
      "Epoch 21, test loss: 0.217756\n",
      "Epoch 22, test loss: 0.209352\n",
      "Epoch 23, test loss: 0.213598\n",
      "Epoch 24, test loss: 0.209848\n",
      "Epoch 25, test loss: 0.213856\n",
      "Epoch 26, test loss: 0.210394\n",
      "Epoch 27, test loss: 0.218443\n",
      "Epoch 28, test loss: 0.212000\n",
      "Epoch 29, test loss: 0.213352\n",
      "Epoch 30, test loss: 0.209366\n",
      "Epoch 31, test loss: 0.209776\n",
      "Epoch 32, test loss: 0.210878\n",
      "Epoch 33, test loss: 0.218229\n",
      "Epoch 34, test loss: 0.209384\n",
      "Epoch 35, test loss: 0.212788\n",
      "Epoch 36, test loss: 0.209405\n",
      "Epoch 37, test loss: 0.216436\n",
      "Epoch 38, test loss: 0.211746\n",
      "Epoch 39, test loss: 0.209588\n",
      "Epoch 40, test loss: 0.210732\n",
      "Epoch 41, test loss: 0.217481\n",
      "Epoch 42, test loss: 0.210444\n",
      "Epoch 43, test loss: 0.210246\n",
      "Epoch 44, test loss: 0.210599\n",
      "Epoch 45, test loss: 0.209815\n",
      "Epoch 46, test loss: 0.214474\n",
      "Epoch 47, test loss: 0.210099\n",
      "Epoch 48, test loss: 0.210297\n",
      "Epoch 49, test loss: 0.209638\n",
      "Epoch 50, test loss: 0.209533\n",
      "Epoch 51, test loss: 0.208770\n",
      "Epoch 52, test loss: 0.211382\n",
      "Epoch 53, test loss: 0.210371\n",
      "Epoch 54, test loss: 0.210132\n",
      "Epoch 55, test loss: 0.211331\n",
      "Epoch 56, test loss: 0.210837\n",
      "Epoch 57, test loss: 0.209548\n",
      "Epoch 58, test loss: 0.215603\n",
      "Epoch 59, test loss: 0.217108\n",
      "Epoch 60, test loss: 0.212955\n",
      "Epoch 61, test loss: 0.209149\n",
      "Epoch 62, test loss: 0.209377\n",
      "Epoch 63, test loss: 0.209510\n",
      "Epoch 64, test loss: 0.214325\n",
      "Epoch 65, test loss: 0.227742\n",
      "Epoch 66, test loss: 0.215654\n",
      "Epoch 67, test loss: 0.209817\n",
      "Epoch 68, test loss: 0.208808\n",
      "Epoch 69, test loss: 0.208853\n",
      "Epoch 70, test loss: 0.210163\n",
      "Epoch 71, test loss: 0.216263\n",
      "Epoch 72, test loss: 0.208673\n",
      "Epoch 73, test loss: 0.209487\n",
      "Epoch 74, test loss: 0.210204\n",
      "Epoch 75, test loss: 0.210804\n",
      "Epoch 76, test loss: 0.212572\n",
      "Epoch 77, test loss: 0.209632\n",
      "Epoch 78, test loss: 0.209429\n",
      "Epoch 79, test loss: 0.209486\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2967CF850>\n",
      "Epoch 0, test loss: 0.210107\n",
      "Epoch 1, test loss: 0.210886\n",
      "Epoch 2, test loss: 0.209404\n",
      "Epoch 3, test loss: 0.209759\n",
      "Epoch 4, test loss: 0.209468\n",
      "Epoch 5, test loss: 0.212017\n",
      "Epoch 6, test loss: 0.211450\n",
      "Epoch 7, test loss: 0.210792\n",
      "Epoch 8, test loss: 0.210236\n",
      "Epoch 9, test loss: 0.211201\n",
      "Epoch 10, test loss: 0.211177\n",
      "Epoch 11, test loss: 0.210826\n",
      "Epoch 12, test loss: 0.211314\n",
      "Epoch 13, test loss: 0.209893\n",
      "Epoch 14, test loss: 0.212439\n",
      "Epoch 15, test loss: 0.214141\n",
      "Epoch 16, test loss: 0.210150\n",
      "Epoch 17, test loss: 0.210249\n",
      "Epoch 18, test loss: 0.210871\n",
      "Epoch 19, test loss: 0.211270\n",
      "Epoch 20, test loss: 0.211634\n",
      "Epoch 21, test loss: 0.211127\n",
      "Epoch 22, test loss: 0.210699\n",
      "Epoch 23, test loss: 0.211562\n",
      "Epoch 24, test loss: 0.213749\n",
      "Epoch 25, test loss: 0.211132\n",
      "Epoch 26, test loss: 0.210448\n",
      "Epoch 27, test loss: 0.210942\n",
      "Epoch 28, test loss: 0.211041\n",
      "Epoch 29, test loss: 0.211017\n",
      "Epoch 30, test loss: 0.212890\n",
      "Epoch 31, test loss: 0.211382\n",
      "Epoch 32, test loss: 0.211217\n",
      "Epoch 33, test loss: 0.214203\n",
      "Epoch 34, test loss: 0.212421\n",
      "Epoch 35, test loss: 0.215033\n",
      "Epoch 36, test loss: 0.211107\n",
      "Epoch 37, test loss: 0.210028\n",
      "Epoch 38, test loss: 0.211133\n",
      "Epoch 39, test loss: 0.210293\n",
      "Epoch 40, test loss: 0.210671\n",
      "Epoch 41, test loss: 0.212945\n",
      "Epoch 42, test loss: 0.213075\n",
      "Epoch 43, test loss: 0.210752\n",
      "Epoch 44, test loss: 0.213004\n",
      "Epoch 45, test loss: 0.216493\n",
      "Epoch 46, test loss: 0.211494\n",
      "Epoch 47, test loss: 0.211283\n",
      "Epoch 48, test loss: 0.210608\n",
      "Epoch 49, test loss: 0.210595\n",
      "Epoch 50, test loss: 0.211055\n",
      "Epoch 51, test loss: 0.214953\n",
      "Epoch 52, test loss: 0.211834\n",
      "Epoch 53, test loss: 0.215592\n",
      "Epoch 54, test loss: 0.212470\n",
      "Epoch 55, test loss: 0.210958\n",
      "Epoch 56, test loss: 0.210893\n",
      "Epoch 57, test loss: 0.211221\n",
      "Epoch 58, test loss: 0.212801\n",
      "Epoch 59, test loss: 0.210741\n",
      "Epoch 60, test loss: 0.210346\n",
      "Epoch 61, test loss: 0.210887\n",
      "Epoch 62, test loss: 0.210857\n",
      "Epoch 63, test loss: 0.212506\n",
      "Epoch 64, test loss: 0.215075\n",
      "Epoch 65, test loss: 0.211212\n",
      "Epoch 66, test loss: 0.210956\n",
      "Epoch 67, test loss: 0.210856\n",
      "Epoch 68, test loss: 0.209994\n",
      "Epoch 69, test loss: 0.210703\n",
      "Epoch 70, test loss: 0.210450\n",
      "Epoch 71, test loss: 0.212900\n",
      "Epoch 72, test loss: 0.211303\n",
      "Epoch 73, test loss: 0.211562\n",
      "Epoch 74, test loss: 0.211868\n",
      "Epoch 75, test loss: 0.212014\n",
      "Epoch 76, test loss: 0.212317\n",
      "Epoch 77, test loss: 0.213720\n",
      "Epoch 78, test loss: 0.210335\n",
      "Epoch 79, test loss: 0.210172\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2967CF850>\n",
      "Epoch 0, test loss: 0.897011\n",
      "Epoch 1, test loss: 0.277418\n",
      "Epoch 2, test loss: 0.243306\n",
      "Epoch 3, test loss: 0.229016\n",
      "Epoch 4, test loss: 0.222883\n",
      "Epoch 5, test loss: 0.220095\n",
      "Epoch 6, test loss: 0.219971\n",
      "Epoch 7, test loss: 0.218260\n",
      "Epoch 8, test loss: 0.216537\n",
      "Epoch 9, test loss: 0.218202\n",
      "Epoch 10, test loss: 0.215467\n",
      "Epoch 11, test loss: 0.216151\n",
      "Epoch 12, test loss: 0.213480\n",
      "Epoch 13, test loss: 0.215650\n",
      "Epoch 14, test loss: 0.215495\n",
      "Epoch 15, test loss: 0.212526\n",
      "Epoch 16, test loss: 0.214504\n",
      "Epoch 17, test loss: 0.212507\n",
      "Epoch 18, test loss: 0.213293\n",
      "Epoch 19, test loss: 0.213122\n",
      "Epoch 20, test loss: 0.211911\n",
      "Epoch 21, test loss: 0.211930\n",
      "Epoch 22, test loss: 0.212563\n",
      "Epoch 23, test loss: 0.212267\n",
      "Epoch 24, test loss: 0.211293\n",
      "Epoch 25, test loss: 0.212044\n",
      "Epoch 26, test loss: 0.210369\n",
      "Epoch 27, test loss: 0.221258\n",
      "Epoch 28, test loss: 0.210871\n",
      "Epoch 29, test loss: 0.210300\n",
      "Epoch 30, test loss: 0.211639\n",
      "Epoch 31, test loss: 0.215497\n",
      "Epoch 32, test loss: 0.211012\n",
      "Epoch 33, test loss: 0.210564\n",
      "Epoch 34, test loss: 0.209705\n",
      "Epoch 35, test loss: 0.210740\n",
      "Epoch 36, test loss: 0.211528\n",
      "Epoch 37, test loss: 0.211688\n",
      "Epoch 38, test loss: 0.210876\n",
      "Epoch 39, test loss: 0.211723\n",
      "Epoch 40, test loss: 0.209391\n",
      "Epoch 41, test loss: 0.210731\n",
      "Epoch 42, test loss: 0.209925\n",
      "Epoch 43, test loss: 0.209081\n",
      "Epoch 44, test loss: 0.208619\n",
      "Epoch 45, test loss: 0.211771\n",
      "Epoch 46, test loss: 0.212450\n",
      "Epoch 47, test loss: 0.212778\n",
      "Epoch 48, test loss: 0.213762\n",
      "Epoch 49, test loss: 0.208844\n",
      "Epoch 50, test loss: 0.215674\n",
      "Epoch 51, test loss: 0.209636\n",
      "Epoch 52, test loss: 0.210957\n",
      "Epoch 53, test loss: 0.210308\n",
      "Epoch 54, test loss: 0.209258\n",
      "Epoch 55, test loss: 0.209488\n",
      "Epoch 56, test loss: 0.210303\n",
      "Epoch 57, test loss: 0.208778\n",
      "Epoch 58, test loss: 0.209597\n",
      "Epoch 59, test loss: 0.209418\n",
      "Epoch 60, test loss: 0.209626\n",
      "Epoch 61, test loss: 0.212116\n",
      "Epoch 62, test loss: 0.209467\n",
      "Epoch 63, test loss: 0.211739\n",
      "Epoch 64, test loss: 0.208709\n",
      "Epoch 65, test loss: 0.210394\n",
      "Epoch 66, test loss: 0.209934\n",
      "Epoch 67, test loss: 0.208375\n",
      "Epoch 68, test loss: 0.210525\n",
      "Epoch 69, test loss: 0.208494\n",
      "Epoch 70, test loss: 0.209107\n",
      "Epoch 71, test loss: 0.208257\n",
      "Epoch 72, test loss: 0.208564\n",
      "Epoch 73, test loss: 0.208223\n",
      "Epoch 74, test loss: 0.212761\n",
      "Epoch 75, test loss: 0.209787\n",
      "Epoch 76, test loss: 0.208047\n",
      "Epoch 77, test loss: 0.208865\n",
      "Epoch 78, test loss: 0.214435\n",
      "Epoch 79, test loss: 0.209051\n",
      "Epoch 80, test loss: 0.211763\n",
      "Epoch 81, test loss: 0.209144\n",
      "Epoch 82, test loss: 0.207989\n",
      "Epoch 83, test loss: 0.208171\n",
      "Epoch 84, test loss: 0.208420\n",
      "Epoch 85, test loss: 0.208130\n",
      "Epoch 86, test loss: 0.212773\n",
      "Epoch 87, test loss: 0.207709\n",
      "Epoch 88, test loss: 0.211020\n",
      "Epoch 89, test loss: 0.210364\n",
      "Epoch 90, test loss: 0.208684\n",
      "Epoch 91, test loss: 0.208065\n",
      "Epoch 92, test loss: 0.207592\n",
      "Epoch 93, test loss: 0.207352\n",
      "Epoch 94, test loss: 0.212811\n",
      "Epoch 95, test loss: 0.207765\n",
      "Epoch 96, test loss: 0.209164\n",
      "Epoch 97, test loss: 0.209057\n",
      "Epoch 98, test loss: 0.208146\n",
      "Epoch 99, test loss: 0.209699\n",
      "Epoch 100, test loss: 0.209718\n",
      "Epoch 101, test loss: 0.208339\n",
      "Epoch 102, test loss: 0.208164\n",
      "Epoch 103, test loss: 0.208742\n",
      "Epoch 104, test loss: 0.208843\n",
      "Epoch 105, test loss: 0.211124\n",
      "Epoch 106, test loss: 0.207967\n",
      "Epoch 107, test loss: 0.208695\n",
      "Epoch 108, test loss: 0.208883\n",
      "Epoch 109, test loss: 0.208839\n",
      "Epoch 110, test loss: 0.208220\n",
      "Epoch 111, test loss: 0.213591\n",
      "Epoch 112, test loss: 0.208442\n",
      "Epoch 113, test loss: 0.207782\n",
      "Epoch 114, test loss: 0.207074\n",
      "Epoch 115, test loss: 0.208300\n",
      "Epoch 116, test loss: 0.207688\n",
      "Epoch 117, test loss: 0.210527\n",
      "Epoch 118, test loss: 0.209168\n",
      "Epoch 119, test loss: 0.208831\n",
      "Epoch 120, test loss: 0.213467\n",
      "Epoch 121, test loss: 0.207727\n",
      "Epoch 122, test loss: 0.209378\n",
      "Epoch 123, test loss: 0.207501\n",
      "Epoch 124, test loss: 0.213624\n",
      "Epoch 125, test loss: 0.211321\n",
      "Epoch 126, test loss: 0.207740\n",
      "Epoch 127, test loss: 0.209289\n",
      "Epoch 128, test loss: 0.207594\n",
      "Epoch 129, test loss: 0.207778\n",
      "Epoch 130, test loss: 0.210129\n",
      "Epoch 131, test loss: 0.208632\n",
      "Epoch 132, test loss: 0.209309\n",
      "Epoch 133, test loss: 0.207425\n",
      "Epoch 134, test loss: 0.210332\n",
      "Epoch 135, test loss: 0.207264\n",
      "Epoch 136, test loss: 0.207782\n",
      "Epoch 137, test loss: 0.209652\n",
      "Epoch 138, test loss: 0.207244\n",
      "Epoch 139, test loss: 0.209656\n",
      "Epoch 140, test loss: 0.207308\n",
      "Epoch 141, test loss: 0.212983\n",
      "Epoch 142, test loss: 0.207622\n",
      "Epoch 143, test loss: 0.207912\n",
      "Epoch 144, test loss: 0.207714\n",
      "Epoch 145, test loss: 0.208620\n",
      "Epoch 146, test loss: 0.207577\n",
      "Epoch 147, test loss: 0.208377\n",
      "Epoch 148, test loss: 0.207480\n",
      "Epoch 149, test loss: 0.207565\n",
      "Epoch 150, test loss: 0.208587\n",
      "Epoch 151, test loss: 0.210860\n",
      "Epoch 152, test loss: 0.208742\n",
      "Epoch 153, test loss: 0.207268\n",
      "Epoch 154, test loss: 0.208312\n",
      "Epoch 155, test loss: 0.207604\n",
      "Epoch 156, test loss: 0.207972\n",
      "Epoch 157, test loss: 0.207366\n",
      "Epoch 158, test loss: 0.209442\n",
      "Epoch 159, test loss: 0.207760\n",
      "Epoch 160, test loss: 0.211719\n",
      "Epoch 161, test loss: 0.210324\n",
      "Epoch 162, test loss: 0.215496\n",
      "Epoch 163, test loss: 0.208777\n",
      "Epoch 164, test loss: 0.209078\n",
      "Epoch 165, test loss: 0.215285\n",
      "Epoch 166, test loss: 0.207881\n",
      "Epoch 167, test loss: 0.207879\n",
      "Epoch 168, test loss: 0.209518\n",
      "Epoch 169, test loss: 0.207468\n",
      "Epoch 170, test loss: 0.208103\n",
      "Epoch 171, test loss: 0.207493\n",
      "Epoch 172, test loss: 0.207996\n",
      "Epoch 173, test loss: 0.207910\n",
      "Epoch 174, test loss: 0.207622\n",
      "Epoch 175, test loss: 0.207384\n",
      "Epoch 176, test loss: 0.207898\n",
      "Epoch 177, test loss: 0.208825\n",
      "Epoch 178, test loss: 0.208592\n",
      "Epoch 179, test loss: 0.208134\n",
      "Epoch 180, test loss: 0.208433\n",
      "Epoch 181, test loss: 0.207787\n",
      "Epoch 182, test loss: 0.209100\n",
      "Epoch 183, test loss: 0.209412\n",
      "Epoch 184, test loss: 0.210660\n",
      "Epoch 185, test loss: 0.208740\n",
      "Epoch 186, test loss: 0.209363\n",
      "Epoch 187, test loss: 0.207650\n",
      "Epoch 188, test loss: 0.207589\n",
      "Epoch 189, test loss: 0.208262\n",
      "Epoch 190, test loss: 0.207607\n",
      "Epoch 191, test loss: 0.207625\n",
      "Epoch 192, test loss: 0.208242\n",
      "Epoch 193, test loss: 0.215468\n",
      "Epoch 194, test loss: 0.208789\n",
      "Epoch 195, test loss: 0.207991\n",
      "Epoch 196, test loss: 0.208884\n",
      "Epoch 197, test loss: 0.208377\n",
      "Epoch 198, test loss: 0.208709\n",
      "Epoch 199, test loss: 0.209364\n",
      "Epoch 200, test loss: 0.207580\n",
      "Epoch 201, test loss: 0.217852\n",
      "Epoch 202, test loss: 0.207215\n",
      "Epoch 203, test loss: 0.208060\n",
      "Epoch 204, test loss: 0.209039\n",
      "Epoch 205, test loss: 0.208078\n",
      "Epoch 206, test loss: 0.208779\n",
      "Epoch 207, test loss: 0.209029\n",
      "Epoch 208, test loss: 0.210587\n",
      "Epoch 209, test loss: 0.210625\n",
      "Epoch 210, test loss: 0.209677\n",
      "Epoch 211, test loss: 0.207533\n",
      "Epoch 212, test loss: 0.209553\n",
      "Epoch 213, test loss: 0.207086\n",
      "Epoch 214, test loss: 0.210131\n",
      "Epoch 215, test loss: 0.207366\n",
      "Epoch 216, test loss: 0.207878\n",
      "Epoch 217, test loss: 0.208482\n",
      "Epoch 218, test loss: 0.207404\n",
      "Epoch 219, test loss: 0.209904\n",
      "Epoch 220, test loss: 0.207446\n",
      "Epoch 221, test loss: 0.207988\n",
      "Epoch 222, test loss: 0.207908\n",
      "Epoch 223, test loss: 0.208115\n",
      "Epoch 224, test loss: 0.207875\n",
      "Epoch 225, test loss: 0.214534\n",
      "Epoch 226, test loss: 0.208040\n",
      "Epoch 227, test loss: 0.212580\n",
      "Epoch 228, test loss: 0.210219\n",
      "Epoch 229, test loss: 0.207346\n",
      "Epoch 230, test loss: 0.208613\n",
      "Epoch 231, test loss: 0.208064\n",
      "Epoch 232, test loss: 0.207439\n",
      "Epoch 233, test loss: 0.207607\n",
      "Epoch 234, test loss: 0.206989\n",
      "Epoch 235, test loss: 0.210454\n",
      "Epoch 236, test loss: 0.208464\n",
      "Epoch 237, test loss: 0.207480\n",
      "Epoch 238, test loss: 0.207304\n",
      "Epoch 239, test loss: 0.210554\n",
      "Pretrain data: 19086006.0\n",
      "Building dataset, requesting data from 0 to 782\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7749/102036\n",
      "Found 782 continuous time series\n",
      "Data shape: (109787, 12), Train/test: 109785/2\n",
      "Train test ratio: 54892.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1715\n",
      "Epoch 0, train loss: 0.196940\n",
      "Epoch 1, train loss: 0.314143\n",
      "Epoch 2, train loss: 0.219914\n",
      "Epoch 3, train loss: 0.170305\n",
      "Epoch 4, train loss: 0.195147\n",
      "Epoch 5, train loss: 0.163899\n",
      "Epoch 6, train loss: 0.184620\n",
      "Epoch 7, train loss: 0.195377\n",
      "Epoch 8, train loss: 0.204443\n",
      "Epoch 9, train loss: 0.222264\n",
      "Epoch 10, train loss: 0.286657\n",
      "Epoch 11, train loss: 0.261060\n",
      "Epoch 12, train loss: 0.166715\n",
      "Epoch 13, train loss: 0.177056\n",
      "Epoch 14, train loss: 0.190312\n",
      "Epoch 15, train loss: 0.195356\n",
      "Epoch 16, train loss: 0.250337\n",
      "Epoch 17, train loss: 0.207016\n",
      "Epoch 18, train loss: 0.181990\n",
      "Epoch 19, train loss: 0.229325\n",
      "Epoch 20, train loss: 0.178140\n",
      "Epoch 21, train loss: 0.177747\n",
      "Epoch 22, train loss: 0.262173\n",
      "Epoch 23, train loss: 0.157414\n",
      "Epoch 24, train loss: 0.204802\n",
      "Epoch 25, train loss: 0.167259\n",
      "Epoch 26, train loss: 0.174538\n",
      "Epoch 27, train loss: 0.179579\n",
      "Epoch 28, train loss: 0.182640\n",
      "Epoch 29, train loss: 0.189072\n",
      "Epoch 30, train loss: 0.246512\n",
      "Epoch 31, train loss: 0.224034\n",
      "Epoch 32, train loss: 0.214440\n",
      "Epoch 33, train loss: 0.280831\n",
      "Epoch 34, train loss: 0.206540\n",
      "Epoch 35, train loss: 0.191075\n",
      "Epoch 36, train loss: 0.169200\n",
      "Epoch 37, train loss: 0.235063\n",
      "Epoch 38, train loss: 0.152653\n",
      "Epoch 39, train loss: 0.212299\n",
      "Epoch 40, train loss: 0.243596\n",
      "Epoch 41, train loss: 0.167085\n",
      "Epoch 42, train loss: 0.201006\n",
      "Epoch 43, train loss: 0.243918\n",
      "Epoch 44, train loss: 0.196636\n",
      "Epoch 45, train loss: 0.237248\n",
      "Epoch 46, train loss: 0.184325\n",
      "Epoch 47, train loss: 0.212128\n",
      "Epoch 48, train loss: 0.198427\n",
      "Epoch 49, train loss: 0.137291\n",
      "Epoch 50, train loss: 0.231578\n",
      "Epoch 51, train loss: 0.180313\n",
      "Epoch 52, train loss: 0.210523\n",
      "Epoch 53, train loss: 0.206357\n",
      "Epoch 54, train loss: 0.198407\n",
      "Epoch 55, train loss: 0.178372\n",
      "Epoch 56, train loss: 0.229315\n",
      "Epoch 57, train loss: 0.168555\n",
      "Epoch 58, train loss: 0.184625\n",
      "Epoch 59, train loss: 0.232017\n",
      "Epoch 60, train loss: 0.285205\n",
      "Epoch 61, train loss: 0.213244\n",
      "Epoch 62, train loss: 0.171647\n",
      "Epoch 63, train loss: 0.216896\n",
      "Epoch 64, train loss: 0.256301\n",
      "Epoch 65, train loss: 0.195888\n",
      "Epoch 66, train loss: 0.217484\n",
      "Epoch 67, train loss: 0.211623\n",
      "Epoch 68, train loss: 0.223605\n",
      "Epoch 69, train loss: 0.184384\n",
      "Epoch 70, train loss: 0.180933\n",
      "Epoch 71, train loss: 0.183590\n",
      "Epoch 72, train loss: 0.228377\n",
      "Epoch 73, train loss: 0.203406\n",
      "Epoch 74, train loss: 0.214773\n",
      "Epoch 75, train loss: 0.147518\n",
      "Epoch 76, train loss: 0.172573\n",
      "Epoch 77, train loss: 0.171346\n",
      "Epoch 78, train loss: 0.212619\n",
      "Epoch 79, train loss: 0.240511\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "x here is\n",
      "[[243. 253. 262. ... 284. 295. 299.]\n",
      " [253. 262. 269. ... 295. 299. 300.]\n",
      " [262. 269. 269. ... 299. 300. 309.]\n",
      " ...\n",
      " [ 58.  59.  57. ...  53.  66.  90.]\n",
      " [ 59.  57.  55. ...  66.  90.  87.]\n",
      " [ 57.  55.  54. ...  90.  87.  86.]]\n",
      "y here is\n",
      "[[311. 311. 311. ... 311. 311. 311.]\n",
      " [314. 314. 314. ... 314. 314. 314.]\n",
      " [318. 318. 318. ... 318. 318. 318.]\n",
      " ...\n",
      " [ 72.  72.  72. ...  72.  72.  72.]\n",
      " [ 78.  78.  78. ...  78.  78.  78.]\n",
      " [ 79.  79.  79. ...  79.  79.  79.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (2393, 12), Train/test: 1/2392\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 60 segments\n",
      "Building dataset, requesting data from 0 to 60\n",
      "x here is\n",
      "[[ 48.  48.  53. ...  47.  44.  44.]\n",
      " [ 48.  53.  63. ...  44.  44.  44.]\n",
      " [ 53.  63.  69. ...  44.  44.  51.]\n",
      " ...\n",
      " [152. 152. 155. ... 212. 223. 227.]\n",
      " [152. 155. 158. ... 223. 227. 223.]\n",
      " [155. 158. 163. ... 227. 223. 215.]]\n",
      "y here is\n",
      "[[114. 114. 114. ... 114. 114. 114.]\n",
      " [119. 119. 119. ... 119. 119. 119.]\n",
      " [121. 121. 121. ... 121. 121. 121.]\n",
      " ...\n",
      " [213. 213. 213. ... 213. 213. 213.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [235. 235. 235. ... 235. 235. 235.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 159/10989\n",
      "Found 60 continuous time series\n",
      "Data shape: (11150, 12), Train/test: 11148/2\n",
      "Train test ratio: 5574.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329E2F0>\n",
      "Epoch 0, test loss: 0.217051\n",
      "Epoch 1, test loss: 0.219675\n",
      "Epoch 2, test loss: 0.227060\n",
      "Epoch 3, test loss: 0.219735\n",
      "Epoch 4, test loss: 0.222365\n",
      "Epoch 5, test loss: 0.221237\n",
      "Epoch 6, test loss: 0.230054\n",
      "Epoch 7, test loss: 0.218716\n",
      "Epoch 8, test loss: 0.219046\n",
      "Epoch 9, test loss: 0.218686\n",
      "Epoch 10, test loss: 0.219561\n",
      "Epoch 11, test loss: 0.218890\n",
      "Epoch 12, test loss: 0.219622\n",
      "Epoch 13, test loss: 0.219372\n",
      "Epoch 14, test loss: 0.218492\n",
      "Epoch 15, test loss: 0.218403\n",
      "Epoch 16, test loss: 0.218884\n",
      "Epoch 17, test loss: 0.218650\n",
      "Epoch 18, test loss: 0.217779\n",
      "Epoch 19, test loss: 0.219154\n",
      "Epoch 20, test loss: 0.218728\n",
      "Epoch 21, test loss: 0.219829\n",
      "Epoch 22, test loss: 0.218363\n",
      "Epoch 23, test loss: 0.218435\n",
      "Epoch 24, test loss: 0.217498\n",
      "Epoch 25, test loss: 0.218770\n",
      "Epoch 26, test loss: 0.223311\n",
      "Epoch 27, test loss: 0.221278\n",
      "Epoch 28, test loss: 0.218155\n",
      "Epoch 29, test loss: 0.222556\n",
      "Epoch 30, test loss: 0.220923\n",
      "Epoch 31, test loss: 0.219089\n",
      "Epoch 32, test loss: 0.217820\n",
      "Epoch 33, test loss: 0.218025\n",
      "Epoch 34, test loss: 0.219067\n",
      "Epoch 35, test loss: 0.218523\n",
      "Epoch 36, test loss: 0.227342\n",
      "Epoch 37, test loss: 0.219721\n",
      "Epoch 38, test loss: 0.217970\n",
      "Epoch 39, test loss: 0.218080\n",
      "Epoch 40, test loss: 0.220120\n",
      "Epoch 41, test loss: 0.218510\n",
      "Epoch 42, test loss: 0.226224\n",
      "Epoch 43, test loss: 0.218802\n",
      "Epoch 44, test loss: 0.219570\n",
      "Epoch 45, test loss: 0.219397\n",
      "Epoch 46, test loss: 0.217631\n",
      "Epoch 47, test loss: 0.219719\n",
      "Epoch 48, test loss: 0.230473\n",
      "Epoch 49, test loss: 0.221888\n",
      "Epoch 50, test loss: 0.218337\n",
      "Epoch 51, test loss: 0.218846\n",
      "Epoch 52, test loss: 0.218054\n",
      "Epoch 53, test loss: 0.219239\n",
      "Epoch 54, test loss: 0.222894\n",
      "Epoch 55, test loss: 0.217600\n",
      "Epoch 56, test loss: 0.217804\n",
      "Epoch 57, test loss: 0.223548\n",
      "Epoch 58, test loss: 0.219320\n",
      "Epoch 59, test loss: 0.219220\n",
      "Epoch 60, test loss: 0.219449\n",
      "Epoch 61, test loss: 0.217627\n",
      "Epoch 62, test loss: 0.219389\n",
      "Epoch 63, test loss: 0.218901\n",
      "Epoch 64, test loss: 0.220657\n",
      "Epoch 65, test loss: 0.221411\n",
      "Epoch 66, test loss: 0.226253\n",
      "Epoch 67, test loss: 0.221683\n",
      "Epoch 68, test loss: 0.232763\n",
      "Epoch 69, test loss: 0.218621\n",
      "Epoch 70, test loss: 0.218489\n",
      "Epoch 71, test loss: 0.217785\n",
      "Epoch 72, test loss: 0.218567\n",
      "Epoch 73, test loss: 0.218223\n",
      "Epoch 74, test loss: 0.218276\n",
      "Epoch 75, test loss: 0.224187\n",
      "Epoch 76, test loss: 0.220858\n",
      "Epoch 77, test loss: 0.217685\n",
      "Epoch 78, test loss: 0.218560\n",
      "Epoch 79, test loss: 0.218263\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329E2F0>\n",
      "Epoch 0, test loss: 0.217343\n",
      "Epoch 1, test loss: 0.217719\n",
      "Epoch 2, test loss: 0.228977\n",
      "Epoch 3, test loss: 0.218887\n",
      "Epoch 4, test loss: 0.220568\n",
      "Epoch 5, test loss: 0.217721\n",
      "Epoch 6, test loss: 0.218063\n",
      "Epoch 7, test loss: 0.216501\n",
      "Epoch 8, test loss: 0.216993\n",
      "Epoch 9, test loss: 0.217368\n",
      "Epoch 10, test loss: 0.217162\n",
      "Epoch 11, test loss: 0.216834\n",
      "Epoch 12, test loss: 0.217259\n",
      "Epoch 13, test loss: 0.217005\n",
      "Epoch 14, test loss: 0.217562\n",
      "Epoch 15, test loss: 0.218389\n",
      "Epoch 16, test loss: 0.217657\n",
      "Epoch 17, test loss: 0.218728\n",
      "Epoch 18, test loss: 0.218764\n",
      "Epoch 19, test loss: 0.217509\n",
      "Epoch 20, test loss: 0.221711\n",
      "Epoch 21, test loss: 0.217139\n",
      "Epoch 22, test loss: 0.216294\n",
      "Epoch 23, test loss: 0.221121\n",
      "Epoch 24, test loss: 0.218266\n",
      "Epoch 25, test loss: 0.217976\n",
      "Epoch 26, test loss: 0.218303\n",
      "Epoch 27, test loss: 0.216448\n",
      "Epoch 28, test loss: 0.216116\n",
      "Epoch 29, test loss: 0.218784\n",
      "Epoch 30, test loss: 0.216806\n",
      "Epoch 31, test loss: 0.217378\n",
      "Epoch 32, test loss: 0.223177\n",
      "Epoch 33, test loss: 0.218762\n",
      "Epoch 34, test loss: 0.216224\n",
      "Epoch 35, test loss: 0.217741\n",
      "Epoch 36, test loss: 0.216934\n",
      "Epoch 37, test loss: 0.223504\n",
      "Epoch 38, test loss: 0.216803\n",
      "Epoch 39, test loss: 0.216999\n",
      "Epoch 40, test loss: 0.218195\n",
      "Epoch 41, test loss: 0.216515\n",
      "Epoch 42, test loss: 0.220092\n",
      "Epoch 43, test loss: 0.218355\n",
      "Epoch 44, test loss: 0.217277\n",
      "Epoch 45, test loss: 0.218016\n",
      "Epoch 46, test loss: 0.217442\n",
      "Epoch 47, test loss: 0.219372\n",
      "Epoch 48, test loss: 0.216715\n",
      "Epoch 49, test loss: 0.215768\n",
      "Epoch 50, test loss: 0.217772\n",
      "Epoch 51, test loss: 0.216653\n",
      "Epoch 52, test loss: 0.216908\n",
      "Epoch 53, test loss: 0.218231\n",
      "Epoch 54, test loss: 0.221028\n",
      "Epoch 55, test loss: 0.217579\n",
      "Epoch 56, test loss: 0.222656\n",
      "Epoch 57, test loss: 0.216914\n",
      "Epoch 58, test loss: 0.218056\n",
      "Epoch 59, test loss: 0.216085\n",
      "Epoch 60, test loss: 0.221758\n",
      "Epoch 61, test loss: 0.219897\n",
      "Epoch 62, test loss: 0.216814\n",
      "Epoch 63, test loss: 0.221345\n",
      "Epoch 64, test loss: 0.217059\n",
      "Epoch 65, test loss: 0.222591\n",
      "Epoch 66, test loss: 0.222968\n",
      "Epoch 67, test loss: 0.216834\n",
      "Epoch 68, test loss: 0.216724\n",
      "Epoch 69, test loss: 0.217199\n",
      "Epoch 70, test loss: 0.218138\n",
      "Epoch 71, test loss: 0.217278\n",
      "Epoch 72, test loss: 0.216643\n",
      "Epoch 73, test loss: 0.219353\n",
      "Epoch 74, test loss: 0.219952\n",
      "Epoch 75, test loss: 0.216304\n",
      "Epoch 76, test loss: 0.223759\n",
      "Epoch 77, test loss: 0.218842\n",
      "Epoch 78, test loss: 0.216383\n",
      "Epoch 79, test loss: 0.217415\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329E2F0>\n",
      "Epoch 0, test loss: 0.259946\n",
      "Epoch 1, test loss: 0.231833\n",
      "Epoch 2, test loss: 0.223612\n",
      "Epoch 3, test loss: 0.220309\n",
      "Epoch 4, test loss: 0.219552\n",
      "Epoch 5, test loss: 0.218707\n",
      "Epoch 6, test loss: 0.218004\n",
      "Epoch 7, test loss: 0.218606\n",
      "Epoch 8, test loss: 0.221058\n",
      "Epoch 9, test loss: 0.219289\n",
      "Epoch 10, test loss: 0.220500\n",
      "Epoch 11, test loss: 0.219945\n",
      "Epoch 12, test loss: 0.224790\n",
      "Epoch 13, test loss: 0.218141\n",
      "Epoch 14, test loss: 0.222582\n",
      "Epoch 15, test loss: 0.217542\n",
      "Epoch 16, test loss: 0.217203\n",
      "Epoch 17, test loss: 0.219582\n",
      "Epoch 18, test loss: 0.219477\n",
      "Epoch 19, test loss: 0.217021\n",
      "Epoch 20, test loss: 0.217030\n",
      "Epoch 21, test loss: 0.217982\n",
      "Epoch 22, test loss: 0.217259\n",
      "Epoch 23, test loss: 0.217723\n",
      "Epoch 24, test loss: 0.216699\n",
      "Epoch 25, test loss: 0.218585\n",
      "Epoch 26, test loss: 0.225226\n",
      "Epoch 27, test loss: 0.218143\n",
      "Epoch 28, test loss: 0.217446\n",
      "Epoch 29, test loss: 0.220282\n",
      "Epoch 30, test loss: 0.217640\n",
      "Epoch 31, test loss: 0.217036\n",
      "Epoch 32, test loss: 0.220498\n",
      "Epoch 33, test loss: 0.216765\n",
      "Epoch 34, test loss: 0.217458\n",
      "Epoch 35, test loss: 0.218490\n",
      "Epoch 36, test loss: 0.216718\n",
      "Epoch 37, test loss: 0.218160\n",
      "Epoch 38, test loss: 0.217338\n",
      "Epoch 39, test loss: 0.216979\n",
      "Epoch 40, test loss: 0.217279\n",
      "Epoch 41, test loss: 0.218279\n",
      "Epoch 42, test loss: 0.220507\n",
      "Epoch 43, test loss: 0.218128\n",
      "Epoch 44, test loss: 0.217124\n",
      "Epoch 45, test loss: 0.220422\n",
      "Epoch 46, test loss: 0.216393\n",
      "Epoch 47, test loss: 0.217443\n",
      "Epoch 48, test loss: 0.217961\n",
      "Epoch 49, test loss: 0.217529\n",
      "Epoch 50, test loss: 0.219398\n",
      "Epoch 51, test loss: 0.219154\n",
      "Epoch 52, test loss: 0.216803\n",
      "Epoch 53, test loss: 0.216669\n",
      "Epoch 54, test loss: 0.216938\n",
      "Epoch 55, test loss: 0.216481\n",
      "Epoch 56, test loss: 0.218625\n",
      "Epoch 57, test loss: 0.217371\n",
      "Epoch 58, test loss: 0.219182\n",
      "Epoch 59, test loss: 0.220029\n",
      "Epoch 60, test loss: 0.219739\n",
      "Epoch 61, test loss: 0.217599\n",
      "Epoch 62, test loss: 0.217105\n",
      "Epoch 63, test loss: 0.216993\n",
      "Epoch 64, test loss: 0.217277\n",
      "Epoch 65, test loss: 0.221238\n",
      "Epoch 66, test loss: 0.217883\n",
      "Epoch 67, test loss: 0.226769\n",
      "Epoch 68, test loss: 0.216047\n",
      "Epoch 69, test loss: 0.222735\n",
      "Epoch 70, test loss: 0.216276\n",
      "Epoch 71, test loss: 0.216537\n",
      "Epoch 72, test loss: 0.216430\n",
      "Epoch 73, test loss: 0.228273\n",
      "Epoch 74, test loss: 0.217109\n",
      "Epoch 75, test loss: 0.217181\n",
      "Epoch 76, test loss: 0.218553\n",
      "Epoch 77, test loss: 0.218160\n",
      "Epoch 78, test loss: 0.218232\n",
      "Epoch 79, test loss: 0.215778\n",
      "Epoch 80, test loss: 0.217093\n",
      "Epoch 81, test loss: 0.216182\n",
      "Epoch 82, test loss: 0.216972\n",
      "Epoch 83, test loss: 0.215890\n",
      "Epoch 84, test loss: 0.216677\n",
      "Epoch 85, test loss: 0.216889\n",
      "Epoch 86, test loss: 0.218327\n",
      "Epoch 87, test loss: 0.219035\n",
      "Epoch 88, test loss: 0.216239\n",
      "Epoch 89, test loss: 0.216275\n",
      "Epoch 90, test loss: 0.216193\n",
      "Epoch 91, test loss: 0.216791\n",
      "Epoch 92, test loss: 0.216066\n",
      "Epoch 93, test loss: 0.218008\n",
      "Epoch 94, test loss: 0.215732\n",
      "Epoch 95, test loss: 0.216061\n",
      "Epoch 96, test loss: 0.217696\n",
      "Epoch 97, test loss: 0.216813\n",
      "Epoch 98, test loss: 0.216978\n",
      "Epoch 99, test loss: 0.216725\n",
      "Epoch 100, test loss: 0.217882\n",
      "Epoch 101, test loss: 0.216694\n",
      "Epoch 102, test loss: 0.216113\n",
      "Epoch 103, test loss: 0.215850\n",
      "Epoch 104, test loss: 0.217145\n",
      "Epoch 105, test loss: 0.216964\n",
      "Epoch 106, test loss: 0.216480\n",
      "Epoch 107, test loss: 0.216563\n",
      "Epoch 108, test loss: 0.216276\n",
      "Epoch 109, test loss: 0.219187\n",
      "Epoch 110, test loss: 0.216789\n",
      "Epoch 111, test loss: 0.218449\n",
      "Epoch 112, test loss: 0.221571\n",
      "Epoch 113, test loss: 0.217046\n",
      "Epoch 114, test loss: 0.217951\n",
      "Epoch 115, test loss: 0.219178\n",
      "Epoch 116, test loss: 0.218023\n",
      "Epoch 117, test loss: 0.216810\n",
      "Epoch 118, test loss: 0.215985\n",
      "Epoch 119, test loss: 0.216902\n",
      "Epoch 120, test loss: 0.216473\n",
      "Epoch 121, test loss: 0.216118\n",
      "Epoch 122, test loss: 0.218369\n",
      "Epoch 123, test loss: 0.217309\n",
      "Epoch 124, test loss: 0.216905\n",
      "Epoch 125, test loss: 0.216717\n",
      "Epoch 126, test loss: 0.215542\n",
      "Epoch 127, test loss: 0.216518\n",
      "Epoch 128, test loss: 0.216011\n",
      "Epoch 129, test loss: 0.217105\n",
      "Epoch 130, test loss: 0.216155\n",
      "Epoch 131, test loss: 0.217123\n",
      "Epoch 132, test loss: 0.216679\n",
      "Epoch 133, test loss: 0.216382\n",
      "Epoch 134, test loss: 0.217053\n",
      "Epoch 135, test loss: 0.217530\n",
      "Epoch 136, test loss: 0.216539\n",
      "Epoch 137, test loss: 0.219196\n",
      "Epoch 138, test loss: 0.219372\n",
      "Epoch 139, test loss: 0.215916\n",
      "Epoch 140, test loss: 0.216972\n",
      "Epoch 141, test loss: 0.216301\n",
      "Epoch 142, test loss: 0.216881\n",
      "Epoch 143, test loss: 0.217271\n",
      "Epoch 144, test loss: 0.218079\n",
      "Epoch 145, test loss: 0.217352\n",
      "Epoch 146, test loss: 0.217889\n",
      "Epoch 147, test loss: 0.225038\n",
      "Epoch 148, test loss: 0.215695\n",
      "Epoch 149, test loss: 0.218894\n",
      "Epoch 150, test loss: 0.219305\n",
      "Epoch 151, test loss: 0.216144\n",
      "Epoch 152, test loss: 0.218037\n",
      "Epoch 153, test loss: 0.222606\n",
      "Epoch 154, test loss: 0.216931\n",
      "Epoch 155, test loss: 0.218925\n",
      "Epoch 156, test loss: 0.216773\n",
      "Epoch 157, test loss: 0.218679\n",
      "Epoch 158, test loss: 0.217651\n",
      "Epoch 159, test loss: 0.218528\n",
      "Epoch 160, test loss: 0.216616\n",
      "Epoch 161, test loss: 0.216681\n",
      "Epoch 162, test loss: 0.226514\n",
      "Epoch 163, test loss: 0.218672\n",
      "Epoch 164, test loss: 0.217701\n",
      "Epoch 165, test loss: 0.217938\n",
      "Epoch 166, test loss: 0.216105\n",
      "Epoch 167, test loss: 0.216799\n",
      "Epoch 168, test loss: 0.216609\n",
      "Epoch 169, test loss: 0.217397\n",
      "Epoch 170, test loss: 0.216619\n",
      "Epoch 171, test loss: 0.216646\n",
      "Epoch 172, test loss: 0.219889\n",
      "Epoch 173, test loss: 0.216339\n",
      "Epoch 174, test loss: 0.217086\n",
      "Epoch 175, test loss: 0.218305\n",
      "Epoch 176, test loss: 0.216285\n",
      "Epoch 177, test loss: 0.219854\n",
      "Epoch 178, test loss: 0.216151\n",
      "Epoch 179, test loss: 0.217533\n",
      "Epoch 180, test loss: 0.221781\n",
      "Epoch 181, test loss: 0.220629\n",
      "Epoch 182, test loss: 0.217465\n",
      "Epoch 183, test loss: 0.217413\n",
      "Epoch 184, test loss: 0.218454\n",
      "Epoch 185, test loss: 0.218093\n",
      "Epoch 186, test loss: 0.217246\n",
      "Epoch 187, test loss: 0.217442\n",
      "Epoch 188, test loss: 0.216362\n",
      "Epoch 189, test loss: 0.218053\n",
      "Epoch 190, test loss: 0.217920\n",
      "Epoch 191, test loss: 0.216473\n",
      "Epoch 192, test loss: 0.216490\n",
      "Epoch 193, test loss: 0.216581\n",
      "Epoch 194, test loss: 0.216872\n",
      "Epoch 195, test loss: 0.216599\n",
      "Epoch 196, test loss: 0.219447\n",
      "Epoch 197, test loss: 0.222555\n",
      "Epoch 198, test loss: 0.218653\n",
      "Epoch 199, test loss: 0.221016\n",
      "Epoch 200, test loss: 0.216999\n",
      "Epoch 201, test loss: 0.216219\n",
      "Epoch 202, test loss: 0.218627\n",
      "Epoch 203, test loss: 0.216587\n",
      "Epoch 204, test loss: 0.217624\n",
      "Epoch 205, test loss: 0.219443\n",
      "Epoch 206, test loss: 0.216385\n",
      "Epoch 207, test loss: 0.218781\n",
      "Epoch 208, test loss: 0.220938\n",
      "Epoch 209, test loss: 0.217352\n",
      "Epoch 210, test loss: 0.215976\n",
      "Epoch 211, test loss: 0.217134\n",
      "Epoch 212, test loss: 0.216817\n",
      "Epoch 213, test loss: 0.216392\n",
      "Epoch 214, test loss: 0.231126\n",
      "Epoch 215, test loss: 0.219847\n",
      "Epoch 216, test loss: 0.216376\n",
      "Epoch 217, test loss: 0.216138\n",
      "Epoch 218, test loss: 0.217473\n",
      "Epoch 219, test loss: 0.218316\n",
      "Epoch 220, test loss: 0.218030\n",
      "Epoch 221, test loss: 0.216549\n",
      "Epoch 222, test loss: 0.216880\n",
      "Epoch 223, test loss: 0.217662\n",
      "Epoch 224, test loss: 0.219731\n",
      "Epoch 225, test loss: 0.216408\n",
      "Epoch 226, test loss: 0.217167\n",
      "Epoch 227, test loss: 0.217618\n",
      "Epoch 228, test loss: 0.217804\n",
      "Epoch 229, test loss: 0.217111\n",
      "Epoch 230, test loss: 0.219405\n",
      "Epoch 231, test loss: 0.219205\n",
      "Epoch 232, test loss: 0.219687\n",
      "Epoch 233, test loss: 0.218101\n",
      "Epoch 234, test loss: 0.217759\n",
      "Epoch 235, test loss: 0.218167\n",
      "Epoch 236, test loss: 0.218193\n",
      "Epoch 237, test loss: 0.217485\n",
      "Epoch 238, test loss: 0.216859\n",
      "Epoch 239, test loss: 0.220502\n",
      "Pretrain data: 19823659.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 139. 141. 142.]\n",
      " [128. 129. 131. ... 141. 142. 141.]\n",
      " [129. 131. 133. ... 142. 141. 143.]\n",
      " ...\n",
      " [323. 328. 264. ... 248. 244. 243.]\n",
      " [328. 264. 264. ... 244. 243. 244.]\n",
      " [264. 264. 264. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [155. 155. 155. ... 155. 155. 155.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7363/103137\n",
      "Found 815 continuous time series\n",
      "Data shape: (110502, 12), Train/test: 110500/2\n",
      "Train test ratio: 55250.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1726\n",
      "Epoch 0, train loss: 0.232572\n",
      "Epoch 1, train loss: 0.201436\n",
      "Epoch 2, train loss: 0.187080\n",
      "Epoch 3, train loss: 0.232398\n",
      "Epoch 4, train loss: 0.191818\n",
      "Epoch 5, train loss: 0.181407\n",
      "Epoch 6, train loss: 0.237488\n",
      "Epoch 7, train loss: 0.224176\n",
      "Epoch 8, train loss: 0.209978\n",
      "Epoch 9, train loss: 0.186937\n",
      "Epoch 10, train loss: 0.259117\n",
      "Epoch 11, train loss: 0.169439\n",
      "Epoch 12, train loss: 0.233822\n",
      "Epoch 13, train loss: 0.223726\n",
      "Epoch 14, train loss: 0.209164\n",
      "Epoch 15, train loss: 0.162999\n",
      "Epoch 16, train loss: 0.270008\n",
      "Epoch 17, train loss: 0.215008\n",
      "Epoch 18, train loss: 0.223585\n",
      "Epoch 19, train loss: 0.270248\n",
      "Epoch 20, train loss: 0.242552\n",
      "Epoch 21, train loss: 0.207993\n",
      "Epoch 22, train loss: 0.211338\n",
      "Epoch 23, train loss: 0.199387\n",
      "Epoch 24, train loss: 0.170824\n",
      "Epoch 25, train loss: 0.212666\n",
      "Epoch 26, train loss: 0.175627\n",
      "Epoch 27, train loss: 0.181478\n",
      "Epoch 28, train loss: 0.207841\n",
      "Epoch 29, train loss: 0.151832\n",
      "Epoch 30, train loss: 0.193246\n",
      "Epoch 31, train loss: 0.215217\n",
      "Epoch 32, train loss: 0.216548\n",
      "Epoch 33, train loss: 0.214866\n",
      "Epoch 34, train loss: 0.164967\n",
      "Epoch 35, train loss: 0.231242\n",
      "Epoch 36, train loss: 0.215705\n",
      "Epoch 37, train loss: 0.160602\n",
      "Epoch 38, train loss: 0.193123\n",
      "Epoch 39, train loss: 0.230474\n",
      "Epoch 40, train loss: 0.200600\n",
      "Epoch 41, train loss: 0.282541\n",
      "Epoch 42, train loss: 0.246462\n",
      "Epoch 43, train loss: 0.248588\n",
      "Epoch 44, train loss: 0.202145\n",
      "Epoch 45, train loss: 0.188821\n",
      "Epoch 46, train loss: 0.198570\n",
      "Epoch 47, train loss: 0.265027\n",
      "Epoch 48, train loss: 0.206870\n",
      "Epoch 49, train loss: 0.187788\n",
      "Epoch 50, train loss: 0.210557\n",
      "Epoch 51, train loss: 0.195544\n",
      "Epoch 52, train loss: 0.186556\n",
      "Epoch 53, train loss: 0.196213\n",
      "Epoch 54, train loss: 0.177829\n",
      "Epoch 55, train loss: 0.218939\n",
      "Epoch 56, train loss: 0.194366\n",
      "Epoch 57, train loss: 0.204766\n",
      "Epoch 58, train loss: 0.188703\n",
      "Epoch 59, train loss: 0.342507\n",
      "Epoch 60, train loss: 0.199639\n",
      "Epoch 61, train loss: 0.213774\n",
      "Epoch 62, train loss: 0.212585\n",
      "Epoch 63, train loss: 0.238329\n",
      "Epoch 64, train loss: 0.226830\n",
      "Epoch 65, train loss: 0.213955\n",
      "Epoch 66, train loss: 0.240049\n",
      "Epoch 67, train loss: 0.193876\n",
      "Epoch 68, train loss: 0.246641\n",
      "Epoch 69, train loss: 0.244283\n",
      "Epoch 70, train loss: 0.191124\n",
      "Epoch 71, train loss: 0.155772\n",
      "Epoch 72, train loss: 0.187445\n",
      "Epoch 73, train loss: 0.166502\n",
      "Epoch 74, train loss: 0.194606\n",
      "Epoch 75, train loss: 0.228525\n",
      "Epoch 76, train loss: 0.168100\n",
      "Epoch 77, train loss: 0.228853\n",
      "Epoch 78, train loss: 0.242054\n",
      "Epoch 79, train loss: 0.218840\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[174. 173. 173. ... 166. 166. 165.]\n",
      " [173. 173. 172. ... 166. 165. 165.]\n",
      " [173. 172. 170. ... 165. 165. 165.]\n",
      " ...\n",
      " [126. 125. 120. ...  92.  88.  85.]\n",
      " [125. 120. 116. ...  88.  85.  84.]\n",
      " [120. 116. 114. ...  85.  84.  86.]]\n",
      "y here is\n",
      "[[163. 163. 163. ... 163. 163. 163.]\n",
      " [163. 163. 163. ... 163. 163. 163.]\n",
      " [163. 163. 163. ... 163. 163. 163.]\n",
      " ...\n",
      " [ 90.  90.  90. ...  90.  90.  90.]\n",
      " [ 86.  86.  86. ...  86.  86.  86.]\n",
      " [ 87.  87.  87. ...  87.  87.  87.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2624, 12), Train/test: 1/2623\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[142. 142. 142. ... 135. 128. 127.]\n",
      " [142. 142. 141. ... 128. 127. 127.]\n",
      " [142. 141. 139. ... 127. 127. 126.]\n",
      " ...\n",
      " [151. 142. 144. ... 163. 166. 168.]\n",
      " [142. 144. 152. ... 166. 168. 167.]\n",
      " [144. 152. 155. ... 168. 167. 168.]]\n",
      "y here is\n",
      "[[124. 124. 124. ... 124. 124. 124.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [124. 124. 124. ... 124. 124. 124.]\n",
      " ...\n",
      " [173. 173. 173. ... 173. 173. 173.]\n",
      " [174. 174. 174. ... 174. 174. 174.]\n",
      " [174. 174. 174. ... 174. 174. 174.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 545/9888\n",
      "Found 27 continuous time series\n",
      "Data shape: (10435, 12), Train/test: 10433/2\n",
      "Train test ratio: 5216.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2932B84C0>\n",
      "Epoch 0, test loss: 0.171970\n",
      "Epoch 1, test loss: 0.171465\n",
      "Epoch 2, test loss: 0.171286\n",
      "Epoch 3, test loss: 0.171400\n",
      "Epoch 4, test loss: 0.173238\n",
      "Epoch 5, test loss: 0.170561\n",
      "Epoch 6, test loss: 0.171532\n",
      "Epoch 7, test loss: 0.172076\n",
      "Epoch 8, test loss: 0.170566\n",
      "Epoch 9, test loss: 0.170569\n",
      "Epoch 10, test loss: 0.172588\n",
      "Epoch 11, test loss: 0.173050\n",
      "Epoch 12, test loss: 0.172112\n",
      "Epoch 13, test loss: 0.170915\n",
      "Epoch 14, test loss: 0.170863\n",
      "Epoch 15, test loss: 0.170688\n",
      "Epoch 16, test loss: 0.170676\n",
      "Epoch 17, test loss: 0.173286\n",
      "Epoch 18, test loss: 0.171942\n",
      "Epoch 19, test loss: 0.171128\n",
      "Epoch 20, test loss: 0.171097\n",
      "Epoch 21, test loss: 0.170989\n",
      "Epoch 22, test loss: 0.171229\n",
      "Epoch 23, test loss: 0.171386\n",
      "Epoch 24, test loss: 0.170531\n",
      "Epoch 25, test loss: 0.172169\n",
      "Epoch 26, test loss: 0.173368\n",
      "Epoch 27, test loss: 0.171395\n",
      "Epoch 28, test loss: 0.171532\n",
      "Epoch 29, test loss: 0.171984\n",
      "Epoch 30, test loss: 0.170854\n",
      "Epoch 31, test loss: 0.171370\n",
      "Epoch 32, test loss: 0.173392\n",
      "Epoch 33, test loss: 0.178770\n",
      "Epoch 34, test loss: 0.170977\n",
      "Epoch 35, test loss: 0.170875\n",
      "Epoch 36, test loss: 0.170623\n",
      "Epoch 37, test loss: 0.173794\n",
      "Epoch 38, test loss: 0.173215\n",
      "Epoch 39, test loss: 0.171968\n",
      "Epoch 40, test loss: 0.170593\n",
      "Epoch 41, test loss: 0.171438\n",
      "Epoch 42, test loss: 0.171362\n",
      "Epoch 43, test loss: 0.171411\n",
      "Epoch 44, test loss: 0.171560\n",
      "Epoch 45, test loss: 0.171969\n",
      "Epoch 46, test loss: 0.171246\n",
      "Epoch 47, test loss: 0.171367\n",
      "Epoch 48, test loss: 0.172077\n",
      "Epoch 49, test loss: 0.172928\n",
      "Epoch 50, test loss: 0.171507\n",
      "Epoch 51, test loss: 0.171395\n",
      "Epoch 52, test loss: 0.170993\n",
      "Epoch 53, test loss: 0.177936\n",
      "Epoch 54, test loss: 0.170808\n",
      "Epoch 55, test loss: 0.170231\n",
      "Epoch 56, test loss: 0.172992\n",
      "Epoch 57, test loss: 0.170684\n",
      "Epoch 58, test loss: 0.170658\n",
      "Epoch 59, test loss: 0.171722\n",
      "Epoch 60, test loss: 0.171854\n",
      "Epoch 61, test loss: 0.170822\n",
      "Epoch 62, test loss: 0.171527\n",
      "Epoch 63, test loss: 0.171309\n",
      "Epoch 64, test loss: 0.171646\n",
      "Epoch 65, test loss: 0.175635\n",
      "Epoch 66, test loss: 0.171225\n",
      "Epoch 67, test loss: 0.171683\n",
      "Epoch 68, test loss: 0.171009\n",
      "Epoch 69, test loss: 0.174865\n",
      "Epoch 70, test loss: 0.172569\n",
      "Epoch 71, test loss: 0.170901\n",
      "Epoch 72, test loss: 0.171359\n",
      "Epoch 73, test loss: 0.171145\n",
      "Epoch 74, test loss: 0.170650\n",
      "Epoch 75, test loss: 0.172704\n",
      "Epoch 76, test loss: 0.172702\n",
      "Epoch 77, test loss: 0.171098\n",
      "Epoch 78, test loss: 0.170740\n",
      "Epoch 79, test loss: 0.170903\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2932B84C0>\n",
      "Epoch 0, test loss: 0.171112\n",
      "Epoch 1, test loss: 0.170527\n",
      "Epoch 2, test loss: 0.171678\n",
      "Epoch 3, test loss: 0.170646\n",
      "Epoch 4, test loss: 0.172142\n",
      "Epoch 5, test loss: 0.171166\n",
      "Epoch 6, test loss: 0.170900\n",
      "Epoch 7, test loss: 0.170368\n",
      "Epoch 8, test loss: 0.170351\n",
      "Epoch 9, test loss: 0.172814\n",
      "Epoch 10, test loss: 0.170688\n",
      "Epoch 11, test loss: 0.171303\n",
      "Epoch 12, test loss: 0.171834\n",
      "Epoch 13, test loss: 0.170410\n",
      "Epoch 14, test loss: 0.171663\n",
      "Epoch 15, test loss: 0.170848\n",
      "Epoch 16, test loss: 0.172638\n",
      "Epoch 17, test loss: 0.170947\n",
      "Epoch 18, test loss: 0.170766\n",
      "Epoch 19, test loss: 0.170702\n",
      "Epoch 20, test loss: 0.170986\n",
      "Epoch 21, test loss: 0.171247\n",
      "Epoch 22, test loss: 0.172260\n",
      "Epoch 23, test loss: 0.170844\n",
      "Epoch 24, test loss: 0.170789\n",
      "Epoch 25, test loss: 0.170640\n",
      "Epoch 26, test loss: 0.170889\n",
      "Epoch 27, test loss: 0.170550\n",
      "Epoch 28, test loss: 0.170633\n",
      "Epoch 29, test loss: 0.170173\n",
      "Epoch 30, test loss: 0.170648\n",
      "Epoch 31, test loss: 0.171426\n",
      "Epoch 32, test loss: 0.171200\n",
      "Epoch 33, test loss: 0.172893\n",
      "Epoch 34, test loss: 0.170405\n",
      "Epoch 35, test loss: 0.170781\n",
      "Epoch 36, test loss: 0.170241\n",
      "Epoch 37, test loss: 0.170322\n",
      "Epoch 38, test loss: 0.173250\n",
      "Epoch 39, test loss: 0.170710\n",
      "Epoch 40, test loss: 0.171862\n",
      "Epoch 41, test loss: 0.170457\n",
      "Epoch 42, test loss: 0.170071\n",
      "Epoch 43, test loss: 0.170796\n",
      "Epoch 44, test loss: 0.170429\n",
      "Epoch 45, test loss: 0.170435\n",
      "Epoch 46, test loss: 0.171027\n",
      "Epoch 47, test loss: 0.170109\n",
      "Epoch 48, test loss: 0.171105\n",
      "Epoch 49, test loss: 0.172766\n",
      "Epoch 50, test loss: 0.170980\n",
      "Epoch 51, test loss: 0.172112\n",
      "Epoch 52, test loss: 0.170781\n",
      "Epoch 53, test loss: 0.170643\n",
      "Epoch 54, test loss: 0.170742\n",
      "Epoch 55, test loss: 0.170600\n",
      "Epoch 56, test loss: 0.170698\n",
      "Epoch 57, test loss: 0.171022\n",
      "Epoch 58, test loss: 0.170852\n",
      "Epoch 59, test loss: 0.170435\n",
      "Epoch 60, test loss: 0.171593\n",
      "Epoch 61, test loss: 0.170257\n",
      "Epoch 62, test loss: 0.170423\n",
      "Epoch 63, test loss: 0.170411\n",
      "Epoch 64, test loss: 0.170458\n",
      "Epoch 65, test loss: 0.170495\n",
      "Epoch 66, test loss: 0.170836\n",
      "Epoch 67, test loss: 0.170644\n",
      "Epoch 68, test loss: 0.171452\n",
      "Epoch 69, test loss: 0.170513\n",
      "Epoch 70, test loss: 0.171385\n",
      "Epoch 71, test loss: 0.170909\n",
      "Epoch 72, test loss: 0.171605\n",
      "Epoch 73, test loss: 0.171252\n",
      "Epoch 74, test loss: 0.170294\n",
      "Epoch 75, test loss: 0.172466\n",
      "Epoch 76, test loss: 0.170483\n",
      "Epoch 77, test loss: 0.170480\n",
      "Epoch 78, test loss: 0.170612\n",
      "Epoch 79, test loss: 0.170535\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh12_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2932B84C0>\n",
      "Epoch 0, test loss: 0.284011\n",
      "Epoch 1, test loss: 0.203083\n",
      "Epoch 2, test loss: 0.183792\n",
      "Epoch 3, test loss: 0.178423\n",
      "Epoch 4, test loss: 0.177473\n",
      "Epoch 5, test loss: 0.176555\n",
      "Epoch 6, test loss: 0.175719\n",
      "Epoch 7, test loss: 0.175958\n",
      "Epoch 8, test loss: 0.175108\n",
      "Epoch 9, test loss: 0.174650\n",
      "Epoch 10, test loss: 0.174598\n",
      "Epoch 11, test loss: 0.176241\n",
      "Epoch 12, test loss: 0.175421\n",
      "Epoch 13, test loss: 0.174206\n",
      "Epoch 14, test loss: 0.175279\n",
      "Epoch 15, test loss: 0.174413\n",
      "Epoch 16, test loss: 0.174711\n",
      "Epoch 17, test loss: 0.173723\n",
      "Epoch 18, test loss: 0.174424\n",
      "Epoch 19, test loss: 0.173685\n",
      "Epoch 20, test loss: 0.174455\n",
      "Epoch 21, test loss: 0.173634\n",
      "Epoch 22, test loss: 0.173927\n",
      "Epoch 23, test loss: 0.173569\n",
      "Epoch 24, test loss: 0.173406\n",
      "Epoch 25, test loss: 0.174543\n",
      "Epoch 26, test loss: 0.173470\n",
      "Epoch 27, test loss: 0.173086\n",
      "Epoch 28, test loss: 0.175208\n",
      "Epoch 29, test loss: 0.172973\n",
      "Epoch 30, test loss: 0.172839\n",
      "Epoch 31, test loss: 0.174068\n",
      "Epoch 32, test loss: 0.173100\n",
      "Epoch 33, test loss: 0.173246\n",
      "Epoch 34, test loss: 0.173366\n",
      "Epoch 35, test loss: 0.173330\n",
      "Epoch 36, test loss: 0.172903\n",
      "Epoch 37, test loss: 0.173461\n",
      "Epoch 38, test loss: 0.174211\n",
      "Epoch 39, test loss: 0.172950\n",
      "Epoch 40, test loss: 0.174020\n",
      "Epoch 41, test loss: 0.172807\n",
      "Epoch 42, test loss: 0.173142\n",
      "Epoch 43, test loss: 0.173407\n",
      "Epoch 44, test loss: 0.173797\n",
      "Epoch 45, test loss: 0.172988\n",
      "Epoch 46, test loss: 0.174736\n",
      "Epoch 47, test loss: 0.172495\n",
      "Epoch 48, test loss: 0.172700\n",
      "Epoch 49, test loss: 0.172873\n",
      "Epoch 50, test loss: 0.173652\n",
      "Epoch 51, test loss: 0.173380\n",
      "Epoch 52, test loss: 0.173523\n",
      "Epoch 53, test loss: 0.175765\n",
      "Epoch 54, test loss: 0.173354\n",
      "Epoch 55, test loss: 0.173673\n",
      "Epoch 56, test loss: 0.173607\n",
      "Epoch 57, test loss: 0.173574\n",
      "Epoch 58, test loss: 0.175136\n",
      "Epoch 59, test loss: 0.173346\n",
      "Epoch 60, test loss: 0.172415\n",
      "Epoch 61, test loss: 0.172956\n",
      "Epoch 62, test loss: 0.173591\n",
      "Epoch 63, test loss: 0.173438\n",
      "Epoch 64, test loss: 0.172295\n",
      "Epoch 65, test loss: 0.173899\n",
      "Epoch 66, test loss: 0.173141\n",
      "Epoch 67, test loss: 0.172443\n",
      "Epoch 68, test loss: 0.172751\n",
      "Epoch 69, test loss: 0.172697\n",
      "Epoch 70, test loss: 0.172060\n",
      "Epoch 71, test loss: 0.173074\n",
      "Epoch 72, test loss: 0.173423\n",
      "Epoch 73, test loss: 0.172415\n",
      "Epoch 74, test loss: 0.172147\n",
      "Epoch 75, test loss: 0.172343\n",
      "Epoch 76, test loss: 0.172762\n",
      "Epoch 77, test loss: 0.172630\n",
      "Epoch 78, test loss: 0.172886\n",
      "Epoch 79, test loss: 0.172585\n",
      "Epoch 80, test loss: 0.172977\n",
      "Epoch 81, test loss: 0.172403\n",
      "Epoch 82, test loss: 0.172832\n",
      "Epoch 83, test loss: 0.172980\n",
      "Epoch 84, test loss: 0.172067\n",
      "Epoch 85, test loss: 0.173656\n",
      "Epoch 86, test loss: 0.172822\n",
      "Epoch 87, test loss: 0.174343\n",
      "Epoch 88, test loss: 0.172966\n",
      "Epoch 89, test loss: 0.173988\n",
      "Epoch 90, test loss: 0.172090\n",
      "Epoch 91, test loss: 0.173801\n",
      "Epoch 92, test loss: 0.175220\n",
      "Epoch 93, test loss: 0.172631\n",
      "Epoch 94, test loss: 0.172666\n",
      "Epoch 95, test loss: 0.173053\n",
      "Epoch 96, test loss: 0.172275\n",
      "Epoch 97, test loss: 0.172493\n",
      "Epoch 98, test loss: 0.172054\n",
      "Epoch 99, test loss: 0.174453\n",
      "Epoch 100, test loss: 0.172559\n",
      "Epoch 101, test loss: 0.172196\n",
      "Epoch 102, test loss: 0.171793\n",
      "Epoch 103, test loss: 0.172856\n",
      "Epoch 104, test loss: 0.172311\n",
      "Epoch 105, test loss: 0.172548\n",
      "Epoch 106, test loss: 0.173316\n",
      "Epoch 107, test loss: 0.177810\n",
      "Epoch 108, test loss: 0.174124\n",
      "Epoch 109, test loss: 0.172547\n",
      "Epoch 110, test loss: 0.172759\n",
      "Epoch 111, test loss: 0.172938\n",
      "Epoch 112, test loss: 0.171669\n",
      "Epoch 113, test loss: 0.172750\n",
      "Epoch 114, test loss: 0.173086\n",
      "Epoch 115, test loss: 0.172349\n",
      "Epoch 116, test loss: 0.172224\n",
      "Epoch 117, test loss: 0.172701\n",
      "Epoch 118, test loss: 0.171965\n",
      "Epoch 119, test loss: 0.174297\n",
      "Epoch 120, test loss: 0.173043\n",
      "Epoch 121, test loss: 0.172221\n",
      "Epoch 122, test loss: 0.172488\n",
      "Epoch 123, test loss: 0.174131\n",
      "Epoch 124, test loss: 0.172090\n",
      "Epoch 125, test loss: 0.172030\n",
      "Epoch 126, test loss: 0.176292\n",
      "Epoch 127, test loss: 0.172136\n",
      "Epoch 128, test loss: 0.172111\n",
      "Epoch 129, test loss: 0.171924\n",
      "Epoch 130, test loss: 0.172355\n",
      "Epoch 131, test loss: 0.172499\n",
      "Epoch 132, test loss: 0.173758\n",
      "Epoch 133, test loss: 0.173827\n",
      "Epoch 134, test loss: 0.172932\n",
      "Epoch 135, test loss: 0.172715\n",
      "Epoch 136, test loss: 0.171978\n",
      "Epoch 137, test loss: 0.171807\n",
      "Epoch 138, test loss: 0.172870\n",
      "Epoch 139, test loss: 0.171873\n",
      "Epoch 140, test loss: 0.171811\n",
      "Epoch 141, test loss: 0.172640\n",
      "Epoch 142, test loss: 0.174289\n",
      "Epoch 143, test loss: 0.171805\n",
      "Epoch 144, test loss: 0.172131\n",
      "Epoch 145, test loss: 0.172153\n",
      "Epoch 146, test loss: 0.172502\n",
      "Epoch 147, test loss: 0.173650\n",
      "Epoch 148, test loss: 0.174146\n",
      "Epoch 149, test loss: 0.171922\n",
      "Epoch 150, test loss: 0.174216\n",
      "Epoch 151, test loss: 0.172118\n",
      "Epoch 152, test loss: 0.171865\n",
      "Epoch 153, test loss: 0.172270\n",
      "Epoch 154, test loss: 0.172546\n",
      "Epoch 155, test loss: 0.171960\n",
      "Epoch 156, test loss: 0.172955\n",
      "Epoch 157, test loss: 0.174499\n",
      "Epoch 158, test loss: 0.171908\n",
      "Epoch 159, test loss: 0.172879\n",
      "Epoch 160, test loss: 0.172639\n",
      "Epoch 161, test loss: 0.172082\n",
      "Epoch 162, test loss: 0.173766\n",
      "Epoch 163, test loss: 0.173290\n",
      "Epoch 164, test loss: 0.172458\n",
      "Epoch 165, test loss: 0.172091\n",
      "Epoch 166, test loss: 0.172291\n",
      "Epoch 167, test loss: 0.174100\n",
      "Epoch 168, test loss: 0.172254\n",
      "Epoch 169, test loss: 0.172155\n",
      "Epoch 170, test loss: 0.172641\n",
      "Epoch 171, test loss: 0.176080\n",
      "Epoch 172, test loss: 0.172295\n",
      "Epoch 173, test loss: 0.173269\n",
      "Epoch 174, test loss: 0.171753\n",
      "Epoch 175, test loss: 0.172178\n",
      "Epoch 176, test loss: 0.173136\n",
      "Epoch 177, test loss: 0.171863\n",
      "Epoch 178, test loss: 0.173349\n",
      "Epoch 179, test loss: 0.172442\n",
      "Epoch 180, test loss: 0.172687\n",
      "Epoch 181, test loss: 0.172990\n",
      "Epoch 182, test loss: 0.172142\n",
      "Epoch 183, test loss: 0.171651\n",
      "Epoch 184, test loss: 0.173084\n",
      "Epoch 185, test loss: 0.172024\n",
      "Epoch 186, test loss: 0.172398\n",
      "Epoch 187, test loss: 0.172789\n",
      "Epoch 188, test loss: 0.172401\n",
      "Epoch 189, test loss: 0.172755\n",
      "Epoch 190, test loss: 0.171909\n",
      "Epoch 191, test loss: 0.171993\n",
      "Epoch 192, test loss: 0.174053\n",
      "Epoch 193, test loss: 0.175721\n",
      "Epoch 194, test loss: 0.172365\n",
      "Epoch 195, test loss: 0.174200\n",
      "Epoch 196, test loss: 0.171892\n",
      "Epoch 197, test loss: 0.173190\n",
      "Epoch 198, test loss: 0.172273\n",
      "Epoch 199, test loss: 0.172082\n",
      "Epoch 200, test loss: 0.172665\n",
      "Epoch 201, test loss: 0.173156\n",
      "Epoch 202, test loss: 0.171554\n",
      "Epoch 203, test loss: 0.173093\n",
      "Epoch 204, test loss: 0.173065\n",
      "Epoch 205, test loss: 0.173984\n",
      "Epoch 206, test loss: 0.172139\n",
      "Epoch 207, test loss: 0.171924\n",
      "Epoch 208, test loss: 0.173684\n",
      "Epoch 209, test loss: 0.172209\n",
      "Epoch 210, test loss: 0.172464\n",
      "Epoch 211, test loss: 0.174358\n",
      "Epoch 212, test loss: 0.173677\n",
      "Epoch 213, test loss: 0.173848\n",
      "Epoch 214, test loss: 0.172292\n",
      "Epoch 215, test loss: 0.176372\n",
      "Epoch 216, test loss: 0.172851\n",
      "Epoch 217, test loss: 0.171408\n",
      "Epoch 218, test loss: 0.171828\n",
      "Epoch 219, test loss: 0.172920\n",
      "Epoch 220, test loss: 0.171875\n",
      "Epoch 221, test loss: 0.173025\n",
      "Epoch 222, test loss: 0.173615\n",
      "Epoch 223, test loss: 0.172030\n",
      "Epoch 224, test loss: 0.175626\n",
      "Epoch 225, test loss: 0.172263\n",
      "Epoch 226, test loss: 0.172340\n",
      "Epoch 227, test loss: 0.171648\n",
      "Epoch 228, test loss: 0.171720\n",
      "Epoch 229, test loss: 0.171749\n",
      "Epoch 230, test loss: 0.172864\n",
      "Epoch 231, test loss: 0.171932\n",
      "Epoch 232, test loss: 0.172104\n",
      "Epoch 233, test loss: 0.172012\n",
      "Epoch 234, test loss: 0.171500\n",
      "Epoch 235, test loss: 0.171947\n",
      "Epoch 236, test loss: 0.172497\n",
      "Epoch 237, test loss: 0.172410\n",
      "Epoch 238, test loss: 0.173081\n",
      "Epoch 239, test loss: 0.172037\n",
      "Reading 44 segments\n",
      "Pretrain data: 19625082.0\n",
      "Building dataset, requesting data from 0 to 798\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6755/99732\n",
      "Found 798 continuous time series\n",
      "Data shape: (106489, 18), Train/test: 106487/2\n",
      "Train test ratio: 53243.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1663\n",
      "Epoch 0, train loss: 0.228272\n",
      "Epoch 1, train loss: 0.174356\n",
      "Epoch 2, train loss: 0.191426\n",
      "Epoch 3, train loss: 0.212398\n",
      "Epoch 4, train loss: 0.168514\n",
      "Epoch 5, train loss: 0.160095\n",
      "Epoch 6, train loss: 0.200306\n",
      "Epoch 7, train loss: 0.215020\n",
      "Epoch 8, train loss: 0.209081\n",
      "Epoch 9, train loss: 0.271346\n",
      "Epoch 10, train loss: 0.157996\n",
      "Epoch 11, train loss: 0.171869\n",
      "Epoch 12, train loss: 0.183511\n",
      "Epoch 13, train loss: 0.187451\n",
      "Epoch 14, train loss: 0.257467\n",
      "Epoch 15, train loss: 0.255001\n",
      "Epoch 16, train loss: 0.210636\n",
      "Epoch 17, train loss: 0.214360\n",
      "Epoch 18, train loss: 0.198519\n",
      "Epoch 19, train loss: 0.208642\n",
      "Epoch 20, train loss: 0.154881\n",
      "Epoch 21, train loss: 0.196036\n",
      "Epoch 22, train loss: 0.179678\n",
      "Epoch 23, train loss: 0.173315\n",
      "Epoch 24, train loss: 0.165145\n",
      "Epoch 25, train loss: 0.206817\n",
      "Epoch 26, train loss: 0.266791\n",
      "Epoch 27, train loss: 0.249612\n",
      "Epoch 28, train loss: 0.178081\n",
      "Epoch 29, train loss: 0.191615\n",
      "Epoch 30, train loss: 0.245468\n",
      "Epoch 31, train loss: 0.219614\n",
      "Epoch 32, train loss: 0.212988\n",
      "Epoch 33, train loss: 0.212885\n",
      "Epoch 34, train loss: 0.214479\n",
      "Epoch 35, train loss: 0.168288\n",
      "Epoch 36, train loss: 0.180209\n",
      "Epoch 37, train loss: 0.189259\n",
      "Epoch 38, train loss: 0.208026\n",
      "Epoch 39, train loss: 0.158538\n",
      "Epoch 40, train loss: 0.180771\n",
      "Epoch 41, train loss: 0.191982\n",
      "Epoch 42, train loss: 0.196347\n",
      "Epoch 43, train loss: 0.220051\n",
      "Epoch 44, train loss: 0.197167\n",
      "Epoch 45, train loss: 0.144378\n",
      "Epoch 46, train loss: 0.167617\n",
      "Epoch 47, train loss: 0.207699\n",
      "Epoch 48, train loss: 0.210953\n",
      "Epoch 49, train loss: 0.228022\n",
      "Epoch 50, train loss: 0.209886\n",
      "Epoch 51, train loss: 0.181919\n",
      "Epoch 52, train loss: 0.255436\n",
      "Epoch 53, train loss: 0.232363\n",
      "Epoch 54, train loss: 0.216989\n",
      "Epoch 55, train loss: 0.268673\n",
      "Epoch 56, train loss: 0.237827\n",
      "Epoch 57, train loss: 0.200143\n",
      "Epoch 58, train loss: 0.234789\n",
      "Epoch 59, train loss: 0.306380\n",
      "Epoch 60, train loss: 0.226764\n",
      "Epoch 61, train loss: 0.194246\n",
      "Epoch 62, train loss: 0.197174\n",
      "Epoch 63, train loss: 0.264568\n",
      "Epoch 64, train loss: 0.238659\n",
      "Epoch 65, train loss: 0.170116\n",
      "Epoch 66, train loss: 0.195982\n",
      "Epoch 67, train loss: 0.148741\n",
      "Epoch 68, train loss: 0.167478\n",
      "Epoch 69, train loss: 0.212919\n",
      "Epoch 70, train loss: 0.203448\n",
      "Epoch 71, train loss: 0.232329\n",
      "Epoch 72, train loss: 0.220712\n",
      "Epoch 73, train loss: 0.138828\n",
      "Epoch 74, train loss: 0.183789\n",
      "Epoch 75, train loss: 0.233603\n",
      "Epoch 76, train loss: 0.228471\n",
      "Epoch 77, train loss: 0.206169\n",
      "Epoch 78, train loss: 0.119225\n",
      "Epoch 79, train loss: 0.199165\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "x here is\n",
      "[[179. 183. 187. ... 241. 246. 248.]\n",
      " [183. 187. 191. ... 246. 248. 250.]\n",
      " [187. 191. 195. ... 248. 250. 252.]\n",
      " ...\n",
      " [187. 185. 181. ... 186. 186. 187.]\n",
      " [185. 181. 180. ... 186. 187. 188.]\n",
      " [181. 180. 178. ... 187. 188. 187.]]\n",
      "y here is\n",
      "[[266. 266. 266. ... 266. 266. 266.]\n",
      " [269. 269. 269. ... 269. 269. 269.]\n",
      " [272. 272. 272. ... 272. 272. 272.]\n",
      " ...\n",
      " [182. 182. 182. ... 182. 182. 182.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " [177. 177. 177. ... 177. 177. 177.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (2238, 18), Train/test: 1/2237\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "x here is\n",
      "[[101.  98. 104. ... 127. 121. 115.]\n",
      " [ 98. 104. 112. ... 121. 115. 111.]\n",
      " [104. 112. 120. ... 115. 111. 109.]\n",
      " ...\n",
      " [128. 133. 137. ... 149. 150. 152.]\n",
      " [133. 137. 140. ... 150. 152. 155.]\n",
      " [137. 140. 142. ... 152. 155. 156.]]\n",
      "y here is\n",
      "[[ 68.  68.  68. ...  68.  68.  68.]\n",
      " [ 64.  64.  64. ...  64.  64.  64.]\n",
      " [ 64.  64.  64. ...  64.  64.  64.]\n",
      " ...\n",
      " [168. 168. 168. ... 168. 168. 168.]\n",
      " [172. 172. 172. ... 172. 172. 172.]\n",
      " [176. 176. 176. ... 176. 176. 176.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 762/9022\n",
      "Found 44 continuous time series\n",
      "Data shape: (9786, 18), Train/test: 9784/2\n",
      "Train test ratio: 4892.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CDBCA0>\n",
      "Epoch 0, test loss: 0.189333\n",
      "Epoch 1, test loss: 0.188896\n",
      "Epoch 2, test loss: 0.187673\n",
      "Epoch 3, test loss: 0.187284\n",
      "Epoch 4, test loss: 0.187422\n",
      "Epoch 5, test loss: 0.199531\n",
      "Epoch 6, test loss: 0.201804\n",
      "Epoch 7, test loss: 0.196127\n",
      "Epoch 8, test loss: 0.194478\n",
      "Epoch 9, test loss: 0.191850\n",
      "Epoch 10, test loss: 0.187280\n",
      "Epoch 11, test loss: 0.186716\n",
      "Epoch 12, test loss: 0.187502\n",
      "Epoch 13, test loss: 0.191209\n",
      "Epoch 14, test loss: 0.189701\n",
      "Epoch 15, test loss: 0.188861\n",
      "Epoch 16, test loss: 0.189276\n",
      "Epoch 17, test loss: 0.196238\n",
      "Epoch 18, test loss: 0.186947\n",
      "Epoch 19, test loss: 0.190582\n",
      "Epoch 20, test loss: 0.190961\n",
      "Epoch 21, test loss: 0.186845\n",
      "Epoch 22, test loss: 0.188275\n",
      "Epoch 23, test loss: 0.192584\n",
      "Epoch 24, test loss: 0.192867\n",
      "Epoch 25, test loss: 0.190715\n",
      "Epoch 26, test loss: 0.187553\n",
      "Epoch 27, test loss: 0.189652\n",
      "Epoch 28, test loss: 0.193019\n",
      "Epoch 29, test loss: 0.189192\n",
      "Epoch 30, test loss: 0.191702\n",
      "Epoch 31, test loss: 0.187740\n",
      "Epoch 32, test loss: 0.190950\n",
      "Epoch 33, test loss: 0.207234\n",
      "Epoch 34, test loss: 0.187233\n",
      "Epoch 35, test loss: 0.186753\n",
      "Epoch 36, test loss: 0.197079\n",
      "Epoch 37, test loss: 0.189798\n",
      "Epoch 38, test loss: 0.196923\n",
      "Epoch 39, test loss: 0.194817\n",
      "Epoch 40, test loss: 0.187358\n",
      "Epoch 41, test loss: 0.188427\n",
      "Epoch 42, test loss: 0.194020\n",
      "Epoch 43, test loss: 0.189096\n",
      "Epoch 44, test loss: 0.190944\n",
      "Epoch 45, test loss: 0.196024\n",
      "Epoch 46, test loss: 0.188114\n",
      "Epoch 47, test loss: 0.188383\n",
      "Epoch 48, test loss: 0.187440\n",
      "Epoch 49, test loss: 0.186913\n",
      "Epoch 50, test loss: 0.191493\n",
      "Epoch 51, test loss: 0.188446\n",
      "Epoch 52, test loss: 0.188877\n",
      "Epoch 53, test loss: 0.189051\n",
      "Epoch 54, test loss: 0.187323\n",
      "Epoch 55, test loss: 0.190696\n",
      "Epoch 56, test loss: 0.187934\n",
      "Epoch 57, test loss: 0.188356\n",
      "Epoch 58, test loss: 0.191076\n",
      "Epoch 59, test loss: 0.187424\n",
      "Epoch 60, test loss: 0.190651\n",
      "Epoch 61, test loss: 0.188665\n",
      "Epoch 62, test loss: 0.188468\n",
      "Epoch 63, test loss: 0.193163\n",
      "Epoch 64, test loss: 0.191623\n",
      "Epoch 65, test loss: 0.193199\n",
      "Epoch 66, test loss: 0.188729\n",
      "Epoch 67, test loss: 0.195098\n",
      "Epoch 68, test loss: 0.196284\n",
      "Epoch 69, test loss: 0.187709\n",
      "Epoch 70, test loss: 0.189278\n",
      "Epoch 71, test loss: 0.190231\n",
      "Epoch 72, test loss: 0.193591\n",
      "Epoch 73, test loss: 0.199550\n",
      "Epoch 74, test loss: 0.192309\n",
      "Epoch 75, test loss: 0.193708\n",
      "Epoch 76, test loss: 0.191647\n",
      "Epoch 77, test loss: 0.187372\n",
      "Epoch 78, test loss: 0.187726\n",
      "Epoch 79, test loss: 0.188557\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CDBCA0>\n",
      "Epoch 0, test loss: 0.186152\n",
      "Epoch 1, test loss: 0.186357\n",
      "Epoch 2, test loss: 0.186431\n",
      "Epoch 3, test loss: 0.186633\n",
      "Epoch 4, test loss: 0.186393\n",
      "Epoch 5, test loss: 0.193237\n",
      "Epoch 6, test loss: 0.188379\n",
      "Epoch 7, test loss: 0.186522\n",
      "Epoch 8, test loss: 0.186033\n",
      "Epoch 9, test loss: 0.187538\n",
      "Epoch 10, test loss: 0.190579\n",
      "Epoch 11, test loss: 0.189399\n",
      "Epoch 12, test loss: 0.186640\n",
      "Epoch 13, test loss: 0.189803\n",
      "Epoch 14, test loss: 0.189338\n",
      "Epoch 15, test loss: 0.187654\n",
      "Epoch 16, test loss: 0.194052\n",
      "Epoch 17, test loss: 0.189823\n",
      "Epoch 18, test loss: 0.186676\n",
      "Epoch 19, test loss: 0.188021\n",
      "Epoch 20, test loss: 0.186496\n",
      "Epoch 21, test loss: 0.186693\n",
      "Epoch 22, test loss: 0.187455\n",
      "Epoch 23, test loss: 0.191977\n",
      "Epoch 24, test loss: 0.186798\n",
      "Epoch 25, test loss: 0.188749\n",
      "Epoch 26, test loss: 0.191892\n",
      "Epoch 27, test loss: 0.196677\n",
      "Epoch 28, test loss: 0.187705\n",
      "Epoch 29, test loss: 0.187073\n",
      "Epoch 30, test loss: 0.186881\n",
      "Epoch 31, test loss: 0.187247\n",
      "Epoch 32, test loss: 0.193713\n",
      "Epoch 33, test loss: 0.190354\n",
      "Epoch 34, test loss: 0.189340\n",
      "Epoch 35, test loss: 0.187537\n",
      "Epoch 36, test loss: 0.186585\n",
      "Epoch 37, test loss: 0.189409\n",
      "Epoch 38, test loss: 0.190114\n",
      "Epoch 39, test loss: 0.188533\n",
      "Epoch 40, test loss: 0.188883\n",
      "Epoch 41, test loss: 0.188276\n",
      "Epoch 42, test loss: 0.186937\n",
      "Epoch 43, test loss: 0.194151\n",
      "Epoch 44, test loss: 0.187491\n",
      "Epoch 45, test loss: 0.188181\n",
      "Epoch 46, test loss: 0.189269\n",
      "Epoch 47, test loss: 0.190369\n",
      "Epoch 48, test loss: 0.191769\n",
      "Epoch 49, test loss: 0.188651\n",
      "Epoch 50, test loss: 0.187597\n",
      "Epoch 51, test loss: 0.188942\n",
      "Epoch 52, test loss: 0.191090\n",
      "Epoch 53, test loss: 0.187594\n",
      "Epoch 54, test loss: 0.187561\n",
      "Epoch 55, test loss: 0.186871\n",
      "Epoch 56, test loss: 0.187984\n",
      "Epoch 57, test loss: 0.187437\n",
      "Epoch 58, test loss: 0.191981\n",
      "Epoch 59, test loss: 0.189297\n",
      "Epoch 60, test loss: 0.187411\n",
      "Epoch 61, test loss: 0.189657\n",
      "Epoch 62, test loss: 0.188338\n",
      "Epoch 63, test loss: 0.189656\n",
      "Epoch 64, test loss: 0.187808\n",
      "Epoch 65, test loss: 0.188124\n",
      "Epoch 66, test loss: 0.191654\n",
      "Epoch 67, test loss: 0.187661\n",
      "Epoch 68, test loss: 0.187058\n",
      "Epoch 69, test loss: 0.187127\n",
      "Epoch 70, test loss: 0.190609\n",
      "Epoch 71, test loss: 0.188322\n",
      "Epoch 72, test loss: 0.186884\n",
      "Epoch 73, test loss: 0.188773\n",
      "Epoch 74, test loss: 0.187823\n",
      "Epoch 75, test loss: 0.188161\n",
      "Epoch 76, test loss: 0.187060\n",
      "Epoch 77, test loss: 0.187895\n",
      "Epoch 78, test loss: 0.187974\n",
      "Epoch 79, test loss: 0.186832\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CDBCA0>\n",
      "Epoch 0, test loss: 0.292956\n",
      "Epoch 1, test loss: 0.225500\n",
      "Epoch 2, test loss: 0.200849\n",
      "Epoch 3, test loss: 0.200235\n",
      "Epoch 4, test loss: 0.194314\n",
      "Epoch 5, test loss: 0.193378\n",
      "Epoch 6, test loss: 0.193853\n",
      "Epoch 7, test loss: 0.194871\n",
      "Epoch 8, test loss: 0.197942\n",
      "Epoch 9, test loss: 0.194051\n",
      "Epoch 10, test loss: 0.198645\n",
      "Epoch 11, test loss: 0.193309\n",
      "Epoch 12, test loss: 0.195029\n",
      "Epoch 13, test loss: 0.192339\n",
      "Epoch 14, test loss: 0.194096\n",
      "Epoch 15, test loss: 0.196944\n",
      "Epoch 16, test loss: 0.192459\n",
      "Epoch 17, test loss: 0.192163\n",
      "Epoch 18, test loss: 0.194438\n",
      "Epoch 19, test loss: 0.191095\n",
      "Epoch 20, test loss: 0.192146\n",
      "Epoch 21, test loss: 0.193007\n",
      "Epoch 22, test loss: 0.198357\n",
      "Epoch 23, test loss: 0.197637\n",
      "Epoch 24, test loss: 0.194949\n",
      "Epoch 25, test loss: 0.192252\n",
      "Epoch 26, test loss: 0.194038\n",
      "Epoch 27, test loss: 0.193872\n",
      "Epoch 28, test loss: 0.191396\n",
      "Epoch 29, test loss: 0.192456\n",
      "Epoch 30, test loss: 0.195680\n",
      "Epoch 31, test loss: 0.192794\n",
      "Epoch 32, test loss: 0.196241\n",
      "Epoch 33, test loss: 0.192526\n",
      "Epoch 34, test loss: 0.194881\n",
      "Epoch 35, test loss: 0.191839\n",
      "Epoch 36, test loss: 0.192005\n",
      "Epoch 37, test loss: 0.193229\n",
      "Epoch 38, test loss: 0.192389\n",
      "Epoch 39, test loss: 0.195801\n",
      "Epoch 40, test loss: 0.194187\n",
      "Epoch 41, test loss: 0.192216\n",
      "Epoch 42, test loss: 0.191725\n",
      "Epoch 43, test loss: 0.192590\n",
      "Epoch 44, test loss: 0.192470\n",
      "Epoch 45, test loss: 0.194862\n",
      "Epoch 46, test loss: 0.194011\n",
      "Epoch 47, test loss: 0.191878\n",
      "Epoch 48, test loss: 0.191489\n",
      "Epoch 49, test loss: 0.195801\n",
      "Epoch 50, test loss: 0.194890\n",
      "Epoch 51, test loss: 0.191725\n",
      "Epoch 52, test loss: 0.192742\n",
      "Epoch 53, test loss: 0.193574\n",
      "Epoch 54, test loss: 0.192548\n",
      "Epoch 55, test loss: 0.190870\n",
      "Epoch 56, test loss: 0.191894\n",
      "Epoch 57, test loss: 0.193627\n",
      "Epoch 58, test loss: 0.191351\n",
      "Epoch 59, test loss: 0.191977\n",
      "Epoch 60, test loss: 0.194335\n",
      "Epoch 61, test loss: 0.192051\n",
      "Epoch 62, test loss: 0.193169\n",
      "Epoch 63, test loss: 0.193678\n",
      "Epoch 64, test loss: 0.190866\n",
      "Epoch 65, test loss: 0.191363\n",
      "Epoch 66, test loss: 0.192000\n",
      "Epoch 67, test loss: 0.192015\n",
      "Epoch 68, test loss: 0.194660\n",
      "Epoch 69, test loss: 0.191585\n",
      "Epoch 70, test loss: 0.192036\n",
      "Epoch 71, test loss: 0.196617\n",
      "Epoch 72, test loss: 0.192508\n",
      "Epoch 73, test loss: 0.191543\n",
      "Epoch 74, test loss: 0.198309\n",
      "Epoch 75, test loss: 0.191751\n",
      "Epoch 76, test loss: 0.191093\n",
      "Epoch 77, test loss: 0.203094\n",
      "Epoch 78, test loss: 0.192898\n",
      "Epoch 79, test loss: 0.190767\n",
      "Epoch 80, test loss: 0.191461\n",
      "Epoch 81, test loss: 0.192141\n",
      "Epoch 82, test loss: 0.192500\n",
      "Epoch 83, test loss: 0.191423\n",
      "Epoch 84, test loss: 0.191164\n",
      "Epoch 85, test loss: 0.199881\n",
      "Epoch 86, test loss: 0.191320\n",
      "Epoch 87, test loss: 0.191677\n",
      "Epoch 88, test loss: 0.190914\n",
      "Epoch 89, test loss: 0.191920\n",
      "Epoch 90, test loss: 0.193886\n",
      "Epoch 91, test loss: 0.190332\n",
      "Epoch 92, test loss: 0.192892\n",
      "Epoch 93, test loss: 0.191458\n",
      "Epoch 94, test loss: 0.191427\n",
      "Epoch 95, test loss: 0.199711\n",
      "Epoch 96, test loss: 0.192886\n",
      "Epoch 97, test loss: 0.192142\n",
      "Epoch 98, test loss: 0.191773\n",
      "Epoch 99, test loss: 0.194330\n",
      "Epoch 100, test loss: 0.193483\n",
      "Epoch 101, test loss: 0.194384\n",
      "Epoch 102, test loss: 0.194051\n",
      "Epoch 103, test loss: 0.195536\n",
      "Epoch 104, test loss: 0.194998\n",
      "Epoch 105, test loss: 0.192852\n",
      "Epoch 106, test loss: 0.191893\n",
      "Epoch 107, test loss: 0.192416\n",
      "Epoch 108, test loss: 0.191956\n",
      "Epoch 109, test loss: 0.190314\n",
      "Epoch 110, test loss: 0.195153\n",
      "Epoch 111, test loss: 0.196186\n",
      "Epoch 112, test loss: 0.193275\n",
      "Epoch 113, test loss: 0.191861\n",
      "Epoch 114, test loss: 0.195452\n",
      "Epoch 115, test loss: 0.193416\n",
      "Epoch 116, test loss: 0.190667\n",
      "Epoch 117, test loss: 0.191763\n",
      "Epoch 118, test loss: 0.192356\n",
      "Epoch 119, test loss: 0.192040\n",
      "Epoch 120, test loss: 0.191511\n",
      "Epoch 121, test loss: 0.191889\n",
      "Epoch 122, test loss: 0.189946\n",
      "Epoch 123, test loss: 0.191265\n",
      "Epoch 124, test loss: 0.192727\n",
      "Epoch 125, test loss: 0.190602\n",
      "Epoch 126, test loss: 0.192478\n",
      "Epoch 127, test loss: 0.190530\n",
      "Epoch 128, test loss: 0.193291\n",
      "Epoch 129, test loss: 0.191449\n",
      "Epoch 130, test loss: 0.192320\n",
      "Epoch 131, test loss: 0.193632\n",
      "Epoch 132, test loss: 0.194120\n",
      "Epoch 133, test loss: 0.190904\n",
      "Epoch 134, test loss: 0.192091\n",
      "Epoch 135, test loss: 0.191033\n",
      "Epoch 136, test loss: 0.191431\n",
      "Epoch 137, test loss: 0.199673\n",
      "Epoch 138, test loss: 0.190810\n",
      "Epoch 139, test loss: 0.190551\n",
      "Epoch 140, test loss: 0.195395\n",
      "Epoch 141, test loss: 0.192427\n",
      "Epoch 142, test loss: 0.191199\n",
      "Epoch 143, test loss: 0.190734\n",
      "Epoch 144, test loss: 0.191805\n",
      "Epoch 145, test loss: 0.194474\n",
      "Epoch 146, test loss: 0.192820\n",
      "Epoch 147, test loss: 0.191567\n",
      "Epoch 148, test loss: 0.190837\n",
      "Epoch 149, test loss: 0.194877\n",
      "Epoch 150, test loss: 0.191265\n",
      "Epoch 151, test loss: 0.190195\n",
      "Epoch 152, test loss: 0.194901\n",
      "Epoch 153, test loss: 0.201050\n",
      "Epoch 154, test loss: 0.190625\n",
      "Epoch 155, test loss: 0.192243\n",
      "Epoch 156, test loss: 0.190273\n",
      "Epoch 157, test loss: 0.191260\n",
      "Epoch 158, test loss: 0.191476\n",
      "Epoch 159, test loss: 0.190172\n",
      "Epoch 160, test loss: 0.190801\n",
      "Epoch 161, test loss: 0.191168\n",
      "Epoch 162, test loss: 0.192643\n",
      "Epoch 163, test loss: 0.194084\n",
      "Epoch 164, test loss: 0.191270\n",
      "Epoch 165, test loss: 0.191066\n",
      "Epoch 166, test loss: 0.192749\n",
      "Epoch 167, test loss: 0.191024\n",
      "Epoch 168, test loss: 0.191124\n",
      "Epoch 169, test loss: 0.192290\n",
      "Epoch 170, test loss: 0.192130\n",
      "Epoch 171, test loss: 0.190310\n",
      "Epoch 172, test loss: 0.190029\n",
      "Epoch 173, test loss: 0.197651\n",
      "Epoch 174, test loss: 0.190442\n",
      "Epoch 175, test loss: 0.190495\n",
      "Epoch 176, test loss: 0.191828\n",
      "Epoch 177, test loss: 0.192820\n",
      "Epoch 178, test loss: 0.191748\n",
      "Epoch 179, test loss: 0.191284\n",
      "Epoch 180, test loss: 0.196994\n",
      "Epoch 181, test loss: 0.190142\n",
      "Epoch 182, test loss: 0.191136\n",
      "Epoch 183, test loss: 0.189748\n",
      "Epoch 184, test loss: 0.189374\n",
      "Epoch 185, test loss: 0.190258\n",
      "Epoch 186, test loss: 0.192738\n",
      "Epoch 187, test loss: 0.190915\n",
      "Epoch 188, test loss: 0.191337\n",
      "Epoch 189, test loss: 0.189642\n",
      "Epoch 190, test loss: 0.195649\n",
      "Epoch 191, test loss: 0.194063\n",
      "Epoch 192, test loss: 0.191412\n",
      "Epoch 193, test loss: 0.191472\n",
      "Epoch 194, test loss: 0.192317\n",
      "Epoch 195, test loss: 0.192730\n",
      "Epoch 196, test loss: 0.189873\n",
      "Epoch 197, test loss: 0.199987\n",
      "Epoch 198, test loss: 0.190567\n",
      "Epoch 199, test loss: 0.193692\n",
      "Epoch 200, test loss: 0.192157\n",
      "Epoch 201, test loss: 0.190825\n",
      "Epoch 202, test loss: 0.190548\n",
      "Epoch 203, test loss: 0.190101\n",
      "Epoch 204, test loss: 0.190399\n",
      "Epoch 205, test loss: 0.193460\n",
      "Epoch 206, test loss: 0.190283\n",
      "Epoch 207, test loss: 0.192645\n",
      "Epoch 208, test loss: 0.190423\n",
      "Epoch 209, test loss: 0.190784\n",
      "Epoch 210, test loss: 0.190042\n",
      "Epoch 211, test loss: 0.189486\n",
      "Epoch 212, test loss: 0.190464\n",
      "Epoch 213, test loss: 0.191966\n",
      "Epoch 214, test loss: 0.189841\n",
      "Epoch 215, test loss: 0.190754\n",
      "Epoch 216, test loss: 0.191043\n",
      "Epoch 217, test loss: 0.192222\n",
      "Epoch 218, test loss: 0.189495\n",
      "Epoch 219, test loss: 0.190197\n",
      "Epoch 220, test loss: 0.189204\n",
      "Epoch 221, test loss: 0.190846\n",
      "Epoch 222, test loss: 0.190867\n",
      "Epoch 223, test loss: 0.189537\n",
      "Epoch 224, test loss: 0.188593\n",
      "Epoch 225, test loss: 0.188577\n",
      "Epoch 226, test loss: 0.190087\n",
      "Epoch 227, test loss: 0.190416\n",
      "Epoch 228, test loss: 0.193749\n",
      "Epoch 229, test loss: 0.189561\n",
      "Epoch 230, test loss: 0.191057\n",
      "Epoch 231, test loss: 0.191994\n",
      "Epoch 232, test loss: 0.190307\n",
      "Epoch 233, test loss: 0.192280\n",
      "Epoch 234, test loss: 0.190123\n",
      "Epoch 235, test loss: 0.194755\n",
      "Epoch 236, test loss: 0.188967\n",
      "Epoch 237, test loss: 0.194919\n",
      "Epoch 238, test loss: 0.191354\n",
      "Epoch 239, test loss: 0.189367\n",
      "Pretrain data: 19653653.0\n",
      "Building dataset, requesting data from 0 to 820\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6840/97815\n",
      "Found 820 continuous time series\n",
      "Data shape: (104657, 18), Train/test: 104655/2\n",
      "Train test ratio: 52327.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1635\n",
      "Epoch 0, train loss: 0.320729\n",
      "Epoch 1, train loss: 0.287395\n",
      "Epoch 2, train loss: 0.224429\n",
      "Epoch 3, train loss: 0.231004\n",
      "Epoch 4, train loss: 0.219420\n",
      "Epoch 5, train loss: 0.179621\n",
      "Epoch 6, train loss: 0.246155\n",
      "Epoch 7, train loss: 0.201407\n",
      "Epoch 8, train loss: 0.232376\n",
      "Epoch 9, train loss: 0.229959\n",
      "Epoch 10, train loss: 0.184865\n",
      "Epoch 11, train loss: 0.254065\n",
      "Epoch 12, train loss: 0.206299\n",
      "Epoch 13, train loss: 0.151879\n",
      "Epoch 14, train loss: 0.176349\n",
      "Epoch 15, train loss: 0.208349\n",
      "Epoch 16, train loss: 0.167878\n",
      "Epoch 17, train loss: 0.151645\n",
      "Epoch 18, train loss: 0.234796\n",
      "Epoch 19, train loss: 0.218614\n",
      "Epoch 20, train loss: 0.220910\n",
      "Epoch 21, train loss: 0.210515\n",
      "Epoch 22, train loss: 0.207937\n",
      "Epoch 23, train loss: 0.229909\n",
      "Epoch 24, train loss: 0.233242\n",
      "Epoch 25, train loss: 0.198850\n",
      "Epoch 26, train loss: 0.203187\n",
      "Epoch 27, train loss: 0.246342\n",
      "Epoch 28, train loss: 0.217762\n",
      "Epoch 29, train loss: 0.199907\n",
      "Epoch 30, train loss: 0.199627\n",
      "Epoch 31, train loss: 0.222308\n",
      "Epoch 32, train loss: 0.229518\n",
      "Epoch 33, train loss: 0.216306\n",
      "Epoch 34, train loss: 0.167441\n",
      "Epoch 35, train loss: 0.198056\n",
      "Epoch 36, train loss: 0.225110\n",
      "Epoch 37, train loss: 0.189634\n",
      "Epoch 38, train loss: 0.194275\n",
      "Epoch 39, train loss: 0.182064\n",
      "Epoch 40, train loss: 0.251154\n",
      "Epoch 41, train loss: 0.249545\n",
      "Epoch 42, train loss: 0.289187\n",
      "Epoch 43, train loss: 0.181313\n",
      "Epoch 44, train loss: 0.175142\n",
      "Epoch 45, train loss: 0.201173\n",
      "Epoch 46, train loss: 0.246027\n",
      "Epoch 47, train loss: 0.274643\n",
      "Epoch 48, train loss: 0.177166\n",
      "Epoch 49, train loss: 0.218164\n",
      "Epoch 50, train loss: 0.198237\n",
      "Epoch 51, train loss: 0.201431\n",
      "Epoch 52, train loss: 0.227040\n",
      "Epoch 53, train loss: 0.244464\n",
      "Epoch 54, train loss: 0.228097\n",
      "Epoch 55, train loss: 0.244233\n",
      "Epoch 56, train loss: 0.190181\n",
      "Epoch 57, train loss: 0.199007\n",
      "Epoch 58, train loss: 0.218060\n",
      "Epoch 59, train loss: 0.221620\n",
      "Epoch 60, train loss: 0.236791\n",
      "Epoch 61, train loss: 0.147648\n",
      "Epoch 62, train loss: 0.176184\n",
      "Epoch 63, train loss: 0.132878\n",
      "Epoch 64, train loss: 0.235839\n",
      "Epoch 65, train loss: 0.173508\n",
      "Epoch 66, train loss: 0.282893\n",
      "Epoch 67, train loss: 0.209600\n",
      "Epoch 68, train loss: 0.224113\n",
      "Epoch 69, train loss: 0.242952\n",
      "Epoch 70, train loss: 0.189106\n",
      "Epoch 71, train loss: 0.207613\n",
      "Epoch 72, train loss: 0.213583\n",
      "Epoch 73, train loss: 0.256423\n",
      "Epoch 74, train loss: 0.193238\n",
      "Epoch 75, train loss: 0.165657\n",
      "Epoch 76, train loss: 0.231307\n",
      "Epoch 77, train loss: 0.237145\n",
      "Epoch 78, train loss: 0.246891\n",
      "Epoch 79, train loss: 0.200157\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "x here is\n",
      "[[239. 238. 235. ... 204. 203. 202.]\n",
      " [238. 235. 233. ... 203. 202. 203.]\n",
      " [235. 233. 231. ... 202. 203. 203.]\n",
      " ...\n",
      " [178. 170. 166. ... 152. 151. 149.]\n",
      " [170. 166. 162. ... 151. 149. 149.]\n",
      " [166. 162. 156. ... 149. 149. 145.]]\n",
      "y here is\n",
      "[[197. 197. 197. ... 197. 197. 197.]\n",
      " [199. 199. 199. ... 199. 199. 199.]\n",
      " [206. 206. 206. ... 206. 206. 206.]\n",
      " ...\n",
      " [144. 144. 144. ... 144. 144. 144.]\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [145. 145. 145. ... 145. 145. 145.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (2478, 18), Train/test: 1/2477\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "x here is\n",
      "[[219. 229. 224. ... 189. 190. 183.]\n",
      " [229. 224. 221. ... 190. 183. 172.]\n",
      " [224. 221. 215. ... 183. 172. 166.]\n",
      " ...\n",
      " [202. 200. 200. ... 237. 241. 243.]\n",
      " [200. 200. 200. ... 241. 243. 251.]\n",
      " [200. 200. 201. ... 243. 251. 257.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [159. 159. 159. ... 159. 159. 159.]\n",
      " [156. 156. 156. ... 156. 156. 156.]\n",
      " ...\n",
      " [250. 250. 250. ... 250. 250. 250.]\n",
      " [246. 246. 246. ... 246. 246. 246.]\n",
      " [240. 240. 240. ... 240. 240. 240.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 677/10939\n",
      "Found 22 continuous time series\n",
      "Data shape: (11618, 18), Train/test: 11616/2\n",
      "Train test ratio: 5808.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901D36A0>\n",
      "Epoch 0, test loss: 0.178517\n",
      "Epoch 1, test loss: 0.178556\n",
      "Epoch 2, test loss: 0.179189\n",
      "Epoch 3, test loss: 0.180873\n",
      "Epoch 4, test loss: 0.178933\n",
      "Epoch 5, test loss: 0.181462\n",
      "Epoch 6, test loss: 0.179309\n",
      "Epoch 7, test loss: 0.179819\n",
      "Epoch 8, test loss: 0.188936\n",
      "Epoch 9, test loss: 0.181327\n",
      "Epoch 10, test loss: 0.180231\n",
      "Epoch 11, test loss: 0.180466\n",
      "Epoch 12, test loss: 0.179688\n",
      "Epoch 13, test loss: 0.180116\n",
      "Epoch 14, test loss: 0.180102\n",
      "Epoch 15, test loss: 0.181047\n",
      "Epoch 16, test loss: 0.182273\n",
      "Epoch 17, test loss: 0.182795\n",
      "Epoch 18, test loss: 0.179822\n",
      "Epoch 19, test loss: 0.180980\n",
      "Epoch 20, test loss: 0.180024\n",
      "Epoch 21, test loss: 0.179889\n",
      "Epoch 22, test loss: 0.182643\n",
      "Epoch 23, test loss: 0.184001\n",
      "Epoch 24, test loss: 0.180440\n",
      "Epoch 25, test loss: 0.183845\n",
      "Epoch 26, test loss: 0.179715\n",
      "Epoch 27, test loss: 0.181558\n",
      "Epoch 28, test loss: 0.180450\n",
      "Epoch 29, test loss: 0.181814\n",
      "Epoch 30, test loss: 0.180185\n",
      "Epoch 31, test loss: 0.184772\n",
      "Epoch 32, test loss: 0.188807\n",
      "Epoch 33, test loss: 0.182646\n",
      "Epoch 34, test loss: 0.184683\n",
      "Epoch 35, test loss: 0.180109\n",
      "Epoch 36, test loss: 0.189916\n",
      "Epoch 37, test loss: 0.181056\n",
      "Epoch 38, test loss: 0.186701\n",
      "Epoch 39, test loss: 0.180849\n",
      "Epoch 40, test loss: 0.184013\n",
      "Epoch 41, test loss: 0.183053\n",
      "Epoch 42, test loss: 0.187655\n",
      "Epoch 43, test loss: 0.187639\n",
      "Epoch 44, test loss: 0.186499\n",
      "Epoch 45, test loss: 0.185026\n",
      "Epoch 46, test loss: 0.184320\n",
      "Epoch 47, test loss: 0.181666\n",
      "Epoch 48, test loss: 0.181395\n",
      "Epoch 49, test loss: 0.183434\n",
      "Epoch 50, test loss: 0.180809\n",
      "Epoch 51, test loss: 0.180675\n",
      "Epoch 52, test loss: 0.182266\n",
      "Epoch 53, test loss: 0.188821\n",
      "Epoch 54, test loss: 0.181624\n",
      "Epoch 55, test loss: 0.181986\n",
      "Epoch 56, test loss: 0.181612\n",
      "Epoch 57, test loss: 0.193303\n",
      "Epoch 58, test loss: 0.182277\n",
      "Epoch 59, test loss: 0.181167\n",
      "Epoch 60, test loss: 0.181724\n",
      "Epoch 61, test loss: 0.182387\n",
      "Epoch 62, test loss: 0.181209\n",
      "Epoch 63, test loss: 0.181666\n",
      "Epoch 64, test loss: 0.182273\n",
      "Epoch 65, test loss: 0.181559\n",
      "Epoch 66, test loss: 0.180940\n",
      "Epoch 67, test loss: 0.181000\n",
      "Epoch 68, test loss: 0.181268\n",
      "Epoch 69, test loss: 0.189656\n",
      "Epoch 70, test loss: 0.182082\n",
      "Epoch 71, test loss: 0.181614\n",
      "Epoch 72, test loss: 0.183958\n",
      "Epoch 73, test loss: 0.190760\n",
      "Epoch 74, test loss: 0.181954\n",
      "Epoch 75, test loss: 0.187469\n",
      "Epoch 76, test loss: 0.186045\n",
      "Epoch 77, test loss: 0.182394\n",
      "Epoch 78, test loss: 0.182218\n",
      "Epoch 79, test loss: 0.182770\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901D36A0>\n",
      "Epoch 0, test loss: 0.183286\n",
      "Epoch 1, test loss: 0.179432\n",
      "Epoch 2, test loss: 0.178790\n",
      "Epoch 3, test loss: 0.178848\n",
      "Epoch 4, test loss: 0.180854\n",
      "Epoch 5, test loss: 0.179911\n",
      "Epoch 6, test loss: 0.179952\n",
      "Epoch 7, test loss: 0.179442\n",
      "Epoch 8, test loss: 0.183925\n",
      "Epoch 9, test loss: 0.178871\n",
      "Epoch 10, test loss: 0.180429\n",
      "Epoch 11, test loss: 0.186482\n",
      "Epoch 12, test loss: 0.182248\n",
      "Epoch 13, test loss: 0.181585\n",
      "Epoch 14, test loss: 0.179889\n",
      "Epoch 15, test loss: 0.180456\n",
      "Epoch 16, test loss: 0.179486\n",
      "Epoch 17, test loss: 0.179741\n",
      "Epoch 18, test loss: 0.181882\n",
      "Epoch 19, test loss: 0.181350\n",
      "Epoch 20, test loss: 0.181932\n",
      "Epoch 21, test loss: 0.180212\n",
      "Epoch 22, test loss: 0.179742\n",
      "Epoch 23, test loss: 0.183987\n",
      "Epoch 24, test loss: 0.181863\n",
      "Epoch 25, test loss: 0.180014\n",
      "Epoch 26, test loss: 0.181022\n",
      "Epoch 27, test loss: 0.180926\n",
      "Epoch 28, test loss: 0.187044\n",
      "Epoch 29, test loss: 0.180335\n",
      "Epoch 30, test loss: 0.180478\n",
      "Epoch 31, test loss: 0.182720\n",
      "Epoch 32, test loss: 0.180279\n",
      "Epoch 33, test loss: 0.181348\n",
      "Epoch 34, test loss: 0.182122\n",
      "Epoch 35, test loss: 0.180205\n",
      "Epoch 36, test loss: 0.181515\n",
      "Epoch 37, test loss: 0.182045\n",
      "Epoch 38, test loss: 0.181130\n",
      "Epoch 39, test loss: 0.182149\n",
      "Epoch 40, test loss: 0.181809\n",
      "Epoch 41, test loss: 0.185883\n",
      "Epoch 42, test loss: 0.182984\n",
      "Epoch 43, test loss: 0.183161\n",
      "Epoch 44, test loss: 0.181132\n",
      "Epoch 45, test loss: 0.182905\n",
      "Epoch 46, test loss: 0.183961\n",
      "Epoch 47, test loss: 0.180896\n",
      "Epoch 48, test loss: 0.181459\n",
      "Epoch 49, test loss: 0.181608\n",
      "Epoch 50, test loss: 0.183871\n",
      "Epoch 51, test loss: 0.184572\n",
      "Epoch 52, test loss: 0.184345\n",
      "Epoch 53, test loss: 0.183184\n",
      "Epoch 54, test loss: 0.181112\n",
      "Epoch 55, test loss: 0.181561\n",
      "Epoch 56, test loss: 0.185353\n",
      "Epoch 57, test loss: 0.182318\n",
      "Epoch 58, test loss: 0.181095\n",
      "Epoch 59, test loss: 0.180922\n",
      "Epoch 60, test loss: 0.182508\n",
      "Epoch 61, test loss: 0.181664\n",
      "Epoch 62, test loss: 0.182197\n",
      "Epoch 63, test loss: 0.181600\n",
      "Epoch 64, test loss: 0.181398\n",
      "Epoch 65, test loss: 0.181696\n",
      "Epoch 66, test loss: 0.182086\n",
      "Epoch 67, test loss: 0.182075\n",
      "Epoch 68, test loss: 0.186154\n",
      "Epoch 69, test loss: 0.182530\n",
      "Epoch 70, test loss: 0.183680\n",
      "Epoch 71, test loss: 0.181793\n",
      "Epoch 72, test loss: 0.183015\n",
      "Epoch 73, test loss: 0.182576\n",
      "Epoch 74, test loss: 0.180986\n",
      "Epoch 75, test loss: 0.181657\n",
      "Epoch 76, test loss: 0.181837\n",
      "Epoch 77, test loss: 0.180985\n",
      "Epoch 78, test loss: 0.183142\n",
      "Epoch 79, test loss: 0.182072\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901D36A0>\n",
      "Epoch 0, test loss: 0.244631\n",
      "Epoch 1, test loss: 0.197533\n",
      "Epoch 2, test loss: 0.188581\n",
      "Epoch 3, test loss: 0.189141\n",
      "Epoch 4, test loss: 0.188673\n",
      "Epoch 5, test loss: 0.188365\n",
      "Epoch 6, test loss: 0.189746\n",
      "Epoch 7, test loss: 0.191908\n",
      "Epoch 8, test loss: 0.190915\n",
      "Epoch 9, test loss: 0.190361\n",
      "Epoch 10, test loss: 0.191670\n",
      "Epoch 11, test loss: 0.192983\n",
      "Epoch 12, test loss: 0.196794\n",
      "Epoch 13, test loss: 0.191637\n",
      "Epoch 14, test loss: 0.190625\n",
      "Epoch 15, test loss: 0.195117\n",
      "Epoch 16, test loss: 0.192249\n",
      "Epoch 17, test loss: 0.191743\n",
      "Epoch 18, test loss: 0.191106\n",
      "Epoch 19, test loss: 0.191958\n",
      "Epoch 20, test loss: 0.195797\n",
      "Epoch 21, test loss: 0.191085\n",
      "Epoch 22, test loss: 0.191652\n",
      "Epoch 23, test loss: 0.193889\n",
      "Epoch 24, test loss: 0.194772\n",
      "Epoch 25, test loss: 0.197112\n",
      "Epoch 26, test loss: 0.192148\n",
      "Epoch 27, test loss: 0.191344\n",
      "Epoch 28, test loss: 0.194116\n",
      "Epoch 29, test loss: 0.191429\n",
      "Epoch 30, test loss: 0.191923\n",
      "Epoch 31, test loss: 0.191802\n",
      "Epoch 32, test loss: 0.193750\n",
      "Epoch 33, test loss: 0.192166\n",
      "Epoch 34, test loss: 0.191770\n",
      "Epoch 35, test loss: 0.194176\n",
      "Epoch 36, test loss: 0.194790\n",
      "Epoch 37, test loss: 0.191189\n",
      "Epoch 38, test loss: 0.196943\n",
      "Epoch 39, test loss: 0.191983\n",
      "Epoch 40, test loss: 0.190928\n",
      "Epoch 41, test loss: 0.195990\n",
      "Epoch 42, test loss: 0.191628\n",
      "Epoch 43, test loss: 0.191717\n",
      "Epoch 44, test loss: 0.193170\n",
      "Epoch 45, test loss: 0.191231\n",
      "Epoch 46, test loss: 0.193205\n",
      "Epoch 47, test loss: 0.192375\n",
      "Epoch 48, test loss: 0.192226\n",
      "Epoch 49, test loss: 0.192724\n",
      "Epoch 50, test loss: 0.193164\n",
      "Epoch 51, test loss: 0.191668\n",
      "Epoch 52, test loss: 0.192940\n",
      "Epoch 53, test loss: 0.192539\n",
      "Epoch 54, test loss: 0.191404\n",
      "Epoch 55, test loss: 0.191277\n",
      "Epoch 56, test loss: 0.192250\n",
      "Epoch 57, test loss: 0.191923\n",
      "Epoch 58, test loss: 0.192652\n",
      "Epoch 59, test loss: 0.191352\n",
      "Epoch 60, test loss: 0.190988\n",
      "Epoch 61, test loss: 0.191925\n",
      "Epoch 62, test loss: 0.192189\n",
      "Epoch 63, test loss: 0.192342\n",
      "Epoch 64, test loss: 0.191805\n",
      "Epoch 65, test loss: 0.190856\n",
      "Epoch 66, test loss: 0.197656\n",
      "Epoch 67, test loss: 0.191535\n",
      "Epoch 68, test loss: 0.190762\n",
      "Epoch 69, test loss: 0.190767\n",
      "Epoch 70, test loss: 0.190900\n",
      "Epoch 71, test loss: 0.192307\n",
      "Epoch 72, test loss: 0.191822\n",
      "Epoch 73, test loss: 0.196679\n",
      "Epoch 74, test loss: 0.193407\n",
      "Epoch 75, test loss: 0.192081\n",
      "Epoch 76, test loss: 0.191078\n",
      "Epoch 77, test loss: 0.191024\n",
      "Epoch 78, test loss: 0.190971\n",
      "Epoch 79, test loss: 0.190032\n",
      "Epoch 80, test loss: 0.191723\n",
      "Epoch 81, test loss: 0.192155\n",
      "Epoch 82, test loss: 0.195762\n",
      "Epoch 83, test loss: 0.191819\n",
      "Epoch 84, test loss: 0.190008\n",
      "Epoch 85, test loss: 0.197125\n",
      "Epoch 86, test loss: 0.191399\n",
      "Epoch 87, test loss: 0.191755\n",
      "Epoch 88, test loss: 0.189996\n",
      "Epoch 89, test loss: 0.190359\n",
      "Epoch 90, test loss: 0.191016\n",
      "Epoch 91, test loss: 0.194683\n",
      "Epoch 92, test loss: 0.189945\n",
      "Epoch 93, test loss: 0.190226\n",
      "Epoch 94, test loss: 0.192905\n",
      "Epoch 95, test loss: 0.192412\n",
      "Epoch 96, test loss: 0.191911\n",
      "Epoch 97, test loss: 0.190867\n",
      "Epoch 98, test loss: 0.191069\n",
      "Epoch 99, test loss: 0.196270\n",
      "Epoch 100, test loss: 0.190757\n",
      "Epoch 101, test loss: 0.189983\n",
      "Epoch 102, test loss: 0.192541\n",
      "Epoch 103, test loss: 0.193159\n",
      "Epoch 104, test loss: 0.190781\n",
      "Epoch 105, test loss: 0.189400\n",
      "Epoch 106, test loss: 0.193013\n",
      "Epoch 107, test loss: 0.189426\n",
      "Epoch 108, test loss: 0.195657\n",
      "Epoch 109, test loss: 0.193265\n",
      "Epoch 110, test loss: 0.194491\n",
      "Epoch 111, test loss: 0.189955\n",
      "Epoch 112, test loss: 0.191354\n",
      "Epoch 113, test loss: 0.192050\n",
      "Epoch 114, test loss: 0.189388\n",
      "Epoch 115, test loss: 0.189882\n",
      "Epoch 116, test loss: 0.190269\n",
      "Epoch 117, test loss: 0.192490\n",
      "Epoch 118, test loss: 0.191997\n",
      "Epoch 119, test loss: 0.192958\n",
      "Epoch 120, test loss: 0.190215\n",
      "Epoch 121, test loss: 0.190291\n",
      "Epoch 122, test loss: 0.190064\n",
      "Epoch 123, test loss: 0.191237\n",
      "Epoch 124, test loss: 0.193183\n",
      "Epoch 125, test loss: 0.191386\n",
      "Epoch 126, test loss: 0.188773\n",
      "Epoch 127, test loss: 0.189613\n",
      "Epoch 128, test loss: 0.192776\n",
      "Epoch 129, test loss: 0.189574\n",
      "Epoch 130, test loss: 0.188430\n",
      "Epoch 131, test loss: 0.189805\n",
      "Epoch 132, test loss: 0.189423\n",
      "Epoch 133, test loss: 0.192133\n",
      "Epoch 134, test loss: 0.190620\n",
      "Epoch 135, test loss: 0.188665\n",
      "Epoch 136, test loss: 0.188803\n",
      "Epoch 137, test loss: 0.189355\n",
      "Epoch 138, test loss: 0.194475\n",
      "Epoch 139, test loss: 0.189139\n",
      "Epoch 140, test loss: 0.193072\n",
      "Epoch 141, test loss: 0.189231\n",
      "Epoch 142, test loss: 0.200726\n",
      "Epoch 143, test loss: 0.188818\n",
      "Epoch 144, test loss: 0.190831\n",
      "Epoch 145, test loss: 0.188884\n",
      "Epoch 146, test loss: 0.196319\n",
      "Epoch 147, test loss: 0.188870\n",
      "Epoch 148, test loss: 0.191510\n",
      "Epoch 149, test loss: 0.196155\n",
      "Epoch 150, test loss: 0.188814\n",
      "Epoch 151, test loss: 0.188550\n",
      "Epoch 152, test loss: 0.188407\n",
      "Epoch 153, test loss: 0.189651\n",
      "Epoch 154, test loss: 0.188916\n",
      "Epoch 155, test loss: 0.195422\n",
      "Epoch 156, test loss: 0.189327\n",
      "Epoch 157, test loss: 0.189757\n",
      "Epoch 158, test loss: 0.189511\n",
      "Epoch 159, test loss: 0.189922\n",
      "Epoch 160, test loss: 0.188841\n",
      "Epoch 161, test loss: 0.191886\n",
      "Epoch 162, test loss: 0.188226\n",
      "Epoch 163, test loss: 0.190021\n",
      "Epoch 164, test loss: 0.188288\n",
      "Epoch 165, test loss: 0.188392\n",
      "Epoch 166, test loss: 0.189286\n",
      "Epoch 167, test loss: 0.188544\n",
      "Epoch 168, test loss: 0.190457\n",
      "Epoch 169, test loss: 0.187930\n",
      "Epoch 170, test loss: 0.188209\n",
      "Epoch 171, test loss: 0.188408\n",
      "Epoch 172, test loss: 0.189416\n",
      "Epoch 173, test loss: 0.188876\n",
      "Epoch 174, test loss: 0.189299\n",
      "Epoch 175, test loss: 0.189550\n",
      "Epoch 176, test loss: 0.188767\n",
      "Epoch 177, test loss: 0.189681\n",
      "Epoch 178, test loss: 0.188879\n",
      "Epoch 179, test loss: 0.188935\n",
      "Epoch 180, test loss: 0.190118\n",
      "Epoch 181, test loss: 0.190835\n",
      "Epoch 182, test loss: 0.190916\n",
      "Epoch 183, test loss: 0.188595\n",
      "Epoch 184, test loss: 0.188154\n",
      "Epoch 185, test loss: 0.188956\n",
      "Epoch 186, test loss: 0.192719\n",
      "Epoch 187, test loss: 0.193041\n",
      "Epoch 188, test loss: 0.188735\n",
      "Epoch 189, test loss: 0.188888\n",
      "Epoch 190, test loss: 0.188591\n",
      "Epoch 191, test loss: 0.187506\n",
      "Epoch 192, test loss: 0.192392\n",
      "Epoch 193, test loss: 0.188158\n",
      "Epoch 194, test loss: 0.190932\n",
      "Epoch 195, test loss: 0.189777\n",
      "Epoch 196, test loss: 0.187520\n",
      "Epoch 197, test loss: 0.188877\n",
      "Epoch 198, test loss: 0.189001\n",
      "Epoch 199, test loss: 0.191710\n",
      "Epoch 200, test loss: 0.188660\n",
      "Epoch 201, test loss: 0.187959\n",
      "Epoch 202, test loss: 0.189512\n",
      "Epoch 203, test loss: 0.189549\n",
      "Epoch 204, test loss: 0.189392\n",
      "Epoch 205, test loss: 0.188901\n",
      "Epoch 206, test loss: 0.189773\n",
      "Epoch 207, test loss: 0.189501\n",
      "Epoch 208, test loss: 0.188005\n",
      "Epoch 209, test loss: 0.188441\n",
      "Epoch 210, test loss: 0.188826\n",
      "Epoch 211, test loss: 0.189896\n",
      "Epoch 212, test loss: 0.187825\n",
      "Epoch 213, test loss: 0.190333\n",
      "Epoch 214, test loss: 0.188581\n",
      "Epoch 215, test loss: 0.193555\n",
      "Epoch 216, test loss: 0.188118\n",
      "Epoch 217, test loss: 0.188813\n",
      "Epoch 218, test loss: 0.188425\n",
      "Epoch 219, test loss: 0.188835\n",
      "Epoch 220, test loss: 0.189357\n",
      "Epoch 221, test loss: 0.192646\n",
      "Epoch 222, test loss: 0.188008\n",
      "Epoch 223, test loss: 0.188323\n",
      "Epoch 224, test loss: 0.187737\n",
      "Epoch 225, test loss: 0.187722\n",
      "Epoch 226, test loss: 0.189679\n",
      "Epoch 227, test loss: 0.189460\n",
      "Epoch 228, test loss: 0.187472\n",
      "Epoch 229, test loss: 0.187508\n",
      "Epoch 230, test loss: 0.188277\n",
      "Epoch 231, test loss: 0.188216\n",
      "Epoch 232, test loss: 0.187940\n",
      "Epoch 233, test loss: 0.187586\n",
      "Epoch 234, test loss: 0.187715\n",
      "Epoch 235, test loss: 0.188549\n",
      "Epoch 236, test loss: 0.188026\n",
      "Epoch 237, test loss: 0.189683\n",
      "Epoch 238, test loss: 0.188453\n",
      "Epoch 239, test loss: 0.188246\n",
      "Pretrain data: 19365644.0\n",
      "Building dataset, requesting data from 0 to 821\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7097/98677\n",
      "Found 821 continuous time series\n",
      "Data shape: (105776, 18), Train/test: 105774/2\n",
      "Train test ratio: 52887.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1652\n",
      "Epoch 0, train loss: 0.272073\n",
      "Epoch 1, train loss: 0.252602\n",
      "Epoch 2, train loss: 0.242999\n",
      "Epoch 3, train loss: 0.158970\n",
      "Epoch 4, train loss: 0.213909\n",
      "Epoch 5, train loss: 0.204119\n",
      "Epoch 6, train loss: 0.172398\n",
      "Epoch 7, train loss: 0.210864\n",
      "Epoch 8, train loss: 0.204064\n",
      "Epoch 9, train loss: 0.283508\n",
      "Epoch 10, train loss: 0.180350\n",
      "Epoch 11, train loss: 0.223140\n",
      "Epoch 12, train loss: 0.183361\n",
      "Epoch 13, train loss: 0.234957\n",
      "Epoch 14, train loss: 0.219310\n",
      "Epoch 15, train loss: 0.219366\n",
      "Epoch 16, train loss: 0.232701\n",
      "Epoch 17, train loss: 0.182142\n",
      "Epoch 18, train loss: 0.197551\n",
      "Epoch 19, train loss: 0.194587\n",
      "Epoch 20, train loss: 0.219340\n",
      "Epoch 21, train loss: 0.237182\n",
      "Epoch 22, train loss: 0.204144\n",
      "Epoch 23, train loss: 0.183159\n",
      "Epoch 24, train loss: 0.216196\n",
      "Epoch 25, train loss: 0.224356\n",
      "Epoch 26, train loss: 0.186317\n",
      "Epoch 27, train loss: 0.202456\n",
      "Epoch 28, train loss: 0.174307\n",
      "Epoch 29, train loss: 0.166774\n",
      "Epoch 30, train loss: 0.234578\n",
      "Epoch 31, train loss: 0.228651\n",
      "Epoch 32, train loss: 0.214399\n",
      "Epoch 33, train loss: 0.207220\n",
      "Epoch 34, train loss: 0.180706\n",
      "Epoch 35, train loss: 0.154596\n",
      "Epoch 36, train loss: 0.204162\n",
      "Epoch 37, train loss: 0.149615\n",
      "Epoch 38, train loss: 0.175627\n",
      "Epoch 39, train loss: 0.165133\n",
      "Epoch 40, train loss: 0.182774\n",
      "Epoch 41, train loss: 0.191744\n",
      "Epoch 42, train loss: 0.249283\n",
      "Epoch 43, train loss: 0.219433\n",
      "Epoch 44, train loss: 0.197302\n",
      "Epoch 45, train loss: 0.215861\n",
      "Epoch 46, train loss: 0.219372\n",
      "Epoch 47, train loss: 0.242429\n",
      "Epoch 48, train loss: 0.224684\n",
      "Epoch 49, train loss: 0.206230\n",
      "Epoch 50, train loss: 0.181567\n",
      "Epoch 51, train loss: 0.261193\n",
      "Epoch 52, train loss: 0.203199\n",
      "Epoch 53, train loss: 0.194152\n",
      "Epoch 54, train loss: 0.176283\n",
      "Epoch 55, train loss: 0.156137\n",
      "Epoch 56, train loss: 0.232806\n",
      "Epoch 57, train loss: 0.207311\n",
      "Epoch 58, train loss: 0.152197\n",
      "Epoch 59, train loss: 0.234143\n",
      "Epoch 60, train loss: 0.179563\n",
      "Epoch 61, train loss: 0.226444\n",
      "Epoch 62, train loss: 0.211714\n",
      "Epoch 63, train loss: 0.241618\n",
      "Epoch 64, train loss: 0.217911\n",
      "Epoch 65, train loss: 0.196777\n",
      "Epoch 66, train loss: 0.202175\n",
      "Epoch 67, train loss: 0.185913\n",
      "Epoch 68, train loss: 0.165477\n",
      "Epoch 69, train loss: 0.230087\n",
      "Epoch 70, train loss: 0.163385\n",
      "Epoch 71, train loss: 0.233789\n",
      "Epoch 72, train loss: 0.224314\n",
      "Epoch 73, train loss: 0.175848\n",
      "Epoch 74, train loss: 0.185075\n",
      "Epoch 75, train loss: 0.235632\n",
      "Epoch 76, train loss: 0.175722\n",
      "Epoch 77, train loss: 0.186903\n",
      "Epoch 78, train loss: 0.220591\n",
      "Epoch 79, train loss: 0.212353\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "x here is\n",
      "[[135. 143. 152. ... 214. 219. 223.]\n",
      " [143. 152. 159. ... 219. 223. 222.]\n",
      " [152. 159. 166. ... 223. 222. 220.]\n",
      " ...\n",
      " [169. 171. 175. ... 204. 202. 201.]\n",
      " [171. 175. 178. ... 202. 201. 201.]\n",
      " [175. 178. 182. ... 201. 201. 201.]]\n",
      "y here is\n",
      "[[229. 229. 229. ... 229. 229. 229.]\n",
      " [232. 232. 232. ... 232. 232. 232.]\n",
      " [234. 234. 234. ... 234. 234. 234.]\n",
      " ...\n",
      " [212. 212. 212. ... 212. 212. 212.]\n",
      " [218. 218. 218. ... 218. 218. 218.]\n",
      " [224. 224. 224. ... 224. 224. 224.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (2515, 18), Train/test: 1/2514\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "x here is\n",
      "[[101. 100. 100. ...  85.  85.  86.]\n",
      " [100. 100.  99. ...  85.  86.  85.]\n",
      " [100.  99.  98. ...  86.  85.  83.]\n",
      " ...\n",
      " [ 77.  75.  74. ...  74.  76.  79.]\n",
      " [ 75.  74.  71. ...  76.  79.  87.]\n",
      " [ 74.  71.  70. ...  79.  87.  95.]]\n",
      "y here is\n",
      "[[ 96.  96.  96. ...  96.  96.  96.]\n",
      " [ 93.  93.  93. ...  93.  93.  93.]\n",
      " [ 93.  93.  93. ...  93.  93.  93.]\n",
      " ...\n",
      " [120. 120. 120. ... 120. 120. 120.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [128. 128. 128. ... 128. 128. 128.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 420/10077\n",
      "Found 21 continuous time series\n",
      "Data shape: (10499, 18), Train/test: 10497/2\n",
      "Train test ratio: 5248.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A477B67490>\n",
      "Epoch 0, test loss: 0.155331\n",
      "Epoch 1, test loss: 0.156296\n",
      "Epoch 2, test loss: 0.158260\n",
      "Epoch 3, test loss: 0.156360\n",
      "Epoch 4, test loss: 0.156681\n",
      "Epoch 5, test loss: 0.155302\n",
      "Epoch 6, test loss: 0.160554\n",
      "Epoch 7, test loss: 0.155533\n",
      "Epoch 8, test loss: 0.159314\n",
      "Epoch 9, test loss: 0.157765\n",
      "Epoch 10, test loss: 0.155046\n",
      "Epoch 11, test loss: 0.160516\n",
      "Epoch 12, test loss: 0.154901\n",
      "Epoch 13, test loss: 0.156365\n",
      "Epoch 14, test loss: 0.161545\n",
      "Epoch 15, test loss: 0.155409\n",
      "Epoch 16, test loss: 0.159435\n",
      "Epoch 17, test loss: 0.157997\n",
      "Epoch 18, test loss: 0.156261\n",
      "Epoch 19, test loss: 0.156898\n",
      "Epoch 20, test loss: 0.161518\n",
      "Epoch 21, test loss: 0.155250\n",
      "Epoch 22, test loss: 0.155785\n",
      "Epoch 23, test loss: 0.168618\n",
      "Epoch 24, test loss: 0.157250\n",
      "Epoch 25, test loss: 0.163700\n",
      "Epoch 26, test loss: 0.159214\n",
      "Epoch 27, test loss: 0.155537\n",
      "Epoch 28, test loss: 0.156240\n",
      "Epoch 29, test loss: 0.154990\n",
      "Epoch 30, test loss: 0.156200\n",
      "Epoch 31, test loss: 0.166313\n",
      "Epoch 32, test loss: 0.164582\n",
      "Epoch 33, test loss: 0.156728\n",
      "Epoch 34, test loss: 0.157511\n",
      "Epoch 35, test loss: 0.156390\n",
      "Epoch 36, test loss: 0.166546\n",
      "Epoch 37, test loss: 0.155325\n",
      "Epoch 38, test loss: 0.156933\n",
      "Epoch 39, test loss: 0.155050\n",
      "Epoch 40, test loss: 0.155380\n",
      "Epoch 41, test loss: 0.156579\n",
      "Epoch 42, test loss: 0.160596\n",
      "Epoch 43, test loss: 0.156795\n",
      "Epoch 44, test loss: 0.167666\n",
      "Epoch 45, test loss: 0.154847\n",
      "Epoch 46, test loss: 0.155959\n",
      "Epoch 47, test loss: 0.178093\n",
      "Epoch 48, test loss: 0.154815\n",
      "Epoch 49, test loss: 0.162613\n",
      "Epoch 50, test loss: 0.155584\n",
      "Epoch 51, test loss: 0.162679\n",
      "Epoch 52, test loss: 0.157224\n",
      "Epoch 53, test loss: 0.155105\n",
      "Epoch 54, test loss: 0.155751\n",
      "Epoch 55, test loss: 0.161264\n",
      "Epoch 56, test loss: 0.156010\n",
      "Epoch 57, test loss: 0.156140\n",
      "Epoch 58, test loss: 0.155358\n",
      "Epoch 59, test loss: 0.155747\n",
      "Epoch 60, test loss: 0.165263\n",
      "Epoch 61, test loss: 0.154909\n",
      "Epoch 62, test loss: 0.154979\n",
      "Epoch 63, test loss: 0.162986\n",
      "Epoch 64, test loss: 0.161086\n",
      "Epoch 65, test loss: 0.161093\n",
      "Epoch 66, test loss: 0.163570\n",
      "Epoch 67, test loss: 0.158681\n",
      "Epoch 68, test loss: 0.156471\n",
      "Epoch 69, test loss: 0.162684\n",
      "Epoch 70, test loss: 0.156358\n",
      "Epoch 71, test loss: 0.172088\n",
      "Epoch 72, test loss: 0.169347\n",
      "Epoch 73, test loss: 0.155095\n",
      "Epoch 74, test loss: 0.155192\n",
      "Epoch 75, test loss: 0.155174\n",
      "Epoch 76, test loss: 0.167178\n",
      "Epoch 77, test loss: 0.155161\n",
      "Epoch 78, test loss: 0.156063\n",
      "Epoch 79, test loss: 0.155490\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A477B67490>\n",
      "Epoch 0, test loss: 0.156334\n",
      "Epoch 1, test loss: 0.156067\n",
      "Epoch 2, test loss: 0.157712\n",
      "Epoch 3, test loss: 0.156223\n",
      "Epoch 4, test loss: 0.155164\n",
      "Epoch 5, test loss: 0.156192\n",
      "Epoch 6, test loss: 0.159618\n",
      "Epoch 7, test loss: 0.157021\n",
      "Epoch 8, test loss: 0.155262\n",
      "Epoch 9, test loss: 0.155997\n",
      "Epoch 10, test loss: 0.155928\n",
      "Epoch 11, test loss: 0.156395\n",
      "Epoch 12, test loss: 0.156906\n",
      "Epoch 13, test loss: 0.156308\n",
      "Epoch 14, test loss: 0.155713\n",
      "Epoch 15, test loss: 0.156210\n",
      "Epoch 16, test loss: 0.157411\n",
      "Epoch 17, test loss: 0.158930\n",
      "Epoch 18, test loss: 0.156169\n",
      "Epoch 19, test loss: 0.157079\n",
      "Epoch 20, test loss: 0.155807\n",
      "Epoch 21, test loss: 0.155848\n",
      "Epoch 22, test loss: 0.155795\n",
      "Epoch 23, test loss: 0.156399\n",
      "Epoch 24, test loss: 0.156056\n",
      "Epoch 25, test loss: 0.157181\n",
      "Epoch 26, test loss: 0.156948\n",
      "Epoch 27, test loss: 0.156741\n",
      "Epoch 28, test loss: 0.157416\n",
      "Epoch 29, test loss: 0.158287\n",
      "Epoch 30, test loss: 0.158322\n",
      "Epoch 31, test loss: 0.155815\n",
      "Epoch 32, test loss: 0.156279\n",
      "Epoch 33, test loss: 0.158431\n",
      "Epoch 34, test loss: 0.157250\n",
      "Epoch 35, test loss: 0.156440\n",
      "Epoch 36, test loss: 0.158128\n",
      "Epoch 37, test loss: 0.155825\n",
      "Epoch 38, test loss: 0.160510\n",
      "Epoch 39, test loss: 0.160058\n",
      "Epoch 40, test loss: 0.158746\n",
      "Epoch 41, test loss: 0.156292\n",
      "Epoch 42, test loss: 0.155979\n",
      "Epoch 43, test loss: 0.156311\n",
      "Epoch 44, test loss: 0.157061\n",
      "Epoch 45, test loss: 0.156437\n",
      "Epoch 46, test loss: 0.155990\n",
      "Epoch 47, test loss: 0.158603\n",
      "Epoch 48, test loss: 0.158652\n",
      "Epoch 49, test loss: 0.156884\n",
      "Epoch 50, test loss: 0.156207\n",
      "Epoch 51, test loss: 0.156999\n",
      "Epoch 52, test loss: 0.156527\n",
      "Epoch 53, test loss: 0.160249\n",
      "Epoch 54, test loss: 0.156377\n",
      "Epoch 55, test loss: 0.156341\n",
      "Epoch 56, test loss: 0.156322\n",
      "Epoch 57, test loss: 0.159012\n",
      "Epoch 58, test loss: 0.157392\n",
      "Epoch 59, test loss: 0.156167\n",
      "Epoch 60, test loss: 0.157523\n",
      "Epoch 61, test loss: 0.158291\n",
      "Epoch 62, test loss: 0.156875\n",
      "Epoch 63, test loss: 0.156647\n",
      "Epoch 64, test loss: 0.159075\n",
      "Epoch 65, test loss: 0.156337\n",
      "Epoch 66, test loss: 0.156603\n",
      "Epoch 67, test loss: 0.157435\n",
      "Epoch 68, test loss: 0.157181\n",
      "Epoch 69, test loss: 0.158592\n",
      "Epoch 70, test loss: 0.156182\n",
      "Epoch 71, test loss: 0.159267\n",
      "Epoch 72, test loss: 0.157082\n",
      "Epoch 73, test loss: 0.156253\n",
      "Epoch 74, test loss: 0.158159\n",
      "Epoch 75, test loss: 0.156452\n",
      "Epoch 76, test loss: 0.157972\n",
      "Epoch 77, test loss: 0.158495\n",
      "Epoch 78, test loss: 0.157317\n",
      "Epoch 79, test loss: 0.161734\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A477B67490>\n",
      "Epoch 0, test loss: 0.212126\n",
      "Epoch 1, test loss: 0.184324\n",
      "Epoch 2, test loss: 0.168587\n",
      "Epoch 3, test loss: 0.177449\n",
      "Epoch 4, test loss: 0.163623\n",
      "Epoch 5, test loss: 0.168924\n",
      "Epoch 6, test loss: 0.160502\n",
      "Epoch 7, test loss: 0.159761\n",
      "Epoch 8, test loss: 0.166637\n",
      "Epoch 9, test loss: 0.159814\n",
      "Epoch 10, test loss: 0.159019\n",
      "Epoch 11, test loss: 0.158583\n",
      "Epoch 12, test loss: 0.158618\n",
      "Epoch 13, test loss: 0.159124\n",
      "Epoch 14, test loss: 0.160236\n",
      "Epoch 15, test loss: 0.159757\n",
      "Epoch 16, test loss: 0.163254\n",
      "Epoch 17, test loss: 0.158704\n",
      "Epoch 18, test loss: 0.160595\n",
      "Epoch 19, test loss: 0.158193\n",
      "Epoch 20, test loss: 0.157873\n",
      "Epoch 21, test loss: 0.164627\n",
      "Epoch 22, test loss: 0.157764\n",
      "Epoch 23, test loss: 0.157382\n",
      "Epoch 24, test loss: 0.157000\n",
      "Epoch 25, test loss: 0.157076\n",
      "Epoch 26, test loss: 0.167143\n",
      "Epoch 27, test loss: 0.158192\n",
      "Epoch 28, test loss: 0.158798\n",
      "Epoch 29, test loss: 0.160114\n",
      "Epoch 30, test loss: 0.159690\n",
      "Epoch 31, test loss: 0.157275\n",
      "Epoch 32, test loss: 0.158444\n",
      "Epoch 33, test loss: 0.159098\n",
      "Epoch 34, test loss: 0.157392\n",
      "Epoch 35, test loss: 0.170728\n",
      "Epoch 36, test loss: 0.160896\n",
      "Epoch 37, test loss: 0.161401\n",
      "Epoch 38, test loss: 0.156949\n",
      "Epoch 39, test loss: 0.159452\n",
      "Epoch 40, test loss: 0.164697\n",
      "Epoch 41, test loss: 0.158337\n",
      "Epoch 42, test loss: 0.158188\n",
      "Epoch 43, test loss: 0.158103\n",
      "Epoch 44, test loss: 0.159651\n",
      "Epoch 45, test loss: 0.158078\n",
      "Epoch 46, test loss: 0.157005\n",
      "Epoch 47, test loss: 0.156833\n",
      "Epoch 48, test loss: 0.157736\n",
      "Epoch 49, test loss: 0.158556\n",
      "Epoch 50, test loss: 0.159354\n",
      "Epoch 51, test loss: 0.161633\n",
      "Epoch 52, test loss: 0.159872\n",
      "Epoch 53, test loss: 0.160881\n",
      "Epoch 54, test loss: 0.157361\n",
      "Epoch 55, test loss: 0.159310\n",
      "Epoch 56, test loss: 0.159720\n",
      "Epoch 57, test loss: 0.157308\n",
      "Epoch 58, test loss: 0.156958\n",
      "Epoch 59, test loss: 0.157088\n",
      "Epoch 60, test loss: 0.158184\n",
      "Epoch 61, test loss: 0.158061\n",
      "Epoch 62, test loss: 0.157073\n",
      "Epoch 63, test loss: 0.159167\n",
      "Epoch 64, test loss: 0.156931\n",
      "Epoch 65, test loss: 0.158922\n",
      "Epoch 66, test loss: 0.157188\n",
      "Epoch 67, test loss: 0.157005\n",
      "Epoch 68, test loss: 0.159017\n",
      "Epoch 69, test loss: 0.158872\n",
      "Epoch 70, test loss: 0.157505\n",
      "Epoch 71, test loss: 0.157780\n",
      "Epoch 72, test loss: 0.156940\n",
      "Epoch 73, test loss: 0.159084\n",
      "Epoch 74, test loss: 0.160850\n",
      "Epoch 75, test loss: 0.159236\n",
      "Epoch 76, test loss: 0.171762\n",
      "Epoch 77, test loss: 0.157086\n",
      "Epoch 78, test loss: 0.165938\n",
      "Epoch 79, test loss: 0.158270\n",
      "Epoch 80, test loss: 0.159225\n",
      "Epoch 81, test loss: 0.165956\n",
      "Epoch 82, test loss: 0.160232\n",
      "Epoch 83, test loss: 0.159526\n",
      "Epoch 84, test loss: 0.157049\n",
      "Epoch 85, test loss: 0.160154\n",
      "Epoch 86, test loss: 0.157327\n",
      "Epoch 87, test loss: 0.162490\n",
      "Epoch 88, test loss: 0.157082\n",
      "Epoch 89, test loss: 0.157516\n",
      "Epoch 90, test loss: 0.162920\n",
      "Epoch 91, test loss: 0.160157\n",
      "Epoch 92, test loss: 0.160566\n",
      "Epoch 93, test loss: 0.158686\n",
      "Epoch 94, test loss: 0.157381\n",
      "Epoch 95, test loss: 0.157258\n",
      "Epoch 96, test loss: 0.157508\n",
      "Epoch 97, test loss: 0.160330\n",
      "Epoch 98, test loss: 0.156981\n",
      "Epoch 99, test loss: 0.157105\n",
      "Epoch 100, test loss: 0.157278\n",
      "Epoch 101, test loss: 0.157925\n",
      "Epoch 102, test loss: 0.157483\n",
      "Epoch 103, test loss: 0.167684\n",
      "Epoch 104, test loss: 0.157615\n",
      "Epoch 105, test loss: 0.157361\n",
      "Epoch 106, test loss: 0.160288\n",
      "Epoch 107, test loss: 0.161371\n",
      "Epoch 108, test loss: 0.157940\n",
      "Epoch 109, test loss: 0.157540\n",
      "Epoch 110, test loss: 0.157999\n",
      "Epoch 111, test loss: 0.157755\n",
      "Epoch 112, test loss: 0.158351\n",
      "Epoch 113, test loss: 0.157281\n",
      "Epoch 114, test loss: 0.157527\n",
      "Epoch 115, test loss: 0.157639\n",
      "Epoch 116, test loss: 0.158595\n",
      "Epoch 117, test loss: 0.159940\n",
      "Epoch 118, test loss: 0.160251\n",
      "Epoch 119, test loss: 0.157933\n",
      "Epoch 120, test loss: 0.156993\n",
      "Epoch 121, test loss: 0.157053\n",
      "Epoch 122, test loss: 0.160146\n",
      "Epoch 123, test loss: 0.157651\n",
      "Epoch 124, test loss: 0.157670\n",
      "Epoch 125, test loss: 0.157032\n",
      "Epoch 126, test loss: 0.157211\n",
      "Epoch 127, test loss: 0.157251\n",
      "Epoch 128, test loss: 0.159794\n",
      "Epoch 129, test loss: 0.157899\n",
      "Epoch 130, test loss: 0.157612\n",
      "Epoch 131, test loss: 0.160487\n",
      "Epoch 132, test loss: 0.159273\n",
      "Epoch 133, test loss: 0.158817\n",
      "Epoch 134, test loss: 0.159141\n",
      "Epoch 135, test loss: 0.160809\n",
      "Epoch 136, test loss: 0.157728\n",
      "Epoch 137, test loss: 0.158483\n",
      "Epoch 138, test loss: 0.163531\n",
      "Epoch 139, test loss: 0.158954\n",
      "Epoch 140, test loss: 0.163634\n",
      "Epoch 141, test loss: 0.161692\n",
      "Epoch 142, test loss: 0.160708\n",
      "Epoch 143, test loss: 0.157290\n",
      "Epoch 144, test loss: 0.161411\n",
      "Epoch 145, test loss: 0.157821\n",
      "Epoch 146, test loss: 0.160257\n",
      "Epoch 147, test loss: 0.159224\n",
      "Epoch 148, test loss: 0.159334\n",
      "Epoch 149, test loss: 0.157171\n",
      "Epoch 150, test loss: 0.157682\n",
      "Epoch 151, test loss: 0.157733\n",
      "Epoch 152, test loss: 0.158261\n",
      "Epoch 153, test loss: 0.158249\n",
      "Epoch 154, test loss: 0.163261\n",
      "Epoch 155, test loss: 0.164162\n",
      "Epoch 156, test loss: 0.158622\n",
      "Epoch 157, test loss: 0.164937\n",
      "Epoch 158, test loss: 0.158460\n",
      "Epoch 159, test loss: 0.158047\n",
      "Epoch 160, test loss: 0.157358\n",
      "Epoch 161, test loss: 0.157102\n",
      "Epoch 162, test loss: 0.166980\n",
      "Epoch 163, test loss: 0.162781\n",
      "Epoch 164, test loss: 0.158019\n",
      "Epoch 165, test loss: 0.159021\n",
      "Epoch 166, test loss: 0.158840\n",
      "Epoch 167, test loss: 0.157655\n",
      "Epoch 168, test loss: 0.157130\n",
      "Epoch 169, test loss: 0.163934\n",
      "Epoch 170, test loss: 0.157678\n",
      "Epoch 171, test loss: 0.159236\n",
      "Epoch 172, test loss: 0.158242\n",
      "Epoch 173, test loss: 0.156894\n",
      "Epoch 174, test loss: 0.158569\n",
      "Epoch 175, test loss: 0.157313\n",
      "Epoch 176, test loss: 0.161955\n",
      "Epoch 177, test loss: 0.157800\n",
      "Epoch 178, test loss: 0.158262\n",
      "Epoch 179, test loss: 0.158183\n",
      "Epoch 180, test loss: 0.158945\n",
      "Epoch 181, test loss: 0.158736\n",
      "Epoch 182, test loss: 0.157360\n",
      "Epoch 183, test loss: 0.157472\n",
      "Epoch 184, test loss: 0.157180\n",
      "Epoch 185, test loss: 0.157282\n",
      "Epoch 186, test loss: 0.157434\n",
      "Epoch 187, test loss: 0.161392\n",
      "Epoch 188, test loss: 0.157385\n",
      "Epoch 189, test loss: 0.158770\n",
      "Epoch 190, test loss: 0.157783\n",
      "Epoch 191, test loss: 0.161128\n",
      "Epoch 192, test loss: 0.157328\n",
      "Epoch 193, test loss: 0.157523\n",
      "Epoch 194, test loss: 0.157133\n",
      "Epoch 195, test loss: 0.157758\n",
      "Epoch 196, test loss: 0.160208\n",
      "Epoch 197, test loss: 0.159512\n",
      "Epoch 198, test loss: 0.165024\n",
      "Epoch 199, test loss: 0.157217\n",
      "Epoch 200, test loss: 0.163321\n",
      "Epoch 201, test loss: 0.160095\n",
      "Epoch 202, test loss: 0.160735\n",
      "Epoch 203, test loss: 0.158117\n",
      "Epoch 204, test loss: 0.158649\n",
      "Epoch 205, test loss: 0.166992\n",
      "Epoch 206, test loss: 0.165291\n",
      "Epoch 207, test loss: 0.157290\n",
      "Epoch 208, test loss: 0.157477\n",
      "Epoch 209, test loss: 0.159500\n",
      "Epoch 210, test loss: 0.157219\n",
      "Epoch 211, test loss: 0.160111\n",
      "Epoch 212, test loss: 0.157804\n",
      "Epoch 213, test loss: 0.158828\n",
      "Epoch 214, test loss: 0.157850\n",
      "Epoch 215, test loss: 0.158635\n",
      "Epoch 216, test loss: 0.157889\n",
      "Epoch 217, test loss: 0.157189\n",
      "Epoch 218, test loss: 0.158760\n",
      "Epoch 219, test loss: 0.157836\n",
      "Epoch 220, test loss: 0.157746\n",
      "Epoch 221, test loss: 0.157069\n",
      "Epoch 222, test loss: 0.157197\n",
      "Epoch 223, test loss: 0.158652\n",
      "Epoch 224, test loss: 0.165736\n",
      "Epoch 225, test loss: 0.159128\n",
      "Epoch 226, test loss: 0.159741\n",
      "Epoch 227, test loss: 0.157892\n",
      "Epoch 228, test loss: 0.157525\n",
      "Epoch 229, test loss: 0.157833\n",
      "Epoch 230, test loss: 0.157649\n",
      "Epoch 231, test loss: 0.157765\n",
      "Epoch 232, test loss: 0.157772\n",
      "Epoch 233, test loss: 0.158484\n",
      "Epoch 234, test loss: 0.159126\n",
      "Epoch 235, test loss: 0.170115\n",
      "Epoch 236, test loss: 0.159176\n",
      "Epoch 237, test loss: 0.157749\n",
      "Epoch 238, test loss: 0.157005\n",
      "Epoch 239, test loss: 0.158338\n",
      "Pretrain data: 19339240.0\n",
      "Building dataset, requesting data from 0 to 831\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7233/96638\n",
      "Found 831 continuous time series\n",
      "Data shape: (103873, 18), Train/test: 103871/2\n",
      "Train test ratio: 51935.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1622\n",
      "Epoch 0, train loss: 0.223425\n",
      "Epoch 1, train loss: 0.218278\n",
      "Epoch 2, train loss: 0.231348\n",
      "Epoch 3, train loss: 0.196061\n",
      "Epoch 4, train loss: 0.209840\n",
      "Epoch 5, train loss: 0.240215\n",
      "Epoch 6, train loss: 0.237030\n",
      "Epoch 7, train loss: 0.245373\n",
      "Epoch 8, train loss: 0.219238\n",
      "Epoch 9, train loss: 0.228405\n",
      "Epoch 10, train loss: 0.251330\n",
      "Epoch 11, train loss: 0.166621\n",
      "Epoch 12, train loss: 0.156925\n",
      "Epoch 13, train loss: 0.216330\n",
      "Epoch 14, train loss: 0.224718\n",
      "Epoch 15, train loss: 0.172406\n",
      "Epoch 16, train loss: 0.267866\n",
      "Epoch 17, train loss: 0.197911\n",
      "Epoch 18, train loss: 0.192616\n",
      "Epoch 19, train loss: 0.225756\n",
      "Epoch 20, train loss: 0.196462\n",
      "Epoch 21, train loss: 0.155084\n",
      "Epoch 22, train loss: 0.281665\n",
      "Epoch 23, train loss: 0.200012\n",
      "Epoch 24, train loss: 0.209280\n",
      "Epoch 25, train loss: 0.193215\n",
      "Epoch 26, train loss: 0.196228\n",
      "Epoch 27, train loss: 0.210223\n",
      "Epoch 28, train loss: 0.171310\n",
      "Epoch 29, train loss: 0.203578\n",
      "Epoch 30, train loss: 0.270671\n",
      "Epoch 31, train loss: 0.203258\n",
      "Epoch 32, train loss: 0.237807\n",
      "Epoch 33, train loss: 0.195775\n",
      "Epoch 34, train loss: 0.208519\n",
      "Epoch 35, train loss: 0.216660\n",
      "Epoch 36, train loss: 0.211327\n",
      "Epoch 37, train loss: 0.297653\n",
      "Epoch 38, train loss: 0.223650\n",
      "Epoch 39, train loss: 0.177056\n",
      "Epoch 40, train loss: 0.199221\n",
      "Epoch 41, train loss: 0.203428\n",
      "Epoch 42, train loss: 0.209638\n",
      "Epoch 43, train loss: 0.187731\n",
      "Epoch 44, train loss: 0.212344\n",
      "Epoch 45, train loss: 0.204486\n",
      "Epoch 46, train loss: 0.162504\n",
      "Epoch 47, train loss: 0.191370\n",
      "Epoch 48, train loss: 0.209131\n",
      "Epoch 49, train loss: 0.175985\n",
      "Epoch 50, train loss: 0.174782\n",
      "Epoch 51, train loss: 0.214018\n",
      "Epoch 52, train loss: 0.189866\n",
      "Epoch 53, train loss: 0.275559\n",
      "Epoch 54, train loss: 0.164003\n",
      "Epoch 55, train loss: 0.215199\n",
      "Epoch 56, train loss: 0.198034\n",
      "Epoch 57, train loss: 0.216679\n",
      "Epoch 58, train loss: 0.219579\n",
      "Epoch 59, train loss: 0.185367\n",
      "Epoch 60, train loss: 0.238182\n",
      "Epoch 61, train loss: 0.186213\n",
      "Epoch 62, train loss: 0.155718\n",
      "Epoch 63, train loss: 0.186005\n",
      "Epoch 64, train loss: 0.248989\n",
      "Epoch 65, train loss: 0.203495\n",
      "Epoch 66, train loss: 0.267152\n",
      "Epoch 67, train loss: 0.207765\n",
      "Epoch 68, train loss: 0.204083\n",
      "Epoch 69, train loss: 0.181032\n",
      "Epoch 70, train loss: 0.221469\n",
      "Epoch 71, train loss: 0.214625\n",
      "Epoch 72, train loss: 0.202393\n",
      "Epoch 73, train loss: 0.243568\n",
      "Epoch 74, train loss: 0.199597\n",
      "Epoch 75, train loss: 0.154529\n",
      "Epoch 76, train loss: 0.188873\n",
      "Epoch 77, train loss: 0.198809\n",
      "Epoch 78, train loss: 0.191957\n",
      "Epoch 79, train loss: 0.189395\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "x here is\n",
      "[[127. 123. 118. ...  80.  78.  76.]\n",
      " [123. 118. 112. ...  78.  76.  74.]\n",
      " [118. 112. 108. ...  76.  74.  83.]\n",
      " ...\n",
      " [171. 168. 170. ... 254. 263. 280.]\n",
      " [168. 170. 174. ... 263. 280. 288.]\n",
      " [170. 174. 173. ... 280. 288. 301.]]\n",
      "y here is\n",
      "[[107. 107. 107. ... 107. 107. 107.]\n",
      " [117. 117. 117. ... 117. 117. 117.]\n",
      " [125. 125. 125. ... 125. 125. 125.]\n",
      " ...\n",
      " [307. 307. 307. ... 307. 307. 307.]\n",
      " [311. 311. 311. ... 311. 311. 311.]\n",
      " [321. 321. 321. ... 321. 321. 321.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (2722, 18), Train/test: 1/2721\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[116. 117. 119. ... 145. 147. 149.]\n",
      " [117. 119. 116. ... 147. 149. 149.]\n",
      " [119. 116. 111. ... 149. 149. 151.]\n",
      " ...\n",
      " [199. 200. 199. ... 180. 175. 171.]\n",
      " [200. 199. 198. ... 175. 171. 168.]\n",
      " [199. 198. 218. ... 171. 168. 162.]]\n",
      "y here is\n",
      "[[152. 152. 152. ... 152. 152. 152.]\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " ...\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [137. 137. 137. ... 137. 137. 137.]\n",
      " [132. 132. 132. ... 132. 132. 132.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 284/12116\n",
      "Found 11 continuous time series\n",
      "Data shape: (12402, 18), Train/test: 12400/2\n",
      "Train test ratio: 6200.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2976E78B0>\n",
      "Epoch 0, test loss: 0.187268\n",
      "Epoch 1, test loss: 0.189434\n",
      "Epoch 2, test loss: 0.187493\n",
      "Epoch 3, test loss: 0.188684\n",
      "Epoch 4, test loss: 0.188152\n",
      "Epoch 5, test loss: 0.187351\n",
      "Epoch 6, test loss: 0.185824\n",
      "Epoch 7, test loss: 0.186320\n",
      "Epoch 8, test loss: 0.187461\n",
      "Epoch 9, test loss: 0.186495\n",
      "Epoch 10, test loss: 0.187843\n",
      "Epoch 11, test loss: 0.190635\n",
      "Epoch 12, test loss: 0.189961\n",
      "Epoch 13, test loss: 0.186743\n",
      "Epoch 14, test loss: 0.188709\n",
      "Epoch 15, test loss: 0.190361\n",
      "Epoch 16, test loss: 0.189599\n",
      "Epoch 17, test loss: 0.193080\n",
      "Epoch 18, test loss: 0.191817\n",
      "Epoch 19, test loss: 0.186388\n",
      "Epoch 20, test loss: 0.187183\n",
      "Epoch 21, test loss: 0.190926\n",
      "Epoch 22, test loss: 0.190027\n",
      "Epoch 23, test loss: 0.186787\n",
      "Epoch 24, test loss: 0.187045\n",
      "Epoch 25, test loss: 0.187919\n",
      "Epoch 26, test loss: 0.186594\n",
      "Epoch 27, test loss: 0.187981\n",
      "Epoch 28, test loss: 0.188124\n",
      "Epoch 29, test loss: 0.186531\n",
      "Epoch 30, test loss: 0.189806\n",
      "Epoch 31, test loss: 0.189443\n",
      "Epoch 32, test loss: 0.187343\n",
      "Epoch 33, test loss: 0.186974\n",
      "Epoch 34, test loss: 0.186181\n",
      "Epoch 35, test loss: 0.186519\n",
      "Epoch 36, test loss: 0.188680\n",
      "Epoch 37, test loss: 0.186304\n",
      "Epoch 38, test loss: 0.185514\n",
      "Epoch 39, test loss: 0.188985\n",
      "Epoch 40, test loss: 0.187784\n",
      "Epoch 41, test loss: 0.191163\n",
      "Epoch 42, test loss: 0.186902\n",
      "Epoch 43, test loss: 0.192353\n",
      "Epoch 44, test loss: 0.186292\n",
      "Epoch 45, test loss: 0.191801\n",
      "Epoch 46, test loss: 0.186241\n",
      "Epoch 47, test loss: 0.191913\n",
      "Epoch 48, test loss: 0.190407\n",
      "Epoch 49, test loss: 0.187716\n",
      "Epoch 50, test loss: 0.187154\n",
      "Epoch 51, test loss: 0.189937\n",
      "Epoch 52, test loss: 0.186189\n",
      "Epoch 53, test loss: 0.190382\n",
      "Epoch 54, test loss: 0.187207\n",
      "Epoch 55, test loss: 0.187655\n",
      "Epoch 56, test loss: 0.190018\n",
      "Epoch 57, test loss: 0.193377\n",
      "Epoch 58, test loss: 0.195271\n",
      "Epoch 59, test loss: 0.194459\n",
      "Epoch 60, test loss: 0.188662\n",
      "Epoch 61, test loss: 0.186546\n",
      "Epoch 62, test loss: 0.188230\n",
      "Epoch 63, test loss: 0.188779\n",
      "Epoch 64, test loss: 0.186788\n",
      "Epoch 65, test loss: 0.187558\n",
      "Epoch 66, test loss: 0.186758\n",
      "Epoch 67, test loss: 0.193267\n",
      "Epoch 68, test loss: 0.187094\n",
      "Epoch 69, test loss: 0.187575\n",
      "Epoch 70, test loss: 0.186700\n",
      "Epoch 71, test loss: 0.186915\n",
      "Epoch 72, test loss: 0.188488\n",
      "Epoch 73, test loss: 0.187434\n",
      "Epoch 74, test loss: 0.186443\n",
      "Epoch 75, test loss: 0.188133\n",
      "Epoch 76, test loss: 0.194084\n",
      "Epoch 77, test loss: 0.186979\n",
      "Epoch 78, test loss: 0.190461\n",
      "Epoch 79, test loss: 0.186472\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2976E78B0>\n",
      "Epoch 0, test loss: 0.188080\n",
      "Epoch 1, test loss: 0.185623\n",
      "Epoch 2, test loss: 0.187918\n",
      "Epoch 3, test loss: 0.185499\n",
      "Epoch 4, test loss: 0.187919\n",
      "Epoch 5, test loss: 0.185312\n",
      "Epoch 6, test loss: 0.188389\n",
      "Epoch 7, test loss: 0.186807\n",
      "Epoch 8, test loss: 0.188032\n",
      "Epoch 9, test loss: 0.185839\n",
      "Epoch 10, test loss: 0.186910\n",
      "Epoch 11, test loss: 0.186308\n",
      "Epoch 12, test loss: 0.188189\n",
      "Epoch 13, test loss: 0.185842\n",
      "Epoch 14, test loss: 0.186614\n",
      "Epoch 15, test loss: 0.186394\n",
      "Epoch 16, test loss: 0.186628\n",
      "Epoch 17, test loss: 0.189595\n",
      "Epoch 18, test loss: 0.186765\n",
      "Epoch 19, test loss: 0.188360\n",
      "Epoch 20, test loss: 0.188671\n",
      "Epoch 21, test loss: 0.186055\n",
      "Epoch 22, test loss: 0.185498\n",
      "Epoch 23, test loss: 0.185907\n",
      "Epoch 24, test loss: 0.187529\n",
      "Epoch 25, test loss: 0.193810\n",
      "Epoch 26, test loss: 0.190987\n",
      "Epoch 27, test loss: 0.187150\n",
      "Epoch 28, test loss: 0.186780\n",
      "Epoch 29, test loss: 0.190024\n",
      "Epoch 30, test loss: 0.191592\n",
      "Epoch 31, test loss: 0.190690\n",
      "Epoch 32, test loss: 0.186139\n",
      "Epoch 33, test loss: 0.186114\n",
      "Epoch 34, test loss: 0.189614\n",
      "Epoch 35, test loss: 0.186996\n",
      "Epoch 36, test loss: 0.188912\n",
      "Epoch 37, test loss: 0.186504\n",
      "Epoch 38, test loss: 0.185722\n",
      "Epoch 39, test loss: 0.185850\n",
      "Epoch 40, test loss: 0.186324\n",
      "Epoch 41, test loss: 0.185808\n",
      "Epoch 42, test loss: 0.186430\n",
      "Epoch 43, test loss: 0.186734\n",
      "Epoch 44, test loss: 0.185896\n",
      "Epoch 45, test loss: 0.187877\n",
      "Epoch 46, test loss: 0.189051\n",
      "Epoch 47, test loss: 0.190976\n",
      "Epoch 48, test loss: 0.188499\n",
      "Epoch 49, test loss: 0.187249\n",
      "Epoch 50, test loss: 0.189933\n",
      "Epoch 51, test loss: 0.185903\n",
      "Epoch 52, test loss: 0.186269\n",
      "Epoch 53, test loss: 0.186198\n",
      "Epoch 54, test loss: 0.186611\n",
      "Epoch 55, test loss: 0.191519\n",
      "Epoch 56, test loss: 0.188051\n",
      "Epoch 57, test loss: 0.193175\n",
      "Epoch 58, test loss: 0.187581\n",
      "Epoch 59, test loss: 0.186201\n",
      "Epoch 60, test loss: 0.188146\n",
      "Epoch 61, test loss: 0.186835\n",
      "Epoch 62, test loss: 0.188309\n",
      "Epoch 63, test loss: 0.191163\n",
      "Epoch 64, test loss: 0.193826\n",
      "Epoch 65, test loss: 0.194899\n",
      "Epoch 66, test loss: 0.188774\n",
      "Epoch 67, test loss: 0.187450\n",
      "Epoch 68, test loss: 0.186929\n",
      "Epoch 69, test loss: 0.186064\n",
      "Epoch 70, test loss: 0.187434\n",
      "Epoch 71, test loss: 0.187564\n",
      "Epoch 72, test loss: 0.186694\n",
      "Epoch 73, test loss: 0.188793\n",
      "Epoch 74, test loss: 0.186395\n",
      "Epoch 75, test loss: 0.188591\n",
      "Epoch 76, test loss: 0.187161\n",
      "Epoch 77, test loss: 0.186294\n",
      "Epoch 78, test loss: 0.185633\n",
      "Epoch 79, test loss: 0.187168\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2976E78B0>\n",
      "Epoch 0, test loss: 0.255812\n",
      "Epoch 1, test loss: 0.224644\n",
      "Epoch 2, test loss: 0.208253\n",
      "Epoch 3, test loss: 0.197595\n",
      "Epoch 4, test loss: 0.193094\n",
      "Epoch 5, test loss: 0.191791\n",
      "Epoch 6, test loss: 0.194324\n",
      "Epoch 7, test loss: 0.193411\n",
      "Epoch 8, test loss: 0.191699\n",
      "Epoch 9, test loss: 0.199671\n",
      "Epoch 10, test loss: 0.190600\n",
      "Epoch 11, test loss: 0.189401\n",
      "Epoch 12, test loss: 0.189358\n",
      "Epoch 13, test loss: 0.191129\n",
      "Epoch 14, test loss: 0.190774\n",
      "Epoch 15, test loss: 0.197176\n",
      "Epoch 16, test loss: 0.188977\n",
      "Epoch 17, test loss: 0.188737\n",
      "Epoch 18, test loss: 0.189208\n",
      "Epoch 19, test loss: 0.196435\n",
      "Epoch 20, test loss: 0.189916\n",
      "Epoch 21, test loss: 0.188329\n",
      "Epoch 22, test loss: 0.194023\n",
      "Epoch 23, test loss: 0.188527\n",
      "Epoch 24, test loss: 0.190624\n",
      "Epoch 25, test loss: 0.192562\n",
      "Epoch 26, test loss: 0.194746\n",
      "Epoch 27, test loss: 0.188412\n",
      "Epoch 28, test loss: 0.192954\n",
      "Epoch 29, test loss: 0.188027\n",
      "Epoch 30, test loss: 0.190092\n",
      "Epoch 31, test loss: 0.188009\n",
      "Epoch 32, test loss: 0.188942\n",
      "Epoch 33, test loss: 0.188438\n",
      "Epoch 34, test loss: 0.189379\n",
      "Epoch 35, test loss: 0.188240\n",
      "Epoch 36, test loss: 0.189327\n",
      "Epoch 37, test loss: 0.188124\n",
      "Epoch 38, test loss: 0.190541\n",
      "Epoch 39, test loss: 0.190107\n",
      "Epoch 40, test loss: 0.188447\n",
      "Epoch 41, test loss: 0.193614\n",
      "Epoch 42, test loss: 0.188302\n",
      "Epoch 43, test loss: 0.189167\n",
      "Epoch 44, test loss: 0.187889\n",
      "Epoch 45, test loss: 0.187946\n",
      "Epoch 46, test loss: 0.188017\n",
      "Epoch 47, test loss: 0.188804\n",
      "Epoch 48, test loss: 0.189379\n",
      "Epoch 49, test loss: 0.188031\n",
      "Epoch 50, test loss: 0.188720\n",
      "Epoch 51, test loss: 0.188985\n",
      "Epoch 52, test loss: 0.189065\n",
      "Epoch 53, test loss: 0.195319\n",
      "Epoch 54, test loss: 0.194460\n",
      "Epoch 55, test loss: 0.191024\n",
      "Epoch 56, test loss: 0.189151\n",
      "Epoch 57, test loss: 0.188796\n",
      "Epoch 58, test loss: 0.191618\n",
      "Epoch 59, test loss: 0.188570\n",
      "Epoch 60, test loss: 0.188452\n",
      "Epoch 61, test loss: 0.189624\n",
      "Epoch 62, test loss: 0.189397\n",
      "Epoch 63, test loss: 0.190665\n",
      "Epoch 64, test loss: 0.187976\n",
      "Epoch 65, test loss: 0.188621\n",
      "Epoch 66, test loss: 0.189140\n",
      "Epoch 67, test loss: 0.188146\n",
      "Epoch 68, test loss: 0.188991\n",
      "Epoch 69, test loss: 0.192508\n",
      "Epoch 70, test loss: 0.198836\n",
      "Epoch 71, test loss: 0.191378\n",
      "Epoch 72, test loss: 0.188685\n",
      "Epoch 73, test loss: 0.191975\n",
      "Epoch 74, test loss: 0.188835\n",
      "Epoch 75, test loss: 0.190563\n",
      "Epoch 76, test loss: 0.188422\n",
      "Epoch 77, test loss: 0.189418\n",
      "Epoch 78, test loss: 0.188259\n",
      "Epoch 79, test loss: 0.190342\n",
      "Epoch 80, test loss: 0.188595\n",
      "Epoch 81, test loss: 0.187976\n",
      "Epoch 82, test loss: 0.188779\n",
      "Epoch 83, test loss: 0.189744\n",
      "Epoch 84, test loss: 0.188035\n",
      "Epoch 85, test loss: 0.188080\n",
      "Epoch 86, test loss: 0.189452\n",
      "Epoch 87, test loss: 0.187902\n",
      "Epoch 88, test loss: 0.194709\n",
      "Epoch 89, test loss: 0.188235\n",
      "Epoch 90, test loss: 0.187762\n",
      "Epoch 91, test loss: 0.195335\n",
      "Epoch 92, test loss: 0.189693\n",
      "Epoch 93, test loss: 0.188591\n",
      "Epoch 94, test loss: 0.198850\n",
      "Epoch 95, test loss: 0.188068\n",
      "Epoch 96, test loss: 0.188191\n",
      "Epoch 97, test loss: 0.191801\n",
      "Epoch 98, test loss: 0.191084\n",
      "Epoch 99, test loss: 0.190250\n",
      "Epoch 100, test loss: 0.190081\n",
      "Epoch 101, test loss: 0.191049\n",
      "Epoch 102, test loss: 0.188089\n",
      "Epoch 103, test loss: 0.189275\n",
      "Epoch 104, test loss: 0.187928\n",
      "Epoch 105, test loss: 0.191023\n",
      "Epoch 106, test loss: 0.193116\n",
      "Epoch 107, test loss: 0.201898\n",
      "Epoch 108, test loss: 0.187960\n",
      "Epoch 109, test loss: 0.194743\n",
      "Epoch 110, test loss: 0.187745\n",
      "Epoch 111, test loss: 0.191804\n",
      "Epoch 112, test loss: 0.187509\n",
      "Epoch 113, test loss: 0.187656\n",
      "Epoch 114, test loss: 0.187968\n",
      "Epoch 115, test loss: 0.192532\n",
      "Epoch 116, test loss: 0.189606\n",
      "Epoch 117, test loss: 0.196524\n",
      "Epoch 118, test loss: 0.188418\n",
      "Epoch 119, test loss: 0.193511\n",
      "Epoch 120, test loss: 0.188529\n",
      "Epoch 121, test loss: 0.189249\n",
      "Epoch 122, test loss: 0.189581\n",
      "Epoch 123, test loss: 0.192263\n",
      "Epoch 124, test loss: 0.187757\n",
      "Epoch 125, test loss: 0.191299\n",
      "Epoch 126, test loss: 0.187224\n",
      "Epoch 127, test loss: 0.186499\n",
      "Epoch 128, test loss: 0.189360\n",
      "Epoch 129, test loss: 0.187041\n",
      "Epoch 130, test loss: 0.189644\n",
      "Epoch 131, test loss: 0.194901\n",
      "Epoch 132, test loss: 0.204489\n",
      "Epoch 133, test loss: 0.187089\n",
      "Epoch 134, test loss: 0.188894\n",
      "Epoch 135, test loss: 0.186731\n",
      "Epoch 136, test loss: 0.187949\n",
      "Epoch 137, test loss: 0.187093\n",
      "Epoch 138, test loss: 0.188228\n",
      "Epoch 139, test loss: 0.186909\n",
      "Epoch 140, test loss: 0.186427\n",
      "Epoch 141, test loss: 0.187037\n",
      "Epoch 142, test loss: 0.186603\n",
      "Epoch 143, test loss: 0.186947\n",
      "Epoch 144, test loss: 0.190784\n",
      "Epoch 145, test loss: 0.187208\n",
      "Epoch 146, test loss: 0.186510\n",
      "Epoch 147, test loss: 0.197036\n",
      "Epoch 148, test loss: 0.190514\n",
      "Epoch 149, test loss: 0.186708\n",
      "Epoch 150, test loss: 0.187281\n",
      "Epoch 151, test loss: 0.187402\n",
      "Epoch 152, test loss: 0.188957\n",
      "Epoch 153, test loss: 0.187133\n",
      "Epoch 154, test loss: 0.187589\n",
      "Epoch 155, test loss: 0.191839\n",
      "Epoch 156, test loss: 0.186420\n",
      "Epoch 157, test loss: 0.187478\n",
      "Epoch 158, test loss: 0.191668\n",
      "Epoch 159, test loss: 0.187617\n",
      "Epoch 160, test loss: 0.186455\n",
      "Epoch 161, test loss: 0.195118\n",
      "Epoch 162, test loss: 0.190723\n",
      "Epoch 163, test loss: 0.187753\n",
      "Epoch 164, test loss: 0.189061\n",
      "Epoch 165, test loss: 0.187792\n",
      "Epoch 166, test loss: 0.188027\n",
      "Epoch 167, test loss: 0.189052\n",
      "Epoch 168, test loss: 0.187019\n",
      "Epoch 169, test loss: 0.187319\n",
      "Epoch 170, test loss: 0.186988\n",
      "Epoch 171, test loss: 0.186380\n",
      "Epoch 172, test loss: 0.192911\n",
      "Epoch 173, test loss: 0.186251\n",
      "Epoch 174, test loss: 0.189699\n",
      "Epoch 175, test loss: 0.186449\n",
      "Epoch 176, test loss: 0.188207\n",
      "Epoch 177, test loss: 0.188920\n",
      "Epoch 178, test loss: 0.186986\n",
      "Epoch 179, test loss: 0.188942\n",
      "Epoch 180, test loss: 0.196273\n",
      "Epoch 181, test loss: 0.187454\n",
      "Epoch 182, test loss: 0.189544\n",
      "Epoch 183, test loss: 0.191154\n",
      "Epoch 184, test loss: 0.188579\n",
      "Epoch 185, test loss: 0.188232\n",
      "Epoch 186, test loss: 0.187062\n",
      "Epoch 187, test loss: 0.187389\n",
      "Epoch 188, test loss: 0.186596\n",
      "Epoch 189, test loss: 0.188088\n",
      "Epoch 190, test loss: 0.186962\n",
      "Epoch 191, test loss: 0.187487\n",
      "Epoch 192, test loss: 0.187610\n",
      "Epoch 193, test loss: 0.186870\n",
      "Epoch 194, test loss: 0.189783\n",
      "Epoch 195, test loss: 0.190847\n",
      "Epoch 196, test loss: 0.186426\n",
      "Epoch 197, test loss: 0.186441\n",
      "Epoch 198, test loss: 0.188605\n",
      "Epoch 199, test loss: 0.191940\n",
      "Epoch 200, test loss: 0.186763\n",
      "Epoch 201, test loss: 0.188165\n",
      "Epoch 202, test loss: 0.189705\n",
      "Epoch 203, test loss: 0.186938\n",
      "Epoch 204, test loss: 0.188071\n",
      "Epoch 205, test loss: 0.187337\n",
      "Epoch 206, test loss: 0.187432\n",
      "Epoch 207, test loss: 0.189825\n",
      "Epoch 208, test loss: 0.188477\n",
      "Epoch 209, test loss: 0.186972\n",
      "Epoch 210, test loss: 0.187130\n",
      "Epoch 211, test loss: 0.186521\n",
      "Epoch 212, test loss: 0.186202\n",
      "Epoch 213, test loss: 0.186871\n",
      "Epoch 214, test loss: 0.187246\n",
      "Epoch 215, test loss: 0.187840\n",
      "Epoch 216, test loss: 0.186245\n",
      "Epoch 217, test loss: 0.186360\n",
      "Epoch 218, test loss: 0.187122\n",
      "Epoch 219, test loss: 0.187372\n",
      "Epoch 220, test loss: 0.187681\n",
      "Epoch 221, test loss: 0.187220\n",
      "Epoch 222, test loss: 0.187277\n",
      "Epoch 223, test loss: 0.186687\n",
      "Epoch 224, test loss: 0.187115\n",
      "Epoch 225, test loss: 0.187216\n",
      "Epoch 226, test loss: 0.188612\n",
      "Epoch 227, test loss: 0.192415\n",
      "Epoch 228, test loss: 0.187236\n",
      "Epoch 229, test loss: 0.186731\n",
      "Epoch 230, test loss: 0.194568\n",
      "Epoch 231, test loss: 0.189763\n",
      "Epoch 232, test loss: 0.187869\n",
      "Epoch 233, test loss: 0.186653\n",
      "Epoch 234, test loss: 0.187088\n",
      "Epoch 235, test loss: 0.192616\n",
      "Epoch 236, test loss: 0.187432\n",
      "Epoch 237, test loss: 0.186909\n",
      "Epoch 238, test loss: 0.188770\n",
      "Epoch 239, test loss: 0.187815\n",
      "Pretrain data: 19742408.0\n",
      "Building dataset, requesting data from 0 to 769\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [304. 299. 291. ... 232. 236. 237.]\n",
      " [299. 291. 282. ... 236. 237. 247.]\n",
      " [291. 282. 274. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6123/99949\n",
      "Found 769 continuous time series\n",
      "Data shape: (106074, 18), Train/test: 106072/2\n",
      "Train test ratio: 53036.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1657\n",
      "Epoch 0, train loss: 0.221711\n",
      "Epoch 1, train loss: 0.218107\n",
      "Epoch 2, train loss: 0.180239\n",
      "Epoch 3, train loss: 0.173014\n",
      "Epoch 4, train loss: 0.197557\n",
      "Epoch 5, train loss: 0.229857\n",
      "Epoch 6, train loss: 0.175522\n",
      "Epoch 7, train loss: 0.234600\n",
      "Epoch 8, train loss: 0.208413\n",
      "Epoch 9, train loss: 0.219245\n",
      "Epoch 10, train loss: 0.199473\n",
      "Epoch 11, train loss: 0.199477\n",
      "Epoch 12, train loss: 0.259700\n",
      "Epoch 13, train loss: 0.155018\n",
      "Epoch 14, train loss: 0.195617\n",
      "Epoch 15, train loss: 0.142038\n",
      "Epoch 16, train loss: 0.164733\n",
      "Epoch 17, train loss: 0.158238\n",
      "Epoch 18, train loss: 0.204595\n",
      "Epoch 19, train loss: 0.216500\n",
      "Epoch 20, train loss: 0.179424\n",
      "Epoch 21, train loss: 0.185334\n",
      "Epoch 22, train loss: 0.234883\n",
      "Epoch 23, train loss: 0.182638\n",
      "Epoch 24, train loss: 0.263172\n",
      "Epoch 25, train loss: 0.215652\n",
      "Epoch 26, train loss: 0.206860\n",
      "Epoch 27, train loss: 0.256564\n",
      "Epoch 28, train loss: 0.212073\n",
      "Epoch 29, train loss: 0.186757\n",
      "Epoch 30, train loss: 0.251164\n",
      "Epoch 31, train loss: 0.223583\n",
      "Epoch 32, train loss: 0.231632\n",
      "Epoch 33, train loss: 0.216981\n",
      "Epoch 34, train loss: 0.186120\n",
      "Epoch 35, train loss: 0.264360\n",
      "Epoch 36, train loss: 0.149148\n",
      "Epoch 37, train loss: 0.197628\n",
      "Epoch 38, train loss: 0.196920\n",
      "Epoch 39, train loss: 0.164541\n",
      "Epoch 40, train loss: 0.236530\n",
      "Epoch 41, train loss: 0.164515\n",
      "Epoch 42, train loss: 0.238052\n",
      "Epoch 43, train loss: 0.245804\n",
      "Epoch 44, train loss: 0.164332\n",
      "Epoch 45, train loss: 0.199514\n",
      "Epoch 46, train loss: 0.229133\n",
      "Epoch 47, train loss: 0.219916\n",
      "Epoch 48, train loss: 0.267953\n",
      "Epoch 49, train loss: 0.202185\n",
      "Epoch 50, train loss: 0.164203\n",
      "Epoch 51, train loss: 0.263775\n",
      "Epoch 52, train loss: 0.188580\n",
      "Epoch 53, train loss: 0.196295\n",
      "Epoch 54, train loss: 0.185839\n",
      "Epoch 55, train loss: 0.168806\n",
      "Epoch 56, train loss: 0.189602\n",
      "Epoch 57, train loss: 0.256221\n",
      "Epoch 58, train loss: 0.268528\n",
      "Epoch 59, train loss: 0.161987\n",
      "Epoch 60, train loss: 0.198047\n",
      "Epoch 61, train loss: 0.204274\n",
      "Epoch 62, train loss: 0.186230\n",
      "Epoch 63, train loss: 0.211830\n",
      "Epoch 64, train loss: 0.194335\n",
      "Epoch 65, train loss: 0.186898\n",
      "Epoch 66, train loss: 0.206678\n",
      "Epoch 67, train loss: 0.208221\n",
      "Epoch 68, train loss: 0.182970\n",
      "Epoch 69, train loss: 0.153716\n",
      "Epoch 70, train loss: 0.190636\n",
      "Epoch 71, train loss: 0.185203\n",
      "Epoch 72, train loss: 0.190054\n",
      "Epoch 73, train loss: 0.222196\n",
      "Epoch 74, train loss: 0.192660\n",
      "Epoch 75, train loss: 0.190571\n",
      "Epoch 76, train loss: 0.169942\n",
      "Epoch 77, train loss: 0.140590\n",
      "Epoch 78, train loss: 0.215791\n",
      "Epoch 79, train loss: 0.158673\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[214. 217. 217. ... 177. 174. 173.]\n",
      " [217. 217. 212. ... 174. 173. 173.]\n",
      " [217. 212. 209. ... 173. 173. 171.]\n",
      " ...\n",
      " [107. 111. 110. ... 128. 130. 124.]\n",
      " [111. 110. 114. ... 130. 124. 120.]\n",
      " [110. 114. 121. ... 124. 120. 117.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [158. 158. 158. ... 158. 158. 158.]\n",
      " [156. 156. 156. ... 156. 156. 156.]\n",
      " ...\n",
      " [129. 129. 129. ... 129. 129. 129.]\n",
      " [139. 139. 139. ... 139. 139. 139.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (2346, 18), Train/test: 1/2345\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 73 segments\n",
      "Building dataset, requesting data from 0 to 73\n",
      "x here is\n",
      "[[128. 123. 120. ... 115. 114. 115.]\n",
      " [123. 120. 124. ... 114. 115. 115.]\n",
      " [120. 124. 121. ... 115. 115. 114.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[113. 113. 113. ... 113. 113. 113.]\n",
      " [110. 110. 110. ... 110. 110. 110.]\n",
      " [107. 107. 107. ... 107. 107. 107.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1394/8805\n",
      "Found 73 continuous time series\n",
      "Data shape: (10201, 18), Train/test: 10199/2\n",
      "Train test ratio: 5099.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291D0BF10>\n",
      "Epoch 0, test loss: 0.221801\n",
      "Epoch 1, test loss: 0.222808\n",
      "Epoch 2, test loss: 0.225205\n",
      "Epoch 3, test loss: 0.222390\n",
      "Epoch 4, test loss: 0.226823\n",
      "Epoch 5, test loss: 0.221530\n",
      "Epoch 6, test loss: 0.222379\n",
      "Epoch 7, test loss: 0.221944\n",
      "Epoch 8, test loss: 0.223121\n",
      "Epoch 9, test loss: 0.222931\n",
      "Epoch 10, test loss: 0.222515\n",
      "Epoch 11, test loss: 0.226853\n",
      "Epoch 12, test loss: 0.224522\n",
      "Epoch 13, test loss: 0.222441\n",
      "Epoch 14, test loss: 0.222497\n",
      "Epoch 15, test loss: 0.230388\n",
      "Epoch 16, test loss: 0.221129\n",
      "Epoch 17, test loss: 0.226081\n",
      "Epoch 18, test loss: 0.226416\n",
      "Epoch 19, test loss: 0.227868\n",
      "Epoch 20, test loss: 0.222337\n",
      "Epoch 21, test loss: 0.228909\n",
      "Epoch 22, test loss: 0.222947\n",
      "Epoch 23, test loss: 0.225934\n",
      "Epoch 24, test loss: 0.223588\n",
      "Epoch 25, test loss: 0.225571\n",
      "Epoch 26, test loss: 0.231095\n",
      "Epoch 27, test loss: 0.223255\n",
      "Epoch 28, test loss: 0.222764\n",
      "Epoch 29, test loss: 0.229528\n",
      "Epoch 30, test loss: 0.223746\n",
      "Epoch 31, test loss: 0.222321\n",
      "Epoch 32, test loss: 0.222767\n",
      "Epoch 33, test loss: 0.222938\n",
      "Epoch 34, test loss: 0.224590\n",
      "Epoch 35, test loss: 0.223010\n",
      "Epoch 36, test loss: 0.225476\n",
      "Epoch 37, test loss: 0.223333\n",
      "Epoch 38, test loss: 0.223675\n",
      "Epoch 39, test loss: 0.224173\n",
      "Epoch 40, test loss: 0.224561\n",
      "Epoch 41, test loss: 0.221509\n",
      "Epoch 42, test loss: 0.223772\n",
      "Epoch 43, test loss: 0.232330\n",
      "Epoch 44, test loss: 0.224885\n",
      "Epoch 45, test loss: 0.226796\n",
      "Epoch 46, test loss: 0.224286\n",
      "Epoch 47, test loss: 0.224729\n",
      "Epoch 48, test loss: 0.224589\n",
      "Epoch 49, test loss: 0.231157\n",
      "Epoch 50, test loss: 0.224959\n",
      "Epoch 51, test loss: 0.222868\n",
      "Epoch 52, test loss: 0.224161\n",
      "Epoch 53, test loss: 0.224573\n",
      "Epoch 54, test loss: 0.223734\n",
      "Epoch 55, test loss: 0.224665\n",
      "Epoch 56, test loss: 0.224878\n",
      "Epoch 57, test loss: 0.225150\n",
      "Epoch 58, test loss: 0.227162\n",
      "Epoch 59, test loss: 0.223827\n",
      "Epoch 60, test loss: 0.225538\n",
      "Epoch 61, test loss: 0.228500\n",
      "Epoch 62, test loss: 0.227339\n",
      "Epoch 63, test loss: 0.224348\n",
      "Epoch 64, test loss: 0.224041\n",
      "Epoch 65, test loss: 0.225267\n",
      "Epoch 66, test loss: 0.223856\n",
      "Epoch 67, test loss: 0.224004\n",
      "Epoch 68, test loss: 0.224288\n",
      "Epoch 69, test loss: 0.225408\n",
      "Epoch 70, test loss: 0.223743\n",
      "Epoch 71, test loss: 0.223537\n",
      "Epoch 72, test loss: 0.223586\n",
      "Epoch 73, test loss: 0.224201\n",
      "Epoch 74, test loss: 0.234926\n",
      "Epoch 75, test loss: 0.224854\n",
      "Epoch 76, test loss: 0.227893\n",
      "Epoch 77, test loss: 0.224465\n",
      "Epoch 78, test loss: 0.224601\n",
      "Epoch 79, test loss: 0.230098\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291D0BF10>\n",
      "Epoch 0, test loss: 0.223150\n",
      "Epoch 1, test loss: 0.226007\n",
      "Epoch 2, test loss: 0.223905\n",
      "Epoch 3, test loss: 0.223533\n",
      "Epoch 4, test loss: 0.225098\n",
      "Epoch 5, test loss: 0.223652\n",
      "Epoch 6, test loss: 0.221029\n",
      "Epoch 7, test loss: 0.224197\n",
      "Epoch 8, test loss: 0.222798\n",
      "Epoch 9, test loss: 0.223987\n",
      "Epoch 10, test loss: 0.224416\n",
      "Epoch 11, test loss: 0.221885\n",
      "Epoch 12, test loss: 0.223424\n",
      "Epoch 13, test loss: 0.225643\n",
      "Epoch 14, test loss: 0.222342\n",
      "Epoch 15, test loss: 0.223945\n",
      "Epoch 16, test loss: 0.223480\n",
      "Epoch 17, test loss: 0.221646\n",
      "Epoch 18, test loss: 0.222652\n",
      "Epoch 19, test loss: 0.223606\n",
      "Epoch 20, test loss: 0.225855\n",
      "Epoch 21, test loss: 0.223389\n",
      "Epoch 22, test loss: 0.222439\n",
      "Epoch 23, test loss: 0.224204\n",
      "Epoch 24, test loss: 0.222332\n",
      "Epoch 25, test loss: 0.222737\n",
      "Epoch 26, test loss: 0.224603\n",
      "Epoch 27, test loss: 0.221335\n",
      "Epoch 28, test loss: 0.222026\n",
      "Epoch 29, test loss: 0.222318\n",
      "Epoch 30, test loss: 0.223300\n",
      "Epoch 31, test loss: 0.223827\n",
      "Epoch 32, test loss: 0.223446\n",
      "Epoch 33, test loss: 0.221320\n",
      "Epoch 34, test loss: 0.224025\n",
      "Epoch 35, test loss: 0.224377\n",
      "Epoch 36, test loss: 0.223345\n",
      "Epoch 37, test loss: 0.223243\n",
      "Epoch 38, test loss: 0.224318\n",
      "Epoch 39, test loss: 0.221329\n",
      "Epoch 40, test loss: 0.223858\n",
      "Epoch 41, test loss: 0.223517\n",
      "Epoch 42, test loss: 0.222831\n",
      "Epoch 43, test loss: 0.226376\n",
      "Epoch 44, test loss: 0.222078\n",
      "Epoch 45, test loss: 0.221861\n",
      "Epoch 46, test loss: 0.224389\n",
      "Epoch 47, test loss: 0.221762\n",
      "Epoch 48, test loss: 0.224434\n",
      "Epoch 49, test loss: 0.223247\n",
      "Epoch 50, test loss: 0.222486\n",
      "Epoch 51, test loss: 0.225037\n",
      "Epoch 52, test loss: 0.222251\n",
      "Epoch 53, test loss: 0.222651\n",
      "Epoch 54, test loss: 0.222341\n",
      "Epoch 55, test loss: 0.222211\n",
      "Epoch 56, test loss: 0.227017\n",
      "Epoch 57, test loss: 0.224369\n",
      "Epoch 58, test loss: 0.223865\n",
      "Epoch 59, test loss: 0.223718\n",
      "Epoch 60, test loss: 0.228280\n",
      "Epoch 61, test loss: 0.223294\n",
      "Epoch 62, test loss: 0.226387\n",
      "Epoch 63, test loss: 0.222809\n",
      "Epoch 64, test loss: 0.223554\n",
      "Epoch 65, test loss: 0.225925\n",
      "Epoch 66, test loss: 0.222263\n",
      "Epoch 67, test loss: 0.221817\n",
      "Epoch 68, test loss: 0.224060\n",
      "Epoch 69, test loss: 0.224898\n",
      "Epoch 70, test loss: 0.224788\n",
      "Epoch 71, test loss: 0.223713\n",
      "Epoch 72, test loss: 0.222386\n",
      "Epoch 73, test loss: 0.224492\n",
      "Epoch 74, test loss: 0.225437\n",
      "Epoch 75, test loss: 0.222578\n",
      "Epoch 76, test loss: 0.225136\n",
      "Epoch 77, test loss: 0.225485\n",
      "Epoch 78, test loss: 0.223900\n",
      "Epoch 79, test loss: 0.222586\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291D0BF10>\n",
      "Epoch 0, test loss: 0.282803\n",
      "Epoch 1, test loss: 0.245505\n",
      "Epoch 2, test loss: 0.243951\n",
      "Epoch 3, test loss: 0.246332\n",
      "Epoch 4, test loss: 0.243816\n",
      "Epoch 5, test loss: 0.241811\n",
      "Epoch 6, test loss: 0.242388\n",
      "Epoch 7, test loss: 0.241235\n",
      "Epoch 8, test loss: 0.243184\n",
      "Epoch 9, test loss: 0.242131\n",
      "Epoch 10, test loss: 0.241720\n",
      "Epoch 11, test loss: 0.242196\n",
      "Epoch 12, test loss: 0.243745\n",
      "Epoch 13, test loss: 0.240953\n",
      "Epoch 14, test loss: 0.242789\n",
      "Epoch 15, test loss: 0.241596\n",
      "Epoch 16, test loss: 0.241075\n",
      "Epoch 17, test loss: 0.242128\n",
      "Epoch 18, test loss: 0.240957\n",
      "Epoch 19, test loss: 0.239634\n",
      "Epoch 20, test loss: 0.241469\n",
      "Epoch 21, test loss: 0.239540\n",
      "Epoch 22, test loss: 0.242249\n",
      "Epoch 23, test loss: 0.238445\n",
      "Epoch 24, test loss: 0.240148\n",
      "Epoch 25, test loss: 0.238157\n",
      "Epoch 26, test loss: 0.240772\n",
      "Epoch 27, test loss: 0.239232\n",
      "Epoch 28, test loss: 0.238003\n",
      "Epoch 29, test loss: 0.238586\n",
      "Epoch 30, test loss: 0.239395\n",
      "Epoch 31, test loss: 0.237023\n",
      "Epoch 32, test loss: 0.237009\n",
      "Epoch 33, test loss: 0.239707\n",
      "Epoch 34, test loss: 0.235952\n",
      "Epoch 35, test loss: 0.235814\n",
      "Epoch 36, test loss: 0.235389\n",
      "Epoch 37, test loss: 0.234961\n",
      "Epoch 38, test loss: 0.235301\n",
      "Epoch 39, test loss: 0.238520\n",
      "Epoch 40, test loss: 0.233606\n",
      "Epoch 41, test loss: 0.233236\n",
      "Epoch 42, test loss: 0.235894\n",
      "Epoch 43, test loss: 0.233953\n",
      "Epoch 44, test loss: 0.236459\n",
      "Epoch 45, test loss: 0.235284\n",
      "Epoch 46, test loss: 0.233697\n",
      "Epoch 47, test loss: 0.232206\n",
      "Epoch 48, test loss: 0.235210\n",
      "Epoch 49, test loss: 0.234152\n",
      "Epoch 50, test loss: 0.234380\n",
      "Epoch 51, test loss: 0.234327\n",
      "Epoch 52, test loss: 0.232000\n",
      "Epoch 53, test loss: 0.232266\n",
      "Epoch 54, test loss: 0.236400\n",
      "Epoch 55, test loss: 0.231682\n",
      "Epoch 56, test loss: 0.236876\n",
      "Epoch 57, test loss: 0.233406\n",
      "Epoch 58, test loss: 0.231399\n",
      "Epoch 59, test loss: 0.230452\n",
      "Epoch 60, test loss: 0.230498\n",
      "Epoch 61, test loss: 0.232816\n",
      "Epoch 62, test loss: 0.230829\n",
      "Epoch 63, test loss: 0.229696\n",
      "Epoch 64, test loss: 0.229723\n",
      "Epoch 65, test loss: 0.228561\n",
      "Epoch 66, test loss: 0.229175\n",
      "Epoch 67, test loss: 0.229874\n",
      "Epoch 68, test loss: 0.230966\n",
      "Epoch 69, test loss: 0.229414\n",
      "Epoch 70, test loss: 0.229937\n",
      "Epoch 71, test loss: 0.229233\n",
      "Epoch 72, test loss: 0.230482\n",
      "Epoch 73, test loss: 0.229008\n",
      "Epoch 74, test loss: 0.227335\n",
      "Epoch 75, test loss: 0.228325\n",
      "Epoch 76, test loss: 0.232158\n",
      "Epoch 77, test loss: 0.231244\n",
      "Epoch 78, test loss: 0.229195\n",
      "Epoch 79, test loss: 0.230054\n",
      "Epoch 80, test loss: 0.229992\n",
      "Epoch 81, test loss: 0.233393\n",
      "Epoch 82, test loss: 0.231780\n",
      "Epoch 83, test loss: 0.227681\n",
      "Epoch 84, test loss: 0.227035\n",
      "Epoch 85, test loss: 0.231086\n",
      "Epoch 86, test loss: 0.227349\n",
      "Epoch 87, test loss: 0.228470\n",
      "Epoch 88, test loss: 0.228679\n",
      "Epoch 89, test loss: 0.229544\n",
      "Epoch 90, test loss: 0.233110\n",
      "Epoch 91, test loss: 0.227779\n",
      "Epoch 92, test loss: 0.226899\n",
      "Epoch 93, test loss: 0.229267\n",
      "Epoch 94, test loss: 0.228624\n",
      "Epoch 95, test loss: 0.226124\n",
      "Epoch 96, test loss: 0.227741\n",
      "Epoch 97, test loss: 0.228686\n",
      "Epoch 98, test loss: 0.226886\n",
      "Epoch 99, test loss: 0.230056\n",
      "Epoch 100, test loss: 0.227361\n",
      "Epoch 101, test loss: 0.227740\n",
      "Epoch 102, test loss: 0.227391\n",
      "Epoch 103, test loss: 0.227361\n",
      "Epoch 104, test loss: 0.229212\n",
      "Epoch 105, test loss: 0.227588\n",
      "Epoch 106, test loss: 0.229588\n",
      "Epoch 107, test loss: 0.227730\n",
      "Epoch 108, test loss: 0.225527\n",
      "Epoch 109, test loss: 0.226203\n",
      "Epoch 110, test loss: 0.228055\n",
      "Epoch 111, test loss: 0.233332\n",
      "Epoch 112, test loss: 0.227491\n",
      "Epoch 113, test loss: 0.226959\n",
      "Epoch 114, test loss: 0.225661\n",
      "Epoch 115, test loss: 0.228630\n",
      "Epoch 116, test loss: 0.226345\n",
      "Epoch 117, test loss: 0.225501\n",
      "Epoch 118, test loss: 0.229103\n",
      "Epoch 119, test loss: 0.230274\n",
      "Epoch 120, test loss: 0.226545\n",
      "Epoch 121, test loss: 0.229580\n",
      "Epoch 122, test loss: 0.225784\n",
      "Epoch 123, test loss: 0.227370\n",
      "Epoch 124, test loss: 0.226448\n",
      "Epoch 125, test loss: 0.228128\n",
      "Epoch 126, test loss: 0.226599\n",
      "Epoch 127, test loss: 0.235257\n",
      "Epoch 128, test loss: 0.227274\n",
      "Epoch 129, test loss: 0.227680\n",
      "Epoch 130, test loss: 0.227574\n",
      "Epoch 131, test loss: 0.226622\n",
      "Epoch 132, test loss: 0.232150\n",
      "Epoch 133, test loss: 0.225910\n",
      "Epoch 134, test loss: 0.226712\n",
      "Epoch 135, test loss: 0.225232\n",
      "Epoch 136, test loss: 0.226230\n",
      "Epoch 137, test loss: 0.238574\n",
      "Epoch 138, test loss: 0.225508\n",
      "Epoch 139, test loss: 0.228149\n",
      "Epoch 140, test loss: 0.227694\n",
      "Epoch 141, test loss: 0.228255\n",
      "Epoch 142, test loss: 0.226667\n",
      "Epoch 143, test loss: 0.227303\n",
      "Epoch 144, test loss: 0.225717\n",
      "Epoch 145, test loss: 0.225330\n",
      "Epoch 146, test loss: 0.225770\n",
      "Epoch 147, test loss: 0.225586\n",
      "Epoch 148, test loss: 0.226085\n",
      "Epoch 149, test loss: 0.224938\n",
      "Epoch 150, test loss: 0.225576\n",
      "Epoch 151, test loss: 0.227271\n",
      "Epoch 152, test loss: 0.226392\n",
      "Epoch 153, test loss: 0.225095\n",
      "Epoch 154, test loss: 0.226997\n",
      "Epoch 155, test loss: 0.230658\n",
      "Epoch 156, test loss: 0.226561\n",
      "Epoch 157, test loss: 0.225933\n",
      "Epoch 158, test loss: 0.226555\n",
      "Epoch 159, test loss: 0.228999\n",
      "Epoch 160, test loss: 0.232714\n",
      "Epoch 161, test loss: 0.225416\n",
      "Epoch 162, test loss: 0.228307\n",
      "Epoch 163, test loss: 0.228491\n",
      "Epoch 164, test loss: 0.225547\n",
      "Epoch 165, test loss: 0.224812\n",
      "Epoch 166, test loss: 0.226111\n",
      "Epoch 167, test loss: 0.226948\n",
      "Epoch 168, test loss: 0.226083\n",
      "Epoch 169, test loss: 0.225797\n",
      "Epoch 170, test loss: 0.226219\n",
      "Epoch 171, test loss: 0.227383\n",
      "Epoch 172, test loss: 0.224827\n",
      "Epoch 173, test loss: 0.227185\n",
      "Epoch 174, test loss: 0.225771\n",
      "Epoch 175, test loss: 0.230673\n",
      "Epoch 176, test loss: 0.226684\n",
      "Epoch 177, test loss: 0.225228\n",
      "Epoch 178, test loss: 0.230366\n",
      "Epoch 179, test loss: 0.234484\n",
      "Epoch 180, test loss: 0.225315\n",
      "Epoch 181, test loss: 0.225359\n",
      "Epoch 182, test loss: 0.226155\n",
      "Epoch 183, test loss: 0.225494\n",
      "Epoch 184, test loss: 0.225322\n",
      "Epoch 185, test loss: 0.224323\n",
      "Epoch 186, test loss: 0.226332\n",
      "Epoch 187, test loss: 0.225459\n",
      "Epoch 188, test loss: 0.224434\n",
      "Epoch 189, test loss: 0.225317\n",
      "Epoch 190, test loss: 0.225273\n",
      "Epoch 191, test loss: 0.225691\n",
      "Epoch 192, test loss: 0.226538\n",
      "Epoch 193, test loss: 0.224622\n",
      "Epoch 194, test loss: 0.225971\n",
      "Epoch 195, test loss: 0.226665\n",
      "Epoch 196, test loss: 0.231567\n",
      "Epoch 197, test loss: 0.226991\n",
      "Epoch 198, test loss: 0.225881\n",
      "Epoch 199, test loss: 0.224563\n",
      "Epoch 200, test loss: 0.227441\n",
      "Epoch 201, test loss: 0.225006\n",
      "Epoch 202, test loss: 0.224774\n",
      "Epoch 203, test loss: 0.228706\n",
      "Epoch 204, test loss: 0.224551\n",
      "Epoch 205, test loss: 0.225664\n",
      "Epoch 206, test loss: 0.230429\n",
      "Epoch 207, test loss: 0.226624\n",
      "Epoch 208, test loss: 0.225798\n",
      "Epoch 209, test loss: 0.226459\n",
      "Epoch 210, test loss: 0.228415\n",
      "Epoch 211, test loss: 0.224591\n",
      "Epoch 212, test loss: 0.224383\n",
      "Epoch 213, test loss: 0.225076\n",
      "Epoch 214, test loss: 0.227487\n",
      "Epoch 215, test loss: 0.230875\n",
      "Epoch 216, test loss: 0.224647\n",
      "Epoch 217, test loss: 0.226541\n",
      "Epoch 218, test loss: 0.225771\n",
      "Epoch 219, test loss: 0.224743\n",
      "Epoch 220, test loss: 0.225048\n",
      "Epoch 221, test loss: 0.228343\n",
      "Epoch 222, test loss: 0.226338\n",
      "Epoch 223, test loss: 0.226967\n",
      "Epoch 224, test loss: 0.228044\n",
      "Epoch 225, test loss: 0.228323\n",
      "Epoch 226, test loss: 0.224755\n",
      "Epoch 227, test loss: 0.228530\n",
      "Epoch 228, test loss: 0.224955\n",
      "Epoch 229, test loss: 0.225288\n",
      "Epoch 230, test loss: 0.225583\n",
      "Epoch 231, test loss: 0.229981\n",
      "Epoch 232, test loss: 0.226090\n",
      "Epoch 233, test loss: 0.226004\n",
      "Epoch 234, test loss: 0.226127\n",
      "Epoch 235, test loss: 0.226869\n",
      "Epoch 236, test loss: 0.227236\n",
      "Epoch 237, test loss: 0.224863\n",
      "Epoch 238, test loss: 0.227146\n",
      "Epoch 239, test loss: 0.227277\n",
      "Pretrain data: 19732321.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6805/99226\n",
      "Found 815 continuous time series\n",
      "Data shape: (106033, 18), Train/test: 106031/2\n",
      "Train test ratio: 53015.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1656\n",
      "Epoch 0, train loss: 0.226899\n",
      "Epoch 1, train loss: 0.222101\n",
      "Epoch 2, train loss: 0.201135\n",
      "Epoch 3, train loss: 0.228059\n",
      "Epoch 4, train loss: 0.232690\n",
      "Epoch 5, train loss: 0.176637\n",
      "Epoch 6, train loss: 0.223178\n",
      "Epoch 7, train loss: 0.184653\n",
      "Epoch 8, train loss: 0.171524\n",
      "Epoch 9, train loss: 0.181575\n",
      "Epoch 10, train loss: 0.156011\n",
      "Epoch 11, train loss: 0.160519\n",
      "Epoch 12, train loss: 0.196515\n",
      "Epoch 13, train loss: 0.136368\n",
      "Epoch 14, train loss: 0.241850\n",
      "Epoch 15, train loss: 0.185788\n",
      "Epoch 16, train loss: 0.213258\n",
      "Epoch 17, train loss: 0.183720\n",
      "Epoch 18, train loss: 0.232723\n",
      "Epoch 19, train loss: 0.245171\n",
      "Epoch 20, train loss: 0.230767\n",
      "Epoch 21, train loss: 0.238389\n",
      "Epoch 22, train loss: 0.212282\n",
      "Epoch 23, train loss: 0.177327\n",
      "Epoch 24, train loss: 0.168047\n",
      "Epoch 25, train loss: 0.149258\n",
      "Epoch 26, train loss: 0.221402\n",
      "Epoch 27, train loss: 0.238117\n",
      "Epoch 28, train loss: 0.208947\n",
      "Epoch 29, train loss: 0.221875\n",
      "Epoch 30, train loss: 0.240568\n",
      "Epoch 31, train loss: 0.188881\n",
      "Epoch 32, train loss: 0.200071\n",
      "Epoch 33, train loss: 0.200086\n",
      "Epoch 34, train loss: 0.191035\n",
      "Epoch 35, train loss: 0.228355\n",
      "Epoch 36, train loss: 0.234850\n",
      "Epoch 37, train loss: 0.217013\n",
      "Epoch 38, train loss: 0.182038\n",
      "Epoch 39, train loss: 0.166483\n",
      "Epoch 40, train loss: 0.160829\n",
      "Epoch 41, train loss: 0.220154\n",
      "Epoch 42, train loss: 0.184629\n",
      "Epoch 43, train loss: 0.168403\n",
      "Epoch 44, train loss: 0.160485\n",
      "Epoch 45, train loss: 0.213612\n",
      "Epoch 46, train loss: 0.218244\n",
      "Epoch 47, train loss: 0.271294\n",
      "Epoch 48, train loss: 0.246705\n",
      "Epoch 49, train loss: 0.159811\n",
      "Epoch 50, train loss: 0.212465\n",
      "Epoch 51, train loss: 0.192705\n",
      "Epoch 52, train loss: 0.219559\n",
      "Epoch 53, train loss: 0.192225\n",
      "Epoch 54, train loss: 0.170877\n",
      "Epoch 55, train loss: 0.169990\n",
      "Epoch 56, train loss: 0.174512\n",
      "Epoch 57, train loss: 0.188597\n",
      "Epoch 58, train loss: 0.222582\n",
      "Epoch 59, train loss: 0.250103\n",
      "Epoch 60, train loss: 0.202180\n",
      "Epoch 61, train loss: 0.194111\n",
      "Epoch 62, train loss: 0.218737\n",
      "Epoch 63, train loss: 0.274807\n",
      "Epoch 64, train loss: 0.160267\n",
      "Epoch 65, train loss: 0.200697\n",
      "Epoch 66, train loss: 0.195539\n",
      "Epoch 67, train loss: 0.207724\n",
      "Epoch 68, train loss: 0.273640\n",
      "Epoch 69, train loss: 0.189866\n",
      "Epoch 70, train loss: 0.212724\n",
      "Epoch 71, train loss: 0.196149\n",
      "Epoch 72, train loss: 0.258137\n",
      "Epoch 73, train loss: 0.173837\n",
      "Epoch 74, train loss: 0.182698\n",
      "Epoch 75, train loss: 0.169655\n",
      "Epoch 76, train loss: 0.201048\n",
      "Epoch 77, train loss: 0.225626\n",
      "Epoch 78, train loss: 0.220985\n",
      "Epoch 79, train loss: 0.228075\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "x here is\n",
      "[[283. 282. 281. ... 179. 154. 138.]\n",
      " [282. 281. 277. ... 154. 138. 151.]\n",
      " [281. 277. 267. ... 138. 151. 146.]\n",
      " ...\n",
      " [197. 194. 190. ... 159. 159. 156.]\n",
      " [194. 190. 189. ... 159. 156. 154.]\n",
      " [190. 189. 186. ... 156. 154. 152.]]\n",
      "y here is\n",
      "[[126. 126. 126. ... 126. 126. 126.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [121. 121. 121. ... 121. 121. 121.]\n",
      " ...\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " [149. 149. 149. ... 149. 149. 149.]\n",
      " [144. 144. 144. ... 144. 144. 144.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (2645, 18), Train/test: 1/2644\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[160. 158. 160. ... 238. 237. 246.]\n",
      " [158. 160. 166. ... 237. 246. 259.]\n",
      " [160. 166. 175. ... 246. 259. 269.]\n",
      " ...\n",
      " [220. 215. 223. ... 211. 202. 196.]\n",
      " [215. 223. 233. ... 202. 196. 199.]\n",
      " [223. 233. 241. ... 196. 199. 224.]]\n",
      "y here is\n",
      "[[251. 251. 251. ... 251. 251. 251.]\n",
      " [238. 238. 238. ... 238. 238. 238.]\n",
      " [232. 232. 232. ... 232. 232. 232.]\n",
      " ...\n",
      " [268. 268. 268. ... 268. 268. 268.]\n",
      " [301. 301. 301. ... 301. 301. 301.]\n",
      " [290. 290. 290. ... 290. 290. 290.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 712/9528\n",
      "Found 27 continuous time series\n",
      "Data shape: (10242, 18), Train/test: 10240/2\n",
      "Train test ratio: 5120.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CD8D30>\n",
      "Epoch 0, test loss: 0.206113\n",
      "Epoch 1, test loss: 0.207978\n",
      "Epoch 2, test loss: 0.206708\n",
      "Epoch 3, test loss: 0.206546\n",
      "Epoch 4, test loss: 0.210686\n",
      "Epoch 5, test loss: 0.206629\n",
      "Epoch 6, test loss: 0.206639\n",
      "Epoch 7, test loss: 0.206178\n",
      "Epoch 8, test loss: 0.208246\n",
      "Epoch 9, test loss: 0.207688\n",
      "Epoch 10, test loss: 0.211211\n",
      "Epoch 11, test loss: 0.207641\n",
      "Epoch 12, test loss: 0.208162\n",
      "Epoch 13, test loss: 0.206609\n",
      "Epoch 14, test loss: 0.209762\n",
      "Epoch 15, test loss: 0.209370\n",
      "Epoch 16, test loss: 0.208584\n",
      "Epoch 17, test loss: 0.208184\n",
      "Epoch 18, test loss: 0.210119\n",
      "Epoch 19, test loss: 0.207166\n",
      "Epoch 20, test loss: 0.207015\n",
      "Epoch 21, test loss: 0.208251\n",
      "Epoch 22, test loss: 0.207260\n",
      "Epoch 23, test loss: 0.207545\n",
      "Epoch 24, test loss: 0.208894\n",
      "Epoch 25, test loss: 0.207687\n",
      "Epoch 26, test loss: 0.209127\n",
      "Epoch 27, test loss: 0.207772\n",
      "Epoch 28, test loss: 0.208701\n",
      "Epoch 29, test loss: 0.208110\n",
      "Epoch 30, test loss: 0.206980\n",
      "Epoch 31, test loss: 0.207955\n",
      "Epoch 32, test loss: 0.207822\n",
      "Epoch 33, test loss: 0.208170\n",
      "Epoch 34, test loss: 0.208149\n",
      "Epoch 35, test loss: 0.210443\n",
      "Epoch 36, test loss: 0.208859\n",
      "Epoch 37, test loss: 0.208018\n",
      "Epoch 38, test loss: 0.208271\n",
      "Epoch 39, test loss: 0.209631\n",
      "Epoch 40, test loss: 0.207042\n",
      "Epoch 41, test loss: 0.208017\n",
      "Epoch 42, test loss: 0.208624\n",
      "Epoch 43, test loss: 0.207570\n",
      "Epoch 44, test loss: 0.207940\n",
      "Epoch 45, test loss: 0.209354\n",
      "Epoch 46, test loss: 0.208187\n",
      "Epoch 47, test loss: 0.207533\n",
      "Epoch 48, test loss: 0.207579\n",
      "Epoch 49, test loss: 0.208512\n",
      "Epoch 50, test loss: 0.208719\n",
      "Epoch 51, test loss: 0.208691\n",
      "Epoch 52, test loss: 0.212928\n",
      "Epoch 53, test loss: 0.209337\n",
      "Epoch 54, test loss: 0.208767\n",
      "Epoch 55, test loss: 0.209589\n",
      "Epoch 56, test loss: 0.207860\n",
      "Epoch 57, test loss: 0.208436\n",
      "Epoch 58, test loss: 0.208868\n",
      "Epoch 59, test loss: 0.208219\n",
      "Epoch 60, test loss: 0.211518\n",
      "Epoch 61, test loss: 0.208445\n",
      "Epoch 62, test loss: 0.208916\n",
      "Epoch 63, test loss: 0.208819\n",
      "Epoch 64, test loss: 0.210093\n",
      "Epoch 65, test loss: 0.211757\n",
      "Epoch 66, test loss: 0.208677\n",
      "Epoch 67, test loss: 0.212475\n",
      "Epoch 68, test loss: 0.217209\n",
      "Epoch 69, test loss: 0.213384\n",
      "Epoch 70, test loss: 0.209608\n",
      "Epoch 71, test loss: 0.209591\n",
      "Epoch 72, test loss: 0.212719\n",
      "Epoch 73, test loss: 0.209861\n",
      "Epoch 74, test loss: 0.209705\n",
      "Epoch 75, test loss: 0.209967\n",
      "Epoch 76, test loss: 0.209664\n",
      "Epoch 77, test loss: 0.209972\n",
      "Epoch 78, test loss: 0.209996\n",
      "Epoch 79, test loss: 0.210170\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CD8D30>\n",
      "Epoch 0, test loss: 0.205641\n",
      "Epoch 1, test loss: 0.206882\n",
      "Epoch 2, test loss: 0.207757\n",
      "Epoch 3, test loss: 0.205654\n",
      "Epoch 4, test loss: 0.206447\n",
      "Epoch 5, test loss: 0.207954\n",
      "Epoch 6, test loss: 0.206259\n",
      "Epoch 7, test loss: 0.207817\n",
      "Epoch 8, test loss: 0.210002\n",
      "Epoch 9, test loss: 0.206855\n",
      "Epoch 10, test loss: 0.206367\n",
      "Epoch 11, test loss: 0.206075\n",
      "Epoch 12, test loss: 0.206224\n",
      "Epoch 13, test loss: 0.208517\n",
      "Epoch 14, test loss: 0.206975\n",
      "Epoch 15, test loss: 0.205882\n",
      "Epoch 16, test loss: 0.206206\n",
      "Epoch 17, test loss: 0.206306\n",
      "Epoch 18, test loss: 0.207839\n",
      "Epoch 19, test loss: 0.206248\n",
      "Epoch 20, test loss: 0.207276\n",
      "Epoch 21, test loss: 0.208243\n",
      "Epoch 22, test loss: 0.207739\n",
      "Epoch 23, test loss: 0.206329\n",
      "Epoch 24, test loss: 0.206148\n",
      "Epoch 25, test loss: 0.206125\n",
      "Epoch 26, test loss: 0.206540\n",
      "Epoch 27, test loss: 0.206374\n",
      "Epoch 28, test loss: 0.206257\n",
      "Epoch 29, test loss: 0.206096\n",
      "Epoch 30, test loss: 0.208520\n",
      "Epoch 31, test loss: 0.207033\n",
      "Epoch 32, test loss: 0.205938\n",
      "Epoch 33, test loss: 0.208321\n",
      "Epoch 34, test loss: 0.206596\n",
      "Epoch 35, test loss: 0.207032\n",
      "Epoch 36, test loss: 0.209254\n",
      "Epoch 37, test loss: 0.207548\n",
      "Epoch 38, test loss: 0.208696\n",
      "Epoch 39, test loss: 0.207581\n",
      "Epoch 40, test loss: 0.206759\n",
      "Epoch 41, test loss: 0.206259\n",
      "Epoch 42, test loss: 0.206728\n",
      "Epoch 43, test loss: 0.209428\n",
      "Epoch 44, test loss: 0.206843\n",
      "Epoch 45, test loss: 0.207646\n",
      "Epoch 46, test loss: 0.207434\n",
      "Epoch 47, test loss: 0.207787\n",
      "Epoch 48, test loss: 0.207807\n",
      "Epoch 49, test loss: 0.209384\n",
      "Epoch 50, test loss: 0.208393\n",
      "Epoch 51, test loss: 0.208315\n",
      "Epoch 52, test loss: 0.207151\n",
      "Epoch 53, test loss: 0.207602\n",
      "Epoch 54, test loss: 0.208505\n",
      "Epoch 55, test loss: 0.206937\n",
      "Epoch 56, test loss: 0.206817\n",
      "Epoch 57, test loss: 0.207929\n",
      "Epoch 58, test loss: 0.207566\n",
      "Epoch 59, test loss: 0.208419\n",
      "Epoch 60, test loss: 0.207640\n",
      "Epoch 61, test loss: 0.210682\n",
      "Epoch 62, test loss: 0.207590\n",
      "Epoch 63, test loss: 0.207848\n",
      "Epoch 64, test loss: 0.209311\n",
      "Epoch 65, test loss: 0.207784\n",
      "Epoch 66, test loss: 0.208535\n",
      "Epoch 67, test loss: 0.209037\n",
      "Epoch 68, test loss: 0.208377\n",
      "Epoch 69, test loss: 0.209020\n",
      "Epoch 70, test loss: 0.207945\n",
      "Epoch 71, test loss: 0.208376\n",
      "Epoch 72, test loss: 0.208317\n",
      "Epoch 73, test loss: 0.208318\n",
      "Epoch 74, test loss: 0.212721\n",
      "Epoch 75, test loss: 0.208436\n",
      "Epoch 76, test loss: 0.209707\n",
      "Epoch 77, test loss: 0.208968\n",
      "Epoch 78, test loss: 0.208307\n",
      "Epoch 79, test loss: 0.208251\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292CD8D30>\n",
      "Epoch 0, test loss: 0.267651\n",
      "Epoch 1, test loss: 0.229280\n",
      "Epoch 2, test loss: 0.219474\n",
      "Epoch 3, test loss: 0.216132\n",
      "Epoch 4, test loss: 0.214674\n",
      "Epoch 5, test loss: 0.214490\n",
      "Epoch 6, test loss: 0.214316\n",
      "Epoch 7, test loss: 0.216629\n",
      "Epoch 8, test loss: 0.213257\n",
      "Epoch 9, test loss: 0.215515\n",
      "Epoch 10, test loss: 0.212392\n",
      "Epoch 11, test loss: 0.213250\n",
      "Epoch 12, test loss: 0.212204\n",
      "Epoch 13, test loss: 0.212438\n",
      "Epoch 14, test loss: 0.211789\n",
      "Epoch 15, test loss: 0.211016\n",
      "Epoch 16, test loss: 0.211350\n",
      "Epoch 17, test loss: 0.211108\n",
      "Epoch 18, test loss: 0.210770\n",
      "Epoch 19, test loss: 0.212007\n",
      "Epoch 20, test loss: 0.212568\n",
      "Epoch 21, test loss: 0.211783\n",
      "Epoch 22, test loss: 0.211336\n",
      "Epoch 23, test loss: 0.210638\n",
      "Epoch 24, test loss: 0.212946\n",
      "Epoch 25, test loss: 0.210416\n",
      "Epoch 26, test loss: 0.211388\n",
      "Epoch 27, test loss: 0.211234\n",
      "Epoch 28, test loss: 0.210538\n",
      "Epoch 29, test loss: 0.210883\n",
      "Epoch 30, test loss: 0.210926\n",
      "Epoch 31, test loss: 0.212567\n",
      "Epoch 32, test loss: 0.211019\n",
      "Epoch 33, test loss: 0.212876\n",
      "Epoch 34, test loss: 0.210690\n",
      "Epoch 35, test loss: 0.211574\n",
      "Epoch 36, test loss: 0.211020\n",
      "Epoch 37, test loss: 0.210550\n",
      "Epoch 38, test loss: 0.211667\n",
      "Epoch 39, test loss: 0.210677\n",
      "Epoch 40, test loss: 0.211031\n",
      "Epoch 41, test loss: 0.210513\n",
      "Epoch 42, test loss: 0.211147\n",
      "Epoch 43, test loss: 0.211147\n",
      "Epoch 44, test loss: 0.211597\n",
      "Epoch 45, test loss: 0.211236\n",
      "Epoch 46, test loss: 0.211910\n",
      "Epoch 47, test loss: 0.215425\n",
      "Epoch 48, test loss: 0.211812\n",
      "Epoch 49, test loss: 0.211721\n",
      "Epoch 50, test loss: 0.211695\n",
      "Epoch 51, test loss: 0.212653\n",
      "Epoch 52, test loss: 0.211367\n",
      "Epoch 53, test loss: 0.210870\n",
      "Epoch 54, test loss: 0.212827\n",
      "Epoch 55, test loss: 0.211593\n",
      "Epoch 56, test loss: 0.211048\n",
      "Epoch 57, test loss: 0.212531\n",
      "Epoch 58, test loss: 0.214012\n",
      "Epoch 59, test loss: 0.211881\n",
      "Epoch 60, test loss: 0.212355\n",
      "Epoch 61, test loss: 0.213202\n",
      "Epoch 62, test loss: 0.213462\n",
      "Epoch 63, test loss: 0.212533\n",
      "Epoch 64, test loss: 0.211640\n",
      "Epoch 65, test loss: 0.211647\n",
      "Epoch 66, test loss: 0.211698\n",
      "Epoch 67, test loss: 0.211624\n",
      "Epoch 68, test loss: 0.213632\n",
      "Epoch 69, test loss: 0.211736\n",
      "Epoch 70, test loss: 0.211331\n",
      "Epoch 71, test loss: 0.211911\n",
      "Epoch 72, test loss: 0.211176\n",
      "Epoch 73, test loss: 0.211393\n",
      "Epoch 74, test loss: 0.212912\n",
      "Epoch 75, test loss: 0.211590\n",
      "Epoch 76, test loss: 0.211974\n",
      "Epoch 77, test loss: 0.213297\n",
      "Epoch 78, test loss: 0.211940\n",
      "Epoch 79, test loss: 0.212761\n",
      "Epoch 80, test loss: 0.212336\n",
      "Epoch 81, test loss: 0.211453\n",
      "Epoch 82, test loss: 0.211475\n",
      "Epoch 83, test loss: 0.211441\n",
      "Epoch 84, test loss: 0.215014\n",
      "Epoch 85, test loss: 0.212776\n",
      "Epoch 86, test loss: 0.211755\n",
      "Epoch 87, test loss: 0.212124\n",
      "Epoch 88, test loss: 0.213153\n",
      "Epoch 89, test loss: 0.211320\n",
      "Epoch 90, test loss: 0.213462\n",
      "Epoch 91, test loss: 0.214583\n",
      "Epoch 92, test loss: 0.213221\n",
      "Epoch 93, test loss: 0.212042\n",
      "Epoch 94, test loss: 0.212436\n",
      "Epoch 95, test loss: 0.211836\n",
      "Epoch 96, test loss: 0.211656\n",
      "Epoch 97, test loss: 0.212543\n",
      "Epoch 98, test loss: 0.211586\n",
      "Epoch 99, test loss: 0.211814\n",
      "Epoch 100, test loss: 0.211768\n",
      "Epoch 101, test loss: 0.212995\n",
      "Epoch 102, test loss: 0.211631\n",
      "Epoch 103, test loss: 0.211738\n",
      "Epoch 104, test loss: 0.212624\n",
      "Epoch 105, test loss: 0.213815\n",
      "Epoch 106, test loss: 0.214319\n",
      "Epoch 107, test loss: 0.211942\n",
      "Epoch 108, test loss: 0.212036\n",
      "Epoch 109, test loss: 0.211520\n",
      "Epoch 110, test loss: 0.211805\n",
      "Epoch 111, test loss: 0.214368\n",
      "Epoch 112, test loss: 0.213433\n",
      "Epoch 113, test loss: 0.211950\n",
      "Epoch 114, test loss: 0.212432\n",
      "Epoch 115, test loss: 0.211903\n",
      "Epoch 116, test loss: 0.212443\n",
      "Epoch 117, test loss: 0.212694\n",
      "Epoch 118, test loss: 0.212149\n",
      "Epoch 119, test loss: 0.212031\n",
      "Epoch 120, test loss: 0.216353\n",
      "Epoch 121, test loss: 0.211747\n",
      "Epoch 122, test loss: 0.212238\n",
      "Epoch 123, test loss: 0.211904\n",
      "Epoch 124, test loss: 0.214237\n",
      "Epoch 125, test loss: 0.212576\n",
      "Epoch 126, test loss: 0.212245\n",
      "Epoch 127, test loss: 0.211777\n",
      "Epoch 128, test loss: 0.212869\n",
      "Epoch 129, test loss: 0.213154\n",
      "Epoch 130, test loss: 0.212783\n",
      "Epoch 131, test loss: 0.212253\n",
      "Epoch 132, test loss: 0.212568\n",
      "Epoch 133, test loss: 0.212344\n",
      "Epoch 134, test loss: 0.212076\n",
      "Epoch 135, test loss: 0.212416\n",
      "Epoch 136, test loss: 0.212169\n",
      "Epoch 137, test loss: 0.212524\n",
      "Epoch 138, test loss: 0.212018\n",
      "Epoch 139, test loss: 0.211661\n",
      "Epoch 140, test loss: 0.213228\n",
      "Epoch 141, test loss: 0.212148\n",
      "Epoch 142, test loss: 0.212692\n",
      "Epoch 143, test loss: 0.211860\n",
      "Epoch 144, test loss: 0.212218\n",
      "Epoch 145, test loss: 0.214973\n",
      "Epoch 146, test loss: 0.215106\n",
      "Epoch 147, test loss: 0.212108\n",
      "Epoch 148, test loss: 0.212343\n",
      "Epoch 149, test loss: 0.212888\n",
      "Epoch 150, test loss: 0.212427\n",
      "Epoch 151, test loss: 0.211859\n",
      "Epoch 152, test loss: 0.213093\n",
      "Epoch 153, test loss: 0.212760\n",
      "Epoch 154, test loss: 0.212125\n",
      "Epoch 155, test loss: 0.212533\n",
      "Epoch 156, test loss: 0.212810\n",
      "Epoch 157, test loss: 0.212089\n",
      "Epoch 158, test loss: 0.212355\n",
      "Epoch 159, test loss: 0.212898\n",
      "Epoch 160, test loss: 0.215459\n",
      "Epoch 161, test loss: 0.212110\n",
      "Epoch 162, test loss: 0.212310\n",
      "Epoch 163, test loss: 0.211809\n",
      "Epoch 164, test loss: 0.212704\n",
      "Epoch 165, test loss: 0.212408\n",
      "Epoch 166, test loss: 0.212219\n",
      "Epoch 167, test loss: 0.212919\n",
      "Epoch 168, test loss: 0.212159\n",
      "Epoch 169, test loss: 0.211937\n",
      "Epoch 170, test loss: 0.212377\n",
      "Epoch 171, test loss: 0.212733\n",
      "Epoch 172, test loss: 0.212883\n",
      "Epoch 173, test loss: 0.213614\n",
      "Epoch 174, test loss: 0.211957\n",
      "Epoch 175, test loss: 0.214042\n",
      "Epoch 176, test loss: 0.211889\n",
      "Epoch 177, test loss: 0.218237\n",
      "Epoch 178, test loss: 0.212403\n",
      "Epoch 179, test loss: 0.217135\n",
      "Epoch 180, test loss: 0.212237\n",
      "Epoch 181, test loss: 0.213294\n",
      "Epoch 182, test loss: 0.212594\n",
      "Epoch 183, test loss: 0.212543\n",
      "Epoch 184, test loss: 0.212952\n",
      "Epoch 185, test loss: 0.211941\n",
      "Epoch 186, test loss: 0.214449\n",
      "Epoch 187, test loss: 0.213134\n",
      "Epoch 188, test loss: 0.213984\n",
      "Epoch 189, test loss: 0.211792\n",
      "Epoch 190, test loss: 0.212049\n",
      "Epoch 191, test loss: 0.213507\n",
      "Epoch 192, test loss: 0.212537\n",
      "Epoch 193, test loss: 0.212761\n",
      "Epoch 194, test loss: 0.213835\n",
      "Epoch 195, test loss: 0.212529\n",
      "Epoch 196, test loss: 0.214796\n",
      "Epoch 197, test loss: 0.214522\n",
      "Epoch 198, test loss: 0.211907\n",
      "Epoch 199, test loss: 0.212720\n",
      "Epoch 200, test loss: 0.212194\n",
      "Epoch 201, test loss: 0.212402\n",
      "Epoch 202, test loss: 0.211981\n",
      "Epoch 203, test loss: 0.212334\n",
      "Epoch 204, test loss: 0.212430\n",
      "Epoch 205, test loss: 0.212387\n",
      "Epoch 206, test loss: 0.211928\n",
      "Epoch 207, test loss: 0.212442\n",
      "Epoch 208, test loss: 0.212383\n",
      "Epoch 209, test loss: 0.211843\n",
      "Epoch 210, test loss: 0.212323\n",
      "Epoch 211, test loss: 0.212799\n",
      "Epoch 212, test loss: 0.211934\n",
      "Epoch 213, test loss: 0.212071\n",
      "Epoch 214, test loss: 0.211992\n",
      "Epoch 215, test loss: 0.211991\n",
      "Epoch 216, test loss: 0.212132\n",
      "Epoch 217, test loss: 0.212211\n",
      "Epoch 218, test loss: 0.212104\n",
      "Epoch 219, test loss: 0.213765\n",
      "Epoch 220, test loss: 0.212402\n",
      "Epoch 221, test loss: 0.212458\n",
      "Epoch 222, test loss: 0.213415\n",
      "Epoch 223, test loss: 0.212812\n",
      "Epoch 224, test loss: 0.213836\n",
      "Epoch 225, test loss: 0.213540\n",
      "Epoch 226, test loss: 0.214982\n",
      "Epoch 227, test loss: 0.212476\n",
      "Epoch 228, test loss: 0.214484\n",
      "Epoch 229, test loss: 0.212008\n",
      "Epoch 230, test loss: 0.215278\n",
      "Epoch 231, test loss: 0.212605\n",
      "Epoch 232, test loss: 0.213674\n",
      "Epoch 233, test loss: 0.212742\n",
      "Epoch 234, test loss: 0.212373\n",
      "Epoch 235, test loss: 0.212253\n",
      "Epoch 236, test loss: 0.212680\n",
      "Epoch 237, test loss: 0.212398\n",
      "Epoch 238, test loss: 0.212781\n",
      "Epoch 239, test loss: 0.212333\n",
      "Pretrain data: 19786775.0\n",
      "Building dataset, requesting data from 0 to 665\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6409/101793\n",
      "Found 665 continuous time series\n",
      "Data shape: (108204, 18), Train/test: 108202/2\n",
      "Train test ratio: 54101.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1690\n",
      "Epoch 0, train loss: 0.247627\n",
      "Epoch 1, train loss: 0.250001\n",
      "Epoch 2, train loss: 0.195920\n",
      "Epoch 3, train loss: 0.211692\n",
      "Epoch 4, train loss: 0.216014\n",
      "Epoch 5, train loss: 0.152422\n",
      "Epoch 6, train loss: 0.190963\n",
      "Epoch 7, train loss: 0.251137\n",
      "Epoch 8, train loss: 0.204489\n",
      "Epoch 9, train loss: 0.218117\n",
      "Epoch 10, train loss: 0.163756\n",
      "Epoch 11, train loss: 0.287165\n",
      "Epoch 12, train loss: 0.256145\n",
      "Epoch 13, train loss: 0.209978\n",
      "Epoch 14, train loss: 0.216040\n",
      "Epoch 15, train loss: 0.249056\n",
      "Epoch 16, train loss: 0.181027\n",
      "Epoch 17, train loss: 0.167162\n",
      "Epoch 18, train loss: 0.212062\n",
      "Epoch 19, train loss: 0.189112\n",
      "Epoch 20, train loss: 0.235062\n",
      "Epoch 21, train loss: 0.232016\n",
      "Epoch 22, train loss: 0.154907\n",
      "Epoch 23, train loss: 0.213001\n",
      "Epoch 24, train loss: 0.172020\n",
      "Epoch 25, train loss: 0.220023\n",
      "Epoch 26, train loss: 0.165659\n",
      "Epoch 27, train loss: 0.237995\n",
      "Epoch 28, train loss: 0.182984\n",
      "Epoch 29, train loss: 0.233472\n",
      "Epoch 30, train loss: 0.204655\n",
      "Epoch 31, train loss: 0.240351\n",
      "Epoch 32, train loss: 0.214562\n",
      "Epoch 33, train loss: 0.232503\n",
      "Epoch 34, train loss: 0.245040\n",
      "Epoch 35, train loss: 0.188057\n",
      "Epoch 36, train loss: 0.277162\n",
      "Epoch 37, train loss: 0.207105\n",
      "Epoch 38, train loss: 0.181267\n",
      "Epoch 39, train loss: 0.189184\n",
      "Epoch 40, train loss: 0.174281\n",
      "Epoch 41, train loss: 0.239811\n",
      "Epoch 42, train loss: 0.255600\n",
      "Epoch 43, train loss: 0.236072\n",
      "Epoch 44, train loss: 0.181394\n",
      "Epoch 45, train loss: 0.204726\n",
      "Epoch 46, train loss: 0.229108\n",
      "Epoch 47, train loss: 0.149921\n",
      "Epoch 48, train loss: 0.201387\n",
      "Epoch 49, train loss: 0.227253\n",
      "Epoch 50, train loss: 0.184085\n",
      "Epoch 51, train loss: 0.198884\n",
      "Epoch 52, train loss: 0.151795\n",
      "Epoch 53, train loss: 0.182096\n",
      "Epoch 54, train loss: 0.168751\n",
      "Epoch 55, train loss: 0.198340\n",
      "Epoch 56, train loss: 0.233381\n",
      "Epoch 57, train loss: 0.248668\n",
      "Epoch 58, train loss: 0.191754\n",
      "Epoch 59, train loss: 0.210961\n",
      "Epoch 60, train loss: 0.223818\n",
      "Epoch 61, train loss: 0.228873\n",
      "Epoch 62, train loss: 0.176605\n",
      "Epoch 63, train loss: 0.198530\n",
      "Epoch 64, train loss: 0.245183\n",
      "Epoch 65, train loss: 0.194354\n",
      "Epoch 66, train loss: 0.229533\n",
      "Epoch 67, train loss: 0.190577\n",
      "Epoch 68, train loss: 0.209518\n",
      "Epoch 69, train loss: 0.250908\n",
      "Epoch 70, train loss: 0.250496\n",
      "Epoch 71, train loss: 0.212440\n",
      "Epoch 72, train loss: 0.269042\n",
      "Epoch 73, train loss: 0.171109\n",
      "Epoch 74, train loss: 0.212759\n",
      "Epoch 75, train loss: 0.236393\n",
      "Epoch 76, train loss: 0.211450\n",
      "Epoch 77, train loss: 0.182262\n",
      "Epoch 78, train loss: 0.218988\n",
      "Epoch 79, train loss: 0.248900\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "x here is\n",
      "[[254. 250. 249. ... 210. 206. 201.]\n",
      " [250. 249. 247. ... 206. 201. 198.]\n",
      " [249. 247. 242. ... 201. 198. 195.]\n",
      " ...\n",
      " [116. 112. 112. ... 143. 159. 177.]\n",
      " [112. 112. 111. ... 159. 177. 181.]\n",
      " [112. 111. 112. ... 177. 181. 205.]]\n",
      "y here is\n",
      "[[185. 185. 185. ... 185. 185. 185.]\n",
      " [184. 184. 184. ... 184. 184. 184.]\n",
      " [182. 182. 182. ... 182. 182. 182.]\n",
      " ...\n",
      " [254. 254. 254. ... 254. 254. 254.]\n",
      " [248. 248. 248. ... 248. 248. 248.]\n",
      " [242. 242. 242. ... 242. 242. 242.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 45 continuous time series\n",
      "Data shape: (1884, 18), Train/test: 1/1883\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "x here is\n",
      "[[ 76.  72.  68. ... 140. 143. 142.]\n",
      " [ 72.  68.  65. ... 143. 142. 136.]\n",
      " [ 68.  65.  63. ... 142. 136. 125.]\n",
      " ...\n",
      " [304. 299. 291. ... 232. 236. 237.]\n",
      " [299. 291. 282. ... 236. 237. 247.]\n",
      " [291. 282. 274. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[129. 129. 129. ... 129. 129. 129.]\n",
      " [129. 129. 129. ... 129. 129. 129.]\n",
      " [127. 127. 127. ... 127. 127. 127.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1108/6961\n",
      "Found 177 continuous time series\n",
      "Data shape: (8071, 18), Train/test: 8069/2\n",
      "Train test ratio: 4034.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292C5F550>\n",
      "Epoch 0, test loss: 0.220811\n",
      "Epoch 1, test loss: 0.221113\n",
      "Epoch 2, test loss: 0.221960\n",
      "Epoch 3, test loss: 0.220510\n",
      "Epoch 4, test loss: 0.220541\n",
      "Epoch 5, test loss: 0.219863\n",
      "Epoch 6, test loss: 0.219784\n",
      "Epoch 7, test loss: 0.219922\n",
      "Epoch 8, test loss: 0.219625\n",
      "Epoch 9, test loss: 0.219612\n",
      "Epoch 10, test loss: 0.220775\n",
      "Epoch 11, test loss: 0.231002\n",
      "Epoch 12, test loss: 0.222366\n",
      "Epoch 13, test loss: 0.220873\n",
      "Epoch 14, test loss: 0.220391\n",
      "Epoch 15, test loss: 0.221814\n",
      "Epoch 16, test loss: 0.220114\n",
      "Epoch 17, test loss: 0.220130\n",
      "Epoch 18, test loss: 0.223066\n",
      "Epoch 19, test loss: 0.221965\n",
      "Epoch 20, test loss: 0.220412\n",
      "Epoch 21, test loss: 0.222119\n",
      "Epoch 22, test loss: 0.222574\n",
      "Epoch 23, test loss: 0.220577\n",
      "Epoch 24, test loss: 0.221489\n",
      "Epoch 25, test loss: 0.220889\n",
      "Epoch 26, test loss: 0.220390\n",
      "Epoch 27, test loss: 0.220621\n",
      "Epoch 28, test loss: 0.220348\n",
      "Epoch 29, test loss: 0.223966\n",
      "Epoch 30, test loss: 0.223187\n",
      "Epoch 31, test loss: 0.220851\n",
      "Epoch 32, test loss: 0.224705\n",
      "Epoch 33, test loss: 0.221694\n",
      "Epoch 34, test loss: 0.220992\n",
      "Epoch 35, test loss: 0.224765\n",
      "Epoch 36, test loss: 0.220346\n",
      "Epoch 37, test loss: 0.222318\n",
      "Epoch 38, test loss: 0.225454\n",
      "Epoch 39, test loss: 0.220842\n",
      "Epoch 40, test loss: 0.220348\n",
      "Epoch 41, test loss: 0.230243\n",
      "Epoch 42, test loss: 0.221173\n",
      "Epoch 43, test loss: 0.220970\n",
      "Epoch 44, test loss: 0.221301\n",
      "Epoch 45, test loss: 0.220911\n",
      "Epoch 46, test loss: 0.220890\n",
      "Epoch 47, test loss: 0.223317\n",
      "Epoch 48, test loss: 0.221001\n",
      "Epoch 49, test loss: 0.220899\n",
      "Epoch 50, test loss: 0.220080\n",
      "Epoch 51, test loss: 0.223298\n",
      "Epoch 52, test loss: 0.219897\n",
      "Epoch 53, test loss: 0.220499\n",
      "Epoch 54, test loss: 0.220278\n",
      "Epoch 55, test loss: 0.220218\n",
      "Epoch 56, test loss: 0.225071\n",
      "Epoch 57, test loss: 0.220840\n",
      "Epoch 58, test loss: 0.221394\n",
      "Epoch 59, test loss: 0.220941\n",
      "Epoch 60, test loss: 0.220780\n",
      "Epoch 61, test loss: 0.219941\n",
      "Epoch 62, test loss: 0.224435\n",
      "Epoch 63, test loss: 0.222692\n",
      "Epoch 64, test loss: 0.233788\n",
      "Epoch 65, test loss: 0.219846\n",
      "Epoch 66, test loss: 0.221780\n",
      "Epoch 67, test loss: 0.219857\n",
      "Epoch 68, test loss: 0.220025\n",
      "Epoch 69, test loss: 0.220365\n",
      "Epoch 70, test loss: 0.226042\n",
      "Epoch 71, test loss: 0.222276\n",
      "Epoch 72, test loss: 0.221522\n",
      "Epoch 73, test loss: 0.223998\n",
      "Epoch 74, test loss: 0.224699\n",
      "Epoch 75, test loss: 0.227868\n",
      "Epoch 76, test loss: 0.221489\n",
      "Epoch 77, test loss: 0.219471\n",
      "Epoch 78, test loss: 0.223556\n",
      "Epoch 79, test loss: 0.219211\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292C5F550>\n",
      "Epoch 0, test loss: 0.229349\n",
      "Epoch 1, test loss: 0.221019\n",
      "Epoch 2, test loss: 0.220711\n",
      "Epoch 3, test loss: 0.222016\n",
      "Epoch 4, test loss: 0.220401\n",
      "Epoch 5, test loss: 0.219571\n",
      "Epoch 6, test loss: 0.220260\n",
      "Epoch 7, test loss: 0.219260\n",
      "Epoch 8, test loss: 0.223244\n",
      "Epoch 9, test loss: 0.219594\n",
      "Epoch 10, test loss: 0.220645\n",
      "Epoch 11, test loss: 0.220887\n",
      "Epoch 12, test loss: 0.221160\n",
      "Epoch 13, test loss: 0.219772\n",
      "Epoch 14, test loss: 0.220553\n",
      "Epoch 15, test loss: 0.220840\n",
      "Epoch 16, test loss: 0.220665\n",
      "Epoch 17, test loss: 0.221238\n",
      "Epoch 18, test loss: 0.223830\n",
      "Epoch 19, test loss: 0.221616\n",
      "Epoch 20, test loss: 0.220506\n",
      "Epoch 21, test loss: 0.221340\n",
      "Epoch 22, test loss: 0.223017\n",
      "Epoch 23, test loss: 0.226417\n",
      "Epoch 24, test loss: 0.220138\n",
      "Epoch 25, test loss: 0.223088\n",
      "Epoch 26, test loss: 0.221303\n",
      "Epoch 27, test loss: 0.220588\n",
      "Epoch 28, test loss: 0.220714\n",
      "Epoch 29, test loss: 0.224324\n",
      "Epoch 30, test loss: 0.220009\n",
      "Epoch 31, test loss: 0.220180\n",
      "Epoch 32, test loss: 0.221053\n",
      "Epoch 33, test loss: 0.219464\n",
      "Epoch 34, test loss: 0.222778\n",
      "Epoch 35, test loss: 0.219527\n",
      "Epoch 36, test loss: 0.227477\n",
      "Epoch 37, test loss: 0.219020\n",
      "Epoch 38, test loss: 0.223775\n",
      "Epoch 39, test loss: 0.219380\n",
      "Epoch 40, test loss: 0.219774\n",
      "Epoch 41, test loss: 0.220588\n",
      "Epoch 42, test loss: 0.223178\n",
      "Epoch 43, test loss: 0.221920\n",
      "Epoch 44, test loss: 0.221316\n",
      "Epoch 45, test loss: 0.222171\n",
      "Epoch 46, test loss: 0.221340\n",
      "Epoch 47, test loss: 0.221357\n",
      "Epoch 48, test loss: 0.221199\n",
      "Epoch 49, test loss: 0.221925\n",
      "Epoch 50, test loss: 0.220938\n",
      "Epoch 51, test loss: 0.226213\n",
      "Epoch 52, test loss: 0.219147\n",
      "Epoch 53, test loss: 0.220435\n",
      "Epoch 54, test loss: 0.220279\n",
      "Epoch 55, test loss: 0.219339\n",
      "Epoch 56, test loss: 0.221160\n",
      "Epoch 57, test loss: 0.220627\n",
      "Epoch 58, test loss: 0.219174\n",
      "Epoch 59, test loss: 0.221089\n",
      "Epoch 60, test loss: 0.221943\n",
      "Epoch 61, test loss: 0.221073\n",
      "Epoch 62, test loss: 0.220703\n",
      "Epoch 63, test loss: 0.221138\n",
      "Epoch 64, test loss: 0.218923\n",
      "Epoch 65, test loss: 0.222067\n",
      "Epoch 66, test loss: 0.220060\n",
      "Epoch 67, test loss: 0.220657\n",
      "Epoch 68, test loss: 0.219512\n",
      "Epoch 69, test loss: 0.221362\n",
      "Epoch 70, test loss: 0.220504\n",
      "Epoch 71, test loss: 0.221037\n",
      "Epoch 72, test loss: 0.220453\n",
      "Epoch 73, test loss: 0.229630\n",
      "Epoch 74, test loss: 0.219770\n",
      "Epoch 75, test loss: 0.220219\n",
      "Epoch 76, test loss: 0.223047\n",
      "Epoch 77, test loss: 0.222546\n",
      "Epoch 78, test loss: 0.220605\n",
      "Epoch 79, test loss: 0.220465\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A292C5F550>\n",
      "Epoch 0, test loss: 0.368361\n",
      "Epoch 1, test loss: 0.284598\n",
      "Epoch 2, test loss: 0.252123\n",
      "Epoch 3, test loss: 0.240173\n",
      "Epoch 4, test loss: 0.236580\n",
      "Epoch 5, test loss: 0.231071\n",
      "Epoch 6, test loss: 0.229085\n",
      "Epoch 7, test loss: 0.228835\n",
      "Epoch 8, test loss: 0.226873\n",
      "Epoch 9, test loss: 0.230237\n",
      "Epoch 10, test loss: 0.226208\n",
      "Epoch 11, test loss: 0.227357\n",
      "Epoch 12, test loss: 0.226136\n",
      "Epoch 13, test loss: 0.228115\n",
      "Epoch 14, test loss: 0.224635\n",
      "Epoch 15, test loss: 0.224458\n",
      "Epoch 16, test loss: 0.237387\n",
      "Epoch 17, test loss: 0.223944\n",
      "Epoch 18, test loss: 0.227673\n",
      "Epoch 19, test loss: 0.223940\n",
      "Epoch 20, test loss: 0.222521\n",
      "Epoch 21, test loss: 0.224789\n",
      "Epoch 22, test loss: 0.231948\n",
      "Epoch 23, test loss: 0.226291\n",
      "Epoch 24, test loss: 0.223957\n",
      "Epoch 25, test loss: 0.222949\n",
      "Epoch 26, test loss: 0.224879\n",
      "Epoch 27, test loss: 0.222694\n",
      "Epoch 28, test loss: 0.226765\n",
      "Epoch 29, test loss: 0.224529\n",
      "Epoch 30, test loss: 0.222420\n",
      "Epoch 31, test loss: 0.224098\n",
      "Epoch 32, test loss: 0.222745\n",
      "Epoch 33, test loss: 0.223030\n",
      "Epoch 34, test loss: 0.222944\n",
      "Epoch 35, test loss: 0.222126\n",
      "Epoch 36, test loss: 0.224011\n",
      "Epoch 37, test loss: 0.223051\n",
      "Epoch 38, test loss: 0.221967\n",
      "Epoch 39, test loss: 0.222521\n",
      "Epoch 40, test loss: 0.226029\n",
      "Epoch 41, test loss: 0.224024\n",
      "Epoch 42, test loss: 0.226255\n",
      "Epoch 43, test loss: 0.231195\n",
      "Epoch 44, test loss: 0.224837\n",
      "Epoch 45, test loss: 0.222550\n",
      "Epoch 46, test loss: 0.233360\n",
      "Epoch 47, test loss: 0.222180\n",
      "Epoch 48, test loss: 0.222640\n",
      "Epoch 49, test loss: 0.227330\n",
      "Epoch 50, test loss: 0.223851\n",
      "Epoch 51, test loss: 0.222508\n",
      "Epoch 52, test loss: 0.226264\n",
      "Epoch 53, test loss: 0.222001\n",
      "Epoch 54, test loss: 0.227690\n",
      "Epoch 55, test loss: 0.223426\n",
      "Epoch 56, test loss: 0.222459\n",
      "Epoch 57, test loss: 0.223896\n",
      "Epoch 58, test loss: 0.225485\n",
      "Epoch 59, test loss: 0.223812\n",
      "Epoch 60, test loss: 0.222281\n",
      "Epoch 61, test loss: 0.222955\n",
      "Epoch 62, test loss: 0.223033\n",
      "Epoch 63, test loss: 0.225211\n",
      "Epoch 64, test loss: 0.222245\n",
      "Epoch 65, test loss: 0.223836\n",
      "Epoch 66, test loss: 0.221838\n",
      "Epoch 67, test loss: 0.226580\n",
      "Epoch 68, test loss: 0.223601\n",
      "Epoch 69, test loss: 0.223426\n",
      "Epoch 70, test loss: 0.223457\n",
      "Epoch 71, test loss: 0.224385\n",
      "Epoch 72, test loss: 0.222195\n",
      "Epoch 73, test loss: 0.222329\n",
      "Epoch 74, test loss: 0.224815\n",
      "Epoch 75, test loss: 0.222115\n",
      "Epoch 76, test loss: 0.224596\n",
      "Epoch 77, test loss: 0.222290\n",
      "Epoch 78, test loss: 0.222855\n",
      "Epoch 79, test loss: 0.224680\n",
      "Epoch 80, test loss: 0.222697\n",
      "Epoch 81, test loss: 0.222738\n",
      "Epoch 82, test loss: 0.222041\n",
      "Epoch 83, test loss: 0.221774\n",
      "Epoch 84, test loss: 0.227567\n",
      "Epoch 85, test loss: 0.227984\n",
      "Epoch 86, test loss: 0.229601\n",
      "Epoch 87, test loss: 0.223069\n",
      "Epoch 88, test loss: 0.222370\n",
      "Epoch 89, test loss: 0.221749\n",
      "Epoch 90, test loss: 0.222026\n",
      "Epoch 91, test loss: 0.224503\n",
      "Epoch 92, test loss: 0.223430\n",
      "Epoch 93, test loss: 0.222599\n",
      "Epoch 94, test loss: 0.221829\n",
      "Epoch 95, test loss: 0.221565\n",
      "Epoch 96, test loss: 0.224334\n",
      "Epoch 97, test loss: 0.221880\n",
      "Epoch 98, test loss: 0.222819\n",
      "Epoch 99, test loss: 0.221937\n",
      "Epoch 100, test loss: 0.223031\n",
      "Epoch 101, test loss: 0.222242\n",
      "Epoch 102, test loss: 0.222589\n",
      "Epoch 103, test loss: 0.229699\n",
      "Epoch 104, test loss: 0.227315\n",
      "Epoch 105, test loss: 0.222269\n",
      "Epoch 106, test loss: 0.222451\n",
      "Epoch 107, test loss: 0.223386\n",
      "Epoch 108, test loss: 0.226253\n",
      "Epoch 109, test loss: 0.221837\n",
      "Epoch 110, test loss: 0.221600\n",
      "Epoch 111, test loss: 0.222004\n",
      "Epoch 112, test loss: 0.222363\n",
      "Epoch 113, test loss: 0.224053\n",
      "Epoch 114, test loss: 0.221648\n",
      "Epoch 115, test loss: 0.225335\n",
      "Epoch 116, test loss: 0.221848\n",
      "Epoch 117, test loss: 0.224510\n",
      "Epoch 118, test loss: 0.222388\n",
      "Epoch 119, test loss: 0.222151\n",
      "Epoch 120, test loss: 0.222459\n",
      "Epoch 121, test loss: 0.222716\n",
      "Epoch 122, test loss: 0.221734\n",
      "Epoch 123, test loss: 0.222223\n",
      "Epoch 124, test loss: 0.221975\n",
      "Epoch 125, test loss: 0.221689\n",
      "Epoch 126, test loss: 0.221911\n",
      "Epoch 127, test loss: 0.221174\n",
      "Epoch 128, test loss: 0.223363\n",
      "Epoch 129, test loss: 0.222912\n",
      "Epoch 130, test loss: 0.222730\n",
      "Epoch 131, test loss: 0.221570\n",
      "Epoch 132, test loss: 0.223199\n",
      "Epoch 133, test loss: 0.232437\n",
      "Epoch 134, test loss: 0.222941\n",
      "Epoch 135, test loss: 0.222226\n",
      "Epoch 136, test loss: 0.222838\n",
      "Epoch 137, test loss: 0.222250\n",
      "Epoch 138, test loss: 0.230060\n",
      "Epoch 139, test loss: 0.222628\n",
      "Epoch 140, test loss: 0.221629\n",
      "Epoch 141, test loss: 0.222218\n",
      "Epoch 142, test loss: 0.222512\n",
      "Epoch 143, test loss: 0.222347\n",
      "Epoch 144, test loss: 0.223662\n",
      "Epoch 145, test loss: 0.226964\n",
      "Epoch 146, test loss: 0.222412\n",
      "Epoch 147, test loss: 0.221926\n",
      "Epoch 148, test loss: 0.222243\n",
      "Epoch 149, test loss: 0.221647\n",
      "Epoch 150, test loss: 0.221556\n",
      "Epoch 151, test loss: 0.222689\n",
      "Epoch 152, test loss: 0.223606\n",
      "Epoch 153, test loss: 0.222152\n",
      "Epoch 154, test loss: 0.223584\n",
      "Epoch 155, test loss: 0.221861\n",
      "Epoch 156, test loss: 0.221836\n",
      "Epoch 157, test loss: 0.221819\n",
      "Epoch 158, test loss: 0.225737\n",
      "Epoch 159, test loss: 0.222348\n",
      "Epoch 160, test loss: 0.221868\n",
      "Epoch 161, test loss: 0.221391\n",
      "Epoch 162, test loss: 0.221691\n",
      "Epoch 163, test loss: 0.223329\n",
      "Epoch 164, test loss: 0.229539\n",
      "Epoch 165, test loss: 0.222256\n",
      "Epoch 166, test loss: 0.222215\n",
      "Epoch 167, test loss: 0.225586\n",
      "Epoch 168, test loss: 0.221381\n",
      "Epoch 169, test loss: 0.221575\n",
      "Epoch 170, test loss: 0.222700\n",
      "Epoch 171, test loss: 0.224592\n",
      "Epoch 172, test loss: 0.221584\n",
      "Epoch 173, test loss: 0.221356\n",
      "Epoch 174, test loss: 0.221782\n",
      "Epoch 175, test loss: 0.221433\n",
      "Epoch 176, test loss: 0.221635\n",
      "Epoch 177, test loss: 0.222623\n",
      "Epoch 178, test loss: 0.222617\n",
      "Epoch 179, test loss: 0.222583\n",
      "Epoch 180, test loss: 0.223971\n",
      "Epoch 181, test loss: 0.223531\n",
      "Epoch 182, test loss: 0.221896\n",
      "Epoch 183, test loss: 0.222073\n",
      "Epoch 184, test loss: 0.222733\n",
      "Epoch 185, test loss: 0.222022\n",
      "Epoch 186, test loss: 0.224355\n",
      "Epoch 187, test loss: 0.222120\n",
      "Epoch 188, test loss: 0.222878\n",
      "Epoch 189, test loss: 0.221728\n",
      "Epoch 190, test loss: 0.224814\n",
      "Epoch 191, test loss: 0.222418\n",
      "Epoch 192, test loss: 0.222132\n",
      "Epoch 193, test loss: 0.222038\n",
      "Epoch 194, test loss: 0.222069\n",
      "Epoch 195, test loss: 0.222704\n",
      "Epoch 196, test loss: 0.223034\n",
      "Epoch 197, test loss: 0.221657\n",
      "Epoch 198, test loss: 0.224813\n",
      "Epoch 199, test loss: 0.222648\n",
      "Epoch 200, test loss: 0.222118\n",
      "Epoch 201, test loss: 0.222934\n",
      "Epoch 202, test loss: 0.221931\n",
      "Epoch 203, test loss: 0.221672\n",
      "Epoch 204, test loss: 0.222135\n",
      "Epoch 205, test loss: 0.224650\n",
      "Epoch 206, test loss: 0.224276\n",
      "Epoch 207, test loss: 0.225476\n",
      "Epoch 208, test loss: 0.223761\n",
      "Epoch 209, test loss: 0.222647\n",
      "Epoch 210, test loss: 0.223799\n",
      "Epoch 211, test loss: 0.223742\n",
      "Epoch 212, test loss: 0.221892\n",
      "Epoch 213, test loss: 0.223796\n",
      "Epoch 214, test loss: 0.222289\n",
      "Epoch 215, test loss: 0.222746\n",
      "Epoch 216, test loss: 0.222641\n",
      "Epoch 217, test loss: 0.222260\n",
      "Epoch 218, test loss: 0.221809\n",
      "Epoch 219, test loss: 0.223595\n",
      "Epoch 220, test loss: 0.227642\n",
      "Epoch 221, test loss: 0.222320\n",
      "Epoch 222, test loss: 0.221706\n",
      "Epoch 223, test loss: 0.222145\n",
      "Epoch 224, test loss: 0.225265\n",
      "Epoch 225, test loss: 0.222676\n",
      "Epoch 226, test loss: 0.221569\n",
      "Epoch 227, test loss: 0.222196\n",
      "Epoch 228, test loss: 0.226699\n",
      "Epoch 229, test loss: 0.224346\n",
      "Epoch 230, test loss: 0.222212\n",
      "Epoch 231, test loss: 0.221438\n",
      "Epoch 232, test loss: 0.222331\n",
      "Epoch 233, test loss: 0.221750\n",
      "Epoch 234, test loss: 0.222755\n",
      "Epoch 235, test loss: 0.221578\n",
      "Epoch 236, test loss: 0.221976\n",
      "Epoch 237, test loss: 0.222182\n",
      "Epoch 238, test loss: 0.221410\n",
      "Epoch 239, test loss: 0.225004\n",
      "Pretrain data: 20092707.0\n",
      "Building dataset, requesting data from 0 to 672\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7123/103835\n",
      "Found 672 continuous time series\n",
      "Data shape: (110960, 18), Train/test: 110958/2\n",
      "Train test ratio: 55479.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1733\n",
      "Epoch 0, train loss: 0.209285\n",
      "Epoch 1, train loss: 0.235261\n",
      "Epoch 2, train loss: 0.233292\n",
      "Epoch 3, train loss: 0.221416\n",
      "Epoch 4, train loss: 0.216170\n",
      "Epoch 5, train loss: 0.206699\n",
      "Epoch 6, train loss: 0.240238\n",
      "Epoch 7, train loss: 0.141967\n",
      "Epoch 8, train loss: 0.175323\n",
      "Epoch 9, train loss: 0.151967\n",
      "Epoch 10, train loss: 0.192131\n",
      "Epoch 11, train loss: 0.194489\n",
      "Epoch 12, train loss: 0.168931\n",
      "Epoch 13, train loss: 0.250360\n",
      "Epoch 14, train loss: 0.233854\n",
      "Epoch 15, train loss: 0.207380\n",
      "Epoch 16, train loss: 0.190747\n",
      "Epoch 17, train loss: 0.159480\n",
      "Epoch 18, train loss: 0.188360\n",
      "Epoch 19, train loss: 0.185794\n",
      "Epoch 20, train loss: 0.208050\n",
      "Epoch 21, train loss: 0.198771\n",
      "Epoch 22, train loss: 0.185823\n",
      "Epoch 23, train loss: 0.189846\n",
      "Epoch 24, train loss: 0.165094\n",
      "Epoch 25, train loss: 0.168706\n",
      "Epoch 26, train loss: 0.232221\n",
      "Epoch 27, train loss: 0.231816\n",
      "Epoch 28, train loss: 0.179005\n",
      "Epoch 29, train loss: 0.195951\n",
      "Epoch 30, train loss: 0.166874\n",
      "Epoch 31, train loss: 0.176750\n",
      "Epoch 32, train loss: 0.223775\n",
      "Epoch 33, train loss: 0.235011\n",
      "Epoch 34, train loss: 0.189525\n",
      "Epoch 35, train loss: 0.379522\n",
      "Epoch 36, train loss: 0.180794\n",
      "Epoch 37, train loss: 0.189315\n",
      "Epoch 38, train loss: 0.225388\n",
      "Epoch 39, train loss: 0.244787\n",
      "Epoch 40, train loss: 0.173898\n",
      "Epoch 41, train loss: 0.194154\n",
      "Epoch 42, train loss: 0.301733\n",
      "Epoch 43, train loss: 0.213968\n",
      "Epoch 44, train loss: 0.261155\n",
      "Epoch 45, train loss: 0.190778\n",
      "Epoch 46, train loss: 0.215816\n",
      "Epoch 47, train loss: 0.213829\n",
      "Epoch 48, train loss: 0.252498\n",
      "Epoch 49, train loss: 0.193660\n",
      "Epoch 50, train loss: 0.155768\n",
      "Epoch 51, train loss: 0.219038\n",
      "Epoch 52, train loss: 0.321335\n",
      "Epoch 53, train loss: 0.201948\n",
      "Epoch 54, train loss: 0.242974\n",
      "Epoch 55, train loss: 0.210375\n",
      "Epoch 56, train loss: 0.182771\n",
      "Epoch 57, train loss: 0.170254\n",
      "Epoch 58, train loss: 0.170844\n",
      "Epoch 59, train loss: 0.200624\n",
      "Epoch 60, train loss: 0.200466\n",
      "Epoch 61, train loss: 0.208728\n",
      "Epoch 62, train loss: 0.249196\n",
      "Epoch 63, train loss: 0.266203\n",
      "Epoch 64, train loss: 0.145083\n",
      "Epoch 65, train loss: 0.235700\n",
      "Epoch 66, train loss: 0.237944\n",
      "Epoch 67, train loss: 0.205853\n",
      "Epoch 68, train loss: 0.219972\n",
      "Epoch 69, train loss: 0.238034\n",
      "Epoch 70, train loss: 0.165431\n",
      "Epoch 71, train loss: 0.205031\n",
      "Epoch 72, train loss: 0.207329\n",
      "Epoch 73, train loss: 0.180585\n",
      "Epoch 74, train loss: 0.222072\n",
      "Epoch 75, train loss: 0.208293\n",
      "Epoch 76, train loss: 0.254079\n",
      "Epoch 77, train loss: 0.190413\n",
      "Epoch 78, train loss: 0.169707\n",
      "Epoch 79, train loss: 0.248073\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[108. 105. 101. ...  63.  61.  58.]\n",
      " [105. 101.  98. ...  61.  58.  57.]\n",
      " [101.  98.  95. ...  58.  57.  64.]\n",
      " ...\n",
      " [257. 257. 262. ... 283. 276. 269.]\n",
      " [257. 262. 268. ... 276. 269. 261.]\n",
      " [262. 268. 278. ... 269. 261. 252.]]\n",
      "y here is\n",
      "[[ 85.  85.  85. ...  85.  85.  85.]\n",
      " [ 96.  96.  96. ...  96.  96.  96.]\n",
      " [109. 109. 109. ... 109. 109. 109.]\n",
      " ...\n",
      " [229. 229. 229. ... 229. 229. 229.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [215. 215. 215. ... 215. 215. 215.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1394, 18), Train/test: 1/1393\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 170 segments\n",
      "Building dataset, requesting data from 0 to 170\n",
      "x here is\n",
      "[[ 95.  86.  81. ...  95.  98.  99.]\n",
      " [ 86.  81.  81. ...  98.  99.  98.]\n",
      " [ 81.  81.  82. ...  99.  98. 100.]\n",
      " ...\n",
      " [301. 306. 311. ... 232. 222. 221.]\n",
      " [306. 311. 316. ... 222. 221. 222.]\n",
      " [311. 316. 315. ... 221. 222. 223.]]\n",
      "y here is\n",
      "[[104. 104. 104. ... 104. 104. 104.]\n",
      " [100. 100. 100. ... 100. 100. 100.]\n",
      " [ 94.  94.  94. ...  94.  94.  94.]\n",
      " ...\n",
      " [225. 225. 225. ... 225. 225. 225.]\n",
      " [227. 227. 227. ... 227. 227. 227.]\n",
      " [226. 226. 226. ... 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 394/4919\n",
      "Found 170 continuous time series\n",
      "Data shape: (5315, 18), Train/test: 5313/2\n",
      "Train test ratio: 2656.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E13C0>\n",
      "Epoch 0, test loss: 0.167962\n",
      "Epoch 1, test loss: 0.168560\n",
      "Epoch 2, test loss: 0.168266\n",
      "Epoch 3, test loss: 0.167252\n",
      "Epoch 4, test loss: 0.166948\n",
      "Epoch 5, test loss: 0.167637\n",
      "Epoch 6, test loss: 0.167475\n",
      "Epoch 7, test loss: 0.168801\n",
      "Epoch 8, test loss: 0.167443\n",
      "Epoch 9, test loss: 0.168631\n",
      "Epoch 10, test loss: 0.167736\n",
      "Epoch 11, test loss: 0.168380\n",
      "Epoch 12, test loss: 0.167590\n",
      "Epoch 13, test loss: 0.170140\n",
      "Epoch 14, test loss: 0.168364\n",
      "Epoch 15, test loss: 0.169790\n",
      "Epoch 16, test loss: 0.168019\n",
      "Epoch 17, test loss: 0.167282\n",
      "Epoch 18, test loss: 0.168059\n",
      "Epoch 19, test loss: 0.170503\n",
      "Epoch 20, test loss: 0.168940\n",
      "Epoch 21, test loss: 0.167720\n",
      "Epoch 22, test loss: 0.168607\n",
      "Epoch 23, test loss: 0.167587\n",
      "Epoch 24, test loss: 0.168201\n",
      "Epoch 25, test loss: 0.168310\n",
      "Epoch 26, test loss: 0.168342\n",
      "Epoch 27, test loss: 0.168467\n",
      "Epoch 28, test loss: 0.169685\n",
      "Epoch 29, test loss: 0.168090\n",
      "Epoch 30, test loss: 0.170108\n",
      "Epoch 31, test loss: 0.168191\n",
      "Epoch 32, test loss: 0.169487\n",
      "Epoch 33, test loss: 0.170828\n",
      "Epoch 34, test loss: 0.167996\n",
      "Epoch 35, test loss: 0.168230\n",
      "Epoch 36, test loss: 0.173276\n",
      "Epoch 37, test loss: 0.169993\n",
      "Epoch 38, test loss: 0.172686\n",
      "Epoch 39, test loss: 0.170455\n",
      "Epoch 40, test loss: 0.170832\n",
      "Epoch 41, test loss: 0.174422\n",
      "Epoch 42, test loss: 0.169887\n",
      "Epoch 43, test loss: 0.167908\n",
      "Epoch 44, test loss: 0.168021\n",
      "Epoch 45, test loss: 0.170554\n",
      "Epoch 46, test loss: 0.170269\n",
      "Epoch 47, test loss: 0.169625\n",
      "Epoch 48, test loss: 0.169691\n",
      "Epoch 49, test loss: 0.175469\n",
      "Epoch 50, test loss: 0.173041\n",
      "Epoch 51, test loss: 0.168947\n",
      "Epoch 52, test loss: 0.169589\n",
      "Epoch 53, test loss: 0.168606\n",
      "Epoch 54, test loss: 0.186450\n",
      "Epoch 55, test loss: 0.184916\n",
      "Epoch 56, test loss: 0.170724\n",
      "Epoch 57, test loss: 0.169148\n",
      "Epoch 58, test loss: 0.170320\n",
      "Epoch 59, test loss: 0.168485\n",
      "Epoch 60, test loss: 0.168305\n",
      "Epoch 61, test loss: 0.168725\n",
      "Epoch 62, test loss: 0.169392\n",
      "Epoch 63, test loss: 0.176008\n",
      "Epoch 64, test loss: 0.172211\n",
      "Epoch 65, test loss: 0.172291\n",
      "Epoch 66, test loss: 0.169481\n",
      "Epoch 67, test loss: 0.168207\n",
      "Epoch 68, test loss: 0.168194\n",
      "Epoch 69, test loss: 0.169112\n",
      "Epoch 70, test loss: 0.168388\n",
      "Epoch 71, test loss: 0.168475\n",
      "Epoch 72, test loss: 0.170266\n",
      "Epoch 73, test loss: 0.168325\n",
      "Epoch 74, test loss: 0.168648\n",
      "Epoch 75, test loss: 0.176453\n",
      "Epoch 76, test loss: 0.170145\n",
      "Epoch 77, test loss: 0.168247\n",
      "Epoch 78, test loss: 0.174028\n",
      "Epoch 79, test loss: 0.179360\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E13C0>\n",
      "Epoch 0, test loss: 0.167567\n",
      "Epoch 1, test loss: 0.171343\n",
      "Epoch 2, test loss: 0.167445\n",
      "Epoch 3, test loss: 0.174621\n",
      "Epoch 4, test loss: 0.166951\n",
      "Epoch 5, test loss: 0.166996\n",
      "Epoch 6, test loss: 0.168052\n",
      "Epoch 7, test loss: 0.167616\n",
      "Epoch 8, test loss: 0.175282\n",
      "Epoch 9, test loss: 0.167540\n",
      "Epoch 10, test loss: 0.167880\n",
      "Epoch 11, test loss: 0.168288\n",
      "Epoch 12, test loss: 0.180279\n",
      "Epoch 13, test loss: 0.168113\n",
      "Epoch 14, test loss: 0.168784\n",
      "Epoch 15, test loss: 0.168421\n",
      "Epoch 16, test loss: 0.170308\n",
      "Epoch 17, test loss: 0.167645\n",
      "Epoch 18, test loss: 0.168804\n",
      "Epoch 19, test loss: 0.168450\n",
      "Epoch 20, test loss: 0.170211\n",
      "Epoch 21, test loss: 0.170146\n",
      "Epoch 22, test loss: 0.169166\n",
      "Epoch 23, test loss: 0.167541\n",
      "Epoch 24, test loss: 0.167995\n",
      "Epoch 25, test loss: 0.167513\n",
      "Epoch 26, test loss: 0.169266\n",
      "Epoch 27, test loss: 0.167825\n",
      "Epoch 28, test loss: 0.168768\n",
      "Epoch 29, test loss: 0.172220\n",
      "Epoch 30, test loss: 0.167621\n",
      "Epoch 31, test loss: 0.173609\n",
      "Epoch 32, test loss: 0.169677\n",
      "Epoch 33, test loss: 0.172789\n",
      "Epoch 34, test loss: 0.169974\n",
      "Epoch 35, test loss: 0.168287\n",
      "Epoch 36, test loss: 0.168244\n",
      "Epoch 37, test loss: 0.169029\n",
      "Epoch 38, test loss: 0.169414\n",
      "Epoch 39, test loss: 0.167496\n",
      "Epoch 40, test loss: 0.168661\n",
      "Epoch 41, test loss: 0.167932\n",
      "Epoch 42, test loss: 0.170327\n",
      "Epoch 43, test loss: 0.167906\n",
      "Epoch 44, test loss: 0.169690\n",
      "Epoch 45, test loss: 0.168770\n",
      "Epoch 46, test loss: 0.168810\n",
      "Epoch 47, test loss: 0.168530\n",
      "Epoch 48, test loss: 0.169165\n",
      "Epoch 49, test loss: 0.168781\n",
      "Epoch 50, test loss: 0.169244\n",
      "Epoch 51, test loss: 0.173124\n",
      "Epoch 52, test loss: 0.169330\n",
      "Epoch 53, test loss: 0.171763\n",
      "Epoch 54, test loss: 0.167961\n",
      "Epoch 55, test loss: 0.167616\n",
      "Epoch 56, test loss: 0.168151\n",
      "Epoch 57, test loss: 0.176240\n",
      "Epoch 58, test loss: 0.169414\n",
      "Epoch 59, test loss: 0.167900\n",
      "Epoch 60, test loss: 0.168358\n",
      "Epoch 61, test loss: 0.168169\n",
      "Epoch 62, test loss: 0.167670\n",
      "Epoch 63, test loss: 0.167637\n",
      "Epoch 64, test loss: 0.169575\n",
      "Epoch 65, test loss: 0.167607\n",
      "Epoch 66, test loss: 0.171631\n",
      "Epoch 67, test loss: 0.167745\n",
      "Epoch 68, test loss: 0.168055\n",
      "Epoch 69, test loss: 0.168437\n",
      "Epoch 70, test loss: 0.170468\n",
      "Epoch 71, test loss: 0.169747\n",
      "Epoch 72, test loss: 0.168060\n",
      "Epoch 73, test loss: 0.168327\n",
      "Epoch 74, test loss: 0.168533\n",
      "Epoch 75, test loss: 0.167586\n",
      "Epoch 76, test loss: 0.172380\n",
      "Epoch 77, test loss: 0.173204\n",
      "Epoch 78, test loss: 0.168982\n",
      "Epoch 79, test loss: 0.169242\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E13C0>\n",
      "Epoch 0, test loss: 0.429524\n",
      "Epoch 1, test loss: 0.245825\n",
      "Epoch 2, test loss: 0.220719\n",
      "Epoch 3, test loss: 0.197998\n",
      "Epoch 4, test loss: 0.186652\n",
      "Epoch 5, test loss: 0.177587\n",
      "Epoch 6, test loss: 0.176151\n",
      "Epoch 7, test loss: 0.176418\n",
      "Epoch 8, test loss: 0.175710\n",
      "Epoch 9, test loss: 0.174136\n",
      "Epoch 10, test loss: 0.175021\n",
      "Epoch 11, test loss: 0.174655\n",
      "Epoch 12, test loss: 0.176620\n",
      "Epoch 13, test loss: 0.179038\n",
      "Epoch 14, test loss: 0.175558\n",
      "Epoch 15, test loss: 0.175476\n",
      "Epoch 16, test loss: 0.172572\n",
      "Epoch 17, test loss: 0.172719\n",
      "Epoch 18, test loss: 0.172420\n",
      "Epoch 19, test loss: 0.173752\n",
      "Epoch 20, test loss: 0.172348\n",
      "Epoch 21, test loss: 0.177125\n",
      "Epoch 22, test loss: 0.177734\n",
      "Epoch 23, test loss: 0.174122\n",
      "Epoch 24, test loss: 0.171596\n",
      "Epoch 25, test loss: 0.172040\n",
      "Epoch 26, test loss: 0.172207\n",
      "Epoch 27, test loss: 0.171654\n",
      "Epoch 28, test loss: 0.171362\n",
      "Epoch 29, test loss: 0.173149\n",
      "Epoch 30, test loss: 0.171178\n",
      "Epoch 31, test loss: 0.174884\n",
      "Epoch 32, test loss: 0.171432\n",
      "Epoch 33, test loss: 0.172955\n",
      "Epoch 34, test loss: 0.171170\n",
      "Epoch 35, test loss: 0.171401\n",
      "Epoch 36, test loss: 0.171113\n",
      "Epoch 37, test loss: 0.170699\n",
      "Epoch 38, test loss: 0.174421\n",
      "Epoch 39, test loss: 0.176017\n",
      "Epoch 40, test loss: 0.174774\n",
      "Epoch 41, test loss: 0.172143\n",
      "Epoch 42, test loss: 0.170239\n",
      "Epoch 43, test loss: 0.172492\n",
      "Epoch 44, test loss: 0.171109\n",
      "Epoch 45, test loss: 0.170654\n",
      "Epoch 46, test loss: 0.171863\n",
      "Epoch 47, test loss: 0.171922\n",
      "Epoch 48, test loss: 0.173474\n",
      "Epoch 49, test loss: 0.175949\n",
      "Epoch 50, test loss: 0.172913\n",
      "Epoch 51, test loss: 0.170631\n",
      "Epoch 52, test loss: 0.172433\n",
      "Epoch 53, test loss: 0.172944\n",
      "Epoch 54, test loss: 0.170663\n",
      "Epoch 55, test loss: 0.172169\n",
      "Epoch 56, test loss: 0.172487\n",
      "Epoch 57, test loss: 0.172439\n",
      "Epoch 58, test loss: 0.179706\n",
      "Epoch 59, test loss: 0.170678\n",
      "Epoch 60, test loss: 0.171824\n",
      "Epoch 61, test loss: 0.174765\n",
      "Epoch 62, test loss: 0.172944\n",
      "Epoch 63, test loss: 0.173203\n",
      "Epoch 64, test loss: 0.171440\n",
      "Epoch 65, test loss: 0.175094\n",
      "Epoch 66, test loss: 0.170593\n",
      "Epoch 67, test loss: 0.171384\n",
      "Epoch 68, test loss: 0.170672\n",
      "Epoch 69, test loss: 0.171386\n",
      "Epoch 70, test loss: 0.172363\n",
      "Epoch 71, test loss: 0.176836\n",
      "Epoch 72, test loss: 0.170505\n",
      "Epoch 73, test loss: 0.174359\n",
      "Epoch 74, test loss: 0.171198\n",
      "Epoch 75, test loss: 0.183879\n",
      "Epoch 76, test loss: 0.173931\n",
      "Epoch 77, test loss: 0.174302\n",
      "Epoch 78, test loss: 0.171111\n",
      "Epoch 79, test loss: 0.172496\n",
      "Epoch 80, test loss: 0.172217\n",
      "Epoch 81, test loss: 0.171012\n",
      "Epoch 82, test loss: 0.170848\n",
      "Epoch 83, test loss: 0.174269\n",
      "Epoch 84, test loss: 0.172025\n",
      "Epoch 85, test loss: 0.173325\n",
      "Epoch 86, test loss: 0.173727\n",
      "Epoch 87, test loss: 0.173246\n",
      "Epoch 88, test loss: 0.173904\n",
      "Epoch 89, test loss: 0.172476\n",
      "Epoch 90, test loss: 0.171220\n",
      "Epoch 91, test loss: 0.171679\n",
      "Epoch 92, test loss: 0.171662\n",
      "Epoch 93, test loss: 0.173052\n",
      "Epoch 94, test loss: 0.170834\n",
      "Epoch 95, test loss: 0.175596\n",
      "Epoch 96, test loss: 0.172885\n",
      "Epoch 97, test loss: 0.171651\n",
      "Epoch 98, test loss: 0.173268\n",
      "Epoch 99, test loss: 0.171632\n",
      "Epoch 100, test loss: 0.171095\n",
      "Epoch 101, test loss: 0.174768\n",
      "Epoch 102, test loss: 0.176723\n",
      "Epoch 103, test loss: 0.171044\n",
      "Epoch 104, test loss: 0.170616\n",
      "Epoch 105, test loss: 0.172004\n",
      "Epoch 106, test loss: 0.171583\n",
      "Epoch 107, test loss: 0.176456\n",
      "Epoch 108, test loss: 0.171237\n",
      "Epoch 109, test loss: 0.172216\n",
      "Epoch 110, test loss: 0.172334\n",
      "Epoch 111, test loss: 0.175455\n",
      "Epoch 112, test loss: 0.171521\n",
      "Epoch 113, test loss: 0.171397\n",
      "Epoch 114, test loss: 0.171434\n",
      "Epoch 115, test loss: 0.170509\n",
      "Epoch 116, test loss: 0.170506\n",
      "Epoch 117, test loss: 0.172008\n",
      "Epoch 118, test loss: 0.172594\n",
      "Epoch 119, test loss: 0.172491\n",
      "Epoch 120, test loss: 0.171972\n",
      "Epoch 121, test loss: 0.176418\n",
      "Epoch 122, test loss: 0.171272\n",
      "Epoch 123, test loss: 0.179690\n",
      "Epoch 124, test loss: 0.171191\n",
      "Epoch 125, test loss: 0.173941\n",
      "Epoch 126, test loss: 0.172118\n",
      "Epoch 127, test loss: 0.174347\n",
      "Epoch 128, test loss: 0.171448\n",
      "Epoch 129, test loss: 0.172198\n",
      "Epoch 130, test loss: 0.171627\n",
      "Epoch 131, test loss: 0.171626\n",
      "Epoch 132, test loss: 0.175896\n",
      "Epoch 133, test loss: 0.171876\n",
      "Epoch 134, test loss: 0.177046\n",
      "Epoch 135, test loss: 0.171742\n",
      "Epoch 136, test loss: 0.172669\n",
      "Epoch 137, test loss: 0.173057\n",
      "Epoch 138, test loss: 0.178001\n",
      "Epoch 139, test loss: 0.176076\n",
      "Epoch 140, test loss: 0.170883\n",
      "Epoch 141, test loss: 0.174307\n",
      "Epoch 142, test loss: 0.170531\n",
      "Epoch 143, test loss: 0.171720\n",
      "Epoch 144, test loss: 0.171598\n",
      "Epoch 145, test loss: 0.171645\n",
      "Epoch 146, test loss: 0.172195\n",
      "Epoch 147, test loss: 0.173133\n",
      "Epoch 148, test loss: 0.172817\n",
      "Epoch 149, test loss: 0.171688\n",
      "Epoch 150, test loss: 0.171255\n",
      "Epoch 151, test loss: 0.172365\n",
      "Epoch 152, test loss: 0.172118\n",
      "Epoch 153, test loss: 0.171890\n",
      "Epoch 154, test loss: 0.171895\n",
      "Epoch 155, test loss: 0.173364\n",
      "Epoch 156, test loss: 0.172746\n",
      "Epoch 157, test loss: 0.171779\n",
      "Epoch 158, test loss: 0.173063\n",
      "Epoch 159, test loss: 0.174190\n",
      "Epoch 160, test loss: 0.171624\n",
      "Epoch 161, test loss: 0.175247\n",
      "Epoch 162, test loss: 0.171761\n",
      "Epoch 163, test loss: 0.172935\n",
      "Epoch 164, test loss: 0.171122\n",
      "Epoch 165, test loss: 0.172927\n",
      "Epoch 166, test loss: 0.172568\n",
      "Epoch 167, test loss: 0.172386\n",
      "Epoch 168, test loss: 0.171217\n",
      "Epoch 169, test loss: 0.174538\n",
      "Epoch 170, test loss: 0.171976\n",
      "Epoch 171, test loss: 0.172139\n",
      "Epoch 172, test loss: 0.171429\n",
      "Epoch 173, test loss: 0.177438\n",
      "Epoch 174, test loss: 0.172210\n",
      "Epoch 175, test loss: 0.171939\n",
      "Epoch 176, test loss: 0.175424\n",
      "Epoch 177, test loss: 0.172716\n",
      "Epoch 178, test loss: 0.174529\n",
      "Epoch 179, test loss: 0.176592\n",
      "Epoch 180, test loss: 0.173537\n",
      "Epoch 181, test loss: 0.174007\n",
      "Epoch 182, test loss: 0.179193\n",
      "Epoch 183, test loss: 0.172862\n",
      "Epoch 184, test loss: 0.178422\n",
      "Epoch 185, test loss: 0.175268\n",
      "Epoch 186, test loss: 0.174151\n",
      "Epoch 187, test loss: 0.173260\n",
      "Epoch 188, test loss: 0.177314\n",
      "Epoch 189, test loss: 0.173839\n",
      "Epoch 190, test loss: 0.172554\n",
      "Epoch 191, test loss: 0.172880\n",
      "Epoch 192, test loss: 0.174992\n",
      "Epoch 193, test loss: 0.174309\n",
      "Epoch 194, test loss: 0.174005\n",
      "Epoch 195, test loss: 0.173533\n",
      "Epoch 196, test loss: 0.174961\n",
      "Epoch 197, test loss: 0.175421\n",
      "Epoch 198, test loss: 0.173581\n",
      "Epoch 199, test loss: 0.173008\n",
      "Epoch 200, test loss: 0.172701\n",
      "Epoch 201, test loss: 0.173206\n",
      "Epoch 202, test loss: 0.173388\n",
      "Epoch 203, test loss: 0.174981\n",
      "Epoch 204, test loss: 0.173842\n",
      "Epoch 205, test loss: 0.177053\n",
      "Epoch 206, test loss: 0.173452\n",
      "Epoch 207, test loss: 0.172669\n",
      "Epoch 208, test loss: 0.172896\n",
      "Epoch 209, test loss: 0.171921\n",
      "Epoch 210, test loss: 0.173808\n",
      "Epoch 211, test loss: 0.175389\n",
      "Epoch 212, test loss: 0.174200\n",
      "Epoch 213, test loss: 0.176103\n",
      "Epoch 214, test loss: 0.176056\n",
      "Epoch 215, test loss: 0.173763\n",
      "Epoch 216, test loss: 0.176828\n",
      "Epoch 217, test loss: 0.175064\n",
      "Epoch 218, test loss: 0.176530\n",
      "Epoch 219, test loss: 0.174675\n",
      "Epoch 220, test loss: 0.175888\n",
      "Epoch 221, test loss: 0.176144\n",
      "Epoch 222, test loss: 0.174331\n",
      "Epoch 223, test loss: 0.172808\n",
      "Epoch 224, test loss: 0.175325\n",
      "Epoch 225, test loss: 0.173192\n",
      "Epoch 226, test loss: 0.175988\n",
      "Epoch 227, test loss: 0.174546\n",
      "Epoch 228, test loss: 0.172185\n",
      "Epoch 229, test loss: 0.172378\n",
      "Epoch 230, test loss: 0.175713\n",
      "Epoch 231, test loss: 0.174329\n",
      "Epoch 232, test loss: 0.173649\n",
      "Epoch 233, test loss: 0.173138\n",
      "Epoch 234, test loss: 0.173619\n",
      "Epoch 235, test loss: 0.174187\n",
      "Epoch 236, test loss: 0.172771\n",
      "Epoch 237, test loss: 0.174053\n",
      "Epoch 238, test loss: 0.172729\n",
      "Epoch 239, test loss: 0.172915\n",
      "Pretrain data: 19669610.0\n",
      "Building dataset, requesting data from 0 to 819\n",
      "x here is\n",
      "[[ 95.  86.  81. ...  95.  98.  99.]\n",
      " [ 86.  81.  81. ...  98.  99.  98.]\n",
      " [ 81.  81.  82. ...  99.  98. 100.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[104. 104. 104. ... 104. 104. 104.]\n",
      " [100. 100. 100. ... 100. 100. 100.]\n",
      " [ 94.  94.  94. ...  94.  94.  94.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7078/99081\n",
      "Found 819 continuous time series\n",
      "Data shape: (106161, 18), Train/test: 106159/2\n",
      "Train test ratio: 53079.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1658\n",
      "Epoch 0, train loss: 0.247382\n",
      "Epoch 1, train loss: 0.215892\n",
      "Epoch 2, train loss: 0.201733\n",
      "Epoch 3, train loss: 0.250785\n",
      "Epoch 4, train loss: 0.208642\n",
      "Epoch 5, train loss: 0.213937\n",
      "Epoch 6, train loss: 0.263722\n",
      "Epoch 7, train loss: 0.214486\n",
      "Epoch 8, train loss: 0.219716\n",
      "Epoch 9, train loss: 0.213826\n",
      "Epoch 10, train loss: 0.193850\n",
      "Epoch 11, train loss: 0.229640\n",
      "Epoch 12, train loss: 0.231060\n",
      "Epoch 13, train loss: 0.241205\n",
      "Epoch 14, train loss: 0.252898\n",
      "Epoch 15, train loss: 0.207629\n",
      "Epoch 16, train loss: 0.212932\n",
      "Epoch 17, train loss: 0.220104\n",
      "Epoch 18, train loss: 0.205347\n",
      "Epoch 19, train loss: 0.189961\n",
      "Epoch 20, train loss: 0.191076\n",
      "Epoch 21, train loss: 0.343929\n",
      "Epoch 22, train loss: 0.183954\n",
      "Epoch 23, train loss: 0.247138\n",
      "Epoch 24, train loss: 0.222386\n",
      "Epoch 25, train loss: 0.203444\n",
      "Epoch 26, train loss: 0.164938\n",
      "Epoch 27, train loss: 0.282093\n",
      "Epoch 28, train loss: 0.179930\n",
      "Epoch 29, train loss: 0.194215\n",
      "Epoch 30, train loss: 0.173102\n",
      "Epoch 31, train loss: 0.209181\n",
      "Epoch 32, train loss: 0.212951\n",
      "Epoch 33, train loss: 0.257724\n",
      "Epoch 34, train loss: 0.205697\n",
      "Epoch 35, train loss: 0.232999\n",
      "Epoch 36, train loss: 0.240760\n",
      "Epoch 37, train loss: 0.245192\n",
      "Epoch 38, train loss: 0.213717\n",
      "Epoch 39, train loss: 0.194561\n",
      "Epoch 40, train loss: 0.243283\n",
      "Epoch 41, train loss: 0.199572\n",
      "Epoch 42, train loss: 0.183028\n",
      "Epoch 43, train loss: 0.139894\n",
      "Epoch 44, train loss: 0.222921\n",
      "Epoch 45, train loss: 0.223544\n",
      "Epoch 46, train loss: 0.197707\n",
      "Epoch 47, train loss: 0.207986\n",
      "Epoch 48, train loss: 0.192130\n",
      "Epoch 49, train loss: 0.223186\n",
      "Epoch 50, train loss: 0.205571\n",
      "Epoch 51, train loss: 0.290736\n",
      "Epoch 52, train loss: 0.195618\n",
      "Epoch 53, train loss: 0.226223\n",
      "Epoch 54, train loss: 0.225567\n",
      "Epoch 55, train loss: 0.261583\n",
      "Epoch 56, train loss: 0.206359\n",
      "Epoch 57, train loss: 0.207023\n",
      "Epoch 58, train loss: 0.195844\n",
      "Epoch 59, train loss: 0.208330\n",
      "Epoch 60, train loss: 0.220466\n",
      "Epoch 61, train loss: 0.203754\n",
      "Epoch 62, train loss: 0.158779\n",
      "Epoch 63, train loss: 0.209593\n",
      "Epoch 64, train loss: 0.199952\n",
      "Epoch 65, train loss: 0.178791\n",
      "Epoch 66, train loss: 0.245333\n",
      "Epoch 67, train loss: 0.261691\n",
      "Epoch 68, train loss: 0.265907\n",
      "Epoch 69, train loss: 0.190231\n",
      "Epoch 70, train loss: 0.192920\n",
      "Epoch 71, train loss: 0.256132\n",
      "Epoch 72, train loss: 0.198927\n",
      "Epoch 73, train loss: 0.223603\n",
      "Epoch 74, train loss: 0.222626\n",
      "Epoch 75, train loss: 0.197983\n",
      "Epoch 76, train loss: 0.190266\n",
      "Epoch 77, train loss: 0.208522\n",
      "Epoch 78, train loss: 0.196672\n",
      "Epoch 79, train loss: 0.202806\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[161. 155. 150. ... 107. 105. 103.]\n",
      " [155. 150. 147. ... 105. 103. 100.]\n",
      " [150. 147. 144. ... 103. 100.  98.]\n",
      " ...\n",
      " [314. 316. 312. ... 293. 301. 307.]\n",
      " [316. 312. 304. ... 301. 307. 306.]\n",
      " [312. 304. 298. ... 307. 306. 300.]]\n",
      "y here is\n",
      "[[ 89.  89.  89. ...  89.  89.  89.]\n",
      " [ 88.  88.  88. ...  88.  88.  88.]\n",
      " [ 87.  87.  87. ...  87.  87.  87.]\n",
      " ...\n",
      " [284. 284. 284. ... 284. 284. 284.]\n",
      " [273. 273. 273. ... 273. 273. 273.]\n",
      " [262. 262. 262. ... 262. 262. 262.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2555, 18), Train/test: 1/2554\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [161. 161. 163. ... 176. 173. 172.]\n",
      " [161. 163. 168. ... 173. 172. 173.]\n",
      " [163. 168. 173. ... 172. 173. 173.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 439/9673\n",
      "Found 23 continuous time series\n",
      "Data shape: (10114, 18), Train/test: 10112/2\n",
      "Train test ratio: 5056.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E3790>\n",
      "Epoch 0, test loss: 0.172419\n",
      "Epoch 1, test loss: 0.174250\n",
      "Epoch 2, test loss: 0.171760\n",
      "Epoch 3, test loss: 0.171294\n",
      "Epoch 4, test loss: 0.172625\n",
      "Epoch 5, test loss: 0.171661\n",
      "Epoch 6, test loss: 0.174073\n",
      "Epoch 7, test loss: 0.171999\n",
      "Epoch 8, test loss: 0.175042\n",
      "Epoch 9, test loss: 0.173433\n",
      "Epoch 10, test loss: 0.172224\n",
      "Epoch 11, test loss: 0.172929\n",
      "Epoch 12, test loss: 0.171542\n",
      "Epoch 13, test loss: 0.172806\n",
      "Epoch 14, test loss: 0.173325\n",
      "Epoch 15, test loss: 0.174642\n",
      "Epoch 16, test loss: 0.172338\n",
      "Epoch 17, test loss: 0.171754\n",
      "Epoch 18, test loss: 0.171980\n",
      "Epoch 19, test loss: 0.172179\n",
      "Epoch 20, test loss: 0.171373\n",
      "Epoch 21, test loss: 0.174473\n",
      "Epoch 22, test loss: 0.174779\n",
      "Epoch 23, test loss: 0.173532\n",
      "Epoch 24, test loss: 0.173441\n",
      "Epoch 25, test loss: 0.173063\n",
      "Epoch 26, test loss: 0.174378\n",
      "Epoch 27, test loss: 0.173081\n",
      "Epoch 28, test loss: 0.172219\n",
      "Epoch 29, test loss: 0.173003\n",
      "Epoch 30, test loss: 0.172505\n",
      "Epoch 31, test loss: 0.173413\n",
      "Epoch 32, test loss: 0.173242\n",
      "Epoch 33, test loss: 0.173509\n",
      "Epoch 34, test loss: 0.172350\n",
      "Epoch 35, test loss: 0.181685\n",
      "Epoch 36, test loss: 0.173338\n",
      "Epoch 37, test loss: 0.172691\n",
      "Epoch 38, test loss: 0.173463\n",
      "Epoch 39, test loss: 0.172370\n",
      "Epoch 40, test loss: 0.175042\n",
      "Epoch 41, test loss: 0.174348\n",
      "Epoch 42, test loss: 0.172029\n",
      "Epoch 43, test loss: 0.172799\n",
      "Epoch 44, test loss: 0.173069\n",
      "Epoch 45, test loss: 0.172927\n",
      "Epoch 46, test loss: 0.173542\n",
      "Epoch 47, test loss: 0.176584\n",
      "Epoch 48, test loss: 0.172726\n",
      "Epoch 49, test loss: 0.172971\n",
      "Epoch 50, test loss: 0.173406\n",
      "Epoch 51, test loss: 0.175559\n",
      "Epoch 52, test loss: 0.179215\n",
      "Epoch 53, test loss: 0.177640\n",
      "Epoch 54, test loss: 0.172614\n",
      "Epoch 55, test loss: 0.172710\n",
      "Epoch 56, test loss: 0.172742\n",
      "Epoch 57, test loss: 0.172517\n",
      "Epoch 58, test loss: 0.173844\n",
      "Epoch 59, test loss: 0.173430\n",
      "Epoch 60, test loss: 0.173970\n",
      "Epoch 61, test loss: 0.172237\n",
      "Epoch 62, test loss: 0.172705\n",
      "Epoch 63, test loss: 0.173044\n",
      "Epoch 64, test loss: 0.173184\n",
      "Epoch 65, test loss: 0.172817\n",
      "Epoch 66, test loss: 0.173446\n",
      "Epoch 67, test loss: 0.173966\n",
      "Epoch 68, test loss: 0.173725\n",
      "Epoch 69, test loss: 0.173584\n",
      "Epoch 70, test loss: 0.174962\n",
      "Epoch 71, test loss: 0.173227\n",
      "Epoch 72, test loss: 0.173413\n",
      "Epoch 73, test loss: 0.174910\n",
      "Epoch 74, test loss: 0.174958\n",
      "Epoch 75, test loss: 0.172747\n",
      "Epoch 76, test loss: 0.174989\n",
      "Epoch 77, test loss: 0.173194\n",
      "Epoch 78, test loss: 0.172774\n",
      "Epoch 79, test loss: 0.172622\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E3790>\n",
      "Epoch 0, test loss: 0.172456\n",
      "Epoch 1, test loss: 0.173758\n",
      "Epoch 2, test loss: 0.172035\n",
      "Epoch 3, test loss: 0.172862\n",
      "Epoch 4, test loss: 0.171723\n",
      "Epoch 5, test loss: 0.173746\n",
      "Epoch 6, test loss: 0.172110\n",
      "Epoch 7, test loss: 0.171490\n",
      "Epoch 8, test loss: 0.171586\n",
      "Epoch 9, test loss: 0.171601\n",
      "Epoch 10, test loss: 0.172481\n",
      "Epoch 11, test loss: 0.172347\n",
      "Epoch 12, test loss: 0.171464\n",
      "Epoch 13, test loss: 0.172929\n",
      "Epoch 14, test loss: 0.172154\n",
      "Epoch 15, test loss: 0.172640\n",
      "Epoch 16, test loss: 0.172186\n",
      "Epoch 17, test loss: 0.172637\n",
      "Epoch 18, test loss: 0.172203\n",
      "Epoch 19, test loss: 0.172739\n",
      "Epoch 20, test loss: 0.174974\n",
      "Epoch 21, test loss: 0.173154\n",
      "Epoch 22, test loss: 0.172881\n",
      "Epoch 23, test loss: 0.173673\n",
      "Epoch 24, test loss: 0.173297\n",
      "Epoch 25, test loss: 0.173285\n",
      "Epoch 26, test loss: 0.172983\n",
      "Epoch 27, test loss: 0.174660\n",
      "Epoch 28, test loss: 0.173798\n",
      "Epoch 29, test loss: 0.173018\n",
      "Epoch 30, test loss: 0.172840\n",
      "Epoch 31, test loss: 0.173726\n",
      "Epoch 32, test loss: 0.174380\n",
      "Epoch 33, test loss: 0.173108\n",
      "Epoch 34, test loss: 0.173895\n",
      "Epoch 35, test loss: 0.174634\n",
      "Epoch 36, test loss: 0.172808\n",
      "Epoch 37, test loss: 0.173053\n",
      "Epoch 38, test loss: 0.173115\n",
      "Epoch 39, test loss: 0.173376\n",
      "Epoch 40, test loss: 0.173124\n",
      "Epoch 41, test loss: 0.173535\n",
      "Epoch 42, test loss: 0.174511\n",
      "Epoch 43, test loss: 0.175365\n",
      "Epoch 44, test loss: 0.174709\n",
      "Epoch 45, test loss: 0.173247\n",
      "Epoch 46, test loss: 0.173976\n",
      "Epoch 47, test loss: 0.174004\n",
      "Epoch 48, test loss: 0.177423\n",
      "Epoch 49, test loss: 0.173588\n",
      "Epoch 50, test loss: 0.173693\n",
      "Epoch 51, test loss: 0.173464\n",
      "Epoch 52, test loss: 0.175016\n",
      "Epoch 53, test loss: 0.173259\n",
      "Epoch 54, test loss: 0.173342\n",
      "Epoch 55, test loss: 0.174983\n",
      "Epoch 56, test loss: 0.174524\n",
      "Epoch 57, test loss: 0.173636\n",
      "Epoch 58, test loss: 0.173539\n",
      "Epoch 59, test loss: 0.174533\n",
      "Epoch 60, test loss: 0.173128\n",
      "Epoch 61, test loss: 0.174221\n",
      "Epoch 62, test loss: 0.174408\n",
      "Epoch 63, test loss: 0.173055\n",
      "Epoch 64, test loss: 0.173672\n",
      "Epoch 65, test loss: 0.174192\n",
      "Epoch 66, test loss: 0.174876\n",
      "Epoch 67, test loss: 0.173589\n",
      "Epoch 68, test loss: 0.174123\n",
      "Epoch 69, test loss: 0.173985\n",
      "Epoch 70, test loss: 0.173794\n",
      "Epoch 71, test loss: 0.174074\n",
      "Epoch 72, test loss: 0.174383\n",
      "Epoch 73, test loss: 0.176971\n",
      "Epoch 74, test loss: 0.174475\n",
      "Epoch 75, test loss: 0.174746\n",
      "Epoch 76, test loss: 0.175472\n",
      "Epoch 77, test loss: 0.174062\n",
      "Epoch 78, test loss: 0.174144\n",
      "Epoch 79, test loss: 0.173889\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2949E3790>\n",
      "Epoch 0, test loss: 0.260896\n",
      "Epoch 1, test loss: 0.190784\n",
      "Epoch 2, test loss: 0.185170\n",
      "Epoch 3, test loss: 0.181643\n",
      "Epoch 4, test loss: 0.179759\n",
      "Epoch 5, test loss: 0.178691\n",
      "Epoch 6, test loss: 0.178084\n",
      "Epoch 7, test loss: 0.177685\n",
      "Epoch 8, test loss: 0.176841\n",
      "Epoch 9, test loss: 0.176606\n",
      "Epoch 10, test loss: 0.176081\n",
      "Epoch 11, test loss: 0.175374\n",
      "Epoch 12, test loss: 0.175072\n",
      "Epoch 13, test loss: 0.175075\n",
      "Epoch 14, test loss: 0.174443\n",
      "Epoch 15, test loss: 0.176253\n",
      "Epoch 16, test loss: 0.174539\n",
      "Epoch 17, test loss: 0.174331\n",
      "Epoch 18, test loss: 0.174643\n",
      "Epoch 19, test loss: 0.175265\n",
      "Epoch 20, test loss: 0.174576\n",
      "Epoch 21, test loss: 0.177070\n",
      "Epoch 22, test loss: 0.176993\n",
      "Epoch 23, test loss: 0.176766\n",
      "Epoch 24, test loss: 0.174560\n",
      "Epoch 25, test loss: 0.174742\n",
      "Epoch 26, test loss: 0.175172\n",
      "Epoch 27, test loss: 0.174311\n",
      "Epoch 28, test loss: 0.174269\n",
      "Epoch 29, test loss: 0.180642\n",
      "Epoch 30, test loss: 0.174719\n",
      "Epoch 31, test loss: 0.175597\n",
      "Epoch 32, test loss: 0.174986\n",
      "Epoch 33, test loss: 0.173951\n",
      "Epoch 34, test loss: 0.174207\n",
      "Epoch 35, test loss: 0.173650\n",
      "Epoch 36, test loss: 0.174959\n",
      "Epoch 37, test loss: 0.173786\n",
      "Epoch 38, test loss: 0.173922\n",
      "Epoch 39, test loss: 0.175825\n",
      "Epoch 40, test loss: 0.175034\n",
      "Epoch 41, test loss: 0.173785\n",
      "Epoch 42, test loss: 0.175615\n",
      "Epoch 43, test loss: 0.173661\n",
      "Epoch 44, test loss: 0.173802\n",
      "Epoch 45, test loss: 0.175190\n",
      "Epoch 46, test loss: 0.175463\n",
      "Epoch 47, test loss: 0.173594\n",
      "Epoch 48, test loss: 0.174489\n",
      "Epoch 49, test loss: 0.174502\n",
      "Epoch 50, test loss: 0.177469\n",
      "Epoch 51, test loss: 0.178020\n",
      "Epoch 52, test loss: 0.173188\n",
      "Epoch 53, test loss: 0.173659\n",
      "Epoch 54, test loss: 0.174429\n",
      "Epoch 55, test loss: 0.174133\n",
      "Epoch 56, test loss: 0.174661\n",
      "Epoch 57, test loss: 0.173704\n",
      "Epoch 58, test loss: 0.173417\n",
      "Epoch 59, test loss: 0.173728\n",
      "Epoch 60, test loss: 0.173783\n",
      "Epoch 61, test loss: 0.177199\n",
      "Epoch 62, test loss: 0.174935\n",
      "Epoch 63, test loss: 0.174125\n",
      "Epoch 64, test loss: 0.173677\n",
      "Epoch 65, test loss: 0.176087\n",
      "Epoch 66, test loss: 0.173911\n",
      "Epoch 67, test loss: 0.174534\n",
      "Epoch 68, test loss: 0.173408\n",
      "Epoch 69, test loss: 0.173830\n",
      "Epoch 70, test loss: 0.176155\n",
      "Epoch 71, test loss: 0.173848\n",
      "Epoch 72, test loss: 0.175024\n",
      "Epoch 73, test loss: 0.173966\n",
      "Epoch 74, test loss: 0.173442\n",
      "Epoch 75, test loss: 0.175866\n",
      "Epoch 76, test loss: 0.179130\n",
      "Epoch 77, test loss: 0.174253\n",
      "Epoch 78, test loss: 0.173946\n",
      "Epoch 79, test loss: 0.174252\n",
      "Epoch 80, test loss: 0.175175\n",
      "Epoch 81, test loss: 0.174243\n",
      "Epoch 82, test loss: 0.174360\n",
      "Epoch 83, test loss: 0.174023\n",
      "Epoch 84, test loss: 0.181655\n",
      "Epoch 85, test loss: 0.177855\n",
      "Epoch 86, test loss: 0.173888\n",
      "Epoch 87, test loss: 0.173821\n",
      "Epoch 88, test loss: 0.174113\n",
      "Epoch 89, test loss: 0.174776\n",
      "Epoch 90, test loss: 0.173538\n",
      "Epoch 91, test loss: 0.174682\n",
      "Epoch 92, test loss: 0.174435\n",
      "Epoch 93, test loss: 0.173998\n",
      "Epoch 94, test loss: 0.173174\n",
      "Epoch 95, test loss: 0.172708\n",
      "Epoch 96, test loss: 0.173304\n",
      "Epoch 97, test loss: 0.174075\n",
      "Epoch 98, test loss: 0.174354\n",
      "Epoch 99, test loss: 0.173916\n",
      "Epoch 100, test loss: 0.174106\n",
      "Epoch 101, test loss: 0.174334\n",
      "Epoch 102, test loss: 0.174110\n",
      "Epoch 103, test loss: 0.173514\n",
      "Epoch 104, test loss: 0.173496\n",
      "Epoch 105, test loss: 0.173334\n",
      "Epoch 106, test loss: 0.173882\n",
      "Epoch 107, test loss: 0.172538\n",
      "Epoch 108, test loss: 0.173508\n",
      "Epoch 109, test loss: 0.175193\n",
      "Epoch 110, test loss: 0.173617\n",
      "Epoch 111, test loss: 0.174137\n",
      "Epoch 112, test loss: 0.174898\n",
      "Epoch 113, test loss: 0.173230\n",
      "Epoch 114, test loss: 0.173712\n",
      "Epoch 115, test loss: 0.175206\n",
      "Epoch 116, test loss: 0.173856\n",
      "Epoch 117, test loss: 0.174774\n",
      "Epoch 118, test loss: 0.174014\n",
      "Epoch 119, test loss: 0.172981\n",
      "Epoch 120, test loss: 0.173042\n",
      "Epoch 121, test loss: 0.175636\n",
      "Epoch 122, test loss: 0.175607\n",
      "Epoch 123, test loss: 0.173617\n",
      "Epoch 124, test loss: 0.176798\n",
      "Epoch 125, test loss: 0.174978\n",
      "Epoch 126, test loss: 0.174584\n",
      "Epoch 127, test loss: 0.173082\n",
      "Epoch 128, test loss: 0.173708\n",
      "Epoch 129, test loss: 0.174941\n",
      "Epoch 130, test loss: 0.173027\n",
      "Epoch 131, test loss: 0.173215\n",
      "Epoch 132, test loss: 0.175826\n",
      "Epoch 133, test loss: 0.172427\n",
      "Epoch 134, test loss: 0.172399\n",
      "Epoch 135, test loss: 0.172693\n",
      "Epoch 136, test loss: 0.172988\n",
      "Epoch 137, test loss: 0.173591\n",
      "Epoch 138, test loss: 0.172588\n",
      "Epoch 139, test loss: 0.175451\n",
      "Epoch 140, test loss: 0.172758\n",
      "Epoch 141, test loss: 0.173240\n",
      "Epoch 142, test loss: 0.177621\n",
      "Epoch 143, test loss: 0.173348\n",
      "Epoch 144, test loss: 0.174965\n",
      "Epoch 145, test loss: 0.172393\n",
      "Epoch 146, test loss: 0.173149\n",
      "Epoch 147, test loss: 0.173413\n",
      "Epoch 148, test loss: 0.174338\n",
      "Epoch 149, test loss: 0.173071\n",
      "Epoch 150, test loss: 0.172501\n",
      "Epoch 151, test loss: 0.174466\n",
      "Epoch 152, test loss: 0.173065\n",
      "Epoch 153, test loss: 0.173136\n",
      "Epoch 154, test loss: 0.172740\n",
      "Epoch 155, test loss: 0.175021\n",
      "Epoch 156, test loss: 0.172603\n",
      "Epoch 157, test loss: 0.174874\n",
      "Epoch 158, test loss: 0.172951\n",
      "Epoch 159, test loss: 0.174610\n",
      "Epoch 160, test loss: 0.178142\n",
      "Epoch 161, test loss: 0.175116\n",
      "Epoch 162, test loss: 0.173839\n",
      "Epoch 163, test loss: 0.173636\n",
      "Epoch 164, test loss: 0.183991\n",
      "Epoch 165, test loss: 0.172261\n",
      "Epoch 166, test loss: 0.172720\n",
      "Epoch 167, test loss: 0.174379\n",
      "Epoch 168, test loss: 0.175428\n",
      "Epoch 169, test loss: 0.173100\n",
      "Epoch 170, test loss: 0.172734\n",
      "Epoch 171, test loss: 0.173096\n",
      "Epoch 172, test loss: 0.173561\n",
      "Epoch 173, test loss: 0.175785\n",
      "Epoch 174, test loss: 0.172687\n",
      "Epoch 175, test loss: 0.172394\n",
      "Epoch 176, test loss: 0.173259\n",
      "Epoch 177, test loss: 0.173025\n",
      "Epoch 178, test loss: 0.171874\n",
      "Epoch 179, test loss: 0.172305\n",
      "Epoch 180, test loss: 0.173765\n",
      "Epoch 181, test loss: 0.173933\n",
      "Epoch 182, test loss: 0.172999\n",
      "Epoch 183, test loss: 0.172212\n",
      "Epoch 184, test loss: 0.172552\n",
      "Epoch 185, test loss: 0.172389\n",
      "Epoch 186, test loss: 0.172154\n",
      "Epoch 187, test loss: 0.171853\n",
      "Epoch 188, test loss: 0.171927\n",
      "Epoch 189, test loss: 0.171734\n",
      "Epoch 190, test loss: 0.172598\n",
      "Epoch 191, test loss: 0.173490\n",
      "Epoch 192, test loss: 0.172477\n",
      "Epoch 193, test loss: 0.177364\n",
      "Epoch 194, test loss: 0.172143\n",
      "Epoch 195, test loss: 0.174786\n",
      "Epoch 196, test loss: 0.173152\n",
      "Epoch 197, test loss: 0.172640\n",
      "Epoch 198, test loss: 0.178865\n",
      "Epoch 199, test loss: 0.176029\n",
      "Epoch 200, test loss: 0.173876\n",
      "Epoch 201, test loss: 0.171970\n",
      "Epoch 202, test loss: 0.173866\n",
      "Epoch 203, test loss: 0.172326\n",
      "Epoch 204, test loss: 0.172568\n",
      "Epoch 205, test loss: 0.177355\n",
      "Epoch 206, test loss: 0.174012\n",
      "Epoch 207, test loss: 0.173710\n",
      "Epoch 208, test loss: 0.172422\n",
      "Epoch 209, test loss: 0.173022\n",
      "Epoch 210, test loss: 0.172917\n",
      "Epoch 211, test loss: 0.174883\n",
      "Epoch 212, test loss: 0.177391\n",
      "Epoch 213, test loss: 0.176292\n",
      "Epoch 214, test loss: 0.174362\n",
      "Epoch 215, test loss: 0.175169\n",
      "Epoch 216, test loss: 0.172279\n",
      "Epoch 217, test loss: 0.172651\n",
      "Epoch 218, test loss: 0.171910\n",
      "Epoch 219, test loss: 0.172886\n",
      "Epoch 220, test loss: 0.173268\n",
      "Epoch 221, test loss: 0.177634\n",
      "Epoch 222, test loss: 0.172071\n",
      "Epoch 223, test loss: 0.173331\n",
      "Epoch 224, test loss: 0.172906\n",
      "Epoch 225, test loss: 0.172808\n",
      "Epoch 226, test loss: 0.172549\n",
      "Epoch 227, test loss: 0.173587\n",
      "Epoch 228, test loss: 0.172697\n",
      "Epoch 229, test loss: 0.171381\n",
      "Epoch 230, test loss: 0.172910\n",
      "Epoch 231, test loss: 0.173156\n",
      "Epoch 232, test loss: 0.173180\n",
      "Epoch 233, test loss: 0.171689\n",
      "Epoch 234, test loss: 0.171735\n",
      "Epoch 235, test loss: 0.172317\n",
      "Epoch 236, test loss: 0.173989\n",
      "Epoch 237, test loss: 0.172124\n",
      "Epoch 238, test loss: 0.172483\n",
      "Epoch 239, test loss: 0.171950\n",
      "Pretrain data: 19754507.0\n",
      "Building dataset, requesting data from 0 to 655\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6869/102467\n",
      "Found 655 continuous time series\n",
      "Data shape: (109338, 18), Train/test: 109336/2\n",
      "Train test ratio: 54668.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1708\n",
      "Epoch 0, train loss: 0.254356\n",
      "Epoch 1, train loss: 0.280475\n",
      "Epoch 2, train loss: 0.262843\n",
      "Epoch 3, train loss: 0.179269\n",
      "Epoch 4, train loss: 0.221033\n",
      "Epoch 5, train loss: 0.205467\n",
      "Epoch 6, train loss: 0.205819\n",
      "Epoch 7, train loss: 0.228790\n",
      "Epoch 8, train loss: 0.264953\n",
      "Epoch 9, train loss: 0.293672\n",
      "Epoch 10, train loss: 0.190189\n",
      "Epoch 11, train loss: 0.213560\n",
      "Epoch 12, train loss: 0.198051\n",
      "Epoch 13, train loss: 0.212030\n",
      "Epoch 14, train loss: 0.204597\n",
      "Epoch 15, train loss: 0.234930\n",
      "Epoch 16, train loss: 0.185681\n",
      "Epoch 17, train loss: 0.221093\n",
      "Epoch 18, train loss: 0.216065\n",
      "Epoch 19, train loss: 0.157474\n",
      "Epoch 20, train loss: 0.217117\n",
      "Epoch 21, train loss: 0.230127\n",
      "Epoch 22, train loss: 0.198408\n",
      "Epoch 23, train loss: 0.174037\n",
      "Epoch 24, train loss: 0.214630\n",
      "Epoch 25, train loss: 0.233037\n",
      "Epoch 26, train loss: 0.231451\n",
      "Epoch 27, train loss: 0.224773\n",
      "Epoch 28, train loss: 0.174934\n",
      "Epoch 29, train loss: 0.207227\n",
      "Epoch 30, train loss: 0.185911\n",
      "Epoch 31, train loss: 0.240151\n",
      "Epoch 32, train loss: 0.222113\n",
      "Epoch 33, train loss: 0.165052\n",
      "Epoch 34, train loss: 0.180981\n",
      "Epoch 35, train loss: 0.235353\n",
      "Epoch 36, train loss: 0.217676\n",
      "Epoch 37, train loss: 0.187119\n",
      "Epoch 38, train loss: 0.249822\n",
      "Epoch 39, train loss: 0.197169\n",
      "Epoch 40, train loss: 0.186460\n",
      "Epoch 41, train loss: 0.253615\n",
      "Epoch 42, train loss: 0.195210\n",
      "Epoch 43, train loss: 0.165272\n",
      "Epoch 44, train loss: 0.212866\n",
      "Epoch 45, train loss: 0.217636\n",
      "Epoch 46, train loss: 0.193861\n",
      "Epoch 47, train loss: 0.173662\n",
      "Epoch 48, train loss: 0.222907\n",
      "Epoch 49, train loss: 0.238304\n",
      "Epoch 50, train loss: 0.202957\n",
      "Epoch 51, train loss: 0.160570\n",
      "Epoch 52, train loss: 0.251220\n",
      "Epoch 53, train loss: 0.221831\n",
      "Epoch 54, train loss: 0.278616\n",
      "Epoch 55, train loss: 0.211722\n",
      "Epoch 56, train loss: 0.185944\n",
      "Epoch 57, train loss: 0.236156\n",
      "Epoch 58, train loss: 0.155593\n",
      "Epoch 59, train loss: 0.178633\n",
      "Epoch 60, train loss: 0.188649\n",
      "Epoch 61, train loss: 0.187996\n",
      "Epoch 62, train loss: 0.174761\n",
      "Epoch 63, train loss: 0.219086\n",
      "Epoch 64, train loss: 0.228516\n",
      "Epoch 65, train loss: 0.202413\n",
      "Epoch 66, train loss: 0.229244\n",
      "Epoch 67, train loss: 0.165145\n",
      "Epoch 68, train loss: 0.209552\n",
      "Epoch 69, train loss: 0.214076\n",
      "Epoch 70, train loss: 0.217925\n",
      "Epoch 71, train loss: 0.204366\n",
      "Epoch 72, train loss: 0.239600\n",
      "Epoch 73, train loss: 0.243970\n",
      "Epoch 74, train loss: 0.175326\n",
      "Epoch 75, train loss: 0.224946\n",
      "Epoch 76, train loss: 0.194094\n",
      "Epoch 77, train loss: 0.218806\n",
      "Epoch 78, train loss: 0.193427\n",
      "Epoch 79, train loss: 0.199480\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[296. 290. 284. ... 210. 204. 198.]\n",
      " [290. 284. 279. ... 204. 198. 192.]\n",
      " [284. 279. 272. ... 198. 192. 186.]\n",
      " ...\n",
      " [267. 265. 256. ... 172. 171. 178.]\n",
      " [265. 256. 247. ... 171. 178. 180.]\n",
      " [256. 247. 238. ... 178. 180. 180.]]\n",
      "y here is\n",
      "[[169. 169. 169. ... 169. 169. 169.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [153. 153. 153. ... 153. 153. 153.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1452, 18), Train/test: 1/1451\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 187 segments\n",
      "Building dataset, requesting data from 0 to 187\n",
      "x here is\n",
      "[[ 71.  71.  71. ...  74.  75.  76.]\n",
      " [ 71.  71.  71. ...  75.  76.  77.]\n",
      " [ 71.  71.  72. ...  76.  77.  79.]\n",
      " ...\n",
      " [232. 236. 240. ... 277. 277. 277.]\n",
      " [236. 240. 245. ... 277. 277. 279.]\n",
      " [240. 245. 254. ... 277. 279. 282.]]\n",
      "y here is\n",
      "[[ 83.  83.  83. ...  83.  83.  83.]\n",
      " [ 86.  86.  86. ...  86.  86.  86.]\n",
      " [ 89.  89.  89. ...  89.  89.  89.]\n",
      " ...\n",
      " [315. 315. 315. ... 315. 315. 315.]\n",
      " [310. 310. 310. ... 310. 310. 310.]\n",
      " [303. 303. 303. ... 303. 303. 303.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 648/6287\n",
      "Found 187 continuous time series\n",
      "Data shape: (6937, 18), Train/test: 6935/2\n",
      "Train test ratio: 3467.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47A1DFA00>\n",
      "Epoch 0, test loss: 0.218018\n",
      "Epoch 1, test loss: 0.216204\n",
      "Epoch 2, test loss: 0.214561\n",
      "Epoch 3, test loss: 0.214023\n",
      "Epoch 4, test loss: 0.216496\n",
      "Epoch 5, test loss: 0.214105\n",
      "Epoch 6, test loss: 0.217059\n",
      "Epoch 7, test loss: 0.219160\n",
      "Epoch 8, test loss: 0.213577\n",
      "Epoch 9, test loss: 0.214586\n",
      "Epoch 10, test loss: 0.214152\n",
      "Epoch 11, test loss: 0.213246\n",
      "Epoch 12, test loss: 0.215444\n",
      "Epoch 13, test loss: 0.213545\n",
      "Epoch 14, test loss: 0.214606\n",
      "Epoch 15, test loss: 0.224780\n",
      "Epoch 16, test loss: 0.216283\n",
      "Epoch 17, test loss: 0.217668\n",
      "Epoch 18, test loss: 0.214400\n",
      "Epoch 19, test loss: 0.215114\n",
      "Epoch 20, test loss: 0.214741\n",
      "Epoch 21, test loss: 0.215190\n",
      "Epoch 22, test loss: 0.218469\n",
      "Epoch 23, test loss: 0.214264\n",
      "Epoch 24, test loss: 0.214253\n",
      "Epoch 25, test loss: 0.214852\n",
      "Epoch 26, test loss: 0.217036\n",
      "Epoch 27, test loss: 0.214994\n",
      "Epoch 28, test loss: 0.216150\n",
      "Epoch 29, test loss: 0.217641\n",
      "Epoch 30, test loss: 0.215126\n",
      "Epoch 31, test loss: 0.220652\n",
      "Epoch 32, test loss: 0.214494\n",
      "Epoch 33, test loss: 0.214340\n",
      "Epoch 34, test loss: 0.218282\n",
      "Epoch 35, test loss: 0.217314\n",
      "Epoch 36, test loss: 0.215228\n",
      "Epoch 37, test loss: 0.214797\n",
      "Epoch 38, test loss: 0.218411\n",
      "Epoch 39, test loss: 0.214891\n",
      "Epoch 40, test loss: 0.214937\n",
      "Epoch 41, test loss: 0.216157\n",
      "Epoch 42, test loss: 0.215894\n",
      "Epoch 43, test loss: 0.214382\n",
      "Epoch 44, test loss: 0.214172\n",
      "Epoch 45, test loss: 0.220949\n",
      "Epoch 46, test loss: 0.214377\n",
      "Epoch 47, test loss: 0.216109\n",
      "Epoch 48, test loss: 0.214600\n",
      "Epoch 49, test loss: 0.217532\n",
      "Epoch 50, test loss: 0.214729\n",
      "Epoch 51, test loss: 0.216097\n",
      "Epoch 52, test loss: 0.216689\n",
      "Epoch 53, test loss: 0.216554\n",
      "Epoch 54, test loss: 0.215129\n",
      "Epoch 55, test loss: 0.219474\n",
      "Epoch 56, test loss: 0.217384\n",
      "Epoch 57, test loss: 0.214734\n",
      "Epoch 58, test loss: 0.213576\n",
      "Epoch 59, test loss: 0.214335\n",
      "Epoch 60, test loss: 0.215746\n",
      "Epoch 61, test loss: 0.214909\n",
      "Epoch 62, test loss: 0.214079\n",
      "Epoch 63, test loss: 0.222160\n",
      "Epoch 64, test loss: 0.218882\n",
      "Epoch 65, test loss: 0.214426\n",
      "Epoch 66, test loss: 0.213469\n",
      "Epoch 67, test loss: 0.214506\n",
      "Epoch 68, test loss: 0.213827\n",
      "Epoch 69, test loss: 0.214200\n",
      "Epoch 70, test loss: 0.215321\n",
      "Epoch 71, test loss: 0.214123\n",
      "Epoch 72, test loss: 0.220851\n",
      "Epoch 73, test loss: 0.216293\n",
      "Epoch 74, test loss: 0.214943\n",
      "Epoch 75, test loss: 0.214059\n",
      "Epoch 76, test loss: 0.213379\n",
      "Epoch 77, test loss: 0.213497\n",
      "Epoch 78, test loss: 0.214543\n",
      "Epoch 79, test loss: 0.214510\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47A1DFA00>\n",
      "Epoch 0, test loss: 0.216516\n",
      "Epoch 1, test loss: 0.217307\n",
      "Epoch 2, test loss: 0.215000\n",
      "Epoch 3, test loss: 0.215539\n",
      "Epoch 4, test loss: 0.214973\n",
      "Epoch 5, test loss: 0.214449\n",
      "Epoch 6, test loss: 0.217454\n",
      "Epoch 7, test loss: 0.217960\n",
      "Epoch 8, test loss: 0.214285\n",
      "Epoch 9, test loss: 0.214402\n",
      "Epoch 10, test loss: 0.217666\n",
      "Epoch 11, test loss: 0.214903\n",
      "Epoch 12, test loss: 0.220868\n",
      "Epoch 13, test loss: 0.214408\n",
      "Epoch 14, test loss: 0.214836\n",
      "Epoch 15, test loss: 0.218450\n",
      "Epoch 16, test loss: 0.214886\n",
      "Epoch 17, test loss: 0.217793\n",
      "Epoch 18, test loss: 0.216212\n",
      "Epoch 19, test loss: 0.215461\n",
      "Epoch 20, test loss: 0.215124\n",
      "Epoch 21, test loss: 0.217291\n",
      "Epoch 22, test loss: 0.216346\n",
      "Epoch 23, test loss: 0.214741\n",
      "Epoch 24, test loss: 0.221611\n",
      "Epoch 25, test loss: 0.218492\n",
      "Epoch 26, test loss: 0.214850\n",
      "Epoch 27, test loss: 0.216423\n",
      "Epoch 28, test loss: 0.217954\n",
      "Epoch 29, test loss: 0.215779\n",
      "Epoch 30, test loss: 0.215091\n",
      "Epoch 31, test loss: 0.214374\n",
      "Epoch 32, test loss: 0.216304\n",
      "Epoch 33, test loss: 0.215363\n",
      "Epoch 34, test loss: 0.217530\n",
      "Epoch 35, test loss: 0.216770\n",
      "Epoch 36, test loss: 0.215645\n",
      "Epoch 37, test loss: 0.214426\n",
      "Epoch 38, test loss: 0.214625\n",
      "Epoch 39, test loss: 0.214818\n",
      "Epoch 40, test loss: 0.214629\n",
      "Epoch 41, test loss: 0.215237\n",
      "Epoch 42, test loss: 0.214651\n",
      "Epoch 43, test loss: 0.214905\n",
      "Epoch 44, test loss: 0.218369\n",
      "Epoch 45, test loss: 0.214659\n",
      "Epoch 46, test loss: 0.214633\n",
      "Epoch 47, test loss: 0.218240\n",
      "Epoch 48, test loss: 0.214987\n",
      "Epoch 49, test loss: 0.215174\n",
      "Epoch 50, test loss: 0.216914\n",
      "Epoch 51, test loss: 0.215851\n",
      "Epoch 52, test loss: 0.214791\n",
      "Epoch 53, test loss: 0.213553\n",
      "Epoch 54, test loss: 0.214027\n",
      "Epoch 55, test loss: 0.214381\n",
      "Epoch 56, test loss: 0.216073\n",
      "Epoch 57, test loss: 0.215122\n",
      "Epoch 58, test loss: 0.214566\n",
      "Epoch 59, test loss: 0.214440\n",
      "Epoch 60, test loss: 0.218954\n",
      "Epoch 61, test loss: 0.213343\n",
      "Epoch 62, test loss: 0.214103\n",
      "Epoch 63, test loss: 0.214519\n",
      "Epoch 64, test loss: 0.214973\n",
      "Epoch 65, test loss: 0.218884\n",
      "Epoch 66, test loss: 0.216772\n",
      "Epoch 67, test loss: 0.213994\n",
      "Epoch 68, test loss: 0.213735\n",
      "Epoch 69, test loss: 0.213710\n",
      "Epoch 70, test loss: 0.214360\n",
      "Epoch 71, test loss: 0.214717\n",
      "Epoch 72, test loss: 0.215488\n",
      "Epoch 73, test loss: 0.215840\n",
      "Epoch 74, test loss: 0.216296\n",
      "Epoch 75, test loss: 0.216972\n",
      "Epoch 76, test loss: 0.214285\n",
      "Epoch 77, test loss: 0.214367\n",
      "Epoch 78, test loss: 0.214936\n",
      "Epoch 79, test loss: 0.216407\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47A1DFA00>\n",
      "Epoch 0, test loss: 0.717579\n",
      "Epoch 1, test loss: 0.291398\n",
      "Epoch 2, test loss: 0.243942\n",
      "Epoch 3, test loss: 0.233882\n",
      "Epoch 4, test loss: 0.229822\n",
      "Epoch 5, test loss: 0.227431\n",
      "Epoch 6, test loss: 0.224378\n",
      "Epoch 7, test loss: 0.223830\n",
      "Epoch 8, test loss: 0.222225\n",
      "Epoch 9, test loss: 0.221214\n",
      "Epoch 10, test loss: 0.221755\n",
      "Epoch 11, test loss: 0.221331\n",
      "Epoch 12, test loss: 0.220054\n",
      "Epoch 13, test loss: 0.222222\n",
      "Epoch 14, test loss: 0.220387\n",
      "Epoch 15, test loss: 0.220319\n",
      "Epoch 16, test loss: 0.218857\n",
      "Epoch 17, test loss: 0.223297\n",
      "Epoch 18, test loss: 0.219440\n",
      "Epoch 19, test loss: 0.219689\n",
      "Epoch 20, test loss: 0.219658\n",
      "Epoch 21, test loss: 0.224910\n",
      "Epoch 22, test loss: 0.219803\n",
      "Epoch 23, test loss: 0.220800\n",
      "Epoch 24, test loss: 0.218956\n",
      "Epoch 25, test loss: 0.220708\n",
      "Epoch 26, test loss: 0.219346\n",
      "Epoch 27, test loss: 0.218392\n",
      "Epoch 28, test loss: 0.217265\n",
      "Epoch 29, test loss: 0.217599\n",
      "Epoch 30, test loss: 0.218880\n",
      "Epoch 31, test loss: 0.217504\n",
      "Epoch 32, test loss: 0.220074\n",
      "Epoch 33, test loss: 0.220136\n",
      "Epoch 34, test loss: 0.217053\n",
      "Epoch 35, test loss: 0.217147\n",
      "Epoch 36, test loss: 0.220917\n",
      "Epoch 37, test loss: 0.221463\n",
      "Epoch 38, test loss: 0.217572\n",
      "Epoch 39, test loss: 0.218589\n",
      "Epoch 40, test loss: 0.216905\n",
      "Epoch 41, test loss: 0.216891\n",
      "Epoch 42, test loss: 0.217288\n",
      "Epoch 43, test loss: 0.218192\n",
      "Epoch 44, test loss: 0.218927\n",
      "Epoch 45, test loss: 0.224995\n",
      "Epoch 46, test loss: 0.216142\n",
      "Epoch 47, test loss: 0.216522\n",
      "Epoch 48, test loss: 0.216255\n",
      "Epoch 49, test loss: 0.216023\n",
      "Epoch 50, test loss: 0.217156\n",
      "Epoch 51, test loss: 0.217674\n",
      "Epoch 52, test loss: 0.217104\n",
      "Epoch 53, test loss: 0.216649\n",
      "Epoch 54, test loss: 0.216716\n",
      "Epoch 55, test loss: 0.216601\n",
      "Epoch 56, test loss: 0.218342\n",
      "Epoch 57, test loss: 0.217258\n",
      "Epoch 58, test loss: 0.217235\n",
      "Epoch 59, test loss: 0.216768\n",
      "Epoch 60, test loss: 0.216384\n",
      "Epoch 61, test loss: 0.216978\n",
      "Epoch 62, test loss: 0.216234\n",
      "Epoch 63, test loss: 0.218939\n",
      "Epoch 64, test loss: 0.216785\n",
      "Epoch 65, test loss: 0.218879\n",
      "Epoch 66, test loss: 0.217372\n",
      "Epoch 67, test loss: 0.224086\n",
      "Epoch 68, test loss: 0.216299\n",
      "Epoch 69, test loss: 0.216145\n",
      "Epoch 70, test loss: 0.215865\n",
      "Epoch 71, test loss: 0.215660\n",
      "Epoch 72, test loss: 0.219004\n",
      "Epoch 73, test loss: 0.215757\n",
      "Epoch 74, test loss: 0.215685\n",
      "Epoch 75, test loss: 0.217436\n",
      "Epoch 76, test loss: 0.215777\n",
      "Epoch 77, test loss: 0.217307\n",
      "Epoch 78, test loss: 0.216358\n",
      "Epoch 79, test loss: 0.220196\n",
      "Epoch 80, test loss: 0.215891\n",
      "Epoch 81, test loss: 0.216436\n",
      "Epoch 82, test loss: 0.216830\n",
      "Epoch 83, test loss: 0.217302\n",
      "Epoch 84, test loss: 0.216986\n",
      "Epoch 85, test loss: 0.216841\n",
      "Epoch 86, test loss: 0.215971\n",
      "Epoch 87, test loss: 0.215329\n",
      "Epoch 88, test loss: 0.215524\n",
      "Epoch 89, test loss: 0.215935\n",
      "Epoch 90, test loss: 0.216522\n",
      "Epoch 91, test loss: 0.216022\n",
      "Epoch 92, test loss: 0.217225\n",
      "Epoch 93, test loss: 0.219198\n",
      "Epoch 94, test loss: 0.217031\n",
      "Epoch 95, test loss: 0.218911\n",
      "Epoch 96, test loss: 0.221186\n",
      "Epoch 97, test loss: 0.216458\n",
      "Epoch 98, test loss: 0.216302\n",
      "Epoch 99, test loss: 0.215989\n",
      "Epoch 100, test loss: 0.215950\n",
      "Epoch 101, test loss: 0.216736\n",
      "Epoch 102, test loss: 0.216214\n",
      "Epoch 103, test loss: 0.216602\n",
      "Epoch 104, test loss: 0.215743\n",
      "Epoch 105, test loss: 0.215885\n",
      "Epoch 106, test loss: 0.215913\n",
      "Epoch 107, test loss: 0.219384\n",
      "Epoch 108, test loss: 0.216284\n",
      "Epoch 109, test loss: 0.216893\n",
      "Epoch 110, test loss: 0.216466\n",
      "Epoch 111, test loss: 0.216708\n",
      "Epoch 112, test loss: 0.215787\n",
      "Epoch 113, test loss: 0.219002\n",
      "Epoch 114, test loss: 0.217151\n",
      "Epoch 115, test loss: 0.216490\n",
      "Epoch 116, test loss: 0.219719\n",
      "Epoch 117, test loss: 0.216041\n",
      "Epoch 118, test loss: 0.215989\n",
      "Epoch 119, test loss: 0.219346\n",
      "Epoch 120, test loss: 0.216698\n",
      "Epoch 121, test loss: 0.219622\n",
      "Epoch 122, test loss: 0.215968\n",
      "Epoch 123, test loss: 0.216944\n",
      "Epoch 124, test loss: 0.216663\n",
      "Epoch 125, test loss: 0.216287\n",
      "Epoch 126, test loss: 0.216164\n",
      "Epoch 127, test loss: 0.216212\n",
      "Epoch 128, test loss: 0.215978\n",
      "Epoch 129, test loss: 0.218577\n",
      "Epoch 130, test loss: 0.216078\n",
      "Epoch 131, test loss: 0.216875\n",
      "Epoch 132, test loss: 0.217491\n",
      "Epoch 133, test loss: 0.217551\n",
      "Epoch 134, test loss: 0.216508\n",
      "Epoch 135, test loss: 0.217474\n",
      "Epoch 136, test loss: 0.216103\n",
      "Epoch 137, test loss: 0.216401\n",
      "Epoch 138, test loss: 0.215397\n",
      "Epoch 139, test loss: 0.215725\n",
      "Epoch 140, test loss: 0.215998\n",
      "Epoch 141, test loss: 0.215991\n",
      "Epoch 142, test loss: 0.216079\n",
      "Epoch 143, test loss: 0.216219\n",
      "Epoch 144, test loss: 0.218102\n",
      "Epoch 145, test loss: 0.216066\n",
      "Epoch 146, test loss: 0.218441\n",
      "Epoch 147, test loss: 0.216470\n",
      "Epoch 148, test loss: 0.216320\n",
      "Epoch 149, test loss: 0.214963\n",
      "Epoch 150, test loss: 0.223331\n",
      "Epoch 151, test loss: 0.216420\n",
      "Epoch 152, test loss: 0.221968\n",
      "Epoch 153, test loss: 0.215734\n",
      "Epoch 154, test loss: 0.216024\n",
      "Epoch 155, test loss: 0.217603\n",
      "Epoch 156, test loss: 0.217400\n",
      "Epoch 157, test loss: 0.220406\n",
      "Epoch 158, test loss: 0.215717\n",
      "Epoch 159, test loss: 0.217526\n",
      "Epoch 160, test loss: 0.216693\n",
      "Epoch 161, test loss: 0.216068\n",
      "Epoch 162, test loss: 0.215569\n",
      "Epoch 163, test loss: 0.216048\n",
      "Epoch 164, test loss: 0.216134\n",
      "Epoch 165, test loss: 0.224041\n",
      "Epoch 166, test loss: 0.215549\n",
      "Epoch 167, test loss: 0.216531\n",
      "Epoch 168, test loss: 0.217152\n",
      "Epoch 169, test loss: 0.215977\n",
      "Epoch 170, test loss: 0.215827\n",
      "Epoch 171, test loss: 0.216016\n",
      "Epoch 172, test loss: 0.216788\n",
      "Epoch 173, test loss: 0.216060\n",
      "Epoch 174, test loss: 0.221912\n",
      "Epoch 175, test loss: 0.218333\n",
      "Epoch 176, test loss: 0.217377\n",
      "Epoch 177, test loss: 0.217590\n",
      "Epoch 178, test loss: 0.215415\n",
      "Epoch 179, test loss: 0.218527\n",
      "Epoch 180, test loss: 0.215638\n",
      "Epoch 181, test loss: 0.216857\n",
      "Epoch 182, test loss: 0.218510\n",
      "Epoch 183, test loss: 0.216124\n",
      "Epoch 184, test loss: 0.216988\n",
      "Epoch 185, test loss: 0.219239\n",
      "Epoch 186, test loss: 0.217082\n",
      "Epoch 187, test loss: 0.218236\n",
      "Epoch 188, test loss: 0.215958\n",
      "Epoch 189, test loss: 0.215894\n",
      "Epoch 190, test loss: 0.217139\n",
      "Epoch 191, test loss: 0.218995\n",
      "Epoch 192, test loss: 0.217330\n",
      "Epoch 193, test loss: 0.216819\n",
      "Epoch 194, test loss: 0.217290\n",
      "Epoch 195, test loss: 0.216315\n",
      "Epoch 196, test loss: 0.216143\n",
      "Epoch 197, test loss: 0.216225\n",
      "Epoch 198, test loss: 0.215374\n",
      "Epoch 199, test loss: 0.217449\n",
      "Epoch 200, test loss: 0.216305\n",
      "Epoch 201, test loss: 0.215754\n",
      "Epoch 202, test loss: 0.217466\n",
      "Epoch 203, test loss: 0.219164\n",
      "Epoch 204, test loss: 0.216488\n",
      "Epoch 205, test loss: 0.215690\n",
      "Epoch 206, test loss: 0.217865\n",
      "Epoch 207, test loss: 0.216237\n",
      "Epoch 208, test loss: 0.216696\n",
      "Epoch 209, test loss: 0.217161\n",
      "Epoch 210, test loss: 0.220169\n",
      "Epoch 211, test loss: 0.216460\n",
      "Epoch 212, test loss: 0.218130\n",
      "Epoch 213, test loss: 0.216683\n",
      "Epoch 214, test loss: 0.216179\n",
      "Epoch 215, test loss: 0.217236\n",
      "Epoch 216, test loss: 0.216500\n",
      "Epoch 217, test loss: 0.217107\n",
      "Epoch 218, test loss: 0.220253\n",
      "Epoch 219, test loss: 0.218939\n",
      "Epoch 220, test loss: 0.217775\n",
      "Epoch 221, test loss: 0.216570\n",
      "Epoch 222, test loss: 0.219356\n",
      "Epoch 223, test loss: 0.216424\n",
      "Epoch 224, test loss: 0.220143\n",
      "Epoch 225, test loss: 0.219489\n",
      "Epoch 226, test loss: 0.215787\n",
      "Epoch 227, test loss: 0.217693\n",
      "Epoch 228, test loss: 0.216921\n",
      "Epoch 229, test loss: 0.217600\n",
      "Epoch 230, test loss: 0.216108\n",
      "Epoch 231, test loss: 0.216756\n",
      "Epoch 232, test loss: 0.216479\n",
      "Epoch 233, test loss: 0.216383\n",
      "Epoch 234, test loss: 0.226509\n",
      "Epoch 235, test loss: 0.217251\n",
      "Epoch 236, test loss: 0.217017\n",
      "Epoch 237, test loss: 0.217421\n",
      "Epoch 238, test loss: 0.221441\n",
      "Epoch 239, test loss: 0.217368\n",
      "Pretrain data: 19086006.0\n",
      "Building dataset, requesting data from 0 to 782\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7358/98113\n",
      "Found 782 continuous time series\n",
      "Data shape: (105473, 18), Train/test: 105471/2\n",
      "Train test ratio: 52735.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1647\n",
      "Epoch 0, train loss: 0.305739\n",
      "Epoch 1, train loss: 0.191635\n",
      "Epoch 2, train loss: 0.214490\n",
      "Epoch 3, train loss: 0.238353\n",
      "Epoch 4, train loss: 0.192398\n",
      "Epoch 5, train loss: 0.194991\n",
      "Epoch 6, train loss: 0.187869\n",
      "Epoch 7, train loss: 0.213626\n",
      "Epoch 8, train loss: 0.199764\n",
      "Epoch 9, train loss: 0.205494\n",
      "Epoch 10, train loss: 0.167548\n",
      "Epoch 11, train loss: 0.187824\n",
      "Epoch 12, train loss: 0.189682\n",
      "Epoch 13, train loss: 0.181286\n",
      "Epoch 14, train loss: 0.206574\n",
      "Epoch 15, train loss: 0.205389\n",
      "Epoch 16, train loss: 0.248918\n",
      "Epoch 17, train loss: 0.215519\n",
      "Epoch 18, train loss: 0.174831\n",
      "Epoch 19, train loss: 0.227286\n",
      "Epoch 20, train loss: 0.248798\n",
      "Epoch 21, train loss: 0.202879\n",
      "Epoch 22, train loss: 0.194106\n",
      "Epoch 23, train loss: 0.197829\n",
      "Epoch 24, train loss: 0.167156\n",
      "Epoch 25, train loss: 0.178685\n",
      "Epoch 26, train loss: 0.196387\n",
      "Epoch 27, train loss: 0.168440\n",
      "Epoch 28, train loss: 0.219891\n",
      "Epoch 29, train loss: 0.244946\n",
      "Epoch 30, train loss: 0.156608\n",
      "Epoch 31, train loss: 0.194222\n",
      "Epoch 32, train loss: 0.144863\n",
      "Epoch 33, train loss: 0.200409\n",
      "Epoch 34, train loss: 0.280180\n",
      "Epoch 35, train loss: 0.179230\n",
      "Epoch 36, train loss: 0.217610\n",
      "Epoch 37, train loss: 0.178255\n",
      "Epoch 38, train loss: 0.170552\n",
      "Epoch 39, train loss: 0.186405\n",
      "Epoch 40, train loss: 0.197427\n",
      "Epoch 41, train loss: 0.198767\n",
      "Epoch 42, train loss: 0.219207\n",
      "Epoch 43, train loss: 0.196678\n",
      "Epoch 44, train loss: 0.219536\n",
      "Epoch 45, train loss: 0.181068\n",
      "Epoch 46, train loss: 0.204361\n",
      "Epoch 47, train loss: 0.218480\n",
      "Epoch 48, train loss: 0.156617\n",
      "Epoch 49, train loss: 0.189408\n",
      "Epoch 50, train loss: 0.189041\n",
      "Epoch 51, train loss: 0.185459\n",
      "Epoch 52, train loss: 0.144700\n",
      "Epoch 53, train loss: 0.217988\n",
      "Epoch 54, train loss: 0.211035\n",
      "Epoch 55, train loss: 0.165448\n",
      "Epoch 56, train loss: 0.158582\n",
      "Epoch 57, train loss: 0.187275\n",
      "Epoch 58, train loss: 0.164243\n",
      "Epoch 59, train loss: 0.164853\n",
      "Epoch 60, train loss: 0.216572\n",
      "Epoch 61, train loss: 0.175100\n",
      "Epoch 62, train loss: 0.189661\n",
      "Epoch 63, train loss: 0.199437\n",
      "Epoch 64, train loss: 0.201349\n",
      "Epoch 65, train loss: 0.244987\n",
      "Epoch 66, train loss: 0.192717\n",
      "Epoch 67, train loss: 0.190533\n",
      "Epoch 68, train loss: 0.173590\n",
      "Epoch 69, train loss: 0.197323\n",
      "Epoch 70, train loss: 0.247349\n",
      "Epoch 71, train loss: 0.154030\n",
      "Epoch 72, train loss: 0.180467\n",
      "Epoch 73, train loss: 0.137315\n",
      "Epoch 74, train loss: 0.279660\n",
      "Epoch 75, train loss: 0.153982\n",
      "Epoch 76, train loss: 0.198933\n",
      "Epoch 77, train loss: 0.233091\n",
      "Epoch 78, train loss: 0.243437\n",
      "Epoch 79, train loss: 0.184455\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "x here is\n",
      "[[243. 253. 262. ... 313. 312. 311.]\n",
      " [253. 262. 269. ... 312. 311. 314.]\n",
      " [262. 269. 269. ... 311. 314. 318.]\n",
      " ...\n",
      " [ 81.  93.  89. ...  53.  66.  90.]\n",
      " [ 93.  89.  74. ...  66.  90.  87.]\n",
      " [ 89.  74.  65. ...  90.  87.  86.]]\n",
      "y here is\n",
      "[[296. 296. 296. ... 296. 296. 296.]\n",
      " [289. 289. 289. ... 289. 289. 289.]\n",
      " [290. 290. 290. ... 290. 290. 290.]\n",
      " ...\n",
      " [ 72.  72.  72. ...  72.  72.  72.]\n",
      " [ 78.  78.  78. ...  78.  78.  78.]\n",
      " [ 79.  79.  79. ...  79.  79.  79.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (2297, 18), Train/test: 1/2296\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 60 segments\n",
      "Building dataset, requesting data from 0 to 60\n",
      "x here is\n",
      "[[ 48.  48.  53. ...  68. 107. 114.]\n",
      " [ 48.  53.  63. ... 107. 114. 119.]\n",
      " [ 53.  63.  69. ... 114. 119. 121.]\n",
      " ...\n",
      " [168. 164. 159. ... 212. 223. 227.]\n",
      " [164. 159. 160. ... 223. 227. 223.]\n",
      " [159. 160. 155. ... 227. 223. 215.]]\n",
      "y here is\n",
      "[[132. 132. 132. ... 132. 132. 132.]\n",
      " [134. 134. 134. ... 134. 134. 134.]\n",
      " [138. 138. 138. ... 138. 138. 138.]\n",
      " ...\n",
      " [213. 213. 213. ... 213. 213. 213.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [235. 235. 235. ... 235. 235. 235.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 159/10641\n",
      "Found 60 continuous time series\n",
      "Data shape: (10802, 18), Train/test: 10800/2\n",
      "Train test ratio: 5400.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2952A3A30>\n",
      "Epoch 0, test loss: 0.218065\n",
      "Epoch 1, test loss: 0.218744\n",
      "Epoch 2, test loss: 0.219130\n",
      "Epoch 3, test loss: 0.219055\n",
      "Epoch 4, test loss: 0.219304\n",
      "Epoch 5, test loss: 0.224534\n",
      "Epoch 6, test loss: 0.218970\n",
      "Epoch 7, test loss: 0.219586\n",
      "Epoch 8, test loss: 0.223288\n",
      "Epoch 9, test loss: 0.223259\n",
      "Epoch 10, test loss: 0.225321\n",
      "Epoch 11, test loss: 0.219358\n",
      "Epoch 12, test loss: 0.219879\n",
      "Epoch 13, test loss: 0.225078\n",
      "Epoch 14, test loss: 0.220336\n",
      "Epoch 15, test loss: 0.218735\n",
      "Epoch 16, test loss: 0.223084\n",
      "Epoch 17, test loss: 0.219803\n",
      "Epoch 18, test loss: 0.221304\n",
      "Epoch 19, test loss: 0.221663\n",
      "Epoch 20, test loss: 0.219528\n",
      "Epoch 21, test loss: 0.218964\n",
      "Epoch 22, test loss: 0.220766\n",
      "Epoch 23, test loss: 0.219371\n",
      "Epoch 24, test loss: 0.219307\n",
      "Epoch 25, test loss: 0.219862\n",
      "Epoch 26, test loss: 0.223966\n",
      "Epoch 27, test loss: 0.219225\n",
      "Epoch 28, test loss: 0.219190\n",
      "Epoch 29, test loss: 0.219698\n",
      "Epoch 30, test loss: 0.221211\n",
      "Epoch 31, test loss: 0.219767\n",
      "Epoch 32, test loss: 0.218833\n",
      "Epoch 33, test loss: 0.219346\n",
      "Epoch 34, test loss: 0.220261\n",
      "Epoch 35, test loss: 0.223158\n",
      "Epoch 36, test loss: 0.219502\n",
      "Epoch 37, test loss: 0.220390\n",
      "Epoch 38, test loss: 0.221029\n",
      "Epoch 39, test loss: 0.220437\n",
      "Epoch 40, test loss: 0.220054\n",
      "Epoch 41, test loss: 0.218680\n",
      "Epoch 42, test loss: 0.229141\n",
      "Epoch 43, test loss: 0.219489\n",
      "Epoch 44, test loss: 0.220987\n",
      "Epoch 45, test loss: 0.218688\n",
      "Epoch 46, test loss: 0.220132\n",
      "Epoch 47, test loss: 0.221633\n",
      "Epoch 48, test loss: 0.218887\n",
      "Epoch 49, test loss: 0.220155\n",
      "Epoch 50, test loss: 0.221144\n",
      "Epoch 51, test loss: 0.219733\n",
      "Epoch 52, test loss: 0.218176\n",
      "Epoch 53, test loss: 0.218605\n",
      "Epoch 54, test loss: 0.218514\n",
      "Epoch 55, test loss: 0.218856\n",
      "Epoch 56, test loss: 0.219157\n",
      "Epoch 57, test loss: 0.220464\n",
      "Epoch 58, test loss: 0.220818\n",
      "Epoch 59, test loss: 0.218821\n",
      "Epoch 60, test loss: 0.219255\n",
      "Epoch 61, test loss: 0.224994\n",
      "Epoch 62, test loss: 0.220299\n",
      "Epoch 63, test loss: 0.219666\n",
      "Epoch 64, test loss: 0.220321\n",
      "Epoch 65, test loss: 0.221826\n",
      "Epoch 66, test loss: 0.225726\n",
      "Epoch 67, test loss: 0.218784\n",
      "Epoch 68, test loss: 0.219760\n",
      "Epoch 69, test loss: 0.219887\n",
      "Epoch 70, test loss: 0.225058\n",
      "Epoch 71, test loss: 0.221261\n",
      "Epoch 72, test loss: 0.220222\n",
      "Epoch 73, test loss: 0.220004\n",
      "Epoch 74, test loss: 0.219673\n",
      "Epoch 75, test loss: 0.221568\n",
      "Epoch 76, test loss: 0.223625\n",
      "Epoch 77, test loss: 0.225250\n",
      "Epoch 78, test loss: 0.220023\n",
      "Epoch 79, test loss: 0.220302\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2952A3A30>\n",
      "Epoch 0, test loss: 0.219716\n",
      "Epoch 1, test loss: 0.220130\n",
      "Epoch 2, test loss: 0.218222\n",
      "Epoch 3, test loss: 0.218491\n",
      "Epoch 4, test loss: 0.223699\n",
      "Epoch 5, test loss: 0.217331\n",
      "Epoch 6, test loss: 0.219345\n",
      "Epoch 7, test loss: 0.218298\n",
      "Epoch 8, test loss: 0.218475\n",
      "Epoch 9, test loss: 0.219267\n",
      "Epoch 10, test loss: 0.217390\n",
      "Epoch 11, test loss: 0.221407\n",
      "Epoch 12, test loss: 0.217560\n",
      "Epoch 13, test loss: 0.223096\n",
      "Epoch 14, test loss: 0.219210\n",
      "Epoch 15, test loss: 0.217645\n",
      "Epoch 16, test loss: 0.218107\n",
      "Epoch 17, test loss: 0.218267\n",
      "Epoch 18, test loss: 0.217401\n",
      "Epoch 19, test loss: 0.218761\n",
      "Epoch 20, test loss: 0.220957\n",
      "Epoch 21, test loss: 0.219629\n",
      "Epoch 22, test loss: 0.219478\n",
      "Epoch 23, test loss: 0.220382\n",
      "Epoch 24, test loss: 0.219762\n",
      "Epoch 25, test loss: 0.218722\n",
      "Epoch 26, test loss: 0.218539\n",
      "Epoch 27, test loss: 0.218378\n",
      "Epoch 28, test loss: 0.221219\n",
      "Epoch 29, test loss: 0.218378\n",
      "Epoch 30, test loss: 0.218287\n",
      "Epoch 31, test loss: 0.218201\n",
      "Epoch 32, test loss: 0.218683\n",
      "Epoch 33, test loss: 0.219970\n",
      "Epoch 34, test loss: 0.220620\n",
      "Epoch 35, test loss: 0.218917\n",
      "Epoch 36, test loss: 0.219147\n",
      "Epoch 37, test loss: 0.219205\n",
      "Epoch 38, test loss: 0.219294\n",
      "Epoch 39, test loss: 0.218138\n",
      "Epoch 40, test loss: 0.218479\n",
      "Epoch 41, test loss: 0.219521\n",
      "Epoch 42, test loss: 0.221021\n",
      "Epoch 43, test loss: 0.218990\n",
      "Epoch 44, test loss: 0.218457\n",
      "Epoch 45, test loss: 0.219220\n",
      "Epoch 46, test loss: 0.218374\n",
      "Epoch 47, test loss: 0.218094\n",
      "Epoch 48, test loss: 0.225162\n",
      "Epoch 49, test loss: 0.218171\n",
      "Epoch 50, test loss: 0.219185\n",
      "Epoch 51, test loss: 0.219143\n",
      "Epoch 52, test loss: 0.219973\n",
      "Epoch 53, test loss: 0.218920\n",
      "Epoch 54, test loss: 0.218964\n",
      "Epoch 55, test loss: 0.218487\n",
      "Epoch 56, test loss: 0.220541\n",
      "Epoch 57, test loss: 0.218165\n",
      "Epoch 58, test loss: 0.218305\n",
      "Epoch 59, test loss: 0.218910\n",
      "Epoch 60, test loss: 0.218634\n",
      "Epoch 61, test loss: 0.220104\n",
      "Epoch 62, test loss: 0.218229\n",
      "Epoch 63, test loss: 0.218875\n",
      "Epoch 64, test loss: 0.218639\n",
      "Epoch 65, test loss: 0.217931\n",
      "Epoch 66, test loss: 0.218081\n",
      "Epoch 67, test loss: 0.218236\n",
      "Epoch 68, test loss: 0.222372\n",
      "Epoch 69, test loss: 0.218940\n",
      "Epoch 70, test loss: 0.218459\n",
      "Epoch 71, test loss: 0.218409\n",
      "Epoch 72, test loss: 0.220954\n",
      "Epoch 73, test loss: 0.220182\n",
      "Epoch 74, test loss: 0.218041\n",
      "Epoch 75, test loss: 0.218938\n",
      "Epoch 76, test loss: 0.221408\n",
      "Epoch 77, test loss: 0.218548\n",
      "Epoch 78, test loss: 0.218813\n",
      "Epoch 79, test loss: 0.221067\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2952A3A30>\n",
      "Epoch 0, test loss: 0.279325\n",
      "Epoch 1, test loss: 0.230745\n",
      "Epoch 2, test loss: 0.225333\n",
      "Epoch 3, test loss: 0.223294\n",
      "Epoch 4, test loss: 0.220977\n",
      "Epoch 5, test loss: 0.220802\n",
      "Epoch 6, test loss: 0.222308\n",
      "Epoch 7, test loss: 0.221106\n",
      "Epoch 8, test loss: 0.219677\n",
      "Epoch 9, test loss: 0.220462\n",
      "Epoch 10, test loss: 0.221781\n",
      "Epoch 11, test loss: 0.220920\n",
      "Epoch 12, test loss: 0.223247\n",
      "Epoch 13, test loss: 0.220643\n",
      "Epoch 14, test loss: 0.219323\n",
      "Epoch 15, test loss: 0.220075\n",
      "Epoch 16, test loss: 0.219459\n",
      "Epoch 17, test loss: 0.219364\n",
      "Epoch 18, test loss: 0.219606\n",
      "Epoch 19, test loss: 0.218427\n",
      "Epoch 20, test loss: 0.219296\n",
      "Epoch 21, test loss: 0.220330\n",
      "Epoch 22, test loss: 0.218905\n",
      "Epoch 23, test loss: 0.219281\n",
      "Epoch 24, test loss: 0.219382\n",
      "Epoch 25, test loss: 0.219711\n",
      "Epoch 26, test loss: 0.219285\n",
      "Epoch 27, test loss: 0.218868\n",
      "Epoch 28, test loss: 0.228832\n",
      "Epoch 29, test loss: 0.219605\n",
      "Epoch 30, test loss: 0.222412\n",
      "Epoch 31, test loss: 0.219483\n",
      "Epoch 32, test loss: 0.223483\n",
      "Epoch 33, test loss: 0.218824\n",
      "Epoch 34, test loss: 0.219487\n",
      "Epoch 35, test loss: 0.218967\n",
      "Epoch 36, test loss: 0.218724\n",
      "Epoch 37, test loss: 0.220649\n",
      "Epoch 38, test loss: 0.218423\n",
      "Epoch 39, test loss: 0.219491\n",
      "Epoch 40, test loss: 0.218713\n",
      "Epoch 41, test loss: 0.221267\n",
      "Epoch 42, test loss: 0.219266\n",
      "Epoch 43, test loss: 0.219397\n",
      "Epoch 44, test loss: 0.218644\n",
      "Epoch 45, test loss: 0.220235\n",
      "Epoch 46, test loss: 0.221131\n",
      "Epoch 47, test loss: 0.219263\n",
      "Epoch 48, test loss: 0.221749\n",
      "Epoch 49, test loss: 0.222333\n",
      "Epoch 50, test loss: 0.220354\n",
      "Epoch 51, test loss: 0.219274\n",
      "Epoch 52, test loss: 0.219363\n",
      "Epoch 53, test loss: 0.218629\n",
      "Epoch 54, test loss: 0.218322\n",
      "Epoch 55, test loss: 0.218339\n",
      "Epoch 56, test loss: 0.218745\n",
      "Epoch 57, test loss: 0.218872\n",
      "Epoch 58, test loss: 0.219018\n",
      "Epoch 59, test loss: 0.219137\n",
      "Epoch 60, test loss: 0.220835\n",
      "Epoch 61, test loss: 0.218895\n",
      "Epoch 62, test loss: 0.219267\n",
      "Epoch 63, test loss: 0.218631\n",
      "Epoch 64, test loss: 0.220068\n",
      "Epoch 65, test loss: 0.220962\n",
      "Epoch 66, test loss: 0.222919\n",
      "Epoch 67, test loss: 0.219221\n",
      "Epoch 68, test loss: 0.223650\n",
      "Epoch 69, test loss: 0.219872\n",
      "Epoch 70, test loss: 0.219682\n",
      "Epoch 71, test loss: 0.219080\n",
      "Epoch 72, test loss: 0.223210\n",
      "Epoch 73, test loss: 0.218552\n",
      "Epoch 74, test loss: 0.222239\n",
      "Epoch 75, test loss: 0.219299\n",
      "Epoch 76, test loss: 0.223126\n",
      "Epoch 77, test loss: 0.219420\n",
      "Epoch 78, test loss: 0.225135\n",
      "Epoch 79, test loss: 0.219073\n",
      "Epoch 80, test loss: 0.218581\n",
      "Epoch 81, test loss: 0.218533\n",
      "Epoch 82, test loss: 0.221647\n",
      "Epoch 83, test loss: 0.220525\n",
      "Epoch 84, test loss: 0.218784\n",
      "Epoch 85, test loss: 0.218595\n",
      "Epoch 86, test loss: 0.220182\n",
      "Epoch 87, test loss: 0.221337\n",
      "Epoch 88, test loss: 0.220268\n",
      "Epoch 89, test loss: 0.218831\n",
      "Epoch 90, test loss: 0.223126\n",
      "Epoch 91, test loss: 0.220672\n",
      "Epoch 92, test loss: 0.221019\n",
      "Epoch 93, test loss: 0.218477\n",
      "Epoch 94, test loss: 0.218911\n",
      "Epoch 95, test loss: 0.219506\n",
      "Epoch 96, test loss: 0.219668\n",
      "Epoch 97, test loss: 0.217962\n",
      "Epoch 98, test loss: 0.219213\n",
      "Epoch 99, test loss: 0.220279\n",
      "Epoch 100, test loss: 0.219409\n",
      "Epoch 101, test loss: 0.219380\n",
      "Epoch 102, test loss: 0.220224\n",
      "Epoch 103, test loss: 0.219082\n",
      "Epoch 104, test loss: 0.218897\n",
      "Epoch 105, test loss: 0.220680\n",
      "Epoch 106, test loss: 0.220490\n",
      "Epoch 107, test loss: 0.220463\n",
      "Epoch 108, test loss: 0.221949\n",
      "Epoch 109, test loss: 0.218823\n",
      "Epoch 110, test loss: 0.220245\n",
      "Epoch 111, test loss: 0.220120\n",
      "Epoch 112, test loss: 0.219490\n",
      "Epoch 113, test loss: 0.219912\n",
      "Epoch 114, test loss: 0.219275\n",
      "Epoch 115, test loss: 0.220494\n",
      "Epoch 116, test loss: 0.222611\n",
      "Epoch 117, test loss: 0.218707\n",
      "Epoch 118, test loss: 0.219799\n",
      "Epoch 119, test loss: 0.219856\n",
      "Epoch 120, test loss: 0.220034\n",
      "Epoch 121, test loss: 0.220277\n",
      "Epoch 122, test loss: 0.221217\n",
      "Epoch 123, test loss: 0.220261\n",
      "Epoch 124, test loss: 0.220153\n",
      "Epoch 125, test loss: 0.221891\n",
      "Epoch 126, test loss: 0.221825\n",
      "Epoch 127, test loss: 0.219459\n",
      "Epoch 128, test loss: 0.219090\n",
      "Epoch 129, test loss: 0.219667\n",
      "Epoch 130, test loss: 0.219199\n",
      "Epoch 131, test loss: 0.219472\n",
      "Epoch 132, test loss: 0.219933\n",
      "Epoch 133, test loss: 0.220058\n",
      "Epoch 134, test loss: 0.219777\n",
      "Epoch 135, test loss: 0.222672\n",
      "Epoch 136, test loss: 0.218819\n",
      "Epoch 137, test loss: 0.219104\n",
      "Epoch 138, test loss: 0.219337\n",
      "Epoch 139, test loss: 0.219662\n",
      "Epoch 140, test loss: 0.221200\n",
      "Epoch 141, test loss: 0.220845\n",
      "Epoch 142, test loss: 0.219690\n",
      "Epoch 143, test loss: 0.220553\n",
      "Epoch 144, test loss: 0.218898\n",
      "Epoch 145, test loss: 0.218950\n",
      "Epoch 146, test loss: 0.218894\n",
      "Epoch 147, test loss: 0.219060\n",
      "Epoch 148, test loss: 0.219219\n",
      "Epoch 149, test loss: 0.219020\n",
      "Epoch 150, test loss: 0.224334\n",
      "Epoch 151, test loss: 0.218969\n",
      "Epoch 152, test loss: 0.219762\n",
      "Epoch 153, test loss: 0.225075\n",
      "Epoch 154, test loss: 0.221483\n",
      "Epoch 155, test loss: 0.220122\n",
      "Epoch 156, test loss: 0.218791\n",
      "Epoch 157, test loss: 0.222889\n",
      "Epoch 158, test loss: 0.218662\n",
      "Epoch 159, test loss: 0.218791\n",
      "Epoch 160, test loss: 0.219478\n",
      "Epoch 161, test loss: 0.219783\n",
      "Epoch 162, test loss: 0.220094\n",
      "Epoch 163, test loss: 0.220008\n",
      "Epoch 164, test loss: 0.225873\n",
      "Epoch 165, test loss: 0.220308\n",
      "Epoch 166, test loss: 0.220789\n",
      "Epoch 167, test loss: 0.220865\n",
      "Epoch 168, test loss: 0.219179\n",
      "Epoch 169, test loss: 0.220956\n",
      "Epoch 170, test loss: 0.219263\n",
      "Epoch 171, test loss: 0.219465\n",
      "Epoch 172, test loss: 0.219364\n",
      "Epoch 173, test loss: 0.227440\n",
      "Epoch 174, test loss: 0.219732\n",
      "Epoch 175, test loss: 0.219646\n",
      "Epoch 176, test loss: 0.219902\n",
      "Epoch 177, test loss: 0.219399\n",
      "Epoch 178, test loss: 0.223062\n",
      "Epoch 179, test loss: 0.220096\n",
      "Epoch 180, test loss: 0.218749\n",
      "Epoch 181, test loss: 0.227901\n",
      "Epoch 182, test loss: 0.218458\n",
      "Epoch 183, test loss: 0.219249\n",
      "Epoch 184, test loss: 0.222845\n",
      "Epoch 185, test loss: 0.219225\n",
      "Epoch 186, test loss: 0.225045\n",
      "Epoch 187, test loss: 0.218949\n",
      "Epoch 188, test loss: 0.218695\n",
      "Epoch 189, test loss: 0.219891\n",
      "Epoch 190, test loss: 0.220494\n",
      "Epoch 191, test loss: 0.221406\n",
      "Epoch 192, test loss: 0.220581\n",
      "Epoch 193, test loss: 0.219881\n",
      "Epoch 194, test loss: 0.219131\n",
      "Epoch 195, test loss: 0.223523\n",
      "Epoch 196, test loss: 0.218730\n",
      "Epoch 197, test loss: 0.220519\n",
      "Epoch 198, test loss: 0.220876\n",
      "Epoch 199, test loss: 0.219483\n",
      "Epoch 200, test loss: 0.219717\n",
      "Epoch 201, test loss: 0.222593\n",
      "Epoch 202, test loss: 0.219098\n",
      "Epoch 203, test loss: 0.221023\n",
      "Epoch 204, test loss: 0.219107\n",
      "Epoch 205, test loss: 0.219030\n",
      "Epoch 206, test loss: 0.219358\n",
      "Epoch 207, test loss: 0.219139\n",
      "Epoch 208, test loss: 0.219247\n",
      "Epoch 209, test loss: 0.220057\n",
      "Epoch 210, test loss: 0.220165\n",
      "Epoch 211, test loss: 0.218713\n",
      "Epoch 212, test loss: 0.219359\n",
      "Epoch 213, test loss: 0.218498\n",
      "Epoch 214, test loss: 0.219213\n",
      "Epoch 215, test loss: 0.220000\n",
      "Epoch 216, test loss: 0.219959\n",
      "Epoch 217, test loss: 0.219357\n",
      "Epoch 218, test loss: 0.218513\n",
      "Epoch 219, test loss: 0.227428\n",
      "Epoch 220, test loss: 0.219448\n",
      "Epoch 221, test loss: 0.219510\n",
      "Epoch 222, test loss: 0.221097\n",
      "Epoch 223, test loss: 0.219538\n",
      "Epoch 224, test loss: 0.220741\n",
      "Epoch 225, test loss: 0.220435\n",
      "Epoch 226, test loss: 0.221857\n",
      "Epoch 227, test loss: 0.220435\n",
      "Epoch 228, test loss: 0.219045\n",
      "Epoch 229, test loss: 0.219087\n",
      "Epoch 230, test loss: 0.220298\n",
      "Epoch 231, test loss: 0.223041\n",
      "Epoch 232, test loss: 0.219977\n",
      "Epoch 233, test loss: 0.218904\n",
      "Epoch 234, test loss: 0.219416\n",
      "Epoch 235, test loss: 0.219923\n",
      "Epoch 236, test loss: 0.224191\n",
      "Epoch 237, test loss: 0.219578\n",
      "Epoch 238, test loss: 0.219299\n",
      "Epoch 239, test loss: 0.219400\n",
      "Pretrain data: 19823659.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 147. 150. 153.]\n",
      " [128. 129. 131. ... 150. 153. 155.]\n",
      " [129. 131. 133. ... 153. 155. 157.]\n",
      " ...\n",
      " [310. 315. 319. ... 248. 244. 243.]\n",
      " [315. 319. 318. ... 244. 243. 244.]\n",
      " [319. 318. 316. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[160. 160. 160. ... 160. 160. 160.]\n",
      " [160. 160. 160. ... 160. 160. 160.]\n",
      " [162. 162. 162. ... 162. 162. 162.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6997/98988\n",
      "Found 815 continuous time series\n",
      "Data shape: (105987, 18), Train/test: 105985/2\n",
      "Train test ratio: 52992.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 18), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 18), dtype=float32)\n",
      "line73: Shape of y: (None, 18)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1656\n",
      "Epoch 0, train loss: 0.232744\n",
      "Epoch 1, train loss: 0.215533\n",
      "Epoch 2, train loss: 0.204997\n",
      "Epoch 3, train loss: 0.212930\n",
      "Epoch 4, train loss: 0.356519\n",
      "Epoch 5, train loss: 0.264141\n",
      "Epoch 6, train loss: 0.185299\n",
      "Epoch 7, train loss: 0.217736\n",
      "Epoch 8, train loss: 0.257933\n",
      "Epoch 9, train loss: 0.217329\n",
      "Epoch 10, train loss: 0.200635\n",
      "Epoch 11, train loss: 0.193962\n",
      "Epoch 12, train loss: 0.240577\n",
      "Epoch 13, train loss: 0.241266\n",
      "Epoch 14, train loss: 0.229723\n",
      "Epoch 15, train loss: 0.231490\n",
      "Epoch 16, train loss: 0.222155\n",
      "Epoch 17, train loss: 0.205419\n",
      "Epoch 18, train loss: 0.230814\n",
      "Epoch 19, train loss: 0.211809\n",
      "Epoch 20, train loss: 0.213219\n",
      "Epoch 21, train loss: 0.171775\n",
      "Epoch 22, train loss: 0.169998\n",
      "Epoch 23, train loss: 0.231108\n",
      "Epoch 24, train loss: 0.176178\n",
      "Epoch 25, train loss: 0.243793\n",
      "Epoch 26, train loss: 0.177609\n",
      "Epoch 27, train loss: 0.191634\n",
      "Epoch 28, train loss: 0.248199\n",
      "Epoch 29, train loss: 0.180294\n",
      "Epoch 30, train loss: 0.170040\n",
      "Epoch 31, train loss: 0.200871\n",
      "Epoch 32, train loss: 0.219927\n",
      "Epoch 33, train loss: 0.222310\n",
      "Epoch 34, train loss: 0.233256\n",
      "Epoch 35, train loss: 0.193273\n",
      "Epoch 36, train loss: 0.257144\n",
      "Epoch 37, train loss: 0.278993\n",
      "Epoch 38, train loss: 0.177947\n",
      "Epoch 39, train loss: 0.223390\n",
      "Epoch 40, train loss: 0.218806\n",
      "Epoch 41, train loss: 0.220733\n",
      "Epoch 42, train loss: 0.211690\n",
      "Epoch 43, train loss: 0.180155\n",
      "Epoch 44, train loss: 0.217337\n",
      "Epoch 45, train loss: 0.221731\n",
      "Epoch 46, train loss: 0.200311\n",
      "Epoch 47, train loss: 0.200839\n",
      "Epoch 48, train loss: 0.244312\n",
      "Epoch 49, train loss: 0.198698\n",
      "Epoch 50, train loss: 0.177883\n",
      "Epoch 51, train loss: 0.164358\n",
      "Epoch 52, train loss: 0.243448\n",
      "Epoch 53, train loss: 0.249585\n",
      "Epoch 54, train loss: 0.203022\n",
      "Epoch 55, train loss: 0.190793\n",
      "Epoch 56, train loss: 0.199802\n",
      "Epoch 57, train loss: 0.174740\n",
      "Epoch 58, train loss: 0.220484\n",
      "Epoch 59, train loss: 0.159178\n",
      "Epoch 60, train loss: 0.240187\n",
      "Epoch 61, train loss: 0.217269\n",
      "Epoch 62, train loss: 0.217054\n",
      "Epoch 63, train loss: 0.263212\n",
      "Epoch 64, train loss: 0.205149\n",
      "Epoch 65, train loss: 0.204927\n",
      "Epoch 66, train loss: 0.229318\n",
      "Epoch 67, train loss: 0.251895\n",
      "Epoch 68, train loss: 0.178816\n",
      "Epoch 69, train loss: 0.186019\n",
      "Epoch 70, train loss: 0.175406\n",
      "Epoch 71, train loss: 0.213779\n",
      "Epoch 72, train loss: 0.224725\n",
      "Epoch 73, train loss: 0.183201\n",
      "Epoch 74, train loss: 0.205891\n",
      "Epoch 75, train loss: 0.249779\n",
      "Epoch 76, train loss: 0.187586\n",
      "Epoch 77, train loss: 0.196379\n",
      "Epoch 78, train loss: 0.177706\n",
      "Epoch 79, train loss: 0.171644\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[174. 173. 173. ... 163. 163. 163.]\n",
      " [173. 173. 172. ... 163. 163. 163.]\n",
      " [173. 172. 170. ... 163. 163. 163.]\n",
      " ...\n",
      " [133. 132. 133. ...  92.  88.  85.]\n",
      " [132. 133. 132. ...  88.  85.  84.]\n",
      " [133. 132. 132. ...  85.  84.  86.]]\n",
      "y here is\n",
      "[[165. 165. 165. ... 165. 165. 165.]\n",
      " [166. 166. 166. ... 166. 166. 166.]\n",
      " [166. 166. 166. ... 166. 166. 166.]\n",
      " ...\n",
      " [ 90.  90.  90. ...  90.  90.  90.]\n",
      " [ 86.  86.  86. ...  86.  86.  86.]\n",
      " [ 87.  87.  87. ...  87.  87.  87.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2582, 18), Train/test: 1/2581\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[142. 142. 142. ... 125. 126. 124.]\n",
      " [142. 142. 141. ... 126. 124. 123.]\n",
      " [142. 141. 139. ... 124. 123. 124.]\n",
      " ...\n",
      " [147. 150. 151. ... 163. 166. 168.]\n",
      " [150. 151. 152. ... 166. 168. 167.]\n",
      " [151. 152. 153. ... 168. 167. 168.]]\n",
      "y here is\n",
      "[[127. 127. 127. ... 127. 127. 127.]\n",
      " [130. 130. 130. ... 130. 130. 130.]\n",
      " [134. 134. 134. ... 134. 134. 134.]\n",
      " ...\n",
      " [173. 173. 173. ... 173. 173. 173.]\n",
      " [174. 174. 174. ... 174. 174. 174.]\n",
      " [174. 174. 174. ... 174. 174. 174.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 520/9766\n",
      "Found 27 continuous time series\n",
      "Data shape: (10288, 18), Train/test: 10286/2\n",
      "Train test ratio: 5143.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293543880>\n",
      "Epoch 0, test loss: 0.171705\n",
      "Epoch 1, test loss: 0.170855\n",
      "Epoch 2, test loss: 0.171413\n",
      "Epoch 3, test loss: 0.171018\n",
      "Epoch 4, test loss: 0.171058\n",
      "Epoch 5, test loss: 0.171869\n",
      "Epoch 6, test loss: 0.171694\n",
      "Epoch 7, test loss: 0.174797\n",
      "Epoch 8, test loss: 0.171468\n",
      "Epoch 9, test loss: 0.174373\n",
      "Epoch 10, test loss: 0.170323\n",
      "Epoch 11, test loss: 0.170548\n",
      "Epoch 12, test loss: 0.176824\n",
      "Epoch 13, test loss: 0.170269\n",
      "Epoch 14, test loss: 0.170959\n",
      "Epoch 15, test loss: 0.171646\n",
      "Epoch 16, test loss: 0.170348\n",
      "Epoch 17, test loss: 0.172801\n",
      "Epoch 18, test loss: 0.170670\n",
      "Epoch 19, test loss: 0.179391\n",
      "Epoch 20, test loss: 0.170687\n",
      "Epoch 21, test loss: 0.171486\n",
      "Epoch 22, test loss: 0.172433\n",
      "Epoch 23, test loss: 0.171721\n",
      "Epoch 24, test loss: 0.172473\n",
      "Epoch 25, test loss: 0.178064\n",
      "Epoch 26, test loss: 0.171163\n",
      "Epoch 27, test loss: 0.170208\n",
      "Epoch 28, test loss: 0.177232\n",
      "Epoch 29, test loss: 0.172089\n",
      "Epoch 30, test loss: 0.173128\n",
      "Epoch 31, test loss: 0.171318\n",
      "Epoch 32, test loss: 0.172840\n",
      "Epoch 33, test loss: 0.171316\n",
      "Epoch 34, test loss: 0.171162\n",
      "Epoch 35, test loss: 0.172040\n",
      "Epoch 36, test loss: 0.173283\n",
      "Epoch 37, test loss: 0.176162\n",
      "Epoch 38, test loss: 0.172518\n",
      "Epoch 39, test loss: 0.177103\n",
      "Epoch 40, test loss: 0.170810\n",
      "Epoch 41, test loss: 0.173026\n",
      "Epoch 42, test loss: 0.172674\n",
      "Epoch 43, test loss: 0.173496\n",
      "Epoch 44, test loss: 0.170698\n",
      "Epoch 45, test loss: 0.175927\n",
      "Epoch 46, test loss: 0.171122\n",
      "Epoch 47, test loss: 0.172730\n",
      "Epoch 48, test loss: 0.170800\n",
      "Epoch 49, test loss: 0.171585\n",
      "Epoch 50, test loss: 0.171239\n",
      "Epoch 51, test loss: 0.171115\n",
      "Epoch 52, test loss: 0.171503\n",
      "Epoch 53, test loss: 0.173819\n",
      "Epoch 54, test loss: 0.171583\n",
      "Epoch 55, test loss: 0.170368\n",
      "Epoch 56, test loss: 0.173711\n",
      "Epoch 57, test loss: 0.171606\n",
      "Epoch 58, test loss: 0.171252\n",
      "Epoch 59, test loss: 0.170952\n",
      "Epoch 60, test loss: 0.171785\n",
      "Epoch 61, test loss: 0.171778\n",
      "Epoch 62, test loss: 0.171432\n",
      "Epoch 63, test loss: 0.175879\n",
      "Epoch 64, test loss: 0.171903\n",
      "Epoch 65, test loss: 0.170133\n",
      "Epoch 66, test loss: 0.171712\n",
      "Epoch 67, test loss: 0.170847\n",
      "Epoch 68, test loss: 0.171869\n",
      "Epoch 69, test loss: 0.170929\n",
      "Epoch 70, test loss: 0.171144\n",
      "Epoch 71, test loss: 0.173319\n",
      "Epoch 72, test loss: 0.172809\n",
      "Epoch 73, test loss: 0.171052\n",
      "Epoch 74, test loss: 0.171033\n",
      "Epoch 75, test loss: 0.174677\n",
      "Epoch 76, test loss: 0.171913\n",
      "Epoch 77, test loss: 0.178403\n",
      "Epoch 78, test loss: 0.171012\n",
      "Epoch 79, test loss: 0.170909\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293543880>\n",
      "Epoch 0, test loss: 0.172173\n",
      "Epoch 1, test loss: 0.171864\n",
      "Epoch 2, test loss: 0.171023\n",
      "Epoch 3, test loss: 0.172606\n",
      "Epoch 4, test loss: 0.172333\n",
      "Epoch 5, test loss: 0.171050\n",
      "Epoch 6, test loss: 0.173163\n",
      "Epoch 7, test loss: 0.171427\n",
      "Epoch 8, test loss: 0.171068\n",
      "Epoch 9, test loss: 0.170136\n",
      "Epoch 10, test loss: 0.175020\n",
      "Epoch 11, test loss: 0.171369\n",
      "Epoch 12, test loss: 0.172261\n",
      "Epoch 13, test loss: 0.170783\n",
      "Epoch 14, test loss: 0.170606\n",
      "Epoch 15, test loss: 0.172338\n",
      "Epoch 16, test loss: 0.170592\n",
      "Epoch 17, test loss: 0.170187\n",
      "Epoch 18, test loss: 0.174600\n",
      "Epoch 19, test loss: 0.170763\n",
      "Epoch 20, test loss: 0.170478\n",
      "Epoch 21, test loss: 0.170068\n",
      "Epoch 22, test loss: 0.170549\n",
      "Epoch 23, test loss: 0.171720\n",
      "Epoch 24, test loss: 0.173007\n",
      "Epoch 25, test loss: 0.170903\n",
      "Epoch 26, test loss: 0.170628\n",
      "Epoch 27, test loss: 0.170550\n",
      "Epoch 28, test loss: 0.171206\n",
      "Epoch 29, test loss: 0.171724\n",
      "Epoch 30, test loss: 0.170771\n",
      "Epoch 31, test loss: 0.170745\n",
      "Epoch 32, test loss: 0.170721\n",
      "Epoch 33, test loss: 0.172749\n",
      "Epoch 34, test loss: 0.171193\n",
      "Epoch 35, test loss: 0.170104\n",
      "Epoch 36, test loss: 0.170873\n",
      "Epoch 37, test loss: 0.170852\n",
      "Epoch 38, test loss: 0.171701\n",
      "Epoch 39, test loss: 0.172375\n",
      "Epoch 40, test loss: 0.171741\n",
      "Epoch 41, test loss: 0.170391\n",
      "Epoch 42, test loss: 0.171372\n",
      "Epoch 43, test loss: 0.170931\n",
      "Epoch 44, test loss: 0.170655\n",
      "Epoch 45, test loss: 0.171309\n",
      "Epoch 46, test loss: 0.175377\n",
      "Epoch 47, test loss: 0.170379\n",
      "Epoch 48, test loss: 0.170900\n",
      "Epoch 49, test loss: 0.170600\n",
      "Epoch 50, test loss: 0.170985\n",
      "Epoch 51, test loss: 0.171300\n",
      "Epoch 52, test loss: 0.170537\n",
      "Epoch 53, test loss: 0.172252\n",
      "Epoch 54, test loss: 0.170318\n",
      "Epoch 55, test loss: 0.170837\n",
      "Epoch 56, test loss: 0.170705\n",
      "Epoch 57, test loss: 0.170848\n",
      "Epoch 58, test loss: 0.170930\n",
      "Epoch 59, test loss: 0.170873\n",
      "Epoch 60, test loss: 0.170894\n",
      "Epoch 61, test loss: 0.171521\n",
      "Epoch 62, test loss: 0.170969\n",
      "Epoch 63, test loss: 0.170543\n",
      "Epoch 64, test loss: 0.171566\n",
      "Epoch 65, test loss: 0.171331\n",
      "Epoch 66, test loss: 0.172651\n",
      "Epoch 67, test loss: 0.171634\n",
      "Epoch 68, test loss: 0.171789\n",
      "Epoch 69, test loss: 0.171796\n",
      "Epoch 70, test loss: 0.171397\n",
      "Epoch 71, test loss: 0.171545\n",
      "Epoch 72, test loss: 0.172528\n",
      "Epoch 73, test loss: 0.177728\n",
      "Epoch 74, test loss: 0.172682\n",
      "Epoch 75, test loss: 0.170730\n",
      "Epoch 76, test loss: 0.175382\n",
      "Epoch 77, test loss: 0.171670\n",
      "Epoch 78, test loss: 0.171520\n",
      "Epoch 79, test loss: 0.171543\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh18_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A293543880>\n",
      "Epoch 0, test loss: 0.252677\n",
      "Epoch 1, test loss: 0.202978\n",
      "Epoch 2, test loss: 0.182893\n",
      "Epoch 3, test loss: 0.178742\n",
      "Epoch 4, test loss: 0.177831\n",
      "Epoch 5, test loss: 0.179742\n",
      "Epoch 6, test loss: 0.176978\n",
      "Epoch 7, test loss: 0.178559\n",
      "Epoch 8, test loss: 0.181122\n",
      "Epoch 9, test loss: 0.176518\n",
      "Epoch 10, test loss: 0.176664\n",
      "Epoch 11, test loss: 0.177319\n",
      "Epoch 12, test loss: 0.179342\n",
      "Epoch 13, test loss: 0.175567\n",
      "Epoch 14, test loss: 0.179481\n",
      "Epoch 15, test loss: 0.176383\n",
      "Epoch 16, test loss: 0.176323\n",
      "Epoch 17, test loss: 0.175471\n",
      "Epoch 18, test loss: 0.178715\n",
      "Epoch 19, test loss: 0.176595\n",
      "Epoch 20, test loss: 0.176095\n",
      "Epoch 21, test loss: 0.176459\n",
      "Epoch 22, test loss: 0.175726\n",
      "Epoch 23, test loss: 0.176968\n",
      "Epoch 24, test loss: 0.175676\n",
      "Epoch 25, test loss: 0.175428\n",
      "Epoch 26, test loss: 0.177314\n",
      "Epoch 27, test loss: 0.176300\n",
      "Epoch 28, test loss: 0.177441\n",
      "Epoch 29, test loss: 0.175627\n",
      "Epoch 30, test loss: 0.175325\n",
      "Epoch 31, test loss: 0.180026\n",
      "Epoch 32, test loss: 0.175531\n",
      "Epoch 33, test loss: 0.176457\n",
      "Epoch 34, test loss: 0.180835\n",
      "Epoch 35, test loss: 0.175508\n",
      "Epoch 36, test loss: 0.177043\n",
      "Epoch 37, test loss: 0.177305\n",
      "Epoch 38, test loss: 0.176185\n",
      "Epoch 39, test loss: 0.180046\n",
      "Epoch 40, test loss: 0.175863\n",
      "Epoch 41, test loss: 0.178309\n",
      "Epoch 42, test loss: 0.177293\n",
      "Epoch 43, test loss: 0.176391\n",
      "Epoch 44, test loss: 0.175331\n",
      "Epoch 45, test loss: 0.175849\n",
      "Epoch 46, test loss: 0.176298\n",
      "Epoch 47, test loss: 0.175236\n",
      "Epoch 48, test loss: 0.175623\n",
      "Epoch 49, test loss: 0.178583\n",
      "Epoch 50, test loss: 0.176607\n",
      "Epoch 51, test loss: 0.176516\n",
      "Epoch 52, test loss: 0.175250\n",
      "Epoch 53, test loss: 0.174788\n",
      "Epoch 54, test loss: 0.175841\n",
      "Epoch 55, test loss: 0.175760\n",
      "Epoch 56, test loss: 0.176317\n",
      "Epoch 57, test loss: 0.174954\n",
      "Epoch 58, test loss: 0.175705\n",
      "Epoch 59, test loss: 0.174998\n",
      "Epoch 60, test loss: 0.176433\n",
      "Epoch 61, test loss: 0.177760\n",
      "Epoch 62, test loss: 0.175314\n",
      "Epoch 63, test loss: 0.177464\n",
      "Epoch 64, test loss: 0.175643\n",
      "Epoch 65, test loss: 0.175405\n",
      "Epoch 66, test loss: 0.174978\n",
      "Epoch 67, test loss: 0.177590\n",
      "Epoch 68, test loss: 0.179363\n",
      "Epoch 69, test loss: 0.175951\n",
      "Epoch 70, test loss: 0.175905\n",
      "Epoch 71, test loss: 0.176119\n",
      "Epoch 72, test loss: 0.178124\n",
      "Epoch 73, test loss: 0.176141\n",
      "Epoch 74, test loss: 0.176791\n",
      "Epoch 75, test loss: 0.176651\n",
      "Epoch 76, test loss: 0.179984\n",
      "Epoch 77, test loss: 0.175468\n",
      "Epoch 78, test loss: 0.177072\n",
      "Epoch 79, test loss: 0.175594\n",
      "Epoch 80, test loss: 0.176476\n",
      "Epoch 81, test loss: 0.176189\n",
      "Epoch 82, test loss: 0.176454\n",
      "Epoch 83, test loss: 0.176082\n",
      "Epoch 84, test loss: 0.175848\n",
      "Epoch 85, test loss: 0.176064\n",
      "Epoch 86, test loss: 0.175672\n",
      "Epoch 87, test loss: 0.176730\n",
      "Epoch 88, test loss: 0.176190\n",
      "Epoch 89, test loss: 0.175328\n",
      "Epoch 90, test loss: 0.175609\n",
      "Epoch 91, test loss: 0.177745\n",
      "Epoch 92, test loss: 0.175821\n",
      "Epoch 93, test loss: 0.176135\n",
      "Epoch 94, test loss: 0.177876\n",
      "Epoch 95, test loss: 0.176334\n",
      "Epoch 96, test loss: 0.176079\n",
      "Epoch 97, test loss: 0.175684\n",
      "Epoch 98, test loss: 0.176190\n",
      "Epoch 99, test loss: 0.177284\n",
      "Epoch 100, test loss: 0.176009\n",
      "Epoch 101, test loss: 0.177354\n",
      "Epoch 102, test loss: 0.176691\n",
      "Epoch 103, test loss: 0.178006\n",
      "Epoch 104, test loss: 0.176590\n",
      "Epoch 105, test loss: 0.176624\n",
      "Epoch 106, test loss: 0.178467\n",
      "Epoch 107, test loss: 0.183728\n",
      "Epoch 108, test loss: 0.175784\n",
      "Epoch 109, test loss: 0.175498\n",
      "Epoch 110, test loss: 0.180274\n",
      "Epoch 111, test loss: 0.175615\n",
      "Epoch 112, test loss: 0.175413\n",
      "Epoch 113, test loss: 0.176484\n",
      "Epoch 114, test loss: 0.176273\n",
      "Epoch 115, test loss: 0.175515\n",
      "Epoch 116, test loss: 0.175568\n",
      "Epoch 117, test loss: 0.180161\n",
      "Epoch 118, test loss: 0.175954\n",
      "Epoch 119, test loss: 0.175591\n",
      "Epoch 120, test loss: 0.175595\n",
      "Epoch 121, test loss: 0.175869\n",
      "Epoch 122, test loss: 0.177405\n",
      "Epoch 123, test loss: 0.178826\n",
      "Epoch 124, test loss: 0.178354\n",
      "Epoch 125, test loss: 0.175818\n",
      "Epoch 126, test loss: 0.177973\n",
      "Epoch 127, test loss: 0.177239\n",
      "Epoch 128, test loss: 0.177468\n",
      "Epoch 129, test loss: 0.175896\n",
      "Epoch 130, test loss: 0.176654\n",
      "Epoch 131, test loss: 0.175620\n",
      "Epoch 132, test loss: 0.176698\n",
      "Epoch 133, test loss: 0.177547\n",
      "Epoch 134, test loss: 0.177191\n",
      "Epoch 135, test loss: 0.175708\n",
      "Epoch 136, test loss: 0.177274\n",
      "Epoch 137, test loss: 0.179488\n",
      "Epoch 138, test loss: 0.175955\n",
      "Epoch 139, test loss: 0.175588\n",
      "Epoch 140, test loss: 0.175450\n",
      "Epoch 141, test loss: 0.175535\n",
      "Epoch 142, test loss: 0.176438\n",
      "Epoch 143, test loss: 0.177020\n",
      "Epoch 144, test loss: 0.175240\n",
      "Epoch 145, test loss: 0.176920\n",
      "Epoch 146, test loss: 0.177074\n",
      "Epoch 147, test loss: 0.178335\n",
      "Epoch 148, test loss: 0.175601\n",
      "Epoch 149, test loss: 0.176519\n",
      "Epoch 150, test loss: 0.177810\n",
      "Epoch 151, test loss: 0.176889\n",
      "Epoch 152, test loss: 0.176385\n",
      "Epoch 153, test loss: 0.176339\n",
      "Epoch 154, test loss: 0.176776\n",
      "Epoch 155, test loss: 0.179912\n",
      "Epoch 156, test loss: 0.176332\n",
      "Epoch 157, test loss: 0.176401\n",
      "Epoch 158, test loss: 0.177980\n",
      "Epoch 159, test loss: 0.176904\n",
      "Epoch 160, test loss: 0.176871\n",
      "Epoch 161, test loss: 0.176613\n",
      "Epoch 162, test loss: 0.176077\n",
      "Epoch 163, test loss: 0.176244\n",
      "Epoch 164, test loss: 0.176212\n",
      "Epoch 165, test loss: 0.176834\n",
      "Epoch 166, test loss: 0.176829\n",
      "Epoch 167, test loss: 0.177025\n",
      "Epoch 168, test loss: 0.179118\n",
      "Epoch 169, test loss: 0.177228\n",
      "Epoch 170, test loss: 0.176332\n",
      "Epoch 171, test loss: 0.175929\n",
      "Epoch 172, test loss: 0.177541\n",
      "Epoch 173, test loss: 0.177134\n",
      "Epoch 174, test loss: 0.176620\n",
      "Epoch 175, test loss: 0.176928\n",
      "Epoch 176, test loss: 0.176746\n",
      "Epoch 177, test loss: 0.176855\n",
      "Epoch 178, test loss: 0.176681\n",
      "Epoch 179, test loss: 0.177681\n",
      "Epoch 180, test loss: 0.180472\n",
      "Epoch 181, test loss: 0.176244\n",
      "Epoch 182, test loss: 0.178309\n",
      "Epoch 183, test loss: 0.180248\n",
      "Epoch 184, test loss: 0.176974\n",
      "Epoch 185, test loss: 0.176047\n",
      "Epoch 186, test loss: 0.177001\n",
      "Epoch 187, test loss: 0.176406\n",
      "Epoch 188, test loss: 0.176283\n",
      "Epoch 189, test loss: 0.176818\n",
      "Epoch 190, test loss: 0.177996\n",
      "Epoch 191, test loss: 0.176197\n",
      "Epoch 192, test loss: 0.177189\n",
      "Epoch 193, test loss: 0.176778\n",
      "Epoch 194, test loss: 0.178367\n",
      "Epoch 195, test loss: 0.176876\n",
      "Epoch 196, test loss: 0.176594\n",
      "Epoch 197, test loss: 0.177503\n",
      "Epoch 198, test loss: 0.177108\n",
      "Epoch 199, test loss: 0.180086\n",
      "Epoch 200, test loss: 0.178009\n",
      "Epoch 201, test loss: 0.176818\n",
      "Epoch 202, test loss: 0.177042\n",
      "Epoch 203, test loss: 0.177754\n",
      "Epoch 204, test loss: 0.176739\n",
      "Epoch 205, test loss: 0.177080\n",
      "Epoch 206, test loss: 0.177652\n",
      "Epoch 207, test loss: 0.176363\n",
      "Epoch 208, test loss: 0.176677\n",
      "Epoch 209, test loss: 0.178626\n",
      "Epoch 210, test loss: 0.176108\n",
      "Epoch 211, test loss: 0.177660\n",
      "Epoch 212, test loss: 0.177073\n",
      "Epoch 213, test loss: 0.177910\n",
      "Epoch 214, test loss: 0.181595\n",
      "Epoch 215, test loss: 0.176664\n",
      "Epoch 216, test loss: 0.176585\n",
      "Epoch 217, test loss: 0.176330\n",
      "Epoch 218, test loss: 0.184344\n",
      "Epoch 219, test loss: 0.176155\n",
      "Epoch 220, test loss: 0.177368\n",
      "Epoch 221, test loss: 0.176548\n",
      "Epoch 222, test loss: 0.177136\n",
      "Epoch 223, test loss: 0.175641\n",
      "Epoch 224, test loss: 0.178264\n",
      "Epoch 225, test loss: 0.176909\n",
      "Epoch 226, test loss: 0.178931\n",
      "Epoch 227, test loss: 0.176419\n",
      "Epoch 228, test loss: 0.177338\n",
      "Epoch 229, test loss: 0.177586\n",
      "Epoch 230, test loss: 0.178879\n",
      "Epoch 231, test loss: 0.177437\n",
      "Epoch 232, test loss: 0.176648\n",
      "Epoch 233, test loss: 0.176879\n",
      "Epoch 234, test loss: 0.177485\n",
      "Epoch 235, test loss: 0.178288\n",
      "Epoch 236, test loss: 0.177376\n",
      "Epoch 237, test loss: 0.176046\n",
      "Epoch 238, test loss: 0.177336\n",
      "Epoch 239, test loss: 0.176664\n",
      "Reading 44 segments\n",
      "Pretrain data: 19625082.0\n",
      "Building dataset, requesting data from 0 to 798\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6359/95924\n",
      "Found 798 continuous time series\n",
      "Data shape: (102285, 24), Train/test: 102283/2\n",
      "Train test ratio: 51141.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1598\n",
      "Epoch 0, train loss: 0.230912\n",
      "Epoch 1, train loss: 0.224363\n",
      "Epoch 2, train loss: 0.228443\n",
      "Epoch 3, train loss: 0.284840\n",
      "Epoch 4, train loss: 0.196171\n",
      "Epoch 5, train loss: 0.225639\n",
      "Epoch 6, train loss: 0.245026\n",
      "Epoch 7, train loss: 0.207669\n",
      "Epoch 8, train loss: 0.208557\n",
      "Epoch 9, train loss: 0.209755\n",
      "Epoch 10, train loss: 0.173858\n",
      "Epoch 11, train loss: 0.261145\n",
      "Epoch 12, train loss: 0.175435\n",
      "Epoch 13, train loss: 0.317008\n",
      "Epoch 14, train loss: 0.187049\n",
      "Epoch 15, train loss: 0.177622\n",
      "Epoch 16, train loss: 0.213053\n",
      "Epoch 17, train loss: 0.230607\n",
      "Epoch 18, train loss: 0.131526\n",
      "Epoch 19, train loss: 0.249177\n",
      "Epoch 20, train loss: 0.163377\n",
      "Epoch 21, train loss: 0.240957\n",
      "Epoch 22, train loss: 0.226099\n",
      "Epoch 23, train loss: 0.191753\n",
      "Epoch 24, train loss: 0.230147\n",
      "Epoch 25, train loss: 0.186685\n",
      "Epoch 26, train loss: 0.223515\n",
      "Epoch 27, train loss: 0.171835\n",
      "Epoch 28, train loss: 0.204757\n",
      "Epoch 29, train loss: 0.161815\n",
      "Epoch 30, train loss: 0.183359\n",
      "Epoch 31, train loss: 0.220086\n",
      "Epoch 32, train loss: 0.203301\n",
      "Epoch 33, train loss: 0.208212\n",
      "Epoch 34, train loss: 0.206961\n",
      "Epoch 35, train loss: 0.229570\n",
      "Epoch 36, train loss: 0.176771\n",
      "Epoch 37, train loss: 0.215679\n",
      "Epoch 38, train loss: 0.191288\n",
      "Epoch 39, train loss: 0.219030\n",
      "Epoch 40, train loss: 0.164438\n",
      "Epoch 41, train loss: 0.276292\n",
      "Epoch 42, train loss: 0.164327\n",
      "Epoch 43, train loss: 0.156072\n",
      "Epoch 44, train loss: 0.221135\n",
      "Epoch 45, train loss: 0.214728\n",
      "Epoch 46, train loss: 0.228608\n",
      "Epoch 47, train loss: 0.230924\n",
      "Epoch 48, train loss: 0.196031\n",
      "Epoch 49, train loss: 0.162278\n",
      "Epoch 50, train loss: 0.206014\n",
      "Epoch 51, train loss: 0.181539\n",
      "Epoch 52, train loss: 0.190928\n",
      "Epoch 53, train loss: 0.253893\n",
      "Epoch 54, train loss: 0.192643\n",
      "Epoch 55, train loss: 0.157127\n",
      "Epoch 56, train loss: 0.191749\n",
      "Epoch 57, train loss: 0.178691\n",
      "Epoch 58, train loss: 0.209111\n",
      "Epoch 59, train loss: 0.256020\n",
      "Epoch 60, train loss: 0.212714\n",
      "Epoch 61, train loss: 0.225604\n",
      "Epoch 62, train loss: 0.215697\n",
      "Epoch 63, train loss: 0.202135\n",
      "Epoch 64, train loss: 0.190855\n",
      "Epoch 65, train loss: 0.186961\n",
      "Epoch 66, train loss: 0.203909\n",
      "Epoch 67, train loss: 0.204433\n",
      "Epoch 68, train loss: 0.187378\n",
      "Epoch 69, train loss: 0.191036\n",
      "Epoch 70, train loss: 0.212853\n",
      "Epoch 71, train loss: 0.224554\n",
      "Epoch 72, train loss: 0.216659\n",
      "Epoch 73, train loss: 0.228088\n",
      "Epoch 74, train loss: 0.225369\n",
      "Epoch 75, train loss: 0.202218\n",
      "Epoch 76, train loss: 0.165439\n",
      "Epoch 77, train loss: 0.196989\n",
      "Epoch 78, train loss: 0.185808\n",
      "Epoch 79, train loss: 0.237891\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "x here is\n",
      "[[179. 183. 187. ... 259. 262. 266.]\n",
      " [183. 187. 191. ... 262. 266. 269.]\n",
      " [187. 191. 195. ... 266. 269. 272.]\n",
      " ...\n",
      " [190. 189. 189. ... 186. 186. 187.]\n",
      " [189. 189. 188. ... 186. 187. 188.]\n",
      " [189. 188. 188. ... 187. 188. 187.]]\n",
      "y here is\n",
      "[[283. 283. 283. ... 283. 283. 283.]\n",
      " [286. 286. 286. ... 286. 286. 286.]\n",
      " [289. 289. 289. ... 289. 289. 289.]\n",
      " ...\n",
      " [182. 182. 182. ... 182. 182. 182.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " [177. 177. 177. ... 177. 177. 177.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (2166, 24), Train/test: 1/2165\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "x here is\n",
      "[[101.  98. 104. ...  89.  76.  68.]\n",
      " [ 98. 104. 112. ...  76.  68.  64.]\n",
      " [104. 112. 120. ...  68.  64.  64.]\n",
      " ...\n",
      " [117. 123. 126. ... 149. 150. 152.]\n",
      " [123. 126. 126. ... 150. 152. 155.]\n",
      " [126. 126. 125. ... 152. 155. 156.]]\n",
      "y here is\n",
      "[[ 66.  66.  66. ...  66.  66.  66.]\n",
      " [ 66.  66.  66. ...  66.  66.  66.]\n",
      " [ 63.  63.  63. ...  63.  63.  63.]\n",
      " ...\n",
      " [168. 168. 168. ... 168. 168. 168.]\n",
      " [172. 172. 172. ... 172. 172. 172.]\n",
      " [176. 176. 176. ... 176. 176. 176.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 750/8779\n",
      "Found 44 continuous time series\n",
      "Data shape: (9531, 24), Train/test: 9529/2\n",
      "Train test ratio: 4764.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294DBA740>\n",
      "Epoch 0, test loss: 0.188585\n",
      "Epoch 1, test loss: 0.183734\n",
      "Epoch 2, test loss: 0.186854\n",
      "Epoch 3, test loss: 0.198470\n",
      "Epoch 4, test loss: 0.191086\n",
      "Epoch 5, test loss: 0.189280\n",
      "Epoch 6, test loss: 0.186597\n",
      "Epoch 7, test loss: 0.184073\n",
      "Epoch 8, test loss: 0.186855\n",
      "Epoch 9, test loss: 0.187803\n",
      "Epoch 10, test loss: 0.190079\n",
      "Epoch 11, test loss: 0.185954\n",
      "Epoch 12, test loss: 0.184968\n",
      "Epoch 13, test loss: 0.192709\n",
      "Epoch 14, test loss: 0.184457\n",
      "Epoch 15, test loss: 0.186597\n",
      "Epoch 16, test loss: 0.191161\n",
      "Epoch 17, test loss: 0.183965\n",
      "Epoch 18, test loss: 0.192104\n",
      "Epoch 19, test loss: 0.185296\n",
      "Epoch 20, test loss: 0.184144\n",
      "Epoch 21, test loss: 0.197838\n",
      "Epoch 22, test loss: 0.187206\n",
      "Epoch 23, test loss: 0.186476\n",
      "Epoch 24, test loss: 0.189008\n",
      "Epoch 25, test loss: 0.186736\n",
      "Epoch 26, test loss: 0.185130\n",
      "Epoch 27, test loss: 0.184941\n",
      "Epoch 28, test loss: 0.184195\n",
      "Epoch 29, test loss: 0.187992\n",
      "Epoch 30, test loss: 0.193106\n",
      "Epoch 31, test loss: 0.196311\n",
      "Epoch 32, test loss: 0.186739\n",
      "Epoch 33, test loss: 0.192208\n",
      "Epoch 34, test loss: 0.185967\n",
      "Epoch 35, test loss: 0.183942\n",
      "Epoch 36, test loss: 0.184164\n",
      "Epoch 37, test loss: 0.185125\n",
      "Epoch 38, test loss: 0.196428\n",
      "Epoch 39, test loss: 0.185064\n",
      "Epoch 40, test loss: 0.185187\n",
      "Epoch 41, test loss: 0.185003\n",
      "Epoch 42, test loss: 0.188132\n",
      "Epoch 43, test loss: 0.185071\n",
      "Epoch 44, test loss: 0.189459\n",
      "Epoch 45, test loss: 0.186212\n",
      "Epoch 46, test loss: 0.189739\n",
      "Epoch 47, test loss: 0.184273\n",
      "Epoch 48, test loss: 0.183284\n",
      "Epoch 49, test loss: 0.186794\n",
      "Epoch 50, test loss: 0.191072\n",
      "Epoch 51, test loss: 0.187135\n",
      "Epoch 52, test loss: 0.188634\n",
      "Epoch 53, test loss: 0.192597\n",
      "Epoch 54, test loss: 0.185991\n",
      "Epoch 55, test loss: 0.187931\n",
      "Epoch 56, test loss: 0.194620\n",
      "Epoch 57, test loss: 0.188168\n",
      "Epoch 58, test loss: 0.197170\n",
      "Epoch 59, test loss: 0.184907\n",
      "Epoch 60, test loss: 0.198386\n",
      "Epoch 61, test loss: 0.185614\n",
      "Epoch 62, test loss: 0.185650\n",
      "Epoch 63, test loss: 0.185327\n",
      "Epoch 64, test loss: 0.187732\n",
      "Epoch 65, test loss: 0.185087\n",
      "Epoch 66, test loss: 0.184777\n",
      "Epoch 67, test loss: 0.185637\n",
      "Epoch 68, test loss: 0.186181\n",
      "Epoch 69, test loss: 0.183850\n",
      "Epoch 70, test loss: 0.194579\n",
      "Epoch 71, test loss: 0.186219\n",
      "Epoch 72, test loss: 0.199315\n",
      "Epoch 73, test loss: 0.185099\n",
      "Epoch 74, test loss: 0.187056\n",
      "Epoch 75, test loss: 0.182906\n",
      "Epoch 76, test loss: 0.184435\n",
      "Epoch 77, test loss: 0.184419\n",
      "Epoch 78, test loss: 0.183576\n",
      "Epoch 79, test loss: 0.183918\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294DBA740>\n",
      "Epoch 0, test loss: 0.190078\n",
      "Epoch 1, test loss: 0.186380\n",
      "Epoch 2, test loss: 0.185414\n",
      "Epoch 3, test loss: 0.184416\n",
      "Epoch 4, test loss: 0.183988\n",
      "Epoch 5, test loss: 0.183693\n",
      "Epoch 6, test loss: 0.183788\n",
      "Epoch 7, test loss: 0.184321\n",
      "Epoch 8, test loss: 0.200878\n",
      "Epoch 9, test loss: 0.187895\n",
      "Epoch 10, test loss: 0.189967\n",
      "Epoch 11, test loss: 0.191011\n",
      "Epoch 12, test loss: 0.185147\n",
      "Epoch 13, test loss: 0.185369\n",
      "Epoch 14, test loss: 0.186547\n",
      "Epoch 15, test loss: 0.186066\n",
      "Epoch 16, test loss: 0.188360\n",
      "Epoch 17, test loss: 0.193363\n",
      "Epoch 18, test loss: 0.186172\n",
      "Epoch 19, test loss: 0.185469\n",
      "Epoch 20, test loss: 0.185151\n",
      "Epoch 21, test loss: 0.184324\n",
      "Epoch 22, test loss: 0.186503\n",
      "Epoch 23, test loss: 0.186370\n",
      "Epoch 24, test loss: 0.183757\n",
      "Epoch 25, test loss: 0.184561\n",
      "Epoch 26, test loss: 0.184842\n",
      "Epoch 27, test loss: 0.188064\n",
      "Epoch 28, test loss: 0.187776\n",
      "Epoch 29, test loss: 0.184828\n",
      "Epoch 30, test loss: 0.184738\n",
      "Epoch 31, test loss: 0.195603\n",
      "Epoch 32, test loss: 0.186749\n",
      "Epoch 33, test loss: 0.188415\n",
      "Epoch 34, test loss: 0.186283\n",
      "Epoch 35, test loss: 0.185226\n",
      "Epoch 36, test loss: 0.191291\n",
      "Epoch 37, test loss: 0.185204\n",
      "Epoch 38, test loss: 0.184117\n",
      "Epoch 39, test loss: 0.189016\n",
      "Epoch 40, test loss: 0.185572\n",
      "Epoch 41, test loss: 0.185692\n",
      "Epoch 42, test loss: 0.187311\n",
      "Epoch 43, test loss: 0.201848\n",
      "Epoch 44, test loss: 0.191203\n",
      "Epoch 45, test loss: 0.185891\n",
      "Epoch 46, test loss: 0.186737\n",
      "Epoch 47, test loss: 0.183923\n",
      "Epoch 48, test loss: 0.185673\n",
      "Epoch 49, test loss: 0.193722\n",
      "Epoch 50, test loss: 0.185985\n",
      "Epoch 51, test loss: 0.192343\n",
      "Epoch 52, test loss: 0.184204\n",
      "Epoch 53, test loss: 0.185497\n",
      "Epoch 54, test loss: 0.184164\n",
      "Epoch 55, test loss: 0.184888\n",
      "Epoch 56, test loss: 0.187510\n",
      "Epoch 57, test loss: 0.185586\n",
      "Epoch 58, test loss: 0.186078\n",
      "Epoch 59, test loss: 0.184605\n",
      "Epoch 60, test loss: 0.183717\n",
      "Epoch 61, test loss: 0.187593\n",
      "Epoch 62, test loss: 0.183192\n",
      "Epoch 63, test loss: 0.185070\n",
      "Epoch 64, test loss: 0.196130\n",
      "Epoch 65, test loss: 0.188069\n",
      "Epoch 66, test loss: 0.189127\n",
      "Epoch 67, test loss: 0.183926\n",
      "Epoch 68, test loss: 0.183919\n",
      "Epoch 69, test loss: 0.183798\n",
      "Epoch 70, test loss: 0.188246\n",
      "Epoch 71, test loss: 0.187258\n",
      "Epoch 72, test loss: 0.190536\n",
      "Epoch 73, test loss: 0.186554\n",
      "Epoch 74, test loss: 0.184569\n",
      "Epoch 75, test loss: 0.183441\n",
      "Epoch 76, test loss: 0.184527\n",
      "Epoch 77, test loss: 0.183754\n",
      "Epoch 78, test loss: 0.185973\n",
      "Epoch 79, test loss: 0.184055\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A294DBA740>\n",
      "Epoch 0, test loss: 0.336732\n",
      "Epoch 1, test loss: 0.226919\n",
      "Epoch 2, test loss: 0.207526\n",
      "Epoch 3, test loss: 0.197227\n",
      "Epoch 4, test loss: 0.193971\n",
      "Epoch 5, test loss: 0.193209\n",
      "Epoch 6, test loss: 0.194544\n",
      "Epoch 7, test loss: 0.191536\n",
      "Epoch 8, test loss: 0.193950\n",
      "Epoch 9, test loss: 0.195485\n",
      "Epoch 10, test loss: 0.189003\n",
      "Epoch 11, test loss: 0.189233\n",
      "Epoch 12, test loss: 0.210168\n",
      "Epoch 13, test loss: 0.189163\n",
      "Epoch 14, test loss: 0.204305\n",
      "Epoch 15, test loss: 0.189412\n",
      "Epoch 16, test loss: 0.188038\n",
      "Epoch 17, test loss: 0.188513\n",
      "Epoch 18, test loss: 0.189666\n",
      "Epoch 19, test loss: 0.188160\n",
      "Epoch 20, test loss: 0.189607\n",
      "Epoch 21, test loss: 0.188059\n",
      "Epoch 22, test loss: 0.189296\n",
      "Epoch 23, test loss: 0.189720\n",
      "Epoch 24, test loss: 0.193836\n",
      "Epoch 25, test loss: 0.191192\n",
      "Epoch 26, test loss: 0.192335\n",
      "Epoch 27, test loss: 0.188369\n",
      "Epoch 28, test loss: 0.200112\n",
      "Epoch 29, test loss: 0.194858\n",
      "Epoch 30, test loss: 0.208261\n",
      "Epoch 31, test loss: 0.187997\n",
      "Epoch 32, test loss: 0.193299\n",
      "Epoch 33, test loss: 0.187820\n",
      "Epoch 34, test loss: 0.187640\n",
      "Epoch 35, test loss: 0.189576\n",
      "Epoch 36, test loss: 0.190516\n",
      "Epoch 37, test loss: 0.191248\n",
      "Epoch 38, test loss: 0.193076\n",
      "Epoch 39, test loss: 0.187530\n",
      "Epoch 40, test loss: 0.188071\n",
      "Epoch 41, test loss: 0.195755\n",
      "Epoch 42, test loss: 0.196347\n",
      "Epoch 43, test loss: 0.188406\n",
      "Epoch 44, test loss: 0.187155\n",
      "Epoch 45, test loss: 0.193606\n",
      "Epoch 46, test loss: 0.199291\n",
      "Epoch 47, test loss: 0.189030\n",
      "Epoch 48, test loss: 0.185825\n",
      "Epoch 49, test loss: 0.187489\n",
      "Epoch 50, test loss: 0.186523\n",
      "Epoch 51, test loss: 0.190120\n",
      "Epoch 52, test loss: 0.191440\n",
      "Epoch 53, test loss: 0.187516\n",
      "Epoch 54, test loss: 0.202713\n",
      "Epoch 55, test loss: 0.187870\n",
      "Epoch 56, test loss: 0.193177\n",
      "Epoch 57, test loss: 0.185522\n",
      "Epoch 58, test loss: 0.186003\n",
      "Epoch 59, test loss: 0.187999\n",
      "Epoch 60, test loss: 0.188123\n",
      "Epoch 61, test loss: 0.189391\n",
      "Epoch 62, test loss: 0.187644\n",
      "Epoch 63, test loss: 0.185477\n",
      "Epoch 64, test loss: 0.185966\n",
      "Epoch 65, test loss: 0.188391\n",
      "Epoch 66, test loss: 0.185248\n",
      "Epoch 67, test loss: 0.185846\n",
      "Epoch 68, test loss: 0.186483\n",
      "Epoch 69, test loss: 0.187060\n",
      "Epoch 70, test loss: 0.191941\n",
      "Epoch 71, test loss: 0.186502\n",
      "Epoch 72, test loss: 0.185247\n",
      "Epoch 73, test loss: 0.194711\n",
      "Epoch 74, test loss: 0.186722\n",
      "Epoch 75, test loss: 0.200089\n",
      "Epoch 76, test loss: 0.187784\n",
      "Epoch 77, test loss: 0.187103\n",
      "Epoch 78, test loss: 0.186416\n",
      "Epoch 79, test loss: 0.193299\n",
      "Epoch 80, test loss: 0.185199\n",
      "Epoch 81, test loss: 0.188198\n",
      "Epoch 82, test loss: 0.188712\n",
      "Epoch 83, test loss: 0.186206\n",
      "Epoch 84, test loss: 0.186022\n",
      "Epoch 85, test loss: 0.187634\n",
      "Epoch 86, test loss: 0.186476\n",
      "Epoch 87, test loss: 0.186020\n",
      "Epoch 88, test loss: 0.191523\n",
      "Epoch 89, test loss: 0.185843\n",
      "Epoch 90, test loss: 0.185085\n",
      "Epoch 91, test loss: 0.196597\n",
      "Epoch 92, test loss: 0.184560\n",
      "Epoch 93, test loss: 0.186031\n",
      "Epoch 94, test loss: 0.188456\n",
      "Epoch 95, test loss: 0.185255\n",
      "Epoch 96, test loss: 0.185221\n",
      "Epoch 97, test loss: 0.185047\n",
      "Epoch 98, test loss: 0.198441\n",
      "Epoch 99, test loss: 0.190740\n",
      "Epoch 100, test loss: 0.184764\n",
      "Epoch 101, test loss: 0.185631\n",
      "Epoch 102, test loss: 0.186362\n",
      "Epoch 103, test loss: 0.187211\n",
      "Epoch 104, test loss: 0.188291\n",
      "Epoch 105, test loss: 0.184917\n",
      "Epoch 106, test loss: 0.188450\n",
      "Epoch 107, test loss: 0.187397\n",
      "Epoch 108, test loss: 0.184756\n",
      "Epoch 109, test loss: 0.193868\n",
      "Epoch 110, test loss: 0.188040\n",
      "Epoch 111, test loss: 0.185377\n",
      "Epoch 112, test loss: 0.188440\n",
      "Epoch 113, test loss: 0.190452\n",
      "Epoch 114, test loss: 0.186992\n",
      "Epoch 115, test loss: 0.192335\n",
      "Epoch 116, test loss: 0.185289\n",
      "Epoch 117, test loss: 0.191363\n",
      "Epoch 118, test loss: 0.194058\n",
      "Epoch 119, test loss: 0.186079\n",
      "Epoch 120, test loss: 0.186875\n",
      "Epoch 121, test loss: 0.194847\n",
      "Epoch 122, test loss: 0.189828\n",
      "Epoch 123, test loss: 0.186304\n",
      "Epoch 124, test loss: 0.187259\n",
      "Epoch 125, test loss: 0.185707\n",
      "Epoch 126, test loss: 0.187356\n",
      "Epoch 127, test loss: 0.187352\n",
      "Epoch 128, test loss: 0.185794\n",
      "Epoch 129, test loss: 0.189069\n",
      "Epoch 130, test loss: 0.185331\n",
      "Epoch 131, test loss: 0.185561\n",
      "Epoch 132, test loss: 0.193044\n",
      "Epoch 133, test loss: 0.189474\n",
      "Epoch 134, test loss: 0.187003\n",
      "Epoch 135, test loss: 0.187096\n",
      "Epoch 136, test loss: 0.186076\n",
      "Epoch 137, test loss: 0.185835\n",
      "Epoch 138, test loss: 0.193268\n",
      "Epoch 139, test loss: 0.185424\n",
      "Epoch 140, test loss: 0.191371\n",
      "Epoch 141, test loss: 0.187921\n",
      "Epoch 142, test loss: 0.192255\n",
      "Epoch 143, test loss: 0.187603\n",
      "Epoch 144, test loss: 0.187452\n",
      "Epoch 145, test loss: 0.195399\n",
      "Epoch 146, test loss: 0.192571\n",
      "Epoch 147, test loss: 0.191903\n",
      "Epoch 148, test loss: 0.187780\n",
      "Epoch 149, test loss: 0.186498\n",
      "Epoch 150, test loss: 0.186280\n",
      "Epoch 151, test loss: 0.196817\n",
      "Epoch 152, test loss: 0.187306\n",
      "Epoch 153, test loss: 0.185504\n",
      "Epoch 154, test loss: 0.188207\n",
      "Epoch 155, test loss: 0.187550\n",
      "Epoch 156, test loss: 0.189463\n",
      "Epoch 157, test loss: 0.187641\n",
      "Epoch 158, test loss: 0.188478\n",
      "Epoch 159, test loss: 0.189963\n",
      "Epoch 160, test loss: 0.186678\n",
      "Epoch 161, test loss: 0.194018\n",
      "Epoch 162, test loss: 0.186501\n",
      "Epoch 163, test loss: 0.186427\n",
      "Epoch 164, test loss: 0.200117\n",
      "Epoch 165, test loss: 0.186141\n",
      "Epoch 166, test loss: 0.187908\n",
      "Epoch 167, test loss: 0.185596\n",
      "Epoch 168, test loss: 0.186636\n",
      "Epoch 169, test loss: 0.188523\n",
      "Epoch 170, test loss: 0.187775\n",
      "Epoch 171, test loss: 0.186658\n",
      "Epoch 172, test loss: 0.186930\n",
      "Epoch 173, test loss: 0.185955\n",
      "Epoch 174, test loss: 0.187013\n",
      "Epoch 175, test loss: 0.188797\n",
      "Epoch 176, test loss: 0.187446\n",
      "Epoch 177, test loss: 0.194097\n",
      "Epoch 178, test loss: 0.191621\n",
      "Epoch 179, test loss: 0.187337\n",
      "Epoch 180, test loss: 0.187625\n",
      "Epoch 181, test loss: 0.187495\n",
      "Epoch 182, test loss: 0.185949\n",
      "Epoch 183, test loss: 0.191463\n",
      "Epoch 184, test loss: 0.194415\n",
      "Epoch 185, test loss: 0.188588\n",
      "Epoch 186, test loss: 0.187502\n",
      "Epoch 187, test loss: 0.192303\n",
      "Epoch 188, test loss: 0.191017\n",
      "Epoch 189, test loss: 0.190187\n",
      "Epoch 190, test loss: 0.195077\n",
      "Epoch 191, test loss: 0.186503\n",
      "Epoch 192, test loss: 0.186708\n",
      "Epoch 193, test loss: 0.187771\n",
      "Epoch 194, test loss: 0.185945\n",
      "Epoch 195, test loss: 0.185427\n",
      "Epoch 196, test loss: 0.185660\n",
      "Epoch 197, test loss: 0.186506\n",
      "Epoch 198, test loss: 0.186456\n",
      "Epoch 199, test loss: 0.186947\n",
      "Epoch 200, test loss: 0.189796\n",
      "Epoch 201, test loss: 0.185841\n",
      "Epoch 202, test loss: 0.192178\n",
      "Epoch 203, test loss: 0.186060\n",
      "Epoch 204, test loss: 0.187800\n",
      "Epoch 205, test loss: 0.186876\n",
      "Epoch 206, test loss: 0.190642\n",
      "Epoch 207, test loss: 0.185379\n",
      "Epoch 208, test loss: 0.185768\n",
      "Epoch 209, test loss: 0.193351\n",
      "Epoch 210, test loss: 0.185821\n",
      "Epoch 211, test loss: 0.185658\n",
      "Epoch 212, test loss: 0.190251\n",
      "Epoch 213, test loss: 0.186338\n",
      "Epoch 214, test loss: 0.185399\n",
      "Epoch 215, test loss: 0.187926\n",
      "Epoch 216, test loss: 0.191372\n",
      "Epoch 217, test loss: 0.187429\n",
      "Epoch 218, test loss: 0.185353\n",
      "Epoch 219, test loss: 0.186415\n",
      "Epoch 220, test loss: 0.188418\n",
      "Epoch 221, test loss: 0.187376\n",
      "Epoch 222, test loss: 0.185515\n",
      "Epoch 223, test loss: 0.185542\n",
      "Epoch 224, test loss: 0.185937\n",
      "Epoch 225, test loss: 0.195219\n",
      "Epoch 226, test loss: 0.198276\n",
      "Epoch 227, test loss: 0.189112\n",
      "Epoch 228, test loss: 0.186812\n",
      "Epoch 229, test loss: 0.187640\n",
      "Epoch 230, test loss: 0.193206\n",
      "Epoch 231, test loss: 0.185831\n",
      "Epoch 232, test loss: 0.185726\n",
      "Epoch 233, test loss: 0.187448\n",
      "Epoch 234, test loss: 0.191498\n",
      "Epoch 235, test loss: 0.198772\n",
      "Epoch 236, test loss: 0.185774\n",
      "Epoch 237, test loss: 0.190263\n",
      "Epoch 238, test loss: 0.189052\n",
      "Epoch 239, test loss: 0.186609\n",
      "Pretrain data: 19653653.0\n",
      "Building dataset, requesting data from 0 to 820\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6438/93890\n",
      "Found 820 continuous time series\n",
      "Data shape: (100330, 24), Train/test: 100328/2\n",
      "Train test ratio: 50164.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1567\n",
      "Epoch 0, train loss: 0.247706\n",
      "Epoch 1, train loss: 0.249761\n",
      "Epoch 2, train loss: 0.151507\n",
      "Epoch 3, train loss: 0.233021\n",
      "Epoch 4, train loss: 0.206433\n",
      "Epoch 5, train loss: 0.211312\n",
      "Epoch 6, train loss: 0.221959\n",
      "Epoch 7, train loss: 0.143736\n",
      "Epoch 8, train loss: 0.209924\n",
      "Epoch 9, train loss: 0.242250\n",
      "Epoch 10, train loss: 0.208741\n",
      "Epoch 11, train loss: 0.202755\n",
      "Epoch 12, train loss: 0.205183\n",
      "Epoch 13, train loss: 0.185160\n",
      "Epoch 14, train loss: 0.216250\n",
      "Epoch 15, train loss: 0.270415\n",
      "Epoch 16, train loss: 0.206992\n",
      "Epoch 17, train loss: 0.161684\n",
      "Epoch 18, train loss: 0.255253\n",
      "Epoch 19, train loss: 0.231561\n",
      "Epoch 20, train loss: 0.271730\n",
      "Epoch 21, train loss: 0.207578\n",
      "Epoch 22, train loss: 0.194704\n",
      "Epoch 23, train loss: 0.265348\n",
      "Epoch 24, train loss: 0.171169\n",
      "Epoch 25, train loss: 0.162692\n",
      "Epoch 26, train loss: 0.214282\n",
      "Epoch 27, train loss: 0.225919\n",
      "Epoch 28, train loss: 0.224889\n",
      "Epoch 29, train loss: 0.194648\n",
      "Epoch 30, train loss: 0.195790\n",
      "Epoch 31, train loss: 0.232210\n",
      "Epoch 32, train loss: 0.198479\n",
      "Epoch 33, train loss: 0.216779\n",
      "Epoch 34, train loss: 0.234652\n",
      "Epoch 35, train loss: 0.267728\n",
      "Epoch 36, train loss: 0.207212\n",
      "Epoch 37, train loss: 0.179002\n",
      "Epoch 38, train loss: 0.234038\n",
      "Epoch 39, train loss: 0.194844\n",
      "Epoch 40, train loss: 0.291336\n",
      "Epoch 41, train loss: 0.185394\n",
      "Epoch 42, train loss: 0.186531\n",
      "Epoch 43, train loss: 0.230706\n",
      "Epoch 44, train loss: 0.219391\n",
      "Epoch 45, train loss: 0.158226\n",
      "Epoch 46, train loss: 0.197105\n",
      "Epoch 47, train loss: 0.257255\n",
      "Epoch 48, train loss: 0.206268\n",
      "Epoch 49, train loss: 0.180585\n",
      "Epoch 50, train loss: 0.210136\n",
      "Epoch 51, train loss: 0.193750\n",
      "Epoch 52, train loss: 0.226967\n",
      "Epoch 53, train loss: 0.193302\n",
      "Epoch 54, train loss: 0.171537\n",
      "Epoch 55, train loss: 0.268661\n",
      "Epoch 56, train loss: 0.207492\n",
      "Epoch 57, train loss: 0.170165\n",
      "Epoch 58, train loss: 0.189735\n",
      "Epoch 59, train loss: 0.240931\n",
      "Epoch 60, train loss: 0.227925\n",
      "Epoch 61, train loss: 0.175280\n",
      "Epoch 62, train loss: 0.202945\n",
      "Epoch 63, train loss: 0.169304\n",
      "Epoch 64, train loss: 0.224496\n",
      "Epoch 65, train loss: 0.226976\n",
      "Epoch 66, train loss: 0.198388\n",
      "Epoch 67, train loss: 0.228521\n",
      "Epoch 68, train loss: 0.213391\n",
      "Epoch 69, train loss: 0.243446\n",
      "Epoch 70, train loss: 0.244128\n",
      "Epoch 71, train loss: 0.265019\n",
      "Epoch 72, train loss: 0.248652\n",
      "Epoch 73, train loss: 0.237735\n",
      "Epoch 74, train loss: 0.184676\n",
      "Epoch 75, train loss: 0.177684\n",
      "Epoch 76, train loss: 0.274945\n",
      "Epoch 77, train loss: 0.241268\n",
      "Epoch 78, train loss: 0.201309\n",
      "Epoch 79, train loss: 0.218715\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "x here is\n",
      "[[239. 238. 235. ... 196. 195. 197.]\n",
      " [238. 235. 233. ... 195. 197. 199.]\n",
      " [235. 233. 231. ... 197. 199. 206.]\n",
      " ...\n",
      " [200. 193. 184. ... 152. 151. 149.]\n",
      " [193. 184. 182. ... 151. 149. 149.]\n",
      " [184. 182. 182. ... 149. 149. 145.]]\n",
      "y here is\n",
      "[[203. 203. 203. ... 203. 203. 203.]\n",
      " [202. 202. 202. ... 202. 202. 202.]\n",
      " [198. 198. 198. ... 198. 198. 198.]\n",
      " ...\n",
      " [144. 144. 144. ... 144. 144. 144.]\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [145. 145. 145. ... 145. 145. 145.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (2454, 24), Train/test: 1/2453\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "x here is\n",
      "[[219. 229. 224. ... 149. 153. 160.]\n",
      " [229. 224. 221. ... 153. 160. 159.]\n",
      " [224. 221. 215. ... 160. 159. 156.]\n",
      " ...\n",
      " [206. 204. 203. ... 237. 241. 243.]\n",
      " [204. 203. 203. ... 241. 243. 251.]\n",
      " [203. 203. 203. ... 243. 251. 257.]]\n",
      "y here is\n",
      "[[149. 149. 149. ... 149. 149. 149.]\n",
      " [137. 137. 137. ... 137. 137. 137.]\n",
      " [127. 127. 127. ... 127. 127. 127.]\n",
      " ...\n",
      " [250. 250. 250. ... 250. 250. 250.]\n",
      " [246. 246. 246. ... 246. 246. 246.]\n",
      " [240. 240. 240. ... 240. 240. 240.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 671/10813\n",
      "Found 22 continuous time series\n",
      "Data shape: (11486, 24), Train/test: 11484/2\n",
      "Train test ratio: 5742.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290187FA0>\n",
      "Epoch 0, test loss: 0.188110\n",
      "Epoch 1, test loss: 0.184020\n",
      "Epoch 2, test loss: 0.180949\n",
      "Epoch 3, test loss: 0.179706\n",
      "Epoch 4, test loss: 0.180367\n",
      "Epoch 5, test loss: 0.180739\n",
      "Epoch 6, test loss: 0.184601\n",
      "Epoch 7, test loss: 0.181907\n",
      "Epoch 8, test loss: 0.186082\n",
      "Epoch 9, test loss: 0.181651\n",
      "Epoch 10, test loss: 0.180712\n",
      "Epoch 11, test loss: 0.184336\n",
      "Epoch 12, test loss: 0.182436\n",
      "Epoch 13, test loss: 0.183641\n",
      "Epoch 14, test loss: 0.181001\n",
      "Epoch 15, test loss: 0.181200\n",
      "Epoch 16, test loss: 0.182328\n",
      "Epoch 17, test loss: 0.181562\n",
      "Epoch 18, test loss: 0.183383\n",
      "Epoch 19, test loss: 0.182692\n",
      "Epoch 20, test loss: 0.181137\n",
      "Epoch 21, test loss: 0.183213\n",
      "Epoch 22, test loss: 0.183920\n",
      "Epoch 23, test loss: 0.183916\n",
      "Epoch 24, test loss: 0.180904\n",
      "Epoch 25, test loss: 0.186083\n",
      "Epoch 26, test loss: 0.185310\n",
      "Epoch 27, test loss: 0.182647\n",
      "Epoch 28, test loss: 0.188400\n",
      "Epoch 29, test loss: 0.183624\n",
      "Epoch 30, test loss: 0.182185\n",
      "Epoch 31, test loss: 0.183397\n",
      "Epoch 32, test loss: 0.184803\n",
      "Epoch 33, test loss: 0.182420\n",
      "Epoch 34, test loss: 0.182419\n",
      "Epoch 35, test loss: 0.182410\n",
      "Epoch 36, test loss: 0.182404\n",
      "Epoch 37, test loss: 0.183671\n",
      "Epoch 38, test loss: 0.182076\n",
      "Epoch 39, test loss: 0.184056\n",
      "Epoch 40, test loss: 0.192354\n",
      "Epoch 41, test loss: 0.182265\n",
      "Epoch 42, test loss: 0.185757\n",
      "Epoch 43, test loss: 0.183379\n",
      "Epoch 44, test loss: 0.188807\n",
      "Epoch 45, test loss: 0.186145\n",
      "Epoch 46, test loss: 0.182514\n",
      "Epoch 47, test loss: 0.182463\n",
      "Epoch 48, test loss: 0.184059\n",
      "Epoch 49, test loss: 0.185421\n",
      "Epoch 50, test loss: 0.183518\n",
      "Epoch 51, test loss: 0.183894\n",
      "Epoch 52, test loss: 0.183621\n",
      "Epoch 53, test loss: 0.184003\n",
      "Epoch 54, test loss: 0.187138\n",
      "Epoch 55, test loss: 0.185401\n",
      "Epoch 56, test loss: 0.187235\n",
      "Epoch 57, test loss: 0.188722\n",
      "Epoch 58, test loss: 0.193813\n",
      "Epoch 59, test loss: 0.183833\n",
      "Epoch 60, test loss: 0.185392\n",
      "Epoch 61, test loss: 0.185480\n",
      "Epoch 62, test loss: 0.183420\n",
      "Epoch 63, test loss: 0.185778\n",
      "Epoch 64, test loss: 0.185201\n",
      "Epoch 65, test loss: 0.184097\n",
      "Epoch 66, test loss: 0.183432\n",
      "Epoch 67, test loss: 0.192029\n",
      "Epoch 68, test loss: 0.183752\n",
      "Epoch 69, test loss: 0.184228\n",
      "Epoch 70, test loss: 0.183075\n",
      "Epoch 71, test loss: 0.186451\n",
      "Epoch 72, test loss: 0.184236\n",
      "Epoch 73, test loss: 0.184300\n",
      "Epoch 74, test loss: 0.194769\n",
      "Epoch 75, test loss: 0.183877\n",
      "Epoch 76, test loss: 0.190709\n",
      "Epoch 77, test loss: 0.183683\n",
      "Epoch 78, test loss: 0.185801\n",
      "Epoch 79, test loss: 0.184305\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290187FA0>\n",
      "Epoch 0, test loss: 0.180643\n",
      "Epoch 1, test loss: 0.181699\n",
      "Epoch 2, test loss: 0.181885\n",
      "Epoch 3, test loss: 0.179873\n",
      "Epoch 4, test loss: 0.182208\n",
      "Epoch 5, test loss: 0.181527\n",
      "Epoch 6, test loss: 0.181642\n",
      "Epoch 7, test loss: 0.180138\n",
      "Epoch 8, test loss: 0.184300\n",
      "Epoch 9, test loss: 0.181891\n",
      "Epoch 10, test loss: 0.184109\n",
      "Epoch 11, test loss: 0.181830\n",
      "Epoch 12, test loss: 0.184587\n",
      "Epoch 13, test loss: 0.181804\n",
      "Epoch 14, test loss: 0.182109\n",
      "Epoch 15, test loss: 0.181566\n",
      "Epoch 16, test loss: 0.182037\n",
      "Epoch 17, test loss: 0.182133\n",
      "Epoch 18, test loss: 0.186558\n",
      "Epoch 19, test loss: 0.182718\n",
      "Epoch 20, test loss: 0.181742\n",
      "Epoch 21, test loss: 0.183557\n",
      "Epoch 22, test loss: 0.182679\n",
      "Epoch 23, test loss: 0.183244\n",
      "Epoch 24, test loss: 0.182398\n",
      "Epoch 25, test loss: 0.187736\n",
      "Epoch 26, test loss: 0.182808\n",
      "Epoch 27, test loss: 0.183044\n",
      "Epoch 28, test loss: 0.185688\n",
      "Epoch 29, test loss: 0.182155\n",
      "Epoch 30, test loss: 0.183914\n",
      "Epoch 31, test loss: 0.182490\n",
      "Epoch 32, test loss: 0.182115\n",
      "Epoch 33, test loss: 0.182199\n",
      "Epoch 34, test loss: 0.182436\n",
      "Epoch 35, test loss: 0.191076\n",
      "Epoch 36, test loss: 0.181365\n",
      "Epoch 37, test loss: 0.182685\n",
      "Epoch 38, test loss: 0.184185\n",
      "Epoch 39, test loss: 0.182678\n",
      "Epoch 40, test loss: 0.193266\n",
      "Epoch 41, test loss: 0.182461\n",
      "Epoch 42, test loss: 0.183910\n",
      "Epoch 43, test loss: 0.182137\n",
      "Epoch 44, test loss: 0.184720\n",
      "Epoch 45, test loss: 0.182922\n",
      "Epoch 46, test loss: 0.182246\n",
      "Epoch 47, test loss: 0.182786\n",
      "Epoch 48, test loss: 0.183380\n",
      "Epoch 49, test loss: 0.183029\n",
      "Epoch 50, test loss: 0.185571\n",
      "Epoch 51, test loss: 0.183509\n",
      "Epoch 52, test loss: 0.181890\n",
      "Epoch 53, test loss: 0.187783\n",
      "Epoch 54, test loss: 0.185845\n",
      "Epoch 55, test loss: 0.184449\n",
      "Epoch 56, test loss: 0.183273\n",
      "Epoch 57, test loss: 0.182919\n",
      "Epoch 58, test loss: 0.182191\n",
      "Epoch 59, test loss: 0.182421\n",
      "Epoch 60, test loss: 0.181575\n",
      "Epoch 61, test loss: 0.183057\n",
      "Epoch 62, test loss: 0.182680\n",
      "Epoch 63, test loss: 0.182361\n",
      "Epoch 64, test loss: 0.182603\n",
      "Epoch 65, test loss: 0.181814\n",
      "Epoch 66, test loss: 0.183595\n",
      "Epoch 67, test loss: 0.182883\n",
      "Epoch 68, test loss: 0.183923\n",
      "Epoch 69, test loss: 0.186394\n",
      "Epoch 70, test loss: 0.183426\n",
      "Epoch 71, test loss: 0.184051\n",
      "Epoch 72, test loss: 0.182587\n",
      "Epoch 73, test loss: 0.184334\n",
      "Epoch 74, test loss: 0.186196\n",
      "Epoch 75, test loss: 0.181928\n",
      "Epoch 76, test loss: 0.188012\n",
      "Epoch 77, test loss: 0.182547\n",
      "Epoch 78, test loss: 0.181681\n",
      "Epoch 79, test loss: 0.181941\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A290187FA0>\n",
      "Epoch 0, test loss: 0.271216\n",
      "Epoch 1, test loss: 0.204442\n",
      "Epoch 2, test loss: 0.200998\n",
      "Epoch 3, test loss: 0.199367\n",
      "Epoch 4, test loss: 0.196597\n",
      "Epoch 5, test loss: 0.195647\n",
      "Epoch 6, test loss: 0.196372\n",
      "Epoch 7, test loss: 0.198341\n",
      "Epoch 8, test loss: 0.196362\n",
      "Epoch 9, test loss: 0.198453\n",
      "Epoch 10, test loss: 0.195159\n",
      "Epoch 11, test loss: 0.195715\n",
      "Epoch 12, test loss: 0.195272\n",
      "Epoch 13, test loss: 0.194559\n",
      "Epoch 14, test loss: 0.196655\n",
      "Epoch 15, test loss: 0.195454\n",
      "Epoch 16, test loss: 0.197992\n",
      "Epoch 17, test loss: 0.195051\n",
      "Epoch 18, test loss: 0.196968\n",
      "Epoch 19, test loss: 0.195491\n",
      "Epoch 20, test loss: 0.199114\n",
      "Epoch 21, test loss: 0.198275\n",
      "Epoch 22, test loss: 0.197321\n",
      "Epoch 23, test loss: 0.195914\n",
      "Epoch 24, test loss: 0.197725\n",
      "Epoch 25, test loss: 0.196019\n",
      "Epoch 26, test loss: 0.195403\n",
      "Epoch 27, test loss: 0.195838\n",
      "Epoch 28, test loss: 0.195306\n",
      "Epoch 29, test loss: 0.195589\n",
      "Epoch 30, test loss: 0.195957\n",
      "Epoch 31, test loss: 0.195773\n",
      "Epoch 32, test loss: 0.196386\n",
      "Epoch 33, test loss: 0.195279\n",
      "Epoch 34, test loss: 0.197512\n",
      "Epoch 35, test loss: 0.194624\n",
      "Epoch 36, test loss: 0.196665\n",
      "Epoch 37, test loss: 0.194676\n",
      "Epoch 38, test loss: 0.195546\n",
      "Epoch 39, test loss: 0.197578\n",
      "Epoch 40, test loss: 0.196645\n",
      "Epoch 41, test loss: 0.196347\n",
      "Epoch 42, test loss: 0.195855\n",
      "Epoch 43, test loss: 0.201566\n",
      "Epoch 44, test loss: 0.197996\n",
      "Epoch 45, test loss: 0.196645\n",
      "Epoch 46, test loss: 0.195439\n",
      "Epoch 47, test loss: 0.196238\n",
      "Epoch 48, test loss: 0.198931\n",
      "Epoch 49, test loss: 0.195985\n",
      "Epoch 50, test loss: 0.198829\n",
      "Epoch 51, test loss: 0.195853\n",
      "Epoch 52, test loss: 0.198130\n",
      "Epoch 53, test loss: 0.194606\n",
      "Epoch 54, test loss: 0.196480\n",
      "Epoch 55, test loss: 0.194181\n",
      "Epoch 56, test loss: 0.195306\n",
      "Epoch 57, test loss: 0.194691\n",
      "Epoch 58, test loss: 0.194645\n",
      "Epoch 59, test loss: 0.195882\n",
      "Epoch 60, test loss: 0.195405\n",
      "Epoch 61, test loss: 0.197863\n",
      "Epoch 62, test loss: 0.200807\n",
      "Epoch 63, test loss: 0.194726\n",
      "Epoch 64, test loss: 0.195562\n",
      "Epoch 65, test loss: 0.196840\n",
      "Epoch 66, test loss: 0.199808\n",
      "Epoch 67, test loss: 0.195372\n",
      "Epoch 68, test loss: 0.194806\n",
      "Epoch 69, test loss: 0.196205\n",
      "Epoch 70, test loss: 0.196788\n",
      "Epoch 71, test loss: 0.194304\n",
      "Epoch 72, test loss: 0.195854\n",
      "Epoch 73, test loss: 0.196933\n",
      "Epoch 74, test loss: 0.204108\n",
      "Epoch 75, test loss: 0.194246\n",
      "Epoch 76, test loss: 0.194376\n",
      "Epoch 77, test loss: 0.194635\n",
      "Epoch 78, test loss: 0.197397\n",
      "Epoch 79, test loss: 0.198000\n",
      "Epoch 80, test loss: 0.199610\n",
      "Epoch 81, test loss: 0.195016\n",
      "Epoch 82, test loss: 0.196988\n",
      "Epoch 83, test loss: 0.196889\n",
      "Epoch 84, test loss: 0.195914\n",
      "Epoch 85, test loss: 0.200376\n",
      "Epoch 86, test loss: 0.193668\n",
      "Epoch 87, test loss: 0.198111\n",
      "Epoch 88, test loss: 0.195242\n",
      "Epoch 89, test loss: 0.193430\n",
      "Epoch 90, test loss: 0.195453\n",
      "Epoch 91, test loss: 0.194339\n",
      "Epoch 92, test loss: 0.193825\n",
      "Epoch 93, test loss: 0.195561\n",
      "Epoch 94, test loss: 0.194546\n",
      "Epoch 95, test loss: 0.194501\n",
      "Epoch 96, test loss: 0.196206\n",
      "Epoch 97, test loss: 0.203997\n",
      "Epoch 98, test loss: 0.194607\n",
      "Epoch 99, test loss: 0.198088\n",
      "Epoch 100, test loss: 0.196832\n",
      "Epoch 101, test loss: 0.195442\n",
      "Epoch 102, test loss: 0.195096\n",
      "Epoch 103, test loss: 0.195306\n",
      "Epoch 104, test loss: 0.195591\n",
      "Epoch 105, test loss: 0.193972\n",
      "Epoch 106, test loss: 0.194888\n",
      "Epoch 107, test loss: 0.194210\n",
      "Epoch 108, test loss: 0.193592\n",
      "Epoch 109, test loss: 0.195002\n",
      "Epoch 110, test loss: 0.197188\n",
      "Epoch 111, test loss: 0.194982\n",
      "Epoch 112, test loss: 0.193427\n",
      "Epoch 113, test loss: 0.194542\n",
      "Epoch 114, test loss: 0.195226\n",
      "Epoch 115, test loss: 0.194379\n",
      "Epoch 116, test loss: 0.194774\n",
      "Epoch 117, test loss: 0.194319\n",
      "Epoch 118, test loss: 0.196746\n",
      "Epoch 119, test loss: 0.194561\n",
      "Epoch 120, test loss: 0.199136\n",
      "Epoch 121, test loss: 0.194348\n",
      "Epoch 122, test loss: 0.198686\n",
      "Epoch 123, test loss: 0.199031\n",
      "Epoch 124, test loss: 0.195554\n",
      "Epoch 125, test loss: 0.195392\n",
      "Epoch 126, test loss: 0.199353\n",
      "Epoch 127, test loss: 0.195822\n",
      "Epoch 128, test loss: 0.195411\n",
      "Epoch 129, test loss: 0.196810\n",
      "Epoch 130, test loss: 0.194562\n",
      "Epoch 131, test loss: 0.194748\n",
      "Epoch 132, test loss: 0.194977\n",
      "Epoch 133, test loss: 0.196328\n",
      "Epoch 134, test loss: 0.193947\n",
      "Epoch 135, test loss: 0.196077\n",
      "Epoch 136, test loss: 0.194218\n",
      "Epoch 137, test loss: 0.195690\n",
      "Epoch 138, test loss: 0.194055\n",
      "Epoch 139, test loss: 0.194557\n",
      "Epoch 140, test loss: 0.197454\n",
      "Epoch 141, test loss: 0.194885\n",
      "Epoch 142, test loss: 0.194599\n",
      "Epoch 143, test loss: 0.197140\n",
      "Epoch 144, test loss: 0.197692\n",
      "Epoch 145, test loss: 0.194177\n",
      "Epoch 146, test loss: 0.197999\n",
      "Epoch 147, test loss: 0.202785\n",
      "Epoch 148, test loss: 0.195106\n",
      "Epoch 149, test loss: 0.194889\n",
      "Epoch 150, test loss: 0.195081\n",
      "Epoch 151, test loss: 0.195074\n",
      "Epoch 152, test loss: 0.196018\n",
      "Epoch 153, test loss: 0.194322\n",
      "Epoch 154, test loss: 0.195544\n",
      "Epoch 155, test loss: 0.198995\n",
      "Epoch 156, test loss: 0.194987\n",
      "Epoch 157, test loss: 0.194539\n",
      "Epoch 158, test loss: 0.199503\n",
      "Epoch 159, test loss: 0.195094\n",
      "Epoch 160, test loss: 0.194436\n",
      "Epoch 161, test loss: 0.194332\n",
      "Epoch 162, test loss: 0.195486\n",
      "Epoch 163, test loss: 0.197695\n",
      "Epoch 164, test loss: 0.200165\n",
      "Epoch 165, test loss: 0.194982\n",
      "Epoch 166, test loss: 0.194144\n",
      "Epoch 167, test loss: 0.195286\n",
      "Epoch 168, test loss: 0.194484\n",
      "Epoch 169, test loss: 0.197147\n",
      "Epoch 170, test loss: 0.203686\n",
      "Epoch 171, test loss: 0.194900\n",
      "Epoch 172, test loss: 0.197963\n",
      "Epoch 173, test loss: 0.194615\n",
      "Epoch 174, test loss: 0.194034\n",
      "Epoch 175, test loss: 0.197827\n",
      "Epoch 176, test loss: 0.195623\n",
      "Epoch 177, test loss: 0.194090\n",
      "Epoch 178, test loss: 0.195032\n",
      "Epoch 179, test loss: 0.193841\n",
      "Epoch 180, test loss: 0.195555\n",
      "Epoch 181, test loss: 0.194179\n",
      "Epoch 182, test loss: 0.206142\n",
      "Epoch 183, test loss: 0.193468\n",
      "Epoch 184, test loss: 0.196171\n",
      "Epoch 185, test loss: 0.194962\n",
      "Epoch 186, test loss: 0.196006\n",
      "Epoch 187, test loss: 0.194298\n",
      "Epoch 188, test loss: 0.196389\n",
      "Epoch 189, test loss: 0.196423\n",
      "Epoch 190, test loss: 0.192903\n",
      "Epoch 191, test loss: 0.193763\n",
      "Epoch 192, test loss: 0.199075\n",
      "Epoch 193, test loss: 0.193558\n",
      "Epoch 194, test loss: 0.195086\n",
      "Epoch 195, test loss: 0.194652\n",
      "Epoch 196, test loss: 0.200340\n",
      "Epoch 197, test loss: 0.194743\n",
      "Epoch 198, test loss: 0.196258\n",
      "Epoch 199, test loss: 0.195199\n",
      "Epoch 200, test loss: 0.194063\n",
      "Epoch 201, test loss: 0.194221\n",
      "Epoch 202, test loss: 0.198163\n",
      "Epoch 203, test loss: 0.193563\n",
      "Epoch 204, test loss: 0.194149\n",
      "Epoch 205, test loss: 0.192796\n",
      "Epoch 206, test loss: 0.195687\n",
      "Epoch 207, test loss: 0.195739\n",
      "Epoch 208, test loss: 0.194103\n",
      "Epoch 209, test loss: 0.192920\n",
      "Epoch 210, test loss: 0.195337\n",
      "Epoch 211, test loss: 0.196179\n",
      "Epoch 212, test loss: 0.195274\n",
      "Epoch 213, test loss: 0.192462\n",
      "Epoch 214, test loss: 0.194993\n",
      "Epoch 215, test loss: 0.193986\n",
      "Epoch 216, test loss: 0.193628\n",
      "Epoch 217, test loss: 0.194500\n",
      "Epoch 218, test loss: 0.193920\n",
      "Epoch 219, test loss: 0.195165\n",
      "Epoch 220, test loss: 0.192127\n",
      "Epoch 221, test loss: 0.193116\n",
      "Epoch 222, test loss: 0.192717\n",
      "Epoch 223, test loss: 0.193855\n",
      "Epoch 224, test loss: 0.198973\n",
      "Epoch 225, test loss: 0.193232\n",
      "Epoch 226, test loss: 0.194211\n",
      "Epoch 227, test loss: 0.192960\n",
      "Epoch 228, test loss: 0.193814\n",
      "Epoch 229, test loss: 0.193689\n",
      "Epoch 230, test loss: 0.193344\n",
      "Epoch 231, test loss: 0.192558\n",
      "Epoch 232, test loss: 0.192522\n",
      "Epoch 233, test loss: 0.194801\n",
      "Epoch 234, test loss: 0.192011\n",
      "Epoch 235, test loss: 0.200418\n",
      "Epoch 236, test loss: 0.194985\n",
      "Epoch 237, test loss: 0.192983\n",
      "Epoch 238, test loss: 0.193900\n",
      "Epoch 239, test loss: 0.193735\n",
      "Pretrain data: 19365644.0\n",
      "Building dataset, requesting data from 0 to 821\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6689/94751\n",
      "Found 821 continuous time series\n",
      "Data shape: (101442, 24), Train/test: 101440/2\n",
      "Train test ratio: 50720.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1585\n",
      "Epoch 0, train loss: 0.183918\n",
      "Epoch 1, train loss: 0.213065\n",
      "Epoch 2, train loss: 0.234708\n",
      "Epoch 3, train loss: 0.175749\n",
      "Epoch 4, train loss: 0.245881\n",
      "Epoch 5, train loss: 0.210084\n",
      "Epoch 6, train loss: 0.201919\n",
      "Epoch 7, train loss: 0.198996\n",
      "Epoch 8, train loss: 0.218467\n",
      "Epoch 9, train loss: 0.239176\n",
      "Epoch 10, train loss: 0.218957\n",
      "Epoch 11, train loss: 0.166047\n",
      "Epoch 12, train loss: 0.219586\n",
      "Epoch 13, train loss: 0.223501\n",
      "Epoch 14, train loss: 0.257437\n",
      "Epoch 15, train loss: 0.174925\n",
      "Epoch 16, train loss: 0.219197\n",
      "Epoch 17, train loss: 0.178098\n",
      "Epoch 18, train loss: 0.212832\n",
      "Epoch 19, train loss: 0.213518\n",
      "Epoch 20, train loss: 0.215127\n",
      "Epoch 21, train loss: 0.203102\n",
      "Epoch 22, train loss: 0.203981\n",
      "Epoch 23, train loss: 0.219072\n",
      "Epoch 24, train loss: 0.184708\n",
      "Epoch 25, train loss: 0.237469\n",
      "Epoch 26, train loss: 0.198642\n",
      "Epoch 27, train loss: 0.179140\n",
      "Epoch 28, train loss: 0.238669\n",
      "Epoch 29, train loss: 0.198896\n",
      "Epoch 30, train loss: 0.202343\n",
      "Epoch 31, train loss: 0.212682\n",
      "Epoch 32, train loss: 0.153760\n",
      "Epoch 33, train loss: 0.205273\n",
      "Epoch 34, train loss: 0.168274\n",
      "Epoch 35, train loss: 0.260683\n",
      "Epoch 36, train loss: 0.183791\n",
      "Epoch 37, train loss: 0.191981\n",
      "Epoch 38, train loss: 0.187932\n",
      "Epoch 39, train loss: 0.187213\n",
      "Epoch 40, train loss: 0.204087\n",
      "Epoch 41, train loss: 0.243274\n",
      "Epoch 42, train loss: 0.202690\n",
      "Epoch 43, train loss: 0.199948\n",
      "Epoch 44, train loss: 0.216493\n",
      "Epoch 45, train loss: 0.162229\n",
      "Epoch 46, train loss: 0.183143\n",
      "Epoch 47, train loss: 0.283282\n",
      "Epoch 48, train loss: 0.201991\n",
      "Epoch 49, train loss: 0.158006\n",
      "Epoch 50, train loss: 0.185635\n",
      "Epoch 51, train loss: 0.196630\n",
      "Epoch 52, train loss: 0.229077\n",
      "Epoch 53, train loss: 0.213236\n",
      "Epoch 54, train loss: 0.215937\n",
      "Epoch 55, train loss: 0.241052\n",
      "Epoch 56, train loss: 0.222282\n",
      "Epoch 57, train loss: 0.203722\n",
      "Epoch 58, train loss: 0.159685\n",
      "Epoch 59, train loss: 0.292586\n",
      "Epoch 60, train loss: 0.186984\n",
      "Epoch 61, train loss: 0.184082\n",
      "Epoch 62, train loss: 0.245579\n",
      "Epoch 63, train loss: 0.272294\n",
      "Epoch 64, train loss: 0.203659\n",
      "Epoch 65, train loss: 0.138843\n",
      "Epoch 66, train loss: 0.195009\n",
      "Epoch 67, train loss: 0.218487\n",
      "Epoch 68, train loss: 0.181147\n",
      "Epoch 69, train loss: 0.153297\n",
      "Epoch 70, train loss: 0.246658\n",
      "Epoch 71, train loss: 0.194177\n",
      "Epoch 72, train loss: 0.217911\n",
      "Epoch 73, train loss: 0.227604\n",
      "Epoch 74, train loss: 0.299243\n",
      "Epoch 75, train loss: 0.187798\n",
      "Epoch 76, train loss: 0.265454\n",
      "Epoch 77, train loss: 0.192412\n",
      "Epoch 78, train loss: 0.216178\n",
      "Epoch 79, train loss: 0.161290\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "x here is\n",
      "[[135. 143. 152. ... 222. 226. 229.]\n",
      " [143. 152. 159. ... 226. 229. 232.]\n",
      " [152. 159. 166. ... 229. 232. 234.]\n",
      " ...\n",
      " [165. 167. 167. ... 204. 202. 201.]\n",
      " [167. 167. 166. ... 202. 201. 201.]\n",
      " [167. 166. 167. ... 201. 201. 201.]]\n",
      "y here is\n",
      "[[241. 241. 241. ... 241. 241. 241.]\n",
      " [244. 244. 244. ... 244. 244. 244.]\n",
      " [249. 249. 249. ... 249. 249. 249.]\n",
      " ...\n",
      " [212. 212. 212. ... 212. 212. 212.]\n",
      " [218. 218. 218. ... 218. 218. 218.]\n",
      " [224. 224. 224. ... 224. 224. 224.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (2455, 24), Train/test: 1/2454\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "x here is\n",
      "[[101. 100. 100. ...  93.  95.  96.]\n",
      " [100. 100.  99. ...  95.  96.  93.]\n",
      " [100.  99.  98. ...  96.  93.  93.]\n",
      " ...\n",
      " [104.  97.  91. ...  74.  76.  79.]\n",
      " [ 97.  91.  88. ...  76.  79.  87.]\n",
      " [ 91.  88.  85. ...  79.  87.  95.]]\n",
      "y here is\n",
      "[[ 94.  94.  94. ...  94.  94.  94.]\n",
      " [ 92.  92.  92. ...  92.  92.  92.]\n",
      " [102. 102. 102. ... 102. 102. 102.]\n",
      " ...\n",
      " [120. 120. 120. ... 120. 120. 120.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [128. 128. 128. ... 128. 128. 128.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 420/9952\n",
      "Found 21 continuous time series\n",
      "Data shape: (10374, 24), Train/test: 10372/2\n",
      "Train test ratio: 5186.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4771336D0>\n",
      "Epoch 0, test loss: 0.154536\n",
      "Epoch 1, test loss: 0.159850\n",
      "Epoch 2, test loss: 0.160508\n",
      "Epoch 3, test loss: 0.164580\n",
      "Epoch 4, test loss: 0.164728\n",
      "Epoch 5, test loss: 0.157234\n",
      "Epoch 6, test loss: 0.159122\n",
      "Epoch 7, test loss: 0.155531\n",
      "Epoch 8, test loss: 0.157362\n",
      "Epoch 9, test loss: 0.159343\n",
      "Epoch 10, test loss: 0.154644\n",
      "Epoch 11, test loss: 0.156181\n",
      "Epoch 12, test loss: 0.164185\n",
      "Epoch 13, test loss: 0.155532\n",
      "Epoch 14, test loss: 0.158547\n",
      "Epoch 15, test loss: 0.160243\n",
      "Epoch 16, test loss: 0.163357\n",
      "Epoch 17, test loss: 0.155440\n",
      "Epoch 18, test loss: 0.155790\n",
      "Epoch 19, test loss: 0.155261\n",
      "Epoch 20, test loss: 0.166417\n",
      "Epoch 21, test loss: 0.157750\n",
      "Epoch 22, test loss: 0.154727\n",
      "Epoch 23, test loss: 0.164782\n",
      "Epoch 24, test loss: 0.155332\n",
      "Epoch 25, test loss: 0.161525\n",
      "Epoch 26, test loss: 0.161982\n",
      "Epoch 27, test loss: 0.154390\n",
      "Epoch 28, test loss: 0.157943\n",
      "Epoch 29, test loss: 0.154826\n",
      "Epoch 30, test loss: 0.154574\n",
      "Epoch 31, test loss: 0.157033\n",
      "Epoch 32, test loss: 0.166293\n",
      "Epoch 33, test loss: 0.159470\n",
      "Epoch 34, test loss: 0.158346\n",
      "Epoch 35, test loss: 0.155254\n",
      "Epoch 36, test loss: 0.156554\n",
      "Epoch 37, test loss: 0.154913\n",
      "Epoch 38, test loss: 0.157607\n",
      "Epoch 39, test loss: 0.155883\n",
      "Epoch 40, test loss: 0.155204\n",
      "Epoch 41, test loss: 0.157101\n",
      "Epoch 42, test loss: 0.164353\n",
      "Epoch 43, test loss: 0.156695\n",
      "Epoch 44, test loss: 0.157853\n",
      "Epoch 45, test loss: 0.159913\n",
      "Epoch 46, test loss: 0.164446\n",
      "Epoch 47, test loss: 0.159385\n",
      "Epoch 48, test loss: 0.158565\n",
      "Epoch 49, test loss: 0.157602\n",
      "Epoch 50, test loss: 0.154052\n",
      "Epoch 51, test loss: 0.154636\n",
      "Epoch 52, test loss: 0.159677\n",
      "Epoch 53, test loss: 0.158611\n",
      "Epoch 54, test loss: 0.175524\n",
      "Epoch 55, test loss: 0.156668\n",
      "Epoch 56, test loss: 0.155007\n",
      "Epoch 57, test loss: 0.155283\n",
      "Epoch 58, test loss: 0.165622\n",
      "Epoch 59, test loss: 0.157455\n",
      "Epoch 60, test loss: 0.159108\n",
      "Epoch 61, test loss: 0.157037\n",
      "Epoch 62, test loss: 0.161920\n",
      "Epoch 63, test loss: 0.155173\n",
      "Epoch 64, test loss: 0.158716\n",
      "Epoch 65, test loss: 0.154790\n",
      "Epoch 66, test loss: 0.157765\n",
      "Epoch 67, test loss: 0.155655\n",
      "Epoch 68, test loss: 0.159298\n",
      "Epoch 69, test loss: 0.154963\n",
      "Epoch 70, test loss: 0.155290\n",
      "Epoch 71, test loss: 0.156606\n",
      "Epoch 72, test loss: 0.155175\n",
      "Epoch 73, test loss: 0.155661\n",
      "Epoch 74, test loss: 0.156134\n",
      "Epoch 75, test loss: 0.159065\n",
      "Epoch 76, test loss: 0.155579\n",
      "Epoch 77, test loss: 0.155864\n",
      "Epoch 78, test loss: 0.156451\n",
      "Epoch 79, test loss: 0.158691\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4771336D0>\n",
      "Epoch 0, test loss: 0.156891\n",
      "Epoch 1, test loss: 0.158619\n",
      "Epoch 2, test loss: 0.154684\n",
      "Epoch 3, test loss: 0.161972\n",
      "Epoch 4, test loss: 0.159498\n",
      "Epoch 5, test loss: 0.168845\n",
      "Epoch 6, test loss: 0.155549\n",
      "Epoch 7, test loss: 0.159558\n",
      "Epoch 8, test loss: 0.156785\n",
      "Epoch 9, test loss: 0.157256\n",
      "Epoch 10, test loss: 0.156152\n",
      "Epoch 11, test loss: 0.165503\n",
      "Epoch 12, test loss: 0.155893\n",
      "Epoch 13, test loss: 0.155943\n",
      "Epoch 14, test loss: 0.155820\n",
      "Epoch 15, test loss: 0.155195\n",
      "Epoch 16, test loss: 0.156275\n",
      "Epoch 17, test loss: 0.156468\n",
      "Epoch 18, test loss: 0.163402\n",
      "Epoch 19, test loss: 0.155083\n",
      "Epoch 20, test loss: 0.159143\n",
      "Epoch 21, test loss: 0.161194\n",
      "Epoch 22, test loss: 0.155301\n",
      "Epoch 23, test loss: 0.155675\n",
      "Epoch 24, test loss: 0.161214\n",
      "Epoch 25, test loss: 0.156742\n",
      "Epoch 26, test loss: 0.159310\n",
      "Epoch 27, test loss: 0.155742\n",
      "Epoch 28, test loss: 0.156818\n",
      "Epoch 29, test loss: 0.155125\n",
      "Epoch 30, test loss: 0.163036\n",
      "Epoch 31, test loss: 0.155989\n",
      "Epoch 32, test loss: 0.157045\n",
      "Epoch 33, test loss: 0.162345\n",
      "Epoch 34, test loss: 0.155678\n",
      "Epoch 35, test loss: 0.155030\n",
      "Epoch 36, test loss: 0.155347\n",
      "Epoch 37, test loss: 0.158411\n",
      "Epoch 38, test loss: 0.158206\n",
      "Epoch 39, test loss: 0.157250\n",
      "Epoch 40, test loss: 0.155222\n",
      "Epoch 41, test loss: 0.155029\n",
      "Epoch 42, test loss: 0.156133\n",
      "Epoch 43, test loss: 0.161361\n",
      "Epoch 44, test loss: 0.155482\n",
      "Epoch 45, test loss: 0.156038\n",
      "Epoch 46, test loss: 0.156285\n",
      "Epoch 47, test loss: 0.155016\n",
      "Epoch 48, test loss: 0.158697\n",
      "Epoch 49, test loss: 0.157980\n",
      "Epoch 50, test loss: 0.155731\n",
      "Epoch 51, test loss: 0.157102\n",
      "Epoch 52, test loss: 0.155387\n",
      "Epoch 53, test loss: 0.155492\n",
      "Epoch 54, test loss: 0.157712\n",
      "Epoch 55, test loss: 0.155013\n",
      "Epoch 56, test loss: 0.156507\n",
      "Epoch 57, test loss: 0.155059\n",
      "Epoch 58, test loss: 0.155416\n",
      "Epoch 59, test loss: 0.155128\n",
      "Epoch 60, test loss: 0.157725\n",
      "Epoch 61, test loss: 0.155521\n",
      "Epoch 62, test loss: 0.157500\n",
      "Epoch 63, test loss: 0.162896\n",
      "Epoch 64, test loss: 0.156900\n",
      "Epoch 65, test loss: 0.155188\n",
      "Epoch 66, test loss: 0.155510\n",
      "Epoch 67, test loss: 0.155476\n",
      "Epoch 68, test loss: 0.155578\n",
      "Epoch 69, test loss: 0.155235\n",
      "Epoch 70, test loss: 0.155714\n",
      "Epoch 71, test loss: 0.155588\n",
      "Epoch 72, test loss: 0.165271\n",
      "Epoch 73, test loss: 0.156906\n",
      "Epoch 74, test loss: 0.155748\n",
      "Epoch 75, test loss: 0.156218\n",
      "Epoch 76, test loss: 0.155427\n",
      "Epoch 77, test loss: 0.156970\n",
      "Epoch 78, test loss: 0.155421\n",
      "Epoch 79, test loss: 0.157677\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4771336D0>\n",
      "Epoch 0, test loss: 0.283068\n",
      "Epoch 1, test loss: 0.226381\n",
      "Epoch 2, test loss: 0.180247\n",
      "Epoch 3, test loss: 0.171678\n",
      "Epoch 4, test loss: 0.167729\n",
      "Epoch 5, test loss: 0.166230\n",
      "Epoch 6, test loss: 0.168983\n",
      "Epoch 7, test loss: 0.164387\n",
      "Epoch 8, test loss: 0.164239\n",
      "Epoch 9, test loss: 0.164033\n",
      "Epoch 10, test loss: 0.162335\n",
      "Epoch 11, test loss: 0.164229\n",
      "Epoch 12, test loss: 0.168386\n",
      "Epoch 13, test loss: 0.164602\n",
      "Epoch 14, test loss: 0.161517\n",
      "Epoch 15, test loss: 0.161297\n",
      "Epoch 16, test loss: 0.164866\n",
      "Epoch 17, test loss: 0.160821\n",
      "Epoch 18, test loss: 0.160254\n",
      "Epoch 19, test loss: 0.161637\n",
      "Epoch 20, test loss: 0.161826\n",
      "Epoch 21, test loss: 0.161507\n",
      "Epoch 22, test loss: 0.163350\n",
      "Epoch 23, test loss: 0.164095\n",
      "Epoch 24, test loss: 0.160649\n",
      "Epoch 25, test loss: 0.161909\n",
      "Epoch 26, test loss: 0.160426\n",
      "Epoch 27, test loss: 0.163464\n",
      "Epoch 28, test loss: 0.161285\n",
      "Epoch 29, test loss: 0.161571\n",
      "Epoch 30, test loss: 0.160841\n",
      "Epoch 31, test loss: 0.160474\n",
      "Epoch 32, test loss: 0.161137\n",
      "Epoch 33, test loss: 0.162169\n",
      "Epoch 34, test loss: 0.160659\n",
      "Epoch 35, test loss: 0.161408\n",
      "Epoch 36, test loss: 0.169520\n",
      "Epoch 37, test loss: 0.160226\n",
      "Epoch 38, test loss: 0.162518\n",
      "Epoch 39, test loss: 0.162109\n",
      "Epoch 40, test loss: 0.160317\n",
      "Epoch 41, test loss: 0.168528\n",
      "Epoch 42, test loss: 0.160462\n",
      "Epoch 43, test loss: 0.159454\n",
      "Epoch 44, test loss: 0.161474\n",
      "Epoch 45, test loss: 0.159818\n",
      "Epoch 46, test loss: 0.163406\n",
      "Epoch 47, test loss: 0.163342\n",
      "Epoch 48, test loss: 0.163643\n",
      "Epoch 49, test loss: 0.161990\n",
      "Epoch 50, test loss: 0.160350\n",
      "Epoch 51, test loss: 0.161313\n",
      "Epoch 52, test loss: 0.159642\n",
      "Epoch 53, test loss: 0.162539\n",
      "Epoch 54, test loss: 0.162198\n",
      "Epoch 55, test loss: 0.158539\n",
      "Epoch 56, test loss: 0.162742\n",
      "Epoch 57, test loss: 0.157799\n",
      "Epoch 58, test loss: 0.159304\n",
      "Epoch 59, test loss: 0.158366\n",
      "Epoch 60, test loss: 0.157592\n",
      "Epoch 61, test loss: 0.158063\n",
      "Epoch 62, test loss: 0.158642\n",
      "Epoch 63, test loss: 0.160595\n",
      "Epoch 64, test loss: 0.159933\n",
      "Epoch 65, test loss: 0.169865\n",
      "Epoch 66, test loss: 0.157727\n",
      "Epoch 67, test loss: 0.158623\n",
      "Epoch 68, test loss: 0.164218\n",
      "Epoch 69, test loss: 0.157255\n",
      "Epoch 70, test loss: 0.157378\n",
      "Epoch 71, test loss: 0.159174\n",
      "Epoch 72, test loss: 0.160815\n",
      "Epoch 73, test loss: 0.163430\n",
      "Epoch 74, test loss: 0.157153\n",
      "Epoch 75, test loss: 0.158410\n",
      "Epoch 76, test loss: 0.161164\n",
      "Epoch 77, test loss: 0.158308\n",
      "Epoch 78, test loss: 0.162224\n",
      "Epoch 79, test loss: 0.159075\n",
      "Epoch 80, test loss: 0.161357\n",
      "Epoch 81, test loss: 0.157292\n",
      "Epoch 82, test loss: 0.157407\n",
      "Epoch 83, test loss: 0.159300\n",
      "Epoch 84, test loss: 0.158123\n",
      "Epoch 85, test loss: 0.156950\n",
      "Epoch 86, test loss: 0.165471\n",
      "Epoch 87, test loss: 0.160037\n",
      "Epoch 88, test loss: 0.157486\n",
      "Epoch 89, test loss: 0.158387\n",
      "Epoch 90, test loss: 0.156640\n",
      "Epoch 91, test loss: 0.156665\n",
      "Epoch 92, test loss: 0.159613\n",
      "Epoch 93, test loss: 0.156897\n",
      "Epoch 94, test loss: 0.157068\n",
      "Epoch 95, test loss: 0.157336\n",
      "Epoch 96, test loss: 0.158316\n",
      "Epoch 97, test loss: 0.157510\n",
      "Epoch 98, test loss: 0.158100\n",
      "Epoch 99, test loss: 0.162709\n",
      "Epoch 100, test loss: 0.163172\n",
      "Epoch 101, test loss: 0.157153\n",
      "Epoch 102, test loss: 0.157002\n",
      "Epoch 103, test loss: 0.156385\n",
      "Epoch 104, test loss: 0.156365\n",
      "Epoch 105, test loss: 0.158521\n",
      "Epoch 106, test loss: 0.156369\n",
      "Epoch 107, test loss: 0.168687\n",
      "Epoch 108, test loss: 0.157699\n",
      "Epoch 109, test loss: 0.162227\n",
      "Epoch 110, test loss: 0.156422\n",
      "Epoch 111, test loss: 0.156503\n",
      "Epoch 112, test loss: 0.161898\n",
      "Epoch 113, test loss: 0.157018\n",
      "Epoch 114, test loss: 0.156966\n",
      "Epoch 115, test loss: 0.156411\n",
      "Epoch 116, test loss: 0.156208\n",
      "Epoch 117, test loss: 0.165839\n",
      "Epoch 118, test loss: 0.156951\n",
      "Epoch 119, test loss: 0.161447\n",
      "Epoch 120, test loss: 0.157161\n",
      "Epoch 121, test loss: 0.166287\n",
      "Epoch 122, test loss: 0.161441\n",
      "Epoch 123, test loss: 0.156371\n",
      "Epoch 124, test loss: 0.156179\n",
      "Epoch 125, test loss: 0.165958\n",
      "Epoch 126, test loss: 0.157025\n",
      "Epoch 127, test loss: 0.161974\n",
      "Epoch 128, test loss: 0.155932\n",
      "Epoch 129, test loss: 0.161270\n",
      "Epoch 130, test loss: 0.157918\n",
      "Epoch 131, test loss: 0.156636\n",
      "Epoch 132, test loss: 0.160893\n",
      "Epoch 133, test loss: 0.156014\n",
      "Epoch 134, test loss: 0.156132\n",
      "Epoch 135, test loss: 0.156141\n",
      "Epoch 136, test loss: 0.158792\n",
      "Epoch 137, test loss: 0.156713\n",
      "Epoch 138, test loss: 0.156085\n",
      "Epoch 139, test loss: 0.162966\n",
      "Epoch 140, test loss: 0.157782\n",
      "Epoch 141, test loss: 0.157381\n",
      "Epoch 142, test loss: 0.156056\n",
      "Epoch 143, test loss: 0.157537\n",
      "Epoch 144, test loss: 0.158067\n",
      "Epoch 145, test loss: 0.158728\n",
      "Epoch 146, test loss: 0.156781\n",
      "Epoch 147, test loss: 0.157769\n",
      "Epoch 148, test loss: 0.156719\n",
      "Epoch 149, test loss: 0.157123\n",
      "Epoch 150, test loss: 0.156086\n",
      "Epoch 151, test loss: 0.158578\n",
      "Epoch 152, test loss: 0.155783\n",
      "Epoch 153, test loss: 0.155869\n",
      "Epoch 154, test loss: 0.157742\n",
      "Epoch 155, test loss: 0.155758\n",
      "Epoch 156, test loss: 0.156637\n",
      "Epoch 157, test loss: 0.156189\n",
      "Epoch 158, test loss: 0.158727\n",
      "Epoch 159, test loss: 0.158325\n",
      "Epoch 160, test loss: 0.158273\n",
      "Epoch 161, test loss: 0.160264\n",
      "Epoch 162, test loss: 0.160078\n",
      "Epoch 163, test loss: 0.155837\n",
      "Epoch 164, test loss: 0.155791\n",
      "Epoch 165, test loss: 0.157928\n",
      "Epoch 166, test loss: 0.165334\n",
      "Epoch 167, test loss: 0.159688\n",
      "Epoch 168, test loss: 0.155702\n",
      "Epoch 169, test loss: 0.162653\n",
      "Epoch 170, test loss: 0.159836\n",
      "Epoch 171, test loss: 0.156524\n",
      "Epoch 172, test loss: 0.158711\n",
      "Epoch 173, test loss: 0.158861\n",
      "Epoch 174, test loss: 0.159115\n",
      "Epoch 175, test loss: 0.155853\n",
      "Epoch 176, test loss: 0.157800\n",
      "Epoch 177, test loss: 0.157088\n",
      "Epoch 178, test loss: 0.156063\n",
      "Epoch 179, test loss: 0.155947\n",
      "Epoch 180, test loss: 0.155775\n",
      "Epoch 181, test loss: 0.159889\n",
      "Epoch 182, test loss: 0.158952\n",
      "Epoch 183, test loss: 0.155730\n",
      "Epoch 184, test loss: 0.156586\n",
      "Epoch 185, test loss: 0.161379\n",
      "Epoch 186, test loss: 0.155909\n",
      "Epoch 187, test loss: 0.155719\n",
      "Epoch 188, test loss: 0.159639\n",
      "Epoch 189, test loss: 0.156063\n",
      "Epoch 190, test loss: 0.155907\n",
      "Epoch 191, test loss: 0.155803\n",
      "Epoch 192, test loss: 0.156175\n",
      "Epoch 193, test loss: 0.161414\n",
      "Epoch 194, test loss: 0.157164\n",
      "Epoch 195, test loss: 0.160551\n",
      "Epoch 196, test loss: 0.157879\n",
      "Epoch 197, test loss: 0.162562\n",
      "Epoch 198, test loss: 0.157096\n",
      "Epoch 199, test loss: 0.158197\n",
      "Epoch 200, test loss: 0.158984\n",
      "Epoch 201, test loss: 0.163121\n",
      "Epoch 202, test loss: 0.155731\n",
      "Epoch 203, test loss: 0.156862\n",
      "Epoch 204, test loss: 0.156244\n",
      "Epoch 205, test loss: 0.156120\n",
      "Epoch 206, test loss: 0.164171\n",
      "Epoch 207, test loss: 0.156760\n",
      "Epoch 208, test loss: 0.156181\n",
      "Epoch 209, test loss: 0.161688\n",
      "Epoch 210, test loss: 0.155866\n",
      "Epoch 211, test loss: 0.157467\n",
      "Epoch 212, test loss: 0.156814\n",
      "Epoch 213, test loss: 0.160717\n",
      "Epoch 214, test loss: 0.157933\n",
      "Epoch 215, test loss: 0.156660\n",
      "Epoch 216, test loss: 0.156648\n",
      "Epoch 217, test loss: 0.155911\n",
      "Epoch 218, test loss: 0.158030\n",
      "Epoch 219, test loss: 0.156372\n",
      "Epoch 220, test loss: 0.156120\n",
      "Epoch 221, test loss: 0.163128\n",
      "Epoch 222, test loss: 0.164855\n",
      "Epoch 223, test loss: 0.164806\n",
      "Epoch 224, test loss: 0.156514\n",
      "Epoch 225, test loss: 0.158262\n",
      "Epoch 226, test loss: 0.155921\n",
      "Epoch 227, test loss: 0.155804\n",
      "Epoch 228, test loss: 0.155836\n",
      "Epoch 229, test loss: 0.157842\n",
      "Epoch 230, test loss: 0.156226\n",
      "Epoch 231, test loss: 0.156474\n",
      "Epoch 232, test loss: 0.156230\n",
      "Epoch 233, test loss: 0.158541\n",
      "Epoch 234, test loss: 0.156993\n",
      "Epoch 235, test loss: 0.158101\n",
      "Epoch 236, test loss: 0.156259\n",
      "Epoch 237, test loss: 0.156192\n",
      "Epoch 238, test loss: 0.157367\n",
      "Epoch 239, test loss: 0.158809\n",
      "Pretrain data: 19339240.0\n",
      "Building dataset, requesting data from 0 to 831\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6825/92647\n",
      "Found 831 continuous time series\n",
      "Data shape: (99474, 24), Train/test: 99472/2\n",
      "Train test ratio: 49736.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1554\n",
      "Epoch 0, train loss: 0.263159\n",
      "Epoch 1, train loss: 0.280375\n",
      "Epoch 2, train loss: 0.244167\n",
      "Epoch 3, train loss: 0.226986\n",
      "Epoch 4, train loss: 0.247935\n",
      "Epoch 5, train loss: 0.216901\n",
      "Epoch 6, train loss: 0.219530\n",
      "Epoch 7, train loss: 0.171957\n",
      "Epoch 8, train loss: 0.193115\n",
      "Epoch 9, train loss: 0.186710\n",
      "Epoch 10, train loss: 0.173472\n",
      "Epoch 11, train loss: 0.243493\n",
      "Epoch 12, train loss: 0.233896\n",
      "Epoch 13, train loss: 0.250895\n",
      "Epoch 14, train loss: 0.181663\n",
      "Epoch 15, train loss: 0.128910\n",
      "Epoch 16, train loss: 0.190217\n",
      "Epoch 17, train loss: 0.264074\n",
      "Epoch 18, train loss: 0.172557\n",
      "Epoch 19, train loss: 0.208276\n",
      "Epoch 20, train loss: 0.163305\n",
      "Epoch 21, train loss: 0.196137\n",
      "Epoch 22, train loss: 0.196909\n",
      "Epoch 23, train loss: 0.213010\n",
      "Epoch 24, train loss: 0.175379\n",
      "Epoch 25, train loss: 0.186444\n",
      "Epoch 26, train loss: 0.186433\n",
      "Epoch 27, train loss: 0.204091\n",
      "Epoch 28, train loss: 0.203377\n",
      "Epoch 29, train loss: 0.253354\n",
      "Epoch 30, train loss: 0.242384\n",
      "Epoch 31, train loss: 0.193179\n",
      "Epoch 32, train loss: 0.275695\n",
      "Epoch 33, train loss: 0.201003\n",
      "Epoch 34, train loss: 0.198097\n",
      "Epoch 35, train loss: 0.191752\n",
      "Epoch 36, train loss: 0.274782\n",
      "Epoch 37, train loss: 0.170667\n",
      "Epoch 38, train loss: 0.270291\n",
      "Epoch 39, train loss: 0.205641\n",
      "Epoch 40, train loss: 0.168845\n",
      "Epoch 41, train loss: 0.231114\n",
      "Epoch 42, train loss: 0.183498\n",
      "Epoch 43, train loss: 0.166241\n",
      "Epoch 44, train loss: 0.171391\n",
      "Epoch 45, train loss: 0.177950\n",
      "Epoch 46, train loss: 0.181439\n",
      "Epoch 47, train loss: 0.298868\n",
      "Epoch 48, train loss: 0.205284\n",
      "Epoch 49, train loss: 0.169146\n",
      "Epoch 50, train loss: 0.209378\n",
      "Epoch 51, train loss: 0.209480\n",
      "Epoch 52, train loss: 0.226679\n",
      "Epoch 53, train loss: 0.190781\n",
      "Epoch 54, train loss: 0.177624\n",
      "Epoch 55, train loss: 0.250462\n",
      "Epoch 56, train loss: 0.151311\n",
      "Epoch 57, train loss: 0.173830\n",
      "Epoch 58, train loss: 0.224028\n",
      "Epoch 59, train loss: 0.164460\n",
      "Epoch 60, train loss: 0.192621\n",
      "Epoch 61, train loss: 0.160910\n",
      "Epoch 62, train loss: 0.235249\n",
      "Epoch 63, train loss: 0.244560\n",
      "Epoch 64, train loss: 0.154641\n",
      "Epoch 65, train loss: 0.161529\n",
      "Epoch 66, train loss: 0.181401\n",
      "Epoch 67, train loss: 0.189862\n",
      "Epoch 68, train loss: 0.209559\n",
      "Epoch 69, train loss: 0.239045\n",
      "Epoch 70, train loss: 0.198879\n",
      "Epoch 71, train loss: 0.196481\n",
      "Epoch 72, train loss: 0.247538\n",
      "Epoch 73, train loss: 0.197437\n",
      "Epoch 74, train loss: 0.233198\n",
      "Epoch 75, train loss: 0.295875\n",
      "Epoch 76, train loss: 0.222119\n",
      "Epoch 77, train loss: 0.227244\n",
      "Epoch 78, train loss: 0.230355\n",
      "Epoch 79, train loss: 0.170840\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "x here is\n",
      "[[127. 123. 118. ...  91.  99. 107.]\n",
      " [123. 118. 112. ...  99. 107. 117.]\n",
      " [118. 112. 108. ... 107. 117. 125.]\n",
      " ...\n",
      " [173. 166. 168. ... 254. 263. 280.]\n",
      " [166. 168. 169. ... 263. 280. 288.]\n",
      " [168. 169. 169. ... 280. 288. 301.]]\n",
      "y here is\n",
      "[[138. 138. 138. ... 138. 138. 138.]\n",
      " [150. 150. 150. ... 150. 150. 150.]\n",
      " [157. 157. 157. ... 157. 157. 157.]\n",
      " ...\n",
      " [307. 307. 307. ... 307. 307. 307.]\n",
      " [311. 311. 311. ... 311. 311. 311.]\n",
      " [321. 321. 321. ... 321. 321. 321.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (2704, 24), Train/test: 1/2703\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[116. 117. 119. ... 152. 152. 152.]\n",
      " [117. 119. 116. ... 152. 152. 151.]\n",
      " [119. 116. 111. ... 152. 151. 151.]\n",
      " ...\n",
      " [173. 178. 183. ... 180. 175. 171.]\n",
      " [178. 183. 188. ... 175. 171. 168.]\n",
      " [183. 188. 193. ... 171. 168. 162.]]\n",
      "y here is\n",
      "[[147. 147. 147. ... 147. 147. 147.]\n",
      " [147. 147. 147. ... 147. 147. 147.]\n",
      " [149. 149. 149. ... 149. 149. 149.]\n",
      " ...\n",
      " [140. 140. 140. ... 140. 140. 140.]\n",
      " [137. 137. 137. ... 137. 137. 137.]\n",
      " [132. 132. 132. ... 132. 132. 132.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 284/12056\n",
      "Found 11 continuous time series\n",
      "Data shape: (12342, 24), Train/test: 12340/2\n",
      "Train test ratio: 6170.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310D240>\n",
      "Epoch 0, test loss: 0.191558\n",
      "Epoch 1, test loss: 0.187826\n",
      "Epoch 2, test loss: 0.187638\n",
      "Epoch 3, test loss: 0.190113\n",
      "Epoch 4, test loss: 0.188647\n",
      "Epoch 5, test loss: 0.188040\n",
      "Epoch 6, test loss: 0.186054\n",
      "Epoch 7, test loss: 0.198222\n",
      "Epoch 8, test loss: 0.186179\n",
      "Epoch 9, test loss: 0.186917\n",
      "Epoch 10, test loss: 0.188053\n",
      "Epoch 11, test loss: 0.186648\n",
      "Epoch 12, test loss: 0.191922\n",
      "Epoch 13, test loss: 0.185817\n",
      "Epoch 14, test loss: 0.187075\n",
      "Epoch 15, test loss: 0.187820\n",
      "Epoch 16, test loss: 0.190968\n",
      "Epoch 17, test loss: 0.186983\n",
      "Epoch 18, test loss: 0.188291\n",
      "Epoch 19, test loss: 0.187826\n",
      "Epoch 20, test loss: 0.187623\n",
      "Epoch 21, test loss: 0.191222\n",
      "Epoch 22, test loss: 0.188297\n",
      "Epoch 23, test loss: 0.202523\n",
      "Epoch 24, test loss: 0.189878\n",
      "Epoch 25, test loss: 0.196044\n",
      "Epoch 26, test loss: 0.185596\n",
      "Epoch 27, test loss: 0.193018\n",
      "Epoch 28, test loss: 0.189254\n",
      "Epoch 29, test loss: 0.192678\n",
      "Epoch 30, test loss: 0.186775\n",
      "Epoch 31, test loss: 0.190240\n",
      "Epoch 32, test loss: 0.188817\n",
      "Epoch 33, test loss: 0.188949\n",
      "Epoch 34, test loss: 0.187135\n",
      "Epoch 35, test loss: 0.191269\n",
      "Epoch 36, test loss: 0.188138\n",
      "Epoch 37, test loss: 0.188677\n",
      "Epoch 38, test loss: 0.192413\n",
      "Epoch 39, test loss: 0.187665\n",
      "Epoch 40, test loss: 0.187141\n",
      "Epoch 41, test loss: 0.190004\n",
      "Epoch 42, test loss: 0.193612\n",
      "Epoch 43, test loss: 0.199000\n",
      "Epoch 44, test loss: 0.192727\n",
      "Epoch 45, test loss: 0.188691\n",
      "Epoch 46, test loss: 0.187902\n",
      "Epoch 47, test loss: 0.194271\n",
      "Epoch 48, test loss: 0.186601\n",
      "Epoch 49, test loss: 0.194627\n",
      "Epoch 50, test loss: 0.188180\n",
      "Epoch 51, test loss: 0.197931\n",
      "Epoch 52, test loss: 0.190150\n",
      "Epoch 53, test loss: 0.188492\n",
      "Epoch 54, test loss: 0.187799\n",
      "Epoch 55, test loss: 0.195072\n",
      "Epoch 56, test loss: 0.189856\n",
      "Epoch 57, test loss: 0.186151\n",
      "Epoch 58, test loss: 0.186665\n",
      "Epoch 59, test loss: 0.187996\n",
      "Epoch 60, test loss: 0.192962\n",
      "Epoch 61, test loss: 0.188148\n",
      "Epoch 62, test loss: 0.187526\n",
      "Epoch 63, test loss: 0.188141\n",
      "Epoch 64, test loss: 0.188501\n",
      "Epoch 65, test loss: 0.188311\n",
      "Epoch 66, test loss: 0.191309\n",
      "Epoch 67, test loss: 0.197178\n",
      "Epoch 68, test loss: 0.191071\n",
      "Epoch 69, test loss: 0.190975\n",
      "Epoch 70, test loss: 0.189751\n",
      "Epoch 71, test loss: 0.193457\n",
      "Epoch 72, test loss: 0.196325\n",
      "Epoch 73, test loss: 0.203086\n",
      "Epoch 74, test loss: 0.193580\n",
      "Epoch 75, test loss: 0.191567\n",
      "Epoch 76, test loss: 0.192080\n",
      "Epoch 77, test loss: 0.195321\n",
      "Epoch 78, test loss: 0.190216\n",
      "Epoch 79, test loss: 0.188943\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310D240>\n",
      "Epoch 0, test loss: 0.192604\n",
      "Epoch 1, test loss: 0.189486\n",
      "Epoch 2, test loss: 0.191411\n",
      "Epoch 3, test loss: 0.186969\n",
      "Epoch 4, test loss: 0.187812\n",
      "Epoch 5, test loss: 0.188032\n",
      "Epoch 6, test loss: 0.186983\n",
      "Epoch 7, test loss: 0.186318\n",
      "Epoch 8, test loss: 0.189259\n",
      "Epoch 9, test loss: 0.188799\n",
      "Epoch 10, test loss: 0.188608\n",
      "Epoch 11, test loss: 0.203436\n",
      "Epoch 12, test loss: 0.187907\n",
      "Epoch 13, test loss: 0.186464\n",
      "Epoch 14, test loss: 0.186052\n",
      "Epoch 15, test loss: 0.190782\n",
      "Epoch 16, test loss: 0.186900\n",
      "Epoch 17, test loss: 0.189747\n",
      "Epoch 18, test loss: 0.193878\n",
      "Epoch 19, test loss: 0.187378\n",
      "Epoch 20, test loss: 0.189058\n",
      "Epoch 21, test loss: 0.189716\n",
      "Epoch 22, test loss: 0.186922\n",
      "Epoch 23, test loss: 0.189232\n",
      "Epoch 24, test loss: 0.186674\n",
      "Epoch 25, test loss: 0.192837\n",
      "Epoch 26, test loss: 0.188079\n",
      "Epoch 27, test loss: 0.188847\n",
      "Epoch 28, test loss: 0.188074\n",
      "Epoch 29, test loss: 0.188056\n",
      "Epoch 30, test loss: 0.193365\n",
      "Epoch 31, test loss: 0.189963\n",
      "Epoch 32, test loss: 0.186954\n",
      "Epoch 33, test loss: 0.187526\n",
      "Epoch 34, test loss: 0.189827\n",
      "Epoch 35, test loss: 0.187392\n",
      "Epoch 36, test loss: 0.189461\n",
      "Epoch 37, test loss: 0.187347\n",
      "Epoch 38, test loss: 0.187751\n",
      "Epoch 39, test loss: 0.188835\n",
      "Epoch 40, test loss: 0.185640\n",
      "Epoch 41, test loss: 0.187586\n",
      "Epoch 42, test loss: 0.193134\n",
      "Epoch 43, test loss: 0.187951\n",
      "Epoch 44, test loss: 0.188815\n",
      "Epoch 45, test loss: 0.196112\n",
      "Epoch 46, test loss: 0.188031\n",
      "Epoch 47, test loss: 0.192750\n",
      "Epoch 48, test loss: 0.192838\n",
      "Epoch 49, test loss: 0.191929\n",
      "Epoch 50, test loss: 0.193253\n",
      "Epoch 51, test loss: 0.189866\n",
      "Epoch 52, test loss: 0.192571\n",
      "Epoch 53, test loss: 0.187119\n",
      "Epoch 54, test loss: 0.187143\n",
      "Epoch 55, test loss: 0.190034\n",
      "Epoch 56, test loss: 0.189552\n",
      "Epoch 57, test loss: 0.188197\n",
      "Epoch 58, test loss: 0.186536\n",
      "Epoch 59, test loss: 0.186083\n",
      "Epoch 60, test loss: 0.188111\n",
      "Epoch 61, test loss: 0.188066\n",
      "Epoch 62, test loss: 0.187776\n",
      "Epoch 63, test loss: 0.187520\n",
      "Epoch 64, test loss: 0.187580\n",
      "Epoch 65, test loss: 0.191009\n",
      "Epoch 66, test loss: 0.187495\n",
      "Epoch 67, test loss: 0.186970\n",
      "Epoch 68, test loss: 0.187985\n",
      "Epoch 69, test loss: 0.188973\n",
      "Epoch 70, test loss: 0.188007\n",
      "Epoch 71, test loss: 0.189706\n",
      "Epoch 72, test loss: 0.189229\n",
      "Epoch 73, test loss: 0.187395\n",
      "Epoch 74, test loss: 0.190843\n",
      "Epoch 75, test loss: 0.193138\n",
      "Epoch 76, test loss: 0.188314\n",
      "Epoch 77, test loss: 0.188456\n",
      "Epoch 78, test loss: 0.187138\n",
      "Epoch 79, test loss: 0.189406\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29310D240>\n",
      "Epoch 0, test loss: 0.267550\n",
      "Epoch 1, test loss: 0.203688\n",
      "Epoch 2, test loss: 0.211287\n",
      "Epoch 3, test loss: 0.195757\n",
      "Epoch 4, test loss: 0.193827\n",
      "Epoch 5, test loss: 0.191537\n",
      "Epoch 6, test loss: 0.191048\n",
      "Epoch 7, test loss: 0.191823\n",
      "Epoch 8, test loss: 0.190749\n",
      "Epoch 9, test loss: 0.190711\n",
      "Epoch 10, test loss: 0.193098\n",
      "Epoch 11, test loss: 0.194463\n",
      "Epoch 12, test loss: 0.191737\n",
      "Epoch 13, test loss: 0.192973\n",
      "Epoch 14, test loss: 0.192535\n",
      "Epoch 15, test loss: 0.201603\n",
      "Epoch 16, test loss: 0.191684\n",
      "Epoch 17, test loss: 0.191371\n",
      "Epoch 18, test loss: 0.190646\n",
      "Epoch 19, test loss: 0.193651\n",
      "Epoch 20, test loss: 0.192577\n",
      "Epoch 21, test loss: 0.191586\n",
      "Epoch 22, test loss: 0.192283\n",
      "Epoch 23, test loss: 0.198492\n",
      "Epoch 24, test loss: 0.191260\n",
      "Epoch 25, test loss: 0.192306\n",
      "Epoch 26, test loss: 0.193056\n",
      "Epoch 27, test loss: 0.192144\n",
      "Epoch 28, test loss: 0.192309\n",
      "Epoch 29, test loss: 0.194314\n",
      "Epoch 30, test loss: 0.191513\n",
      "Epoch 31, test loss: 0.191395\n",
      "Epoch 32, test loss: 0.203224\n",
      "Epoch 33, test loss: 0.192446\n",
      "Epoch 34, test loss: 0.192015\n",
      "Epoch 35, test loss: 0.192798\n",
      "Epoch 36, test loss: 0.193876\n",
      "Epoch 37, test loss: 0.193841\n",
      "Epoch 38, test loss: 0.192674\n",
      "Epoch 39, test loss: 0.201150\n",
      "Epoch 40, test loss: 0.196084\n",
      "Epoch 41, test loss: 0.190209\n",
      "Epoch 42, test loss: 0.189818\n",
      "Epoch 43, test loss: 0.197012\n",
      "Epoch 44, test loss: 0.191278\n",
      "Epoch 45, test loss: 0.191675\n",
      "Epoch 46, test loss: 0.190707\n",
      "Epoch 47, test loss: 0.194327\n",
      "Epoch 48, test loss: 0.192669\n",
      "Epoch 49, test loss: 0.192083\n",
      "Epoch 50, test loss: 0.192503\n",
      "Epoch 51, test loss: 0.192440\n",
      "Epoch 52, test loss: 0.191789\n",
      "Epoch 53, test loss: 0.190583\n",
      "Epoch 54, test loss: 0.194341\n",
      "Epoch 55, test loss: 0.194473\n",
      "Epoch 56, test loss: 0.194822\n",
      "Epoch 57, test loss: 0.194656\n",
      "Epoch 58, test loss: 0.196873\n",
      "Epoch 59, test loss: 0.192007\n",
      "Epoch 60, test loss: 0.191238\n",
      "Epoch 61, test loss: 0.192204\n",
      "Epoch 62, test loss: 0.191393\n",
      "Epoch 63, test loss: 0.191635\n",
      "Epoch 64, test loss: 0.192921\n",
      "Epoch 65, test loss: 0.189542\n",
      "Epoch 66, test loss: 0.191276\n",
      "Epoch 67, test loss: 0.190639\n",
      "Epoch 68, test loss: 0.190691\n",
      "Epoch 69, test loss: 0.190285\n",
      "Epoch 70, test loss: 0.189340\n",
      "Epoch 71, test loss: 0.192848\n",
      "Epoch 72, test loss: 0.190077\n",
      "Epoch 73, test loss: 0.196353\n",
      "Epoch 74, test loss: 0.192332\n",
      "Epoch 75, test loss: 0.190576\n",
      "Epoch 76, test loss: 0.193230\n",
      "Epoch 77, test loss: 0.193508\n",
      "Epoch 78, test loss: 0.196663\n",
      "Epoch 79, test loss: 0.191067\n",
      "Epoch 80, test loss: 0.205162\n",
      "Epoch 81, test loss: 0.192009\n",
      "Epoch 82, test loss: 0.193486\n",
      "Epoch 83, test loss: 0.192873\n",
      "Epoch 84, test loss: 0.192311\n",
      "Epoch 85, test loss: 0.190799\n",
      "Epoch 86, test loss: 0.191576\n",
      "Epoch 87, test loss: 0.192205\n",
      "Epoch 88, test loss: 0.191587\n",
      "Epoch 89, test loss: 0.195267\n",
      "Epoch 90, test loss: 0.191771\n",
      "Epoch 91, test loss: 0.190818\n",
      "Epoch 92, test loss: 0.190584\n",
      "Epoch 93, test loss: 0.192201\n",
      "Epoch 94, test loss: 0.195072\n",
      "Epoch 95, test loss: 0.190045\n",
      "Epoch 96, test loss: 0.192516\n",
      "Epoch 97, test loss: 0.197923\n",
      "Epoch 98, test loss: 0.195714\n",
      "Epoch 99, test loss: 0.193344\n",
      "Epoch 100, test loss: 0.190418\n",
      "Epoch 101, test loss: 0.197384\n",
      "Epoch 102, test loss: 0.193122\n",
      "Epoch 103, test loss: 0.191677\n",
      "Epoch 104, test loss: 0.198443\n",
      "Epoch 105, test loss: 0.189743\n",
      "Epoch 106, test loss: 0.194491\n",
      "Epoch 107, test loss: 0.190892\n",
      "Epoch 108, test loss: 0.191469\n",
      "Epoch 109, test loss: 0.191174\n",
      "Epoch 110, test loss: 0.196707\n",
      "Epoch 111, test loss: 0.192143\n",
      "Epoch 112, test loss: 0.191531\n",
      "Epoch 113, test loss: 0.190994\n",
      "Epoch 114, test loss: 0.191573\n",
      "Epoch 115, test loss: 0.189839\n",
      "Epoch 116, test loss: 0.193230\n",
      "Epoch 117, test loss: 0.190612\n",
      "Epoch 118, test loss: 0.189173\n",
      "Epoch 119, test loss: 0.191538\n",
      "Epoch 120, test loss: 0.190920\n",
      "Epoch 121, test loss: 0.205719\n",
      "Epoch 122, test loss: 0.200052\n",
      "Epoch 123, test loss: 0.194585\n",
      "Epoch 124, test loss: 0.191363\n",
      "Epoch 125, test loss: 0.191077\n",
      "Epoch 126, test loss: 0.192263\n",
      "Epoch 127, test loss: 0.190677\n",
      "Epoch 128, test loss: 0.205883\n",
      "Epoch 129, test loss: 0.194179\n",
      "Epoch 130, test loss: 0.191892\n",
      "Epoch 131, test loss: 0.193553\n",
      "Epoch 132, test loss: 0.192655\n",
      "Epoch 133, test loss: 0.191588\n",
      "Epoch 134, test loss: 0.192024\n",
      "Epoch 135, test loss: 0.192317\n",
      "Epoch 136, test loss: 0.189328\n",
      "Epoch 137, test loss: 0.194984\n",
      "Epoch 138, test loss: 0.190986\n",
      "Epoch 139, test loss: 0.193691\n",
      "Epoch 140, test loss: 0.191595\n",
      "Epoch 141, test loss: 0.193827\n",
      "Epoch 142, test loss: 0.190525\n",
      "Epoch 143, test loss: 0.189737\n",
      "Epoch 144, test loss: 0.191010\n",
      "Epoch 145, test loss: 0.191377\n",
      "Epoch 146, test loss: 0.190145\n",
      "Epoch 147, test loss: 0.197299\n",
      "Epoch 148, test loss: 0.191503\n",
      "Epoch 149, test loss: 0.194353\n",
      "Epoch 150, test loss: 0.191580\n",
      "Epoch 151, test loss: 0.191224\n",
      "Epoch 152, test loss: 0.195698\n",
      "Epoch 153, test loss: 0.190114\n",
      "Epoch 154, test loss: 0.191243\n",
      "Epoch 155, test loss: 0.189751\n",
      "Epoch 156, test loss: 0.191818\n",
      "Epoch 157, test loss: 0.193412\n",
      "Epoch 158, test loss: 0.190138\n",
      "Epoch 159, test loss: 0.195982\n",
      "Epoch 160, test loss: 0.190456\n",
      "Epoch 161, test loss: 0.196984\n",
      "Epoch 162, test loss: 0.190825\n",
      "Epoch 163, test loss: 0.192322\n",
      "Epoch 164, test loss: 0.190806\n",
      "Epoch 165, test loss: 0.192858\n",
      "Epoch 166, test loss: 0.191223\n",
      "Epoch 167, test loss: 0.190109\n",
      "Epoch 168, test loss: 0.190738\n",
      "Epoch 169, test loss: 0.191739\n",
      "Epoch 170, test loss: 0.190505\n",
      "Epoch 171, test loss: 0.189906\n",
      "Epoch 172, test loss: 0.192169\n",
      "Epoch 173, test loss: 0.198975\n",
      "Epoch 174, test loss: 0.189287\n",
      "Epoch 175, test loss: 0.194158\n",
      "Epoch 176, test loss: 0.191554\n",
      "Epoch 177, test loss: 0.189548\n",
      "Epoch 178, test loss: 0.197368\n",
      "Epoch 179, test loss: 0.195390\n",
      "Epoch 180, test loss: 0.191072\n",
      "Epoch 181, test loss: 0.201041\n",
      "Epoch 182, test loss: 0.191064\n",
      "Epoch 183, test loss: 0.198564\n",
      "Epoch 184, test loss: 0.193098\n",
      "Epoch 185, test loss: 0.191730\n",
      "Epoch 186, test loss: 0.199644\n",
      "Epoch 187, test loss: 0.192410\n",
      "Epoch 188, test loss: 0.190475\n",
      "Epoch 189, test loss: 0.193918\n",
      "Epoch 190, test loss: 0.190059\n",
      "Epoch 191, test loss: 0.195051\n",
      "Epoch 192, test loss: 0.191672\n",
      "Epoch 193, test loss: 0.194330\n",
      "Epoch 194, test loss: 0.190630\n",
      "Epoch 195, test loss: 0.191131\n",
      "Epoch 196, test loss: 0.193903\n",
      "Epoch 197, test loss: 0.193074\n",
      "Epoch 198, test loss: 0.191682\n",
      "Epoch 199, test loss: 0.192776\n",
      "Epoch 200, test loss: 0.195582\n",
      "Epoch 201, test loss: 0.193353\n",
      "Epoch 202, test loss: 0.192726\n",
      "Epoch 203, test loss: 0.189412\n",
      "Epoch 204, test loss: 0.195174\n",
      "Epoch 205, test loss: 0.191392\n",
      "Epoch 206, test loss: 0.192614\n",
      "Epoch 207, test loss: 0.197463\n",
      "Epoch 208, test loss: 0.191257\n",
      "Epoch 209, test loss: 0.189792\n",
      "Epoch 210, test loss: 0.195026\n",
      "Epoch 211, test loss: 0.191529\n",
      "Epoch 212, test loss: 0.191835\n",
      "Epoch 213, test loss: 0.191950\n",
      "Epoch 214, test loss: 0.191442\n",
      "Epoch 215, test loss: 0.190786\n",
      "Epoch 216, test loss: 0.194145\n",
      "Epoch 217, test loss: 0.192170\n",
      "Epoch 218, test loss: 0.189603\n",
      "Epoch 219, test loss: 0.189968\n",
      "Epoch 220, test loss: 0.199152\n",
      "Epoch 221, test loss: 0.189884\n",
      "Epoch 222, test loss: 0.191368\n",
      "Epoch 223, test loss: 0.191216\n",
      "Epoch 224, test loss: 0.191158\n",
      "Epoch 225, test loss: 0.199732\n",
      "Epoch 226, test loss: 0.189724\n",
      "Epoch 227, test loss: 0.192774\n",
      "Epoch 228, test loss: 0.195983\n",
      "Epoch 229, test loss: 0.196755\n",
      "Epoch 230, test loss: 0.190214\n",
      "Epoch 231, test loss: 0.190240\n",
      "Epoch 232, test loss: 0.191533\n",
      "Epoch 233, test loss: 0.191955\n",
      "Epoch 234, test loss: 0.195100\n",
      "Epoch 235, test loss: 0.191276\n",
      "Epoch 236, test loss: 0.191007\n",
      "Epoch 237, test loss: 0.195807\n",
      "Epoch 238, test loss: 0.192984\n",
      "Epoch 239, test loss: 0.190684\n",
      "Pretrain data: 19742408.0\n",
      "Building dataset, requesting data from 0 to 769\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [275. 277. 287. ... 232. 236. 237.]\n",
      " [277. 287. 295. ... 236. 237. 247.]\n",
      " [287. 295. 298. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 5734/96307\n",
      "Found 769 continuous time series\n",
      "Data shape: (102043, 24), Train/test: 102041/2\n",
      "Train test ratio: 51020.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1594\n",
      "Epoch 0, train loss: 0.288530\n",
      "Epoch 1, train loss: 0.260558\n",
      "Epoch 2, train loss: 0.221001\n",
      "Epoch 3, train loss: 0.277482\n",
      "Epoch 4, train loss: 0.180006\n",
      "Epoch 5, train loss: 0.198657\n",
      "Epoch 6, train loss: 0.252000\n",
      "Epoch 7, train loss: 0.198932\n",
      "Epoch 8, train loss: 0.225305\n",
      "Epoch 9, train loss: 0.220764\n",
      "Epoch 10, train loss: 0.222485\n",
      "Epoch 11, train loss: 0.185367\n",
      "Epoch 12, train loss: 0.136470\n",
      "Epoch 13, train loss: 0.208213\n",
      "Epoch 14, train loss: 0.231036\n",
      "Epoch 15, train loss: 0.202459\n",
      "Epoch 16, train loss: 0.178513\n",
      "Epoch 17, train loss: 0.217066\n",
      "Epoch 18, train loss: 0.232832\n",
      "Epoch 19, train loss: 0.275378\n",
      "Epoch 20, train loss: 0.177348\n",
      "Epoch 21, train loss: 0.152789\n",
      "Epoch 22, train loss: 0.166193\n",
      "Epoch 23, train loss: 0.213125\n",
      "Epoch 24, train loss: 0.244123\n",
      "Epoch 25, train loss: 0.257330\n",
      "Epoch 26, train loss: 0.172326\n",
      "Epoch 27, train loss: 0.248872\n",
      "Epoch 28, train loss: 0.271082\n",
      "Epoch 29, train loss: 0.216409\n",
      "Epoch 30, train loss: 0.188868\n",
      "Epoch 31, train loss: 0.150851\n",
      "Epoch 32, train loss: 0.175448\n",
      "Epoch 33, train loss: 0.146388\n",
      "Epoch 34, train loss: 0.271046\n",
      "Epoch 35, train loss: 0.191896\n",
      "Epoch 36, train loss: 0.173857\n",
      "Epoch 37, train loss: 0.246946\n",
      "Epoch 38, train loss: 0.200223\n",
      "Epoch 39, train loss: 0.314530\n",
      "Epoch 40, train loss: 0.209814\n",
      "Epoch 41, train loss: 0.207848\n",
      "Epoch 42, train loss: 0.200886\n",
      "Epoch 43, train loss: 0.244413\n",
      "Epoch 44, train loss: 0.183291\n",
      "Epoch 45, train loss: 0.218154\n",
      "Epoch 46, train loss: 0.224084\n",
      "Epoch 47, train loss: 0.178766\n",
      "Epoch 48, train loss: 0.203351\n",
      "Epoch 49, train loss: 0.182680\n",
      "Epoch 50, train loss: 0.198198\n",
      "Epoch 51, train loss: 0.188701\n",
      "Epoch 52, train loss: 0.209278\n",
      "Epoch 53, train loss: 0.183169\n",
      "Epoch 54, train loss: 0.209973\n",
      "Epoch 55, train loss: 0.212097\n",
      "Epoch 56, train loss: 0.198910\n",
      "Epoch 57, train loss: 0.265309\n",
      "Epoch 58, train loss: 0.239176\n",
      "Epoch 59, train loss: 0.258067\n",
      "Epoch 60, train loss: 0.226191\n",
      "Epoch 61, train loss: 0.148856\n",
      "Epoch 62, train loss: 0.256792\n",
      "Epoch 63, train loss: 0.251069\n",
      "Epoch 64, train loss: 0.169022\n",
      "Epoch 65, train loss: 0.242041\n",
      "Epoch 66, train loss: 0.220480\n",
      "Epoch 67, train loss: 0.178658\n",
      "Epoch 68, train loss: 0.231151\n",
      "Epoch 69, train loss: 0.210123\n",
      "Epoch 70, train loss: 0.191733\n",
      "Epoch 71, train loss: 0.185743\n",
      "Epoch 72, train loss: 0.184641\n",
      "Epoch 73, train loss: 0.205215\n",
      "Epoch 74, train loss: 0.229627\n",
      "Epoch 75, train loss: 0.200229\n",
      "Epoch 76, train loss: 0.176366\n",
      "Epoch 77, train loss: 0.226476\n",
      "Epoch 78, train loss: 0.231857\n",
      "Epoch 79, train loss: 0.239025\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[214. 217. 217. ... 167. 162. 160.]\n",
      " [217. 217. 212. ... 162. 160. 158.]\n",
      " [217. 212. 209. ... 160. 158. 156.]\n",
      " ...\n",
      " [ 85.  88. 100. ... 128. 130. 124.]\n",
      " [ 88. 100.  89. ... 130. 124. 120.]\n",
      " [100.  89.  96. ... 124. 120. 117.]]\n",
      "y here is\n",
      "[[153. 153. 153. ... 153. 153. 153.]\n",
      " [153. 153. 153. ... 153. 153. 153.]\n",
      " [153. 153. 153. ... 153. 153. 153.]\n",
      " ...\n",
      " [129. 129. 129. ... 129. 129. 129.]\n",
      " [139. 139. 139. ... 139. 139. 139.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (2286, 24), Train/test: 1/2285\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 73 segments\n",
      "Building dataset, requesting data from 0 to 73\n",
      "x here is\n",
      "[[128. 123. 120. ... 114. 113. 113.]\n",
      " [123. 120. 124. ... 113. 113. 110.]\n",
      " [120. 124. 121. ... 113. 110. 107.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[105. 105. 105. ... 105. 105. 105.]\n",
      " [115. 115. 115. ... 115. 115. 115.]\n",
      " [109. 109. 109. ... 109. 109. 109.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1375/8396\n",
      "Found 73 continuous time series\n",
      "Data shape: (9773, 24), Train/test: 9771/2\n",
      "Train test ratio: 4885.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2935D55A0>\n",
      "Epoch 0, test loss: 0.215920\n",
      "Epoch 1, test loss: 0.214886\n",
      "Epoch 2, test loss: 0.217302\n",
      "Epoch 3, test loss: 0.214855\n",
      "Epoch 4, test loss: 0.215409\n",
      "Epoch 5, test loss: 0.216496\n",
      "Epoch 6, test loss: 0.217426\n",
      "Epoch 7, test loss: 0.219607\n",
      "Epoch 8, test loss: 0.214637\n",
      "Epoch 9, test loss: 0.215812\n",
      "Epoch 10, test loss: 0.214787\n",
      "Epoch 11, test loss: 0.215294\n",
      "Epoch 12, test loss: 0.216635\n",
      "Epoch 13, test loss: 0.215213\n",
      "Epoch 14, test loss: 0.215959\n",
      "Epoch 15, test loss: 0.216147\n",
      "Epoch 16, test loss: 0.216888\n",
      "Epoch 17, test loss: 0.215149\n",
      "Epoch 18, test loss: 0.215948\n",
      "Epoch 19, test loss: 0.216118\n",
      "Epoch 20, test loss: 0.215496\n",
      "Epoch 21, test loss: 0.216797\n",
      "Epoch 22, test loss: 0.216426\n",
      "Epoch 23, test loss: 0.215480\n",
      "Epoch 24, test loss: 0.217650\n",
      "Epoch 25, test loss: 0.219077\n",
      "Epoch 26, test loss: 0.215665\n",
      "Epoch 27, test loss: 0.219806\n",
      "Epoch 28, test loss: 0.216139\n",
      "Epoch 29, test loss: 0.220132\n",
      "Epoch 30, test loss: 0.215961\n",
      "Epoch 31, test loss: 0.216710\n",
      "Epoch 32, test loss: 0.217251\n",
      "Epoch 33, test loss: 0.216637\n",
      "Epoch 34, test loss: 0.215850\n",
      "Epoch 35, test loss: 0.218714\n",
      "Epoch 36, test loss: 0.217116\n",
      "Epoch 37, test loss: 0.216273\n",
      "Epoch 38, test loss: 0.217705\n",
      "Epoch 39, test loss: 0.217255\n",
      "Epoch 40, test loss: 0.216764\n",
      "Epoch 41, test loss: 0.215854\n",
      "Epoch 42, test loss: 0.216632\n",
      "Epoch 43, test loss: 0.216707\n",
      "Epoch 44, test loss: 0.217657\n",
      "Epoch 45, test loss: 0.219155\n",
      "Epoch 46, test loss: 0.218545\n",
      "Epoch 47, test loss: 0.221359\n",
      "Epoch 48, test loss: 0.217457\n",
      "Epoch 49, test loss: 0.217794\n",
      "Epoch 50, test loss: 0.221802\n",
      "Epoch 51, test loss: 0.217828\n",
      "Epoch 52, test loss: 0.218635\n",
      "Epoch 53, test loss: 0.219280\n",
      "Epoch 54, test loss: 0.217609\n",
      "Epoch 55, test loss: 0.216658\n",
      "Epoch 56, test loss: 0.216493\n",
      "Epoch 57, test loss: 0.227753\n",
      "Epoch 58, test loss: 0.216964\n",
      "Epoch 59, test loss: 0.217733\n",
      "Epoch 60, test loss: 0.216844\n",
      "Epoch 61, test loss: 0.217903\n",
      "Epoch 62, test loss: 0.216607\n",
      "Epoch 63, test loss: 0.217505\n",
      "Epoch 64, test loss: 0.220300\n",
      "Epoch 65, test loss: 0.217558\n",
      "Epoch 66, test loss: 0.217144\n",
      "Epoch 67, test loss: 0.219116\n",
      "Epoch 68, test loss: 0.218204\n",
      "Epoch 69, test loss: 0.218938\n",
      "Epoch 70, test loss: 0.217257\n",
      "Epoch 71, test loss: 0.218923\n",
      "Epoch 72, test loss: 0.218797\n",
      "Epoch 73, test loss: 0.218012\n",
      "Epoch 74, test loss: 0.218166\n",
      "Epoch 75, test loss: 0.222141\n",
      "Epoch 76, test loss: 0.217299\n",
      "Epoch 77, test loss: 0.222817\n",
      "Epoch 78, test loss: 0.217349\n",
      "Epoch 79, test loss: 0.218844\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2935D55A0>\n",
      "Epoch 0, test loss: 0.213372\n",
      "Epoch 1, test loss: 0.215232\n",
      "Epoch 2, test loss: 0.215712\n",
      "Epoch 3, test loss: 0.217405\n",
      "Epoch 4, test loss: 0.214683\n",
      "Epoch 5, test loss: 0.217035\n",
      "Epoch 6, test loss: 0.217795\n",
      "Epoch 7, test loss: 0.214659\n",
      "Epoch 8, test loss: 0.215573\n",
      "Epoch 9, test loss: 0.217364\n",
      "Epoch 10, test loss: 0.216784\n",
      "Epoch 11, test loss: 0.215900\n",
      "Epoch 12, test loss: 0.215026\n",
      "Epoch 13, test loss: 0.216318\n",
      "Epoch 14, test loss: 0.215732\n",
      "Epoch 15, test loss: 0.216353\n",
      "Epoch 16, test loss: 0.215956\n",
      "Epoch 17, test loss: 0.216045\n",
      "Epoch 18, test loss: 0.215683\n",
      "Epoch 19, test loss: 0.217425\n",
      "Epoch 20, test loss: 0.216514\n",
      "Epoch 21, test loss: 0.216805\n",
      "Epoch 22, test loss: 0.216478\n",
      "Epoch 23, test loss: 0.215918\n",
      "Epoch 24, test loss: 0.215473\n",
      "Epoch 25, test loss: 0.217355\n",
      "Epoch 26, test loss: 0.216017\n",
      "Epoch 27, test loss: 0.215272\n",
      "Epoch 28, test loss: 0.217815\n",
      "Epoch 29, test loss: 0.215565\n",
      "Epoch 30, test loss: 0.216741\n",
      "Epoch 31, test loss: 0.217679\n",
      "Epoch 32, test loss: 0.215442\n",
      "Epoch 33, test loss: 0.215770\n",
      "Epoch 34, test loss: 0.216737\n",
      "Epoch 35, test loss: 0.216876\n",
      "Epoch 36, test loss: 0.216556\n",
      "Epoch 37, test loss: 0.218542\n",
      "Epoch 38, test loss: 0.216380\n",
      "Epoch 39, test loss: 0.216495\n",
      "Epoch 40, test loss: 0.216579\n",
      "Epoch 41, test loss: 0.216270\n",
      "Epoch 42, test loss: 0.216094\n",
      "Epoch 43, test loss: 0.221079\n",
      "Epoch 44, test loss: 0.218876\n",
      "Epoch 45, test loss: 0.216846\n",
      "Epoch 46, test loss: 0.216172\n",
      "Epoch 47, test loss: 0.216748\n",
      "Epoch 48, test loss: 0.218316\n",
      "Epoch 49, test loss: 0.220010\n",
      "Epoch 50, test loss: 0.217008\n",
      "Epoch 51, test loss: 0.216700\n",
      "Epoch 52, test loss: 0.215989\n",
      "Epoch 53, test loss: 0.216570\n",
      "Epoch 54, test loss: 0.216873\n",
      "Epoch 55, test loss: 0.216976\n",
      "Epoch 56, test loss: 0.221659\n",
      "Epoch 57, test loss: 0.215771\n",
      "Epoch 58, test loss: 0.217158\n",
      "Epoch 59, test loss: 0.216245\n",
      "Epoch 60, test loss: 0.220957\n",
      "Epoch 61, test loss: 0.215817\n",
      "Epoch 62, test loss: 0.220567\n",
      "Epoch 63, test loss: 0.217256\n",
      "Epoch 64, test loss: 0.215826\n",
      "Epoch 65, test loss: 0.217586\n",
      "Epoch 66, test loss: 0.218061\n",
      "Epoch 67, test loss: 0.216631\n",
      "Epoch 68, test loss: 0.217590\n",
      "Epoch 69, test loss: 0.220388\n",
      "Epoch 70, test loss: 0.215868\n",
      "Epoch 71, test loss: 0.216556\n",
      "Epoch 72, test loss: 0.216093\n",
      "Epoch 73, test loss: 0.216563\n",
      "Epoch 74, test loss: 0.216521\n",
      "Epoch 75, test loss: 0.215408\n",
      "Epoch 76, test loss: 0.217396\n",
      "Epoch 77, test loss: 0.217679\n",
      "Epoch 78, test loss: 0.219042\n",
      "Epoch 79, test loss: 0.216118\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2935D55A0>\n",
      "Epoch 0, test loss: 0.312357\n",
      "Epoch 1, test loss: 0.238780\n",
      "Epoch 2, test loss: 0.242756\n",
      "Epoch 3, test loss: 0.240335\n",
      "Epoch 4, test loss: 0.239987\n",
      "Epoch 5, test loss: 0.240037\n",
      "Epoch 6, test loss: 0.238182\n",
      "Epoch 7, test loss: 0.236897\n",
      "Epoch 8, test loss: 0.235639\n",
      "Epoch 9, test loss: 0.239315\n",
      "Epoch 10, test loss: 0.242444\n",
      "Epoch 11, test loss: 0.235017\n",
      "Epoch 12, test loss: 0.234949\n",
      "Epoch 13, test loss: 0.234939\n",
      "Epoch 14, test loss: 0.234859\n",
      "Epoch 15, test loss: 0.233196\n",
      "Epoch 16, test loss: 0.237008\n",
      "Epoch 17, test loss: 0.236019\n",
      "Epoch 18, test loss: 0.233633\n",
      "Epoch 19, test loss: 0.233164\n",
      "Epoch 20, test loss: 0.233205\n",
      "Epoch 21, test loss: 0.232367\n",
      "Epoch 22, test loss: 0.235522\n",
      "Epoch 23, test loss: 0.232585\n",
      "Epoch 24, test loss: 0.237638\n",
      "Epoch 25, test loss: 0.233979\n",
      "Epoch 26, test loss: 0.230871\n",
      "Epoch 27, test loss: 0.236748\n",
      "Epoch 28, test loss: 0.238780\n",
      "Epoch 29, test loss: 0.230203\n",
      "Epoch 30, test loss: 0.231050\n",
      "Epoch 31, test loss: 0.231794\n",
      "Epoch 32, test loss: 0.231950\n",
      "Epoch 33, test loss: 0.231246\n",
      "Epoch 34, test loss: 0.232727\n",
      "Epoch 35, test loss: 0.231431\n",
      "Epoch 36, test loss: 0.229038\n",
      "Epoch 37, test loss: 0.229624\n",
      "Epoch 38, test loss: 0.233565\n",
      "Epoch 39, test loss: 0.230005\n",
      "Epoch 40, test loss: 0.228156\n",
      "Epoch 41, test loss: 0.228191\n",
      "Epoch 42, test loss: 0.228972\n",
      "Epoch 43, test loss: 0.229817\n",
      "Epoch 44, test loss: 0.228271\n",
      "Epoch 45, test loss: 0.228818\n",
      "Epoch 46, test loss: 0.227218\n",
      "Epoch 47, test loss: 0.228795\n",
      "Epoch 48, test loss: 0.227042\n",
      "Epoch 49, test loss: 0.226953\n",
      "Epoch 50, test loss: 0.227204\n",
      "Epoch 51, test loss: 0.228162\n",
      "Epoch 52, test loss: 0.226508\n",
      "Epoch 53, test loss: 0.228931\n",
      "Epoch 54, test loss: 0.225126\n",
      "Epoch 55, test loss: 0.226798\n",
      "Epoch 56, test loss: 0.224722\n",
      "Epoch 57, test loss: 0.226622\n",
      "Epoch 58, test loss: 0.225486\n",
      "Epoch 59, test loss: 0.230965\n",
      "Epoch 60, test loss: 0.226984\n",
      "Epoch 61, test loss: 0.226706\n",
      "Epoch 62, test loss: 0.225727\n",
      "Epoch 63, test loss: 0.227348\n",
      "Epoch 64, test loss: 0.227228\n",
      "Epoch 65, test loss: 0.226517\n",
      "Epoch 66, test loss: 0.224787\n",
      "Epoch 67, test loss: 0.226098\n",
      "Epoch 68, test loss: 0.227467\n",
      "Epoch 69, test loss: 0.223965\n",
      "Epoch 70, test loss: 0.227636\n",
      "Epoch 71, test loss: 0.224698\n",
      "Epoch 72, test loss: 0.226697\n",
      "Epoch 73, test loss: 0.224766\n",
      "Epoch 74, test loss: 0.224927\n",
      "Epoch 75, test loss: 0.225533\n",
      "Epoch 76, test loss: 0.224929\n",
      "Epoch 77, test loss: 0.225193\n",
      "Epoch 78, test loss: 0.225737\n",
      "Epoch 79, test loss: 0.224454\n",
      "Epoch 80, test loss: 0.226776\n",
      "Epoch 81, test loss: 0.226541\n",
      "Epoch 82, test loss: 0.224954\n",
      "Epoch 83, test loss: 0.225773\n",
      "Epoch 84, test loss: 0.225694\n",
      "Epoch 85, test loss: 0.225247\n",
      "Epoch 86, test loss: 0.226272\n",
      "Epoch 87, test loss: 0.225910\n",
      "Epoch 88, test loss: 0.224639\n",
      "Epoch 89, test loss: 0.225858\n",
      "Epoch 90, test loss: 0.226919\n",
      "Epoch 91, test loss: 0.223492\n",
      "Epoch 92, test loss: 0.224223\n",
      "Epoch 93, test loss: 0.223177\n",
      "Epoch 94, test loss: 0.222776\n",
      "Epoch 95, test loss: 0.224095\n",
      "Epoch 96, test loss: 0.226266\n",
      "Epoch 97, test loss: 0.223535\n",
      "Epoch 98, test loss: 0.229289\n",
      "Epoch 99, test loss: 0.224402\n",
      "Epoch 100, test loss: 0.221842\n",
      "Epoch 101, test loss: 0.222821\n",
      "Epoch 102, test loss: 0.231569\n",
      "Epoch 103, test loss: 0.229959\n",
      "Epoch 104, test loss: 0.224101\n",
      "Epoch 105, test loss: 0.225311\n",
      "Epoch 106, test loss: 0.225428\n",
      "Epoch 107, test loss: 0.226340\n",
      "Epoch 108, test loss: 0.222870\n",
      "Epoch 109, test loss: 0.222247\n",
      "Epoch 110, test loss: 0.224600\n",
      "Epoch 111, test loss: 0.225701\n",
      "Epoch 112, test loss: 0.222746\n",
      "Epoch 113, test loss: 0.222784\n",
      "Epoch 114, test loss: 0.227906\n",
      "Epoch 115, test loss: 0.224746\n",
      "Epoch 116, test loss: 0.224856\n",
      "Epoch 117, test loss: 0.224387\n",
      "Epoch 118, test loss: 0.226282\n",
      "Epoch 119, test loss: 0.223530\n",
      "Epoch 120, test loss: 0.223917\n",
      "Epoch 121, test loss: 0.223906\n",
      "Epoch 122, test loss: 0.224495\n",
      "Epoch 123, test loss: 0.230475\n",
      "Epoch 124, test loss: 0.227991\n",
      "Epoch 125, test loss: 0.226586\n",
      "Epoch 126, test loss: 0.226641\n",
      "Epoch 127, test loss: 0.224067\n",
      "Epoch 128, test loss: 0.223437\n",
      "Epoch 129, test loss: 0.224069\n",
      "Epoch 130, test loss: 0.223113\n",
      "Epoch 131, test loss: 0.223731\n",
      "Epoch 132, test loss: 0.229178\n",
      "Epoch 133, test loss: 0.224884\n",
      "Epoch 134, test loss: 0.222569\n",
      "Epoch 135, test loss: 0.225548\n",
      "Epoch 136, test loss: 0.222492\n",
      "Epoch 137, test loss: 0.225150\n",
      "Epoch 138, test loss: 0.229689\n",
      "Epoch 139, test loss: 0.222472\n",
      "Epoch 140, test loss: 0.222169\n",
      "Epoch 141, test loss: 0.225386\n",
      "Epoch 142, test loss: 0.224846\n",
      "Epoch 143, test loss: 0.221427\n",
      "Epoch 144, test loss: 0.224810\n",
      "Epoch 145, test loss: 0.224631\n",
      "Epoch 146, test loss: 0.224730\n",
      "Epoch 147, test loss: 0.229071\n",
      "Epoch 148, test loss: 0.232935\n",
      "Epoch 149, test loss: 0.226040\n",
      "Epoch 150, test loss: 0.223339\n",
      "Epoch 151, test loss: 0.224128\n",
      "Epoch 152, test loss: 0.226176\n",
      "Epoch 153, test loss: 0.223001\n",
      "Epoch 154, test loss: 0.223233\n",
      "Epoch 155, test loss: 0.225262\n",
      "Epoch 156, test loss: 0.225779\n",
      "Epoch 157, test loss: 0.224060\n",
      "Epoch 158, test loss: 0.223353\n",
      "Epoch 159, test loss: 0.231200\n",
      "Epoch 160, test loss: 0.223577\n",
      "Epoch 161, test loss: 0.226851\n",
      "Epoch 162, test loss: 0.222903\n",
      "Epoch 163, test loss: 0.226210\n",
      "Epoch 164, test loss: 0.224988\n",
      "Epoch 165, test loss: 0.225427\n",
      "Epoch 166, test loss: 0.226359\n",
      "Epoch 167, test loss: 0.225404\n",
      "Epoch 168, test loss: 0.228923\n",
      "Epoch 169, test loss: 0.225273\n",
      "Epoch 170, test loss: 0.226682\n",
      "Epoch 171, test loss: 0.222967\n",
      "Epoch 172, test loss: 0.225711\n",
      "Epoch 173, test loss: 0.224646\n",
      "Epoch 174, test loss: 0.230541\n",
      "Epoch 175, test loss: 0.226519\n",
      "Epoch 176, test loss: 0.224645\n",
      "Epoch 177, test loss: 0.226098\n",
      "Epoch 178, test loss: 0.224716\n",
      "Epoch 179, test loss: 0.224790\n",
      "Epoch 180, test loss: 0.224004\n",
      "Epoch 181, test loss: 0.223682\n",
      "Epoch 182, test loss: 0.226556\n",
      "Epoch 183, test loss: 0.224749\n",
      "Epoch 184, test loss: 0.222611\n",
      "Epoch 185, test loss: 0.224059\n",
      "Epoch 186, test loss: 0.225033\n",
      "Epoch 187, test loss: 0.227610\n",
      "Epoch 188, test loss: 0.228871\n",
      "Epoch 189, test loss: 0.223367\n",
      "Epoch 190, test loss: 0.228698\n",
      "Epoch 191, test loss: 0.227850\n",
      "Epoch 192, test loss: 0.232362\n",
      "Epoch 193, test loss: 0.223953\n",
      "Epoch 194, test loss: 0.222184\n",
      "Epoch 195, test loss: 0.222527\n",
      "Epoch 196, test loss: 0.225515\n",
      "Epoch 197, test loss: 0.223648\n",
      "Epoch 198, test loss: 0.224404\n",
      "Epoch 199, test loss: 0.224395\n",
      "Epoch 200, test loss: 0.223946\n",
      "Epoch 201, test loss: 0.223738\n",
      "Epoch 202, test loss: 0.224972\n",
      "Epoch 203, test loss: 0.229798\n",
      "Epoch 204, test loss: 0.222252\n",
      "Epoch 205, test loss: 0.226963\n",
      "Epoch 206, test loss: 0.225498\n",
      "Epoch 207, test loss: 0.227028\n",
      "Epoch 208, test loss: 0.223137\n",
      "Epoch 209, test loss: 0.224176\n",
      "Epoch 210, test loss: 0.226572\n",
      "Epoch 211, test loss: 0.227375\n",
      "Epoch 212, test loss: 0.225215\n",
      "Epoch 213, test loss: 0.227527\n",
      "Epoch 214, test loss: 0.225773\n",
      "Epoch 215, test loss: 0.224336\n",
      "Epoch 216, test loss: 0.224205\n",
      "Epoch 217, test loss: 0.227168\n",
      "Epoch 218, test loss: 0.223614\n",
      "Epoch 219, test loss: 0.225927\n",
      "Epoch 220, test loss: 0.225384\n",
      "Epoch 221, test loss: 0.224043\n",
      "Epoch 222, test loss: 0.225740\n",
      "Epoch 223, test loss: 0.227422\n",
      "Epoch 224, test loss: 0.224903\n",
      "Epoch 225, test loss: 0.225229\n",
      "Epoch 226, test loss: 0.224991\n",
      "Epoch 227, test loss: 0.222666\n",
      "Epoch 228, test loss: 0.226111\n",
      "Epoch 229, test loss: 0.225514\n",
      "Epoch 230, test loss: 0.223302\n",
      "Epoch 231, test loss: 0.226407\n",
      "Epoch 232, test loss: 0.225792\n",
      "Epoch 233, test loss: 0.230204\n",
      "Epoch 234, test loss: 0.229307\n",
      "Epoch 235, test loss: 0.235666\n",
      "Epoch 236, test loss: 0.225167\n",
      "Epoch 237, test loss: 0.228110\n",
      "Epoch 238, test loss: 0.223683\n",
      "Epoch 239, test loss: 0.226397\n",
      "Pretrain data: 19732321.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6423/95305\n",
      "Found 815 continuous time series\n",
      "Data shape: (101730, 24), Train/test: 101728/2\n",
      "Train test ratio: 50864.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1589\n",
      "Epoch 0, train loss: 0.204048\n",
      "Epoch 1, train loss: 0.222778\n",
      "Epoch 2, train loss: 0.168777\n",
      "Epoch 3, train loss: 0.292965\n",
      "Epoch 4, train loss: 0.270339\n",
      "Epoch 5, train loss: 0.277002\n",
      "Epoch 6, train loss: 0.216312\n",
      "Epoch 7, train loss: 0.174739\n",
      "Epoch 8, train loss: 0.187766\n",
      "Epoch 9, train loss: 0.188419\n",
      "Epoch 10, train loss: 0.193769\n",
      "Epoch 11, train loss: 0.214226\n",
      "Epoch 12, train loss: 0.195973\n",
      "Epoch 13, train loss: 0.188247\n",
      "Epoch 14, train loss: 0.197748\n",
      "Epoch 15, train loss: 0.177003\n",
      "Epoch 16, train loss: 0.189363\n",
      "Epoch 17, train loss: 0.244524\n",
      "Epoch 18, train loss: 0.191548\n",
      "Epoch 19, train loss: 0.203216\n",
      "Epoch 20, train loss: 0.205238\n",
      "Epoch 21, train loss: 0.211218\n",
      "Epoch 22, train loss: 0.202055\n",
      "Epoch 23, train loss: 0.194161\n",
      "Epoch 24, train loss: 0.225680\n",
      "Epoch 25, train loss: 0.265566\n",
      "Epoch 26, train loss: 0.188810\n",
      "Epoch 27, train loss: 0.174342\n",
      "Epoch 28, train loss: 0.193811\n",
      "Epoch 29, train loss: 0.257902\n",
      "Epoch 30, train loss: 0.180780\n",
      "Epoch 31, train loss: 0.200159\n",
      "Epoch 32, train loss: 0.187820\n",
      "Epoch 33, train loss: 0.158849\n",
      "Epoch 34, train loss: 0.241996\n",
      "Epoch 35, train loss: 0.167436\n",
      "Epoch 36, train loss: 0.260313\n",
      "Epoch 37, train loss: 0.188237\n",
      "Epoch 38, train loss: 0.165087\n",
      "Epoch 39, train loss: 0.223514\n",
      "Epoch 40, train loss: 0.253065\n",
      "Epoch 41, train loss: 0.190626\n",
      "Epoch 42, train loss: 0.278917\n",
      "Epoch 43, train loss: 0.214316\n",
      "Epoch 44, train loss: 0.239802\n",
      "Epoch 45, train loss: 0.171677\n",
      "Epoch 46, train loss: 0.181728\n",
      "Epoch 47, train loss: 0.247729\n",
      "Epoch 48, train loss: 0.230423\n",
      "Epoch 49, train loss: 0.206759\n",
      "Epoch 50, train loss: 0.196250\n",
      "Epoch 51, train loss: 0.197907\n",
      "Epoch 52, train loss: 0.225763\n",
      "Epoch 53, train loss: 0.194577\n",
      "Epoch 54, train loss: 0.221860\n",
      "Epoch 55, train loss: 0.289640\n",
      "Epoch 56, train loss: 0.169073\n",
      "Epoch 57, train loss: 0.160434\n",
      "Epoch 58, train loss: 0.250860\n",
      "Epoch 59, train loss: 0.269401\n",
      "Epoch 60, train loss: 0.241970\n",
      "Epoch 61, train loss: 0.204507\n",
      "Epoch 62, train loss: 0.203898\n",
      "Epoch 63, train loss: 0.187330\n",
      "Epoch 64, train loss: 0.170267\n",
      "Epoch 65, train loss: 0.272065\n",
      "Epoch 66, train loss: 0.234740\n",
      "Epoch 67, train loss: 0.208879\n",
      "Epoch 68, train loss: 0.225262\n",
      "Epoch 69, train loss: 0.160069\n",
      "Epoch 70, train loss: 0.209523\n",
      "Epoch 71, train loss: 0.221631\n",
      "Epoch 72, train loss: 0.199683\n",
      "Epoch 73, train loss: 0.253782\n",
      "Epoch 74, train loss: 0.210495\n",
      "Epoch 75, train loss: 0.262130\n",
      "Epoch 76, train loss: 0.259077\n",
      "Epoch 77, train loss: 0.171175\n",
      "Epoch 78, train loss: 0.129217\n",
      "Epoch 79, train loss: 0.238631\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "x here is\n",
      "[[283. 282. 281. ... 135. 130. 126.]\n",
      " [282. 281. 277. ... 130. 126. 123.]\n",
      " [281. 277. 267. ... 126. 123. 121.]\n",
      " ...\n",
      " [222. 217. 213. ... 159. 159. 156.]\n",
      " [217. 213. 211. ... 159. 156. 154.]\n",
      " [213. 211. 208. ... 156. 154. 152.]]\n",
      "y here is\n",
      "[[ 95.  95.  95. ...  95.  95.  95.]\n",
      " [ 89.  89.  89. ...  89.  89.  89.]\n",
      " [ 98.  98.  98. ...  98.  98.  98.]\n",
      " ...\n",
      " [151. 151. 151. ... 151. 151. 151.]\n",
      " [149. 149. 149. ... 149. 149. 149.]\n",
      " [144. 144. 144. ... 144. 144. 144.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (2616, 24), Train/test: 1/2615\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[160. 158. 160. ... 271. 262. 251.]\n",
      " [158. 160. 166. ... 262. 251. 238.]\n",
      " [160. 166. 175. ... 251. 238. 232.]\n",
      " ...\n",
      " [121. 154. 184. ... 211. 202. 196.]\n",
      " [154. 184. 204. ... 202. 196. 199.]\n",
      " [184. 204. 218. ... 196. 199. 224.]]\n",
      "y here is\n",
      "[[243. 243. 243. ... 243. 243. 243.]\n",
      " [236. 236. 236. ... 236. 236. 236.]\n",
      " [231. 231. 231. ... 231. 231. 231.]\n",
      " ...\n",
      " [268. 268. 268. ... 268. 268. 268.]\n",
      " [301. 301. 301. ... 301. 301. 301.]\n",
      " [290. 290. 290. ... 290. 290. 290.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 686/9398\n",
      "Found 27 continuous time series\n",
      "Data shape: (10086, 24), Train/test: 10084/2\n",
      "Train test ratio: 5042.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47722CBE0>\n",
      "Epoch 0, test loss: 0.211440\n",
      "Epoch 1, test loss: 0.206419\n",
      "Epoch 2, test loss: 0.208784\n",
      "Epoch 3, test loss: 0.209480\n",
      "Epoch 4, test loss: 0.207124\n",
      "Epoch 5, test loss: 0.207366\n",
      "Epoch 6, test loss: 0.208826\n",
      "Epoch 7, test loss: 0.209493\n",
      "Epoch 8, test loss: 0.208753\n",
      "Epoch 9, test loss: 0.208132\n",
      "Epoch 10, test loss: 0.209685\n",
      "Epoch 11, test loss: 0.213383\n",
      "Epoch 12, test loss: 0.207915\n",
      "Epoch 13, test loss: 0.216059\n",
      "Epoch 14, test loss: 0.207342\n",
      "Epoch 15, test loss: 0.209320\n",
      "Epoch 16, test loss: 0.210367\n",
      "Epoch 17, test loss: 0.207813\n",
      "Epoch 18, test loss: 0.209902\n",
      "Epoch 19, test loss: 0.207340\n",
      "Epoch 20, test loss: 0.209124\n",
      "Epoch 21, test loss: 0.208063\n",
      "Epoch 22, test loss: 0.207813\n",
      "Epoch 23, test loss: 0.207631\n",
      "Epoch 24, test loss: 0.209743\n",
      "Epoch 25, test loss: 0.209729\n",
      "Epoch 26, test loss: 0.207729\n",
      "Epoch 27, test loss: 0.207572\n",
      "Epoch 28, test loss: 0.208184\n",
      "Epoch 29, test loss: 0.208258\n",
      "Epoch 30, test loss: 0.207396\n",
      "Epoch 31, test loss: 0.209870\n",
      "Epoch 32, test loss: 0.207606\n",
      "Epoch 33, test loss: 0.210976\n",
      "Epoch 34, test loss: 0.207839\n",
      "Epoch 35, test loss: 0.208806\n",
      "Epoch 36, test loss: 0.211759\n",
      "Epoch 37, test loss: 0.208194\n",
      "Epoch 38, test loss: 0.207903\n",
      "Epoch 39, test loss: 0.213588\n",
      "Epoch 40, test loss: 0.210632\n",
      "Epoch 41, test loss: 0.209685\n",
      "Epoch 42, test loss: 0.208614\n",
      "Epoch 43, test loss: 0.210336\n",
      "Epoch 44, test loss: 0.208379\n",
      "Epoch 45, test loss: 0.208451\n",
      "Epoch 46, test loss: 0.209179\n",
      "Epoch 47, test loss: 0.208811\n",
      "Epoch 48, test loss: 0.208098\n",
      "Epoch 49, test loss: 0.208167\n",
      "Epoch 50, test loss: 0.207967\n",
      "Epoch 51, test loss: 0.207924\n",
      "Epoch 52, test loss: 0.209730\n",
      "Epoch 53, test loss: 0.207640\n",
      "Epoch 54, test loss: 0.207853\n",
      "Epoch 55, test loss: 0.207653\n",
      "Epoch 56, test loss: 0.209777\n",
      "Epoch 57, test loss: 0.209435\n",
      "Epoch 58, test loss: 0.207805\n",
      "Epoch 59, test loss: 0.208155\n",
      "Epoch 60, test loss: 0.207673\n",
      "Epoch 61, test loss: 0.207816\n",
      "Epoch 62, test loss: 0.213824\n",
      "Epoch 63, test loss: 0.208273\n",
      "Epoch 64, test loss: 0.208254\n",
      "Epoch 65, test loss: 0.211090\n",
      "Epoch 66, test loss: 0.211320\n",
      "Epoch 67, test loss: 0.207914\n",
      "Epoch 68, test loss: 0.208124\n",
      "Epoch 69, test loss: 0.208138\n",
      "Epoch 70, test loss: 0.213793\n",
      "Epoch 71, test loss: 0.208151\n",
      "Epoch 72, test loss: 0.212462\n",
      "Epoch 73, test loss: 0.208338\n",
      "Epoch 74, test loss: 0.208084\n",
      "Epoch 75, test loss: 0.208008\n",
      "Epoch 76, test loss: 0.211801\n",
      "Epoch 77, test loss: 0.210562\n",
      "Epoch 78, test loss: 0.209316\n",
      "Epoch 79, test loss: 0.209265\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47722CBE0>\n",
      "Epoch 0, test loss: 0.206523\n",
      "Epoch 1, test loss: 0.206136\n",
      "Epoch 2, test loss: 0.207130\n",
      "Epoch 3, test loss: 0.206688\n",
      "Epoch 4, test loss: 0.207172\n",
      "Epoch 5, test loss: 0.206671\n",
      "Epoch 6, test loss: 0.213715\n",
      "Epoch 7, test loss: 0.207857\n",
      "Epoch 8, test loss: 0.207823\n",
      "Epoch 9, test loss: 0.207300\n",
      "Epoch 10, test loss: 0.207647\n",
      "Epoch 11, test loss: 0.207735\n",
      "Epoch 12, test loss: 0.207437\n",
      "Epoch 13, test loss: 0.209152\n",
      "Epoch 14, test loss: 0.207402\n",
      "Epoch 15, test loss: 0.207740\n",
      "Epoch 16, test loss: 0.207633\n",
      "Epoch 17, test loss: 0.207598\n",
      "Epoch 18, test loss: 0.208486\n",
      "Epoch 19, test loss: 0.209279\n",
      "Epoch 20, test loss: 0.208959\n",
      "Epoch 21, test loss: 0.208801\n",
      "Epoch 22, test loss: 0.208268\n",
      "Epoch 23, test loss: 0.207866\n",
      "Epoch 24, test loss: 0.209019\n",
      "Epoch 25, test loss: 0.208677\n",
      "Epoch 26, test loss: 0.207671\n",
      "Epoch 27, test loss: 0.207725\n",
      "Epoch 28, test loss: 0.208596\n",
      "Epoch 29, test loss: 0.210162\n",
      "Epoch 30, test loss: 0.208530\n",
      "Epoch 31, test loss: 0.209006\n",
      "Epoch 32, test loss: 0.213011\n",
      "Epoch 33, test loss: 0.208383\n",
      "Epoch 34, test loss: 0.208952\n",
      "Epoch 35, test loss: 0.210675\n",
      "Epoch 36, test loss: 0.209253\n",
      "Epoch 37, test loss: 0.209979\n",
      "Epoch 38, test loss: 0.207736\n",
      "Epoch 39, test loss: 0.208412\n",
      "Epoch 40, test loss: 0.208819\n",
      "Epoch 41, test loss: 0.209121\n",
      "Epoch 42, test loss: 0.209146\n",
      "Epoch 43, test loss: 0.209218\n",
      "Epoch 44, test loss: 0.208585\n",
      "Epoch 45, test loss: 0.209412\n",
      "Epoch 46, test loss: 0.208332\n",
      "Epoch 47, test loss: 0.208890\n",
      "Epoch 48, test loss: 0.208343\n",
      "Epoch 49, test loss: 0.208695\n",
      "Epoch 50, test loss: 0.209140\n",
      "Epoch 51, test loss: 0.209078\n",
      "Epoch 52, test loss: 0.208783\n",
      "Epoch 53, test loss: 0.210202\n",
      "Epoch 54, test loss: 0.210209\n",
      "Epoch 55, test loss: 0.208390\n",
      "Epoch 56, test loss: 0.208803\n",
      "Epoch 57, test loss: 0.210730\n",
      "Epoch 58, test loss: 0.208424\n",
      "Epoch 59, test loss: 0.208733\n",
      "Epoch 60, test loss: 0.212338\n",
      "Epoch 61, test loss: 0.208967\n",
      "Epoch 62, test loss: 0.210213\n",
      "Epoch 63, test loss: 0.208839\n",
      "Epoch 64, test loss: 0.208913\n",
      "Epoch 65, test loss: 0.208688\n",
      "Epoch 66, test loss: 0.208076\n",
      "Epoch 67, test loss: 0.209121\n",
      "Epoch 68, test loss: 0.208343\n",
      "Epoch 69, test loss: 0.210096\n",
      "Epoch 70, test loss: 0.208815\n",
      "Epoch 71, test loss: 0.210654\n",
      "Epoch 72, test loss: 0.208698\n",
      "Epoch 73, test loss: 0.208916\n",
      "Epoch 74, test loss: 0.209063\n",
      "Epoch 75, test loss: 0.208781\n",
      "Epoch 76, test loss: 0.213806\n",
      "Epoch 77, test loss: 0.209710\n",
      "Epoch 78, test loss: 0.212814\n",
      "Epoch 79, test loss: 0.208553\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A47722CBE0>\n",
      "Epoch 0, test loss: 0.301825\n",
      "Epoch 1, test loss: 0.226229\n",
      "Epoch 2, test loss: 0.220595\n",
      "Epoch 3, test loss: 0.216493\n",
      "Epoch 4, test loss: 0.215864\n",
      "Epoch 5, test loss: 0.217454\n",
      "Epoch 6, test loss: 0.220316\n",
      "Epoch 7, test loss: 0.215879\n",
      "Epoch 8, test loss: 0.218737\n",
      "Epoch 9, test loss: 0.218218\n",
      "Epoch 10, test loss: 0.215264\n",
      "Epoch 11, test loss: 0.215134\n",
      "Epoch 12, test loss: 0.215068\n",
      "Epoch 13, test loss: 0.214894\n",
      "Epoch 14, test loss: 0.214915\n",
      "Epoch 15, test loss: 0.218751\n",
      "Epoch 16, test loss: 0.215610\n",
      "Epoch 17, test loss: 0.217184\n",
      "Epoch 18, test loss: 0.215170\n",
      "Epoch 19, test loss: 0.214454\n",
      "Epoch 20, test loss: 0.216504\n",
      "Epoch 21, test loss: 0.215122\n",
      "Epoch 22, test loss: 0.215985\n",
      "Epoch 23, test loss: 0.214313\n",
      "Epoch 24, test loss: 0.214714\n",
      "Epoch 25, test loss: 0.221068\n",
      "Epoch 26, test loss: 0.215513\n",
      "Epoch 27, test loss: 0.214090\n",
      "Epoch 28, test loss: 0.214217\n",
      "Epoch 29, test loss: 0.214244\n",
      "Epoch 30, test loss: 0.214264\n",
      "Epoch 31, test loss: 0.214405\n",
      "Epoch 32, test loss: 0.214664\n",
      "Epoch 33, test loss: 0.214321\n",
      "Epoch 34, test loss: 0.214458\n",
      "Epoch 35, test loss: 0.216927\n",
      "Epoch 36, test loss: 0.215022\n",
      "Epoch 37, test loss: 0.215125\n",
      "Epoch 38, test loss: 0.214065\n",
      "Epoch 39, test loss: 0.214400\n",
      "Epoch 40, test loss: 0.214384\n",
      "Epoch 41, test loss: 0.214650\n",
      "Epoch 42, test loss: 0.214161\n",
      "Epoch 43, test loss: 0.214119\n",
      "Epoch 44, test loss: 0.217292\n",
      "Epoch 45, test loss: 0.214261\n",
      "Epoch 46, test loss: 0.213893\n",
      "Epoch 47, test loss: 0.214429\n",
      "Epoch 48, test loss: 0.216741\n",
      "Epoch 49, test loss: 0.213755\n",
      "Epoch 50, test loss: 0.213522\n",
      "Epoch 51, test loss: 0.213976\n",
      "Epoch 52, test loss: 0.220347\n",
      "Epoch 53, test loss: 0.213264\n",
      "Epoch 54, test loss: 0.213335\n",
      "Epoch 55, test loss: 0.212254\n",
      "Epoch 56, test loss: 0.212863\n",
      "Epoch 57, test loss: 0.212984\n",
      "Epoch 58, test loss: 0.212704\n",
      "Epoch 59, test loss: 0.214929\n",
      "Epoch 60, test loss: 0.212423\n",
      "Epoch 61, test loss: 0.212478\n",
      "Epoch 62, test loss: 0.211650\n",
      "Epoch 63, test loss: 0.212975\n",
      "Epoch 64, test loss: 0.212942\n",
      "Epoch 65, test loss: 0.212094\n",
      "Epoch 66, test loss: 0.212206\n",
      "Epoch 67, test loss: 0.211693\n",
      "Epoch 68, test loss: 0.211454\n",
      "Epoch 69, test loss: 0.212635\n",
      "Epoch 70, test loss: 0.212589\n",
      "Epoch 71, test loss: 0.213346\n",
      "Epoch 72, test loss: 0.212604\n",
      "Epoch 73, test loss: 0.212336\n",
      "Epoch 74, test loss: 0.212030\n",
      "Epoch 75, test loss: 0.212241\n",
      "Epoch 76, test loss: 0.217316\n",
      "Epoch 77, test loss: 0.213118\n",
      "Epoch 78, test loss: 0.214854\n",
      "Epoch 79, test loss: 0.211530\n",
      "Epoch 80, test loss: 0.212260\n",
      "Epoch 81, test loss: 0.212441\n",
      "Epoch 82, test loss: 0.212099\n",
      "Epoch 83, test loss: 0.213372\n",
      "Epoch 84, test loss: 0.212614\n",
      "Epoch 85, test loss: 0.212091\n",
      "Epoch 86, test loss: 0.211686\n",
      "Epoch 87, test loss: 0.212356\n",
      "Epoch 88, test loss: 0.212293\n",
      "Epoch 89, test loss: 0.212996\n",
      "Epoch 90, test loss: 0.212801\n",
      "Epoch 91, test loss: 0.212063\n",
      "Epoch 92, test loss: 0.212441\n",
      "Epoch 93, test loss: 0.212541\n",
      "Epoch 94, test loss: 0.212148\n",
      "Epoch 95, test loss: 0.214200\n",
      "Epoch 96, test loss: 0.213358\n",
      "Epoch 97, test loss: 0.214952\n",
      "Epoch 98, test loss: 0.214608\n",
      "Epoch 99, test loss: 0.216734\n",
      "Epoch 100, test loss: 0.213388\n",
      "Epoch 101, test loss: 0.213657\n",
      "Epoch 102, test loss: 0.214120\n",
      "Epoch 103, test loss: 0.213276\n",
      "Epoch 104, test loss: 0.215374\n",
      "Epoch 105, test loss: 0.213890\n",
      "Epoch 106, test loss: 0.214672\n",
      "Epoch 107, test loss: 0.217400\n",
      "Epoch 108, test loss: 0.215975\n",
      "Epoch 109, test loss: 0.213811\n",
      "Epoch 110, test loss: 0.215210\n",
      "Epoch 111, test loss: 0.215670\n",
      "Epoch 112, test loss: 0.213420\n",
      "Epoch 113, test loss: 0.214047\n",
      "Epoch 114, test loss: 0.214412\n",
      "Epoch 115, test loss: 0.215887\n",
      "Epoch 116, test loss: 0.213164\n",
      "Epoch 117, test loss: 0.214342\n",
      "Epoch 118, test loss: 0.218458\n",
      "Epoch 119, test loss: 0.213775\n",
      "Epoch 120, test loss: 0.213879\n",
      "Epoch 121, test loss: 0.213782\n",
      "Epoch 122, test loss: 0.213416\n",
      "Epoch 123, test loss: 0.214884\n",
      "Epoch 124, test loss: 0.213567\n",
      "Epoch 125, test loss: 0.215888\n",
      "Epoch 126, test loss: 0.214562\n",
      "Epoch 127, test loss: 0.213354\n",
      "Epoch 128, test loss: 0.213679\n",
      "Epoch 129, test loss: 0.217029\n",
      "Epoch 130, test loss: 0.215453\n",
      "Epoch 131, test loss: 0.214374\n",
      "Epoch 132, test loss: 0.215112\n",
      "Epoch 133, test loss: 0.214647\n",
      "Epoch 134, test loss: 0.215646\n",
      "Epoch 135, test loss: 0.214748\n",
      "Epoch 136, test loss: 0.213938\n",
      "Epoch 137, test loss: 0.214316\n",
      "Epoch 138, test loss: 0.214745\n",
      "Epoch 139, test loss: 0.214976\n",
      "Epoch 140, test loss: 0.215920\n",
      "Epoch 141, test loss: 0.217318\n",
      "Epoch 142, test loss: 0.216867\n",
      "Epoch 143, test loss: 0.219304\n",
      "Epoch 144, test loss: 0.214779\n",
      "Epoch 145, test loss: 0.214802\n",
      "Epoch 146, test loss: 0.215127\n",
      "Epoch 147, test loss: 0.215356\n",
      "Epoch 148, test loss: 0.214373\n",
      "Epoch 149, test loss: 0.213944\n",
      "Epoch 150, test loss: 0.214708\n",
      "Epoch 151, test loss: 0.215234\n",
      "Epoch 152, test loss: 0.216734\n",
      "Epoch 153, test loss: 0.215642\n",
      "Epoch 154, test loss: 0.222635\n",
      "Epoch 155, test loss: 0.214277\n",
      "Epoch 156, test loss: 0.214751\n",
      "Epoch 157, test loss: 0.215319\n",
      "Epoch 158, test loss: 0.217793\n",
      "Epoch 159, test loss: 0.219218\n",
      "Epoch 160, test loss: 0.214684\n",
      "Epoch 161, test loss: 0.222931\n",
      "Epoch 162, test loss: 0.214585\n",
      "Epoch 163, test loss: 0.217647\n",
      "Epoch 164, test loss: 0.216234\n",
      "Epoch 165, test loss: 0.214841\n",
      "Epoch 166, test loss: 0.215852\n",
      "Epoch 167, test loss: 0.217048\n",
      "Epoch 168, test loss: 0.215253\n",
      "Epoch 169, test loss: 0.215357\n",
      "Epoch 170, test loss: 0.215318\n",
      "Epoch 171, test loss: 0.215169\n",
      "Epoch 172, test loss: 0.220539\n",
      "Epoch 173, test loss: 0.218987\n",
      "Epoch 174, test loss: 0.216418\n",
      "Epoch 175, test loss: 0.215097\n",
      "Epoch 176, test loss: 0.216025\n",
      "Epoch 177, test loss: 0.216518\n",
      "Epoch 178, test loss: 0.215100\n",
      "Epoch 179, test loss: 0.216242\n",
      "Epoch 180, test loss: 0.216274\n",
      "Epoch 181, test loss: 0.215871\n",
      "Epoch 182, test loss: 0.216350\n",
      "Epoch 183, test loss: 0.215558\n",
      "Epoch 184, test loss: 0.218028\n",
      "Epoch 185, test loss: 0.215730\n",
      "Epoch 186, test loss: 0.215683\n",
      "Epoch 187, test loss: 0.215729\n",
      "Epoch 188, test loss: 0.216335\n",
      "Epoch 189, test loss: 0.215678\n",
      "Epoch 190, test loss: 0.215691\n",
      "Epoch 191, test loss: 0.216864\n",
      "Epoch 192, test loss: 0.215870\n",
      "Epoch 193, test loss: 0.216655\n",
      "Epoch 194, test loss: 0.216308\n",
      "Epoch 195, test loss: 0.217084\n",
      "Epoch 196, test loss: 0.216107\n",
      "Epoch 197, test loss: 0.218159\n",
      "Epoch 198, test loss: 0.218703\n",
      "Epoch 199, test loss: 0.216265\n",
      "Epoch 200, test loss: 0.217663\n",
      "Epoch 201, test loss: 0.216469\n",
      "Epoch 202, test loss: 0.217210\n",
      "Epoch 203, test loss: 0.216761\n",
      "Epoch 204, test loss: 0.216778\n",
      "Epoch 205, test loss: 0.215670\n",
      "Epoch 206, test loss: 0.219400\n",
      "Epoch 207, test loss: 0.216098\n",
      "Epoch 208, test loss: 0.217070\n",
      "Epoch 209, test loss: 0.216516\n",
      "Epoch 210, test loss: 0.219227\n",
      "Epoch 211, test loss: 0.217025\n",
      "Epoch 212, test loss: 0.216397\n",
      "Epoch 213, test loss: 0.217335\n",
      "Epoch 214, test loss: 0.218550\n",
      "Epoch 215, test loss: 0.216674\n",
      "Epoch 216, test loss: 0.216224\n",
      "Epoch 217, test loss: 0.216928\n",
      "Epoch 218, test loss: 0.216863\n",
      "Epoch 219, test loss: 0.216844\n",
      "Epoch 220, test loss: 0.216558\n",
      "Epoch 221, test loss: 0.217190\n",
      "Epoch 222, test loss: 0.216900\n",
      "Epoch 223, test loss: 0.215983\n",
      "Epoch 224, test loss: 0.216603\n",
      "Epoch 225, test loss: 0.216665\n",
      "Epoch 226, test loss: 0.217781\n",
      "Epoch 227, test loss: 0.216337\n",
      "Epoch 228, test loss: 0.217402\n",
      "Epoch 229, test loss: 0.216782\n",
      "Epoch 230, test loss: 0.216736\n",
      "Epoch 231, test loss: 0.215860\n",
      "Epoch 232, test loss: 0.217529\n",
      "Epoch 233, test loss: 0.216181\n",
      "Epoch 234, test loss: 0.216509\n",
      "Epoch 235, test loss: 0.218097\n",
      "Epoch 236, test loss: 0.216814\n",
      "Epoch 237, test loss: 0.216526\n",
      "Epoch 238, test loss: 0.219151\n",
      "Epoch 239, test loss: 0.216081\n",
      "Pretrain data: 19786775.0\n",
      "Building dataset, requesting data from 0 to 665\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6142/98536\n",
      "Found 665 continuous time series\n",
      "Data shape: (104680, 24), Train/test: 104678/2\n",
      "Train test ratio: 52339.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1635\n",
      "Epoch 0, train loss: 0.184158\n",
      "Epoch 1, train loss: 0.287849\n",
      "Epoch 2, train loss: 0.179485\n",
      "Epoch 3, train loss: 0.238315\n",
      "Epoch 4, train loss: 0.202616\n",
      "Epoch 5, train loss: 0.189937\n",
      "Epoch 6, train loss: 0.214982\n",
      "Epoch 7, train loss: 0.159666\n",
      "Epoch 8, train loss: 0.169446\n",
      "Epoch 9, train loss: 0.216697\n",
      "Epoch 10, train loss: 0.181000\n",
      "Epoch 11, train loss: 0.144186\n",
      "Epoch 12, train loss: 0.236796\n",
      "Epoch 13, train loss: 0.160200\n",
      "Epoch 14, train loss: 0.178212\n",
      "Epoch 15, train loss: 0.233219\n",
      "Epoch 16, train loss: 0.232210\n",
      "Epoch 17, train loss: 0.134092\n",
      "Epoch 18, train loss: 0.157243\n",
      "Epoch 19, train loss: 0.186599\n",
      "Epoch 20, train loss: 0.192803\n",
      "Epoch 21, train loss: 0.174049\n",
      "Epoch 22, train loss: 0.177588\n",
      "Epoch 23, train loss: 0.188164\n",
      "Epoch 24, train loss: 0.196845\n",
      "Epoch 25, train loss: 0.346073\n",
      "Epoch 26, train loss: 0.232985\n",
      "Epoch 27, train loss: 0.274902\n",
      "Epoch 28, train loss: 0.304166\n",
      "Epoch 29, train loss: 0.193484\n",
      "Epoch 30, train loss: 0.216849\n",
      "Epoch 31, train loss: 0.163439\n",
      "Epoch 32, train loss: 0.181058\n",
      "Epoch 33, train loss: 0.175407\n",
      "Epoch 34, train loss: 0.181603\n",
      "Epoch 35, train loss: 0.187071\n",
      "Epoch 36, train loss: 0.206786\n",
      "Epoch 37, train loss: 0.187239\n",
      "Epoch 38, train loss: 0.189541\n",
      "Epoch 39, train loss: 0.200577\n",
      "Epoch 40, train loss: 0.207497\n",
      "Epoch 41, train loss: 0.264080\n",
      "Epoch 42, train loss: 0.175501\n",
      "Epoch 43, train loss: 0.197947\n",
      "Epoch 44, train loss: 0.214117\n",
      "Epoch 45, train loss: 0.192782\n",
      "Epoch 46, train loss: 0.259563\n",
      "Epoch 47, train loss: 0.198951\n",
      "Epoch 48, train loss: 0.182498\n",
      "Epoch 49, train loss: 0.234857\n",
      "Epoch 50, train loss: 0.169541\n",
      "Epoch 51, train loss: 0.143501\n",
      "Epoch 52, train loss: 0.183804\n",
      "Epoch 53, train loss: 0.194415\n",
      "Epoch 54, train loss: 0.190993\n",
      "Epoch 55, train loss: 0.188573\n",
      "Epoch 56, train loss: 0.216600\n",
      "Epoch 57, train loss: 0.159207\n",
      "Epoch 58, train loss: 0.166448\n",
      "Epoch 59, train loss: 0.258817\n",
      "Epoch 60, train loss: 0.193466\n",
      "Epoch 61, train loss: 0.186439\n",
      "Epoch 62, train loss: 0.166369\n",
      "Epoch 63, train loss: 0.209662\n",
      "Epoch 64, train loss: 0.228588\n",
      "Epoch 65, train loss: 0.238367\n",
      "Epoch 66, train loss: 0.158081\n",
      "Epoch 67, train loss: 0.245247\n",
      "Epoch 68, train loss: 0.204864\n",
      "Epoch 69, train loss: 0.190642\n",
      "Epoch 70, train loss: 0.170821\n",
      "Epoch 71, train loss: 0.195482\n",
      "Epoch 72, train loss: 0.190135\n",
      "Epoch 73, train loss: 0.242228\n",
      "Epoch 74, train loss: 0.239560\n",
      "Epoch 75, train loss: 0.155967\n",
      "Epoch 76, train loss: 0.245506\n",
      "Epoch 77, train loss: 0.180921\n",
      "Epoch 78, train loss: 0.212995\n",
      "Epoch 79, train loss: 0.167444\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "x here is\n",
      "[[254. 250. 249. ... 191. 188. 185.]\n",
      " [250. 249. 247. ... 188. 185. 184.]\n",
      " [249. 247. 242. ... 185. 184. 182.]\n",
      " ...\n",
      " [224. 223. 222. ... 173. 169. 163.]\n",
      " [223. 222. 221. ... 169. 163. 155.]\n",
      " [222. 221. 216. ... 163. 155. 149.]]\n",
      "y here is\n",
      "[[176. 176. 176. ... 176. 176. 176.]\n",
      " [174. 174. 174. ... 174. 174. 174.]\n",
      " [170. 170. 170. ... 170. 170. 170.]\n",
      " ...\n",
      " [128. 128. 128. ... 128. 128. 128.]\n",
      " [125. 125. 125. ... 125. 125. 125.]\n",
      " [121. 121. 121. ... 121. 121. 121.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 45 continuous time series\n",
      "Data shape: (1635, 24), Train/test: 1/1634\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "x here is\n",
      "[[ 76.  72.  68. ... 127. 128. 129.]\n",
      " [ 72.  68.  65. ... 128. 129. 129.]\n",
      " [ 68.  65.  63. ... 129. 129. 127.]\n",
      " ...\n",
      " [275. 277. 287. ... 232. 236. 237.]\n",
      " [277. 287. 295. ... 236. 237. 247.]\n",
      " [287. 295. 298. ... 237. 247. 254.]]\n",
      "y here is\n",
      "[[126. 126. 126. ... 126. 126. 126.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " ...\n",
      " [280. 280. 280. ... 280. 280. 280.]\n",
      " [283. 283. 283. ... 283. 283. 283.]\n",
      " [282. 282. 282. ... 282. 282. 282.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 967/6167\n",
      "Found 177 continuous time series\n",
      "Data shape: (7136, 24), Train/test: 7134/2\n",
      "Train test ratio: 3567.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29432F7F0>\n",
      "Epoch 0, test loss: 0.217036\n",
      "Epoch 1, test loss: 0.218058\n",
      "Epoch 2, test loss: 0.216635\n",
      "Epoch 3, test loss: 0.217107\n",
      "Epoch 4, test loss: 0.220992\n",
      "Epoch 5, test loss: 0.223042\n",
      "Epoch 6, test loss: 0.218445\n",
      "Epoch 7, test loss: 0.219199\n",
      "Epoch 8, test loss: 0.218608\n",
      "Epoch 9, test loss: 0.216510\n",
      "Epoch 10, test loss: 0.217519\n",
      "Epoch 11, test loss: 0.218044\n",
      "Epoch 12, test loss: 0.217941\n",
      "Epoch 13, test loss: 0.217179\n",
      "Epoch 14, test loss: 0.216879\n",
      "Epoch 15, test loss: 0.217450\n",
      "Epoch 16, test loss: 0.221400\n",
      "Epoch 17, test loss: 0.217201\n",
      "Epoch 18, test loss: 0.216610\n",
      "Epoch 19, test loss: 0.216807\n",
      "Epoch 20, test loss: 0.217121\n",
      "Epoch 21, test loss: 0.217333\n",
      "Epoch 22, test loss: 0.220868\n",
      "Epoch 23, test loss: 0.217527\n",
      "Epoch 24, test loss: 0.221692\n",
      "Epoch 25, test loss: 0.217039\n",
      "Epoch 26, test loss: 0.216725\n",
      "Epoch 27, test loss: 0.220758\n",
      "Epoch 28, test loss: 0.217460\n",
      "Epoch 29, test loss: 0.217370\n",
      "Epoch 30, test loss: 0.217448\n",
      "Epoch 31, test loss: 0.219165\n",
      "Epoch 32, test loss: 0.218530\n",
      "Epoch 33, test loss: 0.224232\n",
      "Epoch 34, test loss: 0.217177\n",
      "Epoch 35, test loss: 0.217081\n",
      "Epoch 36, test loss: 0.217151\n",
      "Epoch 37, test loss: 0.223809\n",
      "Epoch 38, test loss: 0.218977\n",
      "Epoch 39, test loss: 0.216984\n",
      "Epoch 40, test loss: 0.217767\n",
      "Epoch 41, test loss: 0.218586\n",
      "Epoch 42, test loss: 0.216350\n",
      "Epoch 43, test loss: 0.219473\n",
      "Epoch 44, test loss: 0.219208\n",
      "Epoch 45, test loss: 0.219317\n",
      "Epoch 46, test loss: 0.219151\n",
      "Epoch 47, test loss: 0.217628\n",
      "Epoch 48, test loss: 0.217802\n",
      "Epoch 49, test loss: 0.217833\n",
      "Epoch 50, test loss: 0.218057\n",
      "Epoch 51, test loss: 0.217885\n",
      "Epoch 52, test loss: 0.217373\n",
      "Epoch 53, test loss: 0.219150\n",
      "Epoch 54, test loss: 0.219310\n",
      "Epoch 55, test loss: 0.219496\n",
      "Epoch 56, test loss: 0.217774\n",
      "Epoch 57, test loss: 0.217855\n",
      "Epoch 58, test loss: 0.217815\n",
      "Epoch 59, test loss: 0.216900\n",
      "Epoch 60, test loss: 0.216918\n",
      "Epoch 61, test loss: 0.216846\n",
      "Epoch 62, test loss: 0.220141\n",
      "Epoch 63, test loss: 0.217516\n",
      "Epoch 64, test loss: 0.218822\n",
      "Epoch 65, test loss: 0.222544\n",
      "Epoch 66, test loss: 0.220855\n",
      "Epoch 67, test loss: 0.217518\n",
      "Epoch 68, test loss: 0.217085\n",
      "Epoch 69, test loss: 0.221364\n",
      "Epoch 70, test loss: 0.217765\n",
      "Epoch 71, test loss: 0.218408\n",
      "Epoch 72, test loss: 0.217595\n",
      "Epoch 73, test loss: 0.217486\n",
      "Epoch 74, test loss: 0.218355\n",
      "Epoch 75, test loss: 0.217015\n",
      "Epoch 76, test loss: 0.218091\n",
      "Epoch 77, test loss: 0.217242\n",
      "Epoch 78, test loss: 0.217490\n",
      "Epoch 79, test loss: 0.218804\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29432F7F0>\n",
      "Epoch 0, test loss: 0.220471\n",
      "Epoch 1, test loss: 0.218013\n",
      "Epoch 2, test loss: 0.216120\n",
      "Epoch 3, test loss: 0.218095\n",
      "Epoch 4, test loss: 0.216556\n",
      "Epoch 5, test loss: 0.217491\n",
      "Epoch 6, test loss: 0.217941\n",
      "Epoch 7, test loss: 0.216699\n",
      "Epoch 8, test loss: 0.217795\n",
      "Epoch 9, test loss: 0.217752\n",
      "Epoch 10, test loss: 0.219003\n",
      "Epoch 11, test loss: 0.216629\n",
      "Epoch 12, test loss: 0.218643\n",
      "Epoch 13, test loss: 0.218607\n",
      "Epoch 14, test loss: 0.221552\n",
      "Epoch 15, test loss: 0.217642\n",
      "Epoch 16, test loss: 0.220013\n",
      "Epoch 17, test loss: 0.217643\n",
      "Epoch 18, test loss: 0.217375\n",
      "Epoch 19, test loss: 0.216578\n",
      "Epoch 20, test loss: 0.218143\n",
      "Epoch 21, test loss: 0.217250\n",
      "Epoch 22, test loss: 0.217591\n",
      "Epoch 23, test loss: 0.216788\n",
      "Epoch 24, test loss: 0.217564\n",
      "Epoch 25, test loss: 0.217410\n",
      "Epoch 26, test loss: 0.217514\n",
      "Epoch 27, test loss: 0.218126\n",
      "Epoch 28, test loss: 0.217934\n",
      "Epoch 29, test loss: 0.217534\n",
      "Epoch 30, test loss: 0.220799\n",
      "Epoch 31, test loss: 0.216672\n",
      "Epoch 32, test loss: 0.215966\n",
      "Epoch 33, test loss: 0.219065\n",
      "Epoch 34, test loss: 0.219608\n",
      "Epoch 35, test loss: 0.218042\n",
      "Epoch 36, test loss: 0.216692\n",
      "Epoch 37, test loss: 0.218156\n",
      "Epoch 38, test loss: 0.217939\n",
      "Epoch 39, test loss: 0.216444\n",
      "Epoch 40, test loss: 0.217115\n",
      "Epoch 41, test loss: 0.220723\n",
      "Epoch 42, test loss: 0.217451\n",
      "Epoch 43, test loss: 0.217152\n",
      "Epoch 44, test loss: 0.219948\n",
      "Epoch 45, test loss: 0.217518\n",
      "Epoch 46, test loss: 0.216609\n",
      "Epoch 47, test loss: 0.217488\n",
      "Epoch 48, test loss: 0.218198\n",
      "Epoch 49, test loss: 0.217082\n",
      "Epoch 50, test loss: 0.216785\n",
      "Epoch 51, test loss: 0.216880\n",
      "Epoch 52, test loss: 0.219251\n",
      "Epoch 53, test loss: 0.222257\n",
      "Epoch 54, test loss: 0.217015\n",
      "Epoch 55, test loss: 0.216920\n",
      "Epoch 56, test loss: 0.217654\n",
      "Epoch 57, test loss: 0.216551\n",
      "Epoch 58, test loss: 0.219180\n",
      "Epoch 59, test loss: 0.217026\n",
      "Epoch 60, test loss: 0.216705\n",
      "Epoch 61, test loss: 0.218024\n",
      "Epoch 62, test loss: 0.217104\n",
      "Epoch 63, test loss: 0.217749\n",
      "Epoch 64, test loss: 0.222642\n",
      "Epoch 65, test loss: 0.218091\n",
      "Epoch 66, test loss: 0.216918\n",
      "Epoch 67, test loss: 0.216459\n",
      "Epoch 68, test loss: 0.218238\n",
      "Epoch 69, test loss: 0.218194\n",
      "Epoch 70, test loss: 0.216976\n",
      "Epoch 71, test loss: 0.215912\n",
      "Epoch 72, test loss: 0.216502\n",
      "Epoch 73, test loss: 0.216069\n",
      "Epoch 74, test loss: 0.215866\n",
      "Epoch 75, test loss: 0.217382\n",
      "Epoch 76, test loss: 0.217871\n",
      "Epoch 77, test loss: 0.216337\n",
      "Epoch 78, test loss: 0.216574\n",
      "Epoch 79, test loss: 0.217728\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29432F7F0>\n",
      "Epoch 0, test loss: 0.424398\n",
      "Epoch 1, test loss: 0.326506\n",
      "Epoch 2, test loss: 0.256912\n",
      "Epoch 3, test loss: 0.237637\n",
      "Epoch 4, test loss: 0.231227\n",
      "Epoch 5, test loss: 0.229316\n",
      "Epoch 6, test loss: 0.226297\n",
      "Epoch 7, test loss: 0.224782\n",
      "Epoch 8, test loss: 0.224316\n",
      "Epoch 9, test loss: 0.223786\n",
      "Epoch 10, test loss: 0.223709\n",
      "Epoch 11, test loss: 0.223058\n",
      "Epoch 12, test loss: 0.222661\n",
      "Epoch 13, test loss: 0.222033\n",
      "Epoch 14, test loss: 0.224098\n",
      "Epoch 15, test loss: 0.228847\n",
      "Epoch 16, test loss: 0.223461\n",
      "Epoch 17, test loss: 0.221744\n",
      "Epoch 18, test loss: 0.222054\n",
      "Epoch 19, test loss: 0.226813\n",
      "Epoch 20, test loss: 0.221927\n",
      "Epoch 21, test loss: 0.221962\n",
      "Epoch 22, test loss: 0.221642\n",
      "Epoch 23, test loss: 0.221575\n",
      "Epoch 24, test loss: 0.221551\n",
      "Epoch 25, test loss: 0.227068\n",
      "Epoch 26, test loss: 0.223773\n",
      "Epoch 27, test loss: 0.222405\n",
      "Epoch 28, test loss: 0.222159\n",
      "Epoch 29, test loss: 0.222085\n",
      "Epoch 30, test loss: 0.222196\n",
      "Epoch 31, test loss: 0.226383\n",
      "Epoch 32, test loss: 0.222315\n",
      "Epoch 33, test loss: 0.223203\n",
      "Epoch 34, test loss: 0.222759\n",
      "Epoch 35, test loss: 0.221697\n",
      "Epoch 36, test loss: 0.224133\n",
      "Epoch 37, test loss: 0.222063\n",
      "Epoch 38, test loss: 0.221793\n",
      "Epoch 39, test loss: 0.221738\n",
      "Epoch 40, test loss: 0.221839\n",
      "Epoch 41, test loss: 0.222026\n",
      "Epoch 42, test loss: 0.221965\n",
      "Epoch 43, test loss: 0.223768\n",
      "Epoch 44, test loss: 0.222052\n",
      "Epoch 45, test loss: 0.222258\n",
      "Epoch 46, test loss: 0.222823\n",
      "Epoch 47, test loss: 0.222063\n",
      "Epoch 48, test loss: 0.223384\n",
      "Epoch 49, test loss: 0.222140\n",
      "Epoch 50, test loss: 0.224032\n",
      "Epoch 51, test loss: 0.224010\n",
      "Epoch 52, test loss: 0.222154\n",
      "Epoch 53, test loss: 0.228500\n",
      "Epoch 54, test loss: 0.226538\n",
      "Epoch 55, test loss: 0.222170\n",
      "Epoch 56, test loss: 0.227500\n",
      "Epoch 57, test loss: 0.229061\n",
      "Epoch 58, test loss: 0.222240\n",
      "Epoch 59, test loss: 0.222382\n",
      "Epoch 60, test loss: 0.222135\n",
      "Epoch 61, test loss: 0.222995\n",
      "Epoch 62, test loss: 0.223646\n",
      "Epoch 63, test loss: 0.222876\n",
      "Epoch 64, test loss: 0.222257\n",
      "Epoch 65, test loss: 0.222097\n",
      "Epoch 66, test loss: 0.224035\n",
      "Epoch 67, test loss: 0.223500\n",
      "Epoch 68, test loss: 0.222260\n",
      "Epoch 69, test loss: 0.223712\n",
      "Epoch 70, test loss: 0.229976\n",
      "Epoch 71, test loss: 0.224178\n",
      "Epoch 72, test loss: 0.222398\n",
      "Epoch 73, test loss: 0.222882\n",
      "Epoch 74, test loss: 0.222724\n",
      "Epoch 75, test loss: 0.221973\n",
      "Epoch 76, test loss: 0.222037\n",
      "Epoch 77, test loss: 0.221795\n",
      "Epoch 78, test loss: 0.221830\n",
      "Epoch 79, test loss: 0.222549\n",
      "Epoch 80, test loss: 0.221850\n",
      "Epoch 81, test loss: 0.223033\n",
      "Epoch 82, test loss: 0.224328\n",
      "Epoch 83, test loss: 0.224277\n",
      "Epoch 84, test loss: 0.222900\n",
      "Epoch 85, test loss: 0.222716\n",
      "Epoch 86, test loss: 0.221735\n",
      "Epoch 87, test loss: 0.222668\n",
      "Epoch 88, test loss: 0.222169\n",
      "Epoch 89, test loss: 0.221862\n",
      "Epoch 90, test loss: 0.221902\n",
      "Epoch 91, test loss: 0.222074\n",
      "Epoch 92, test loss: 0.222303\n",
      "Epoch 93, test loss: 0.222151\n",
      "Epoch 94, test loss: 0.222131\n",
      "Epoch 95, test loss: 0.222592\n",
      "Epoch 96, test loss: 0.221867\n",
      "Epoch 97, test loss: 0.222826\n",
      "Epoch 98, test loss: 0.228475\n",
      "Epoch 99, test loss: 0.223031\n",
      "Epoch 100, test loss: 0.222645\n",
      "Epoch 101, test loss: 0.223304\n",
      "Epoch 102, test loss: 0.222462\n",
      "Epoch 103, test loss: 0.222006\n",
      "Epoch 104, test loss: 0.222602\n",
      "Epoch 105, test loss: 0.230190\n",
      "Epoch 106, test loss: 0.221765\n",
      "Epoch 107, test loss: 0.223659\n",
      "Epoch 108, test loss: 0.225688\n",
      "Epoch 109, test loss: 0.222104\n",
      "Epoch 110, test loss: 0.221850\n",
      "Epoch 111, test loss: 0.222030\n",
      "Epoch 112, test loss: 0.221788\n",
      "Epoch 113, test loss: 0.228488\n",
      "Epoch 114, test loss: 0.224465\n",
      "Epoch 115, test loss: 0.223589\n",
      "Epoch 116, test loss: 0.230686\n",
      "Epoch 117, test loss: 0.222014\n",
      "Epoch 118, test loss: 0.222968\n",
      "Epoch 119, test loss: 0.221813\n",
      "Epoch 120, test loss: 0.221906\n",
      "Epoch 121, test loss: 0.222034\n",
      "Epoch 122, test loss: 0.221908\n",
      "Epoch 123, test loss: 0.222872\n",
      "Epoch 124, test loss: 0.222765\n",
      "Epoch 125, test loss: 0.222396\n",
      "Epoch 126, test loss: 0.221880\n",
      "Epoch 127, test loss: 0.222529\n",
      "Epoch 128, test loss: 0.221716\n",
      "Epoch 129, test loss: 0.222159\n",
      "Epoch 130, test loss: 0.227982\n",
      "Epoch 131, test loss: 0.222189\n",
      "Epoch 132, test loss: 0.221551\n",
      "Epoch 133, test loss: 0.221648\n",
      "Epoch 134, test loss: 0.222933\n",
      "Epoch 135, test loss: 0.221830\n",
      "Epoch 136, test loss: 0.223283\n",
      "Epoch 137, test loss: 0.221588\n",
      "Epoch 138, test loss: 0.222375\n",
      "Epoch 139, test loss: 0.221850\n",
      "Epoch 140, test loss: 0.221850\n",
      "Epoch 141, test loss: 0.222418\n",
      "Epoch 142, test loss: 0.223481\n",
      "Epoch 143, test loss: 0.224461\n",
      "Epoch 144, test loss: 0.221956\n",
      "Epoch 145, test loss: 0.222185\n",
      "Epoch 146, test loss: 0.224373\n",
      "Epoch 147, test loss: 0.224904\n",
      "Epoch 148, test loss: 0.231676\n",
      "Epoch 149, test loss: 0.222198\n",
      "Epoch 150, test loss: 0.221596\n",
      "Epoch 151, test loss: 0.222347\n",
      "Epoch 152, test loss: 0.221718\n",
      "Epoch 153, test loss: 0.222213\n",
      "Epoch 154, test loss: 0.221734\n",
      "Epoch 155, test loss: 0.221479\n",
      "Epoch 156, test loss: 0.222088\n",
      "Epoch 157, test loss: 0.225273\n",
      "Epoch 158, test loss: 0.222564\n",
      "Epoch 159, test loss: 0.222403\n",
      "Epoch 160, test loss: 0.221949\n",
      "Epoch 161, test loss: 0.222079\n",
      "Epoch 162, test loss: 0.221739\n",
      "Epoch 163, test loss: 0.224010\n",
      "Epoch 164, test loss: 0.222191\n",
      "Epoch 165, test loss: 0.224697\n",
      "Epoch 166, test loss: 0.225998\n",
      "Epoch 167, test loss: 0.221966\n",
      "Epoch 168, test loss: 0.223997\n",
      "Epoch 169, test loss: 0.224011\n",
      "Epoch 170, test loss: 0.222643\n",
      "Epoch 171, test loss: 0.226502\n",
      "Epoch 172, test loss: 0.221914\n",
      "Epoch 173, test loss: 0.222570\n",
      "Epoch 174, test loss: 0.222983\n",
      "Epoch 175, test loss: 0.222954\n",
      "Epoch 176, test loss: 0.223184\n",
      "Epoch 177, test loss: 0.222225\n",
      "Epoch 178, test loss: 0.223216\n",
      "Epoch 179, test loss: 0.222230\n",
      "Epoch 180, test loss: 0.221888\n",
      "Epoch 181, test loss: 0.223145\n",
      "Epoch 182, test loss: 0.224149\n",
      "Epoch 183, test loss: 0.225225\n",
      "Epoch 184, test loss: 0.221576\n",
      "Epoch 185, test loss: 0.222313\n",
      "Epoch 186, test loss: 0.224892\n",
      "Epoch 187, test loss: 0.223586\n",
      "Epoch 188, test loss: 0.221878\n",
      "Epoch 189, test loss: 0.224081\n",
      "Epoch 190, test loss: 0.221910\n",
      "Epoch 191, test loss: 0.221632\n",
      "Epoch 192, test loss: 0.221580\n",
      "Epoch 193, test loss: 0.221698\n",
      "Epoch 194, test loss: 0.221556\n",
      "Epoch 195, test loss: 0.225333\n",
      "Epoch 196, test loss: 0.226665\n",
      "Epoch 197, test loss: 0.226110\n",
      "Epoch 198, test loss: 0.221635\n",
      "Epoch 199, test loss: 0.223482\n",
      "Epoch 200, test loss: 0.221571\n",
      "Epoch 201, test loss: 0.221866\n",
      "Epoch 202, test loss: 0.221448\n",
      "Epoch 203, test loss: 0.225760\n",
      "Epoch 204, test loss: 0.221553\n",
      "Epoch 205, test loss: 0.224681\n",
      "Epoch 206, test loss: 0.223804\n",
      "Epoch 207, test loss: 0.221918\n",
      "Epoch 208, test loss: 0.226037\n",
      "Epoch 209, test loss: 0.221717\n",
      "Epoch 210, test loss: 0.223857\n",
      "Epoch 211, test loss: 0.228311\n",
      "Epoch 212, test loss: 0.222308\n",
      "Epoch 213, test loss: 0.227031\n",
      "Epoch 214, test loss: 0.221436\n",
      "Epoch 215, test loss: 0.221655\n",
      "Epoch 216, test loss: 0.224078\n",
      "Epoch 217, test loss: 0.221723\n",
      "Epoch 218, test loss: 0.223287\n",
      "Epoch 219, test loss: 0.222653\n",
      "Epoch 220, test loss: 0.221584\n",
      "Epoch 221, test loss: 0.223251\n",
      "Epoch 222, test loss: 0.221765\n",
      "Epoch 223, test loss: 0.227540\n",
      "Epoch 224, test loss: 0.226674\n",
      "Epoch 225, test loss: 0.223384\n",
      "Epoch 226, test loss: 0.226797\n",
      "Epoch 227, test loss: 0.222737\n",
      "Epoch 228, test loss: 0.223819\n",
      "Epoch 229, test loss: 0.221295\n",
      "Epoch 230, test loss: 0.223056\n",
      "Epoch 231, test loss: 0.222878\n",
      "Epoch 232, test loss: 0.222464\n",
      "Epoch 233, test loss: 0.227392\n",
      "Epoch 234, test loss: 0.222000\n",
      "Epoch 235, test loss: 0.221386\n",
      "Epoch 236, test loss: 0.222201\n",
      "Epoch 237, test loss: 0.221315\n",
      "Epoch 238, test loss: 0.222075\n",
      "Epoch 239, test loss: 0.221225\n",
      "Pretrain data: 20092707.0\n",
      "Building dataset, requesting data from 0 to 672\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6790/100545\n",
      "Found 672 continuous time series\n",
      "Data shape: (107337, 24), Train/test: 107335/2\n",
      "Train test ratio: 53667.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1677\n",
      "Epoch 0, train loss: 0.219076\n",
      "Epoch 1, train loss: 0.248206\n",
      "Epoch 2, train loss: 0.227454\n",
      "Epoch 3, train loss: 0.211683\n",
      "Epoch 4, train loss: 0.192776\n",
      "Epoch 5, train loss: 0.220003\n",
      "Epoch 6, train loss: 0.203612\n",
      "Epoch 7, train loss: 0.232716\n",
      "Epoch 8, train loss: 0.162861\n",
      "Epoch 9, train loss: 0.220126\n",
      "Epoch 10, train loss: 0.210154\n",
      "Epoch 11, train loss: 0.192801\n",
      "Epoch 12, train loss: 0.271325\n",
      "Epoch 13, train loss: 0.185067\n",
      "Epoch 14, train loss: 0.160354\n",
      "Epoch 15, train loss: 0.184395\n",
      "Epoch 16, train loss: 0.288293\n",
      "Epoch 17, train loss: 0.170827\n",
      "Epoch 18, train loss: 0.228955\n",
      "Epoch 19, train loss: 0.187426\n",
      "Epoch 20, train loss: 0.219225\n",
      "Epoch 21, train loss: 0.179449\n",
      "Epoch 22, train loss: 0.167575\n",
      "Epoch 23, train loss: 0.221908\n",
      "Epoch 24, train loss: 0.171723\n",
      "Epoch 25, train loss: 0.190012\n",
      "Epoch 26, train loss: 0.197698\n",
      "Epoch 27, train loss: 0.185081\n",
      "Epoch 28, train loss: 0.159607\n",
      "Epoch 29, train loss: 0.207754\n",
      "Epoch 30, train loss: 0.178663\n",
      "Epoch 31, train loss: 0.204280\n",
      "Epoch 32, train loss: 0.206758\n",
      "Epoch 33, train loss: 0.198008\n",
      "Epoch 34, train loss: 0.145400\n",
      "Epoch 35, train loss: 0.173253\n",
      "Epoch 36, train loss: 0.205213\n",
      "Epoch 37, train loss: 0.248047\n",
      "Epoch 38, train loss: 0.157168\n",
      "Epoch 39, train loss: 0.237305\n",
      "Epoch 40, train loss: 0.278409\n",
      "Epoch 41, train loss: 0.225384\n",
      "Epoch 42, train loss: 0.210375\n",
      "Epoch 43, train loss: 0.198545\n",
      "Epoch 44, train loss: 0.161419\n",
      "Epoch 45, train loss: 0.185618\n",
      "Epoch 46, train loss: 0.174081\n",
      "Epoch 47, train loss: 0.215299\n",
      "Epoch 48, train loss: 0.172009\n",
      "Epoch 49, train loss: 0.204025\n",
      "Epoch 50, train loss: 0.285382\n",
      "Epoch 51, train loss: 0.202067\n",
      "Epoch 52, train loss: 0.175607\n",
      "Epoch 53, train loss: 0.208529\n",
      "Epoch 54, train loss: 0.251352\n",
      "Epoch 55, train loss: 0.161282\n",
      "Epoch 56, train loss: 0.160355\n",
      "Epoch 57, train loss: 0.221890\n",
      "Epoch 58, train loss: 0.222299\n",
      "Epoch 59, train loss: 0.177058\n",
      "Epoch 60, train loss: 0.169412\n",
      "Epoch 61, train loss: 0.207491\n",
      "Epoch 62, train loss: 0.184570\n",
      "Epoch 63, train loss: 0.194618\n",
      "Epoch 64, train loss: 0.209734\n",
      "Epoch 65, train loss: 0.239830\n",
      "Epoch 66, train loss: 0.195325\n",
      "Epoch 67, train loss: 0.253934\n",
      "Epoch 68, train loss: 0.198580\n",
      "Epoch 69, train loss: 0.203128\n",
      "Epoch 70, train loss: 0.175678\n",
      "Epoch 71, train loss: 0.188959\n",
      "Epoch 72, train loss: 0.210839\n",
      "Epoch 73, train loss: 0.205794\n",
      "Epoch 74, train loss: 0.218218\n",
      "Epoch 75, train loss: 0.135034\n",
      "Epoch 76, train loss: 0.245931\n",
      "Epoch 77, train loss: 0.215657\n",
      "Epoch 78, train loss: 0.209204\n",
      "Epoch 79, train loss: 0.227265\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[108. 105. 101. ...  75.  79.  85.]\n",
      " [105. 101.  98. ...  79.  85.  96.]\n",
      " [101.  98.  95. ...  85.  96. 109.]\n",
      " ...\n",
      " [242. 245. 247. ... 283. 276. 269.]\n",
      " [245. 247. 251. ... 276. 269. 261.]\n",
      " [247. 251. 255. ... 269. 261. 252.]]\n",
      "y here is\n",
      "[[117. 117. 117. ... 117. 117. 117.]\n",
      " [117. 117. 117. ... 117. 117. 117.]\n",
      " [115. 115. 115. ... 115. 115. 115.]\n",
      " ...\n",
      " [229. 229. 229. ... 229. 229. 229.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [215. 215. 215. ... 215. 215. 215.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1172, 24), Train/test: 1/1171\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 170 segments\n",
      "Building dataset, requesting data from 0 to 170\n",
      "x here is\n",
      "[[ 95.  86.  81. ... 106. 106. 104.]\n",
      " [ 86.  81.  81. ... 106. 104. 100.]\n",
      " [ 81.  81.  82. ... 104. 100.  94.]\n",
      " ...\n",
      " [282. 286. 284. ... 232. 222. 221.]\n",
      " [286. 284. 275. ... 222. 221. 222.]\n",
      " [284. 275. 277. ... 221. 222. 223.]]\n",
      "y here is\n",
      "[[100. 100. 100. ... 100. 100. 100.]\n",
      " [ 95.  95.  95. ...  95.  95.  95.]\n",
      " [ 93.  93.  93. ...  93.  93.  93.]\n",
      " ...\n",
      " [225. 225. 225. ... 225. 225. 225.]\n",
      " [227. 227. 227. ... 227. 227. 227.]\n",
      " [226. 226. 226. ... 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 319/4158\n",
      "Found 170 continuous time series\n",
      "Data shape: (4479, 24), Train/test: 4477/2\n",
      "Train test ratio: 2238.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329D3C0>\n",
      "Epoch 0, test loss: 0.165263\n",
      "Epoch 1, test loss: 0.163002\n",
      "Epoch 2, test loss: 0.164367\n",
      "Epoch 3, test loss: 0.162806\n",
      "Epoch 4, test loss: 0.163536\n",
      "Epoch 5, test loss: 0.163333\n",
      "Epoch 6, test loss: 0.165989\n",
      "Epoch 7, test loss: 0.167259\n",
      "Epoch 8, test loss: 0.166936\n",
      "Epoch 9, test loss: 0.164014\n",
      "Epoch 10, test loss: 0.164128\n",
      "Epoch 11, test loss: 0.165732\n",
      "Epoch 12, test loss: 0.169343\n",
      "Epoch 13, test loss: 0.167738\n",
      "Epoch 14, test loss: 0.164649\n",
      "Epoch 15, test loss: 0.164019\n",
      "Epoch 16, test loss: 0.169258\n",
      "Epoch 17, test loss: 0.163860\n",
      "Epoch 18, test loss: 0.164117\n",
      "Epoch 19, test loss: 0.164032\n",
      "Epoch 20, test loss: 0.164761\n",
      "Epoch 21, test loss: 0.163682\n",
      "Epoch 22, test loss: 0.166983\n",
      "Epoch 23, test loss: 0.168070\n",
      "Epoch 24, test loss: 0.164405\n",
      "Epoch 25, test loss: 0.164203\n",
      "Epoch 26, test loss: 0.164856\n",
      "Epoch 27, test loss: 0.164901\n",
      "Epoch 28, test loss: 0.164320\n",
      "Epoch 29, test loss: 0.165378\n",
      "Epoch 30, test loss: 0.164804\n",
      "Epoch 31, test loss: 0.166692\n",
      "Epoch 32, test loss: 0.164651\n",
      "Epoch 33, test loss: 0.165963\n",
      "Epoch 34, test loss: 0.163740\n",
      "Epoch 35, test loss: 0.165479\n",
      "Epoch 36, test loss: 0.168798\n",
      "Epoch 37, test loss: 0.164787\n",
      "Epoch 38, test loss: 0.166271\n",
      "Epoch 39, test loss: 0.165146\n",
      "Epoch 40, test loss: 0.165824\n",
      "Epoch 41, test loss: 0.167989\n",
      "Epoch 42, test loss: 0.165204\n",
      "Epoch 43, test loss: 0.165920\n",
      "Epoch 44, test loss: 0.166218\n",
      "Epoch 45, test loss: 0.164450\n",
      "Epoch 46, test loss: 0.168580\n",
      "Epoch 47, test loss: 0.164138\n",
      "Epoch 48, test loss: 0.164708\n",
      "Epoch 49, test loss: 0.165499\n",
      "Epoch 50, test loss: 0.163852\n",
      "Epoch 51, test loss: 0.164706\n",
      "Epoch 52, test loss: 0.165044\n",
      "Epoch 53, test loss: 0.166265\n",
      "Epoch 54, test loss: 0.164626\n",
      "Epoch 55, test loss: 0.180690\n",
      "Epoch 56, test loss: 0.164286\n",
      "Epoch 57, test loss: 0.168258\n",
      "Epoch 58, test loss: 0.167229\n",
      "Epoch 59, test loss: 0.168122\n",
      "Epoch 60, test loss: 0.166980\n",
      "Epoch 61, test loss: 0.166118\n",
      "Epoch 62, test loss: 0.176681\n",
      "Epoch 63, test loss: 0.170600\n",
      "Epoch 64, test loss: 0.165386\n",
      "Epoch 65, test loss: 0.167085\n",
      "Epoch 66, test loss: 0.169498\n",
      "Epoch 67, test loss: 0.168755\n",
      "Epoch 68, test loss: 0.164746\n",
      "Epoch 69, test loss: 0.165400\n",
      "Epoch 70, test loss: 0.166602\n",
      "Epoch 71, test loss: 0.165469\n",
      "Epoch 72, test loss: 0.165517\n",
      "Epoch 73, test loss: 0.165104\n",
      "Epoch 74, test loss: 0.165358\n",
      "Epoch 75, test loss: 0.165851\n",
      "Epoch 76, test loss: 0.167798\n",
      "Epoch 77, test loss: 0.166401\n",
      "Epoch 78, test loss: 0.165169\n",
      "Epoch 79, test loss: 0.167869\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329D3C0>\n",
      "Epoch 0, test loss: 0.168793\n",
      "Epoch 1, test loss: 0.163934\n",
      "Epoch 2, test loss: 0.163175\n",
      "Epoch 3, test loss: 0.165089\n",
      "Epoch 4, test loss: 0.168646\n",
      "Epoch 5, test loss: 0.165297\n",
      "Epoch 6, test loss: 0.164900\n",
      "Epoch 7, test loss: 0.166583\n",
      "Epoch 8, test loss: 0.164890\n",
      "Epoch 9, test loss: 0.166961\n",
      "Epoch 10, test loss: 0.167936\n",
      "Epoch 11, test loss: 0.166800\n",
      "Epoch 12, test loss: 0.165679\n",
      "Epoch 13, test loss: 0.166111\n",
      "Epoch 14, test loss: 0.167212\n",
      "Epoch 15, test loss: 0.166453\n",
      "Epoch 16, test loss: 0.166189\n",
      "Epoch 17, test loss: 0.165152\n",
      "Epoch 18, test loss: 0.165600\n",
      "Epoch 19, test loss: 0.167019\n",
      "Epoch 20, test loss: 0.167404\n",
      "Epoch 21, test loss: 0.167200\n",
      "Epoch 22, test loss: 0.166288\n",
      "Epoch 23, test loss: 0.171113\n",
      "Epoch 24, test loss: 0.167247\n",
      "Epoch 25, test loss: 0.167689\n",
      "Epoch 26, test loss: 0.171722\n",
      "Epoch 27, test loss: 0.166810\n",
      "Epoch 28, test loss: 0.166234\n",
      "Epoch 29, test loss: 0.168613\n",
      "Epoch 30, test loss: 0.169343\n",
      "Epoch 31, test loss: 0.167527\n",
      "Epoch 32, test loss: 0.170038\n",
      "Epoch 33, test loss: 0.170472\n",
      "Epoch 34, test loss: 0.166644\n",
      "Epoch 35, test loss: 0.165508\n",
      "Epoch 36, test loss: 0.165521\n",
      "Epoch 37, test loss: 0.166576\n",
      "Epoch 38, test loss: 0.168197\n",
      "Epoch 39, test loss: 0.166747\n",
      "Epoch 40, test loss: 0.166999\n",
      "Epoch 41, test loss: 0.168424\n",
      "Epoch 42, test loss: 0.166102\n",
      "Epoch 43, test loss: 0.168344\n",
      "Epoch 44, test loss: 0.167587\n",
      "Epoch 45, test loss: 0.166667\n",
      "Epoch 46, test loss: 0.168242\n",
      "Epoch 47, test loss: 0.166838\n",
      "Epoch 48, test loss: 0.166306\n",
      "Epoch 49, test loss: 0.165999\n",
      "Epoch 50, test loss: 0.166360\n",
      "Epoch 51, test loss: 0.168051\n",
      "Epoch 52, test loss: 0.166729\n",
      "Epoch 53, test loss: 0.166843\n",
      "Epoch 54, test loss: 0.168387\n",
      "Epoch 55, test loss: 0.167190\n",
      "Epoch 56, test loss: 0.166773\n",
      "Epoch 57, test loss: 0.166624\n",
      "Epoch 58, test loss: 0.168396\n",
      "Epoch 59, test loss: 0.167115\n",
      "Epoch 60, test loss: 0.167528\n",
      "Epoch 61, test loss: 0.167214\n",
      "Epoch 62, test loss: 0.165846\n",
      "Epoch 63, test loss: 0.167707\n",
      "Epoch 64, test loss: 0.167772\n",
      "Epoch 65, test loss: 0.169191\n",
      "Epoch 66, test loss: 0.167804\n",
      "Epoch 67, test loss: 0.167056\n",
      "Epoch 68, test loss: 0.166336\n",
      "Epoch 69, test loss: 0.166254\n",
      "Epoch 70, test loss: 0.168954\n",
      "Epoch 71, test loss: 0.168882\n",
      "Epoch 72, test loss: 0.165501\n",
      "Epoch 73, test loss: 0.168821\n",
      "Epoch 74, test loss: 0.166702\n",
      "Epoch 75, test loss: 0.166189\n",
      "Epoch 76, test loss: 0.166962\n",
      "Epoch 77, test loss: 0.165875\n",
      "Epoch 78, test loss: 0.167333\n",
      "Epoch 79, test loss: 0.168350\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29329D3C0>\n",
      "Epoch 0, test loss: 0.773769\n",
      "Epoch 1, test loss: 0.250318\n",
      "Epoch 2, test loss: 0.219264\n",
      "Epoch 3, test loss: 0.195363\n",
      "Epoch 4, test loss: 0.181134\n",
      "Epoch 5, test loss: 0.177007\n",
      "Epoch 6, test loss: 0.174588\n",
      "Epoch 7, test loss: 0.177274\n",
      "Epoch 8, test loss: 0.173271\n",
      "Epoch 9, test loss: 0.175833\n",
      "Epoch 10, test loss: 0.172815\n",
      "Epoch 11, test loss: 0.170394\n",
      "Epoch 12, test loss: 0.172601\n",
      "Epoch 13, test loss: 0.170643\n",
      "Epoch 14, test loss: 0.171145\n",
      "Epoch 15, test loss: 0.170919\n",
      "Epoch 16, test loss: 0.170567\n",
      "Epoch 17, test loss: 0.169355\n",
      "Epoch 18, test loss: 0.170013\n",
      "Epoch 19, test loss: 0.171018\n",
      "Epoch 20, test loss: 0.174905\n",
      "Epoch 21, test loss: 0.172677\n",
      "Epoch 22, test loss: 0.170392\n",
      "Epoch 23, test loss: 0.168807\n",
      "Epoch 24, test loss: 0.169394\n",
      "Epoch 25, test loss: 0.169208\n",
      "Epoch 26, test loss: 0.168633\n",
      "Epoch 27, test loss: 0.170770\n",
      "Epoch 28, test loss: 0.168721\n",
      "Epoch 29, test loss: 0.168423\n",
      "Epoch 30, test loss: 0.175269\n",
      "Epoch 31, test loss: 0.168459\n",
      "Epoch 32, test loss: 0.168296\n",
      "Epoch 33, test loss: 0.168301\n",
      "Epoch 34, test loss: 0.169831\n",
      "Epoch 35, test loss: 0.170264\n",
      "Epoch 36, test loss: 0.168345\n",
      "Epoch 37, test loss: 0.168156\n",
      "Epoch 38, test loss: 0.171163\n",
      "Epoch 39, test loss: 0.167698\n",
      "Epoch 40, test loss: 0.170405\n",
      "Epoch 41, test loss: 0.168310\n",
      "Epoch 42, test loss: 0.168160\n",
      "Epoch 43, test loss: 0.168995\n",
      "Epoch 44, test loss: 0.173126\n",
      "Epoch 45, test loss: 0.168538\n",
      "Epoch 46, test loss: 0.167545\n",
      "Epoch 47, test loss: 0.169753\n",
      "Epoch 48, test loss: 0.167871\n",
      "Epoch 49, test loss: 0.169477\n",
      "Epoch 50, test loss: 0.167960\n",
      "Epoch 51, test loss: 0.168661\n",
      "Epoch 52, test loss: 0.168334\n",
      "Epoch 53, test loss: 0.168007\n",
      "Epoch 54, test loss: 0.170007\n",
      "Epoch 55, test loss: 0.167868\n",
      "Epoch 56, test loss: 0.167216\n",
      "Epoch 57, test loss: 0.169198\n",
      "Epoch 58, test loss: 0.169081\n",
      "Epoch 59, test loss: 0.168070\n",
      "Epoch 60, test loss: 0.171447\n",
      "Epoch 61, test loss: 0.167839\n",
      "Epoch 62, test loss: 0.167000\n",
      "Epoch 63, test loss: 0.168408\n",
      "Epoch 64, test loss: 0.175378\n",
      "Epoch 65, test loss: 0.168326\n",
      "Epoch 66, test loss: 0.168786\n",
      "Epoch 67, test loss: 0.170626\n",
      "Epoch 68, test loss: 0.168702\n",
      "Epoch 69, test loss: 0.167911\n",
      "Epoch 70, test loss: 0.168434\n",
      "Epoch 71, test loss: 0.167700\n",
      "Epoch 72, test loss: 0.167296\n",
      "Epoch 73, test loss: 0.167438\n",
      "Epoch 74, test loss: 0.169114\n",
      "Epoch 75, test loss: 0.169647\n",
      "Epoch 76, test loss: 0.168539\n",
      "Epoch 77, test loss: 0.167269\n",
      "Epoch 78, test loss: 0.166948\n",
      "Epoch 79, test loss: 0.168338\n",
      "Epoch 80, test loss: 0.167176\n",
      "Epoch 81, test loss: 0.174589\n",
      "Epoch 82, test loss: 0.176964\n",
      "Epoch 83, test loss: 0.167267\n",
      "Epoch 84, test loss: 0.167121\n",
      "Epoch 85, test loss: 0.167994\n",
      "Epoch 86, test loss: 0.168056\n",
      "Epoch 87, test loss: 0.167175\n",
      "Epoch 88, test loss: 0.170953\n",
      "Epoch 89, test loss: 0.169639\n",
      "Epoch 90, test loss: 0.167641\n",
      "Epoch 91, test loss: 0.169456\n",
      "Epoch 92, test loss: 0.169588\n",
      "Epoch 93, test loss: 0.167977\n",
      "Epoch 94, test loss: 0.169925\n",
      "Epoch 95, test loss: 0.168666\n",
      "Epoch 96, test loss: 0.167182\n",
      "Epoch 97, test loss: 0.165827\n",
      "Epoch 98, test loss: 0.170246\n",
      "Epoch 99, test loss: 0.171336\n",
      "Epoch 100, test loss: 0.172420\n",
      "Epoch 101, test loss: 0.168625\n",
      "Epoch 102, test loss: 0.166725\n",
      "Epoch 103, test loss: 0.168507\n",
      "Epoch 104, test loss: 0.168289\n",
      "Epoch 105, test loss: 0.169633\n",
      "Epoch 106, test loss: 0.166867\n",
      "Epoch 107, test loss: 0.166349\n",
      "Epoch 108, test loss: 0.166290\n",
      "Epoch 109, test loss: 0.166455\n",
      "Epoch 110, test loss: 0.166985\n",
      "Epoch 111, test loss: 0.167197\n",
      "Epoch 112, test loss: 0.168345\n",
      "Epoch 113, test loss: 0.167071\n",
      "Epoch 114, test loss: 0.169300\n",
      "Epoch 115, test loss: 0.166381\n",
      "Epoch 116, test loss: 0.166374\n",
      "Epoch 117, test loss: 0.167678\n",
      "Epoch 118, test loss: 0.167526\n",
      "Epoch 119, test loss: 0.167848\n",
      "Epoch 120, test loss: 0.166064\n",
      "Epoch 121, test loss: 0.166613\n",
      "Epoch 122, test loss: 0.166939\n",
      "Epoch 123, test loss: 0.173374\n",
      "Epoch 124, test loss: 0.166064\n",
      "Epoch 125, test loss: 0.170046\n",
      "Epoch 126, test loss: 0.175977\n",
      "Epoch 127, test loss: 0.166684\n",
      "Epoch 128, test loss: 0.170163\n",
      "Epoch 129, test loss: 0.166777\n",
      "Epoch 130, test loss: 0.169539\n",
      "Epoch 131, test loss: 0.166752\n",
      "Epoch 132, test loss: 0.171941\n",
      "Epoch 133, test loss: 0.165530\n",
      "Epoch 134, test loss: 0.169684\n",
      "Epoch 135, test loss: 0.168405\n",
      "Epoch 136, test loss: 0.166585\n",
      "Epoch 137, test loss: 0.166277\n",
      "Epoch 138, test loss: 0.166341\n",
      "Epoch 139, test loss: 0.165940\n",
      "Epoch 140, test loss: 0.166610\n",
      "Epoch 141, test loss: 0.165784\n",
      "Epoch 142, test loss: 0.167042\n",
      "Epoch 143, test loss: 0.166085\n",
      "Epoch 144, test loss: 0.167553\n",
      "Epoch 145, test loss: 0.166385\n",
      "Epoch 146, test loss: 0.166770\n",
      "Epoch 147, test loss: 0.166538\n",
      "Epoch 148, test loss: 0.167576\n",
      "Epoch 149, test loss: 0.167856\n",
      "Epoch 150, test loss: 0.167948\n",
      "Epoch 151, test loss: 0.168187\n",
      "Epoch 152, test loss: 0.166636\n",
      "Epoch 153, test loss: 0.169318\n",
      "Epoch 154, test loss: 0.169268\n",
      "Epoch 155, test loss: 0.167922\n",
      "Epoch 156, test loss: 0.168309\n",
      "Epoch 157, test loss: 0.170273\n",
      "Epoch 158, test loss: 0.168568\n",
      "Epoch 159, test loss: 0.168725\n",
      "Epoch 160, test loss: 0.171397\n",
      "Epoch 161, test loss: 0.169143\n",
      "Epoch 162, test loss: 0.169251\n",
      "Epoch 163, test loss: 0.170349\n",
      "Epoch 164, test loss: 0.171297\n",
      "Epoch 165, test loss: 0.169224\n",
      "Epoch 166, test loss: 0.171786\n",
      "Epoch 167, test loss: 0.172234\n",
      "Epoch 168, test loss: 0.170064\n",
      "Epoch 169, test loss: 0.174288\n",
      "Epoch 170, test loss: 0.170070\n",
      "Epoch 171, test loss: 0.169844\n",
      "Epoch 172, test loss: 0.178169\n",
      "Epoch 173, test loss: 0.171959\n",
      "Epoch 174, test loss: 0.169273\n",
      "Epoch 175, test loss: 0.169628\n",
      "Epoch 176, test loss: 0.169882\n",
      "Epoch 177, test loss: 0.170969\n",
      "Epoch 178, test loss: 0.170874\n",
      "Epoch 179, test loss: 0.172246\n",
      "Epoch 180, test loss: 0.172247\n",
      "Epoch 181, test loss: 0.170904\n",
      "Epoch 182, test loss: 0.170353\n",
      "Epoch 183, test loss: 0.169981\n",
      "Epoch 184, test loss: 0.170893\n",
      "Epoch 185, test loss: 0.170428\n",
      "Epoch 186, test loss: 0.171581\n",
      "Epoch 187, test loss: 0.172938\n",
      "Epoch 188, test loss: 0.174633\n",
      "Epoch 189, test loss: 0.169970\n",
      "Epoch 190, test loss: 0.171098\n",
      "Epoch 191, test loss: 0.169712\n",
      "Epoch 192, test loss: 0.170681\n",
      "Epoch 193, test loss: 0.169806\n",
      "Epoch 194, test loss: 0.173390\n",
      "Epoch 195, test loss: 0.171412\n",
      "Epoch 196, test loss: 0.169504\n",
      "Epoch 197, test loss: 0.170930\n",
      "Epoch 198, test loss: 0.169540\n",
      "Epoch 199, test loss: 0.171447\n",
      "Epoch 200, test loss: 0.171902\n",
      "Epoch 201, test loss: 0.171128\n",
      "Epoch 202, test loss: 0.170764\n",
      "Epoch 203, test loss: 0.168927\n",
      "Epoch 204, test loss: 0.174664\n",
      "Epoch 205, test loss: 0.170416\n",
      "Epoch 206, test loss: 0.170431\n",
      "Epoch 207, test loss: 0.170925\n",
      "Epoch 208, test loss: 0.174099\n",
      "Epoch 209, test loss: 0.169439\n",
      "Epoch 210, test loss: 0.174216\n",
      "Epoch 211, test loss: 0.170992\n",
      "Epoch 212, test loss: 0.171865\n",
      "Epoch 213, test loss: 0.172877\n",
      "Epoch 214, test loss: 0.170348\n",
      "Epoch 215, test loss: 0.172368\n",
      "Epoch 216, test loss: 0.169950\n",
      "Epoch 217, test loss: 0.171104\n",
      "Epoch 218, test loss: 0.171112\n",
      "Epoch 219, test loss: 0.170174\n",
      "Epoch 220, test loss: 0.169896\n",
      "Epoch 221, test loss: 0.169375\n",
      "Epoch 222, test loss: 0.169452\n",
      "Epoch 223, test loss: 0.169600\n",
      "Epoch 224, test loss: 0.169150\n",
      "Epoch 225, test loss: 0.169398\n",
      "Epoch 226, test loss: 0.171177\n",
      "Epoch 227, test loss: 0.170110\n",
      "Epoch 228, test loss: 0.173100\n",
      "Epoch 229, test loss: 0.170627\n",
      "Epoch 230, test loss: 0.169826\n",
      "Epoch 231, test loss: 0.169674\n",
      "Epoch 232, test loss: 0.170247\n",
      "Epoch 233, test loss: 0.169884\n",
      "Epoch 234, test loss: 0.172683\n",
      "Epoch 235, test loss: 0.171517\n",
      "Epoch 236, test loss: 0.170602\n",
      "Epoch 237, test loss: 0.171063\n",
      "Epoch 238, test loss: 0.170422\n",
      "Epoch 239, test loss: 0.170045\n",
      "Pretrain data: 19669610.0\n",
      "Building dataset, requesting data from 0 to 819\n",
      "x here is\n",
      "[[ 95.  86.  81. ... 106. 106. 104.]\n",
      " [ 86.  81.  81. ... 106. 104. 100.]\n",
      " [ 81.  81.  82. ... 104. 100.  94.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[100. 100. 100. ... 100. 100. 100.]\n",
      " [ 95.  95.  95. ...  95.  95.  95.]\n",
      " [ 93.  93.  93. ...  93.  93.  93.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6671/95161\n",
      "Found 819 continuous time series\n",
      "Data shape: (101834, 24), Train/test: 101832/2\n",
      "Train test ratio: 50916.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1591\n",
      "Epoch 0, train loss: 0.217342\n",
      "Epoch 1, train loss: 0.234822\n",
      "Epoch 2, train loss: 0.173262\n",
      "Epoch 3, train loss: 0.205172\n",
      "Epoch 4, train loss: 0.223416\n",
      "Epoch 5, train loss: 0.250910\n",
      "Epoch 6, train loss: 0.201047\n",
      "Epoch 7, train loss: 0.192525\n",
      "Epoch 8, train loss: 0.225582\n",
      "Epoch 9, train loss: 0.227474\n",
      "Epoch 10, train loss: 0.199458\n",
      "Epoch 11, train loss: 0.162213\n",
      "Epoch 12, train loss: 0.202348\n",
      "Epoch 13, train loss: 0.219966\n",
      "Epoch 14, train loss: 0.225528\n",
      "Epoch 15, train loss: 0.224659\n",
      "Epoch 16, train loss: 0.282639\n",
      "Epoch 17, train loss: 0.225237\n",
      "Epoch 18, train loss: 0.214973\n",
      "Epoch 19, train loss: 0.236598\n",
      "Epoch 20, train loss: 0.233801\n",
      "Epoch 21, train loss: 0.184330\n",
      "Epoch 22, train loss: 0.233979\n",
      "Epoch 23, train loss: 0.188763\n",
      "Epoch 24, train loss: 0.154543\n",
      "Epoch 25, train loss: 0.247711\n",
      "Epoch 26, train loss: 0.249845\n",
      "Epoch 27, train loss: 0.242679\n",
      "Epoch 28, train loss: 0.168668\n",
      "Epoch 29, train loss: 0.198439\n",
      "Epoch 30, train loss: 0.197210\n",
      "Epoch 31, train loss: 0.244981\n",
      "Epoch 32, train loss: 0.205111\n",
      "Epoch 33, train loss: 0.251603\n",
      "Epoch 34, train loss: 0.212330\n",
      "Epoch 35, train loss: 0.202741\n",
      "Epoch 36, train loss: 0.207572\n",
      "Epoch 37, train loss: 0.240452\n",
      "Epoch 38, train loss: 0.252013\n",
      "Epoch 39, train loss: 0.273922\n",
      "Epoch 40, train loss: 0.163726\n",
      "Epoch 41, train loss: 0.186062\n",
      "Epoch 42, train loss: 0.218396\n",
      "Epoch 43, train loss: 0.233247\n",
      "Epoch 44, train loss: 0.219277\n",
      "Epoch 45, train loss: 0.197039\n",
      "Epoch 46, train loss: 0.196186\n",
      "Epoch 47, train loss: 0.257371\n",
      "Epoch 48, train loss: 0.207759\n",
      "Epoch 49, train loss: 0.213615\n",
      "Epoch 50, train loss: 0.201749\n",
      "Epoch 51, train loss: 0.170565\n",
      "Epoch 52, train loss: 0.178874\n",
      "Epoch 53, train loss: 0.223127\n",
      "Epoch 54, train loss: 0.211534\n",
      "Epoch 55, train loss: 0.188187\n",
      "Epoch 56, train loss: 0.153780\n",
      "Epoch 57, train loss: 0.298512\n",
      "Epoch 58, train loss: 0.185807\n",
      "Epoch 59, train loss: 0.204170\n",
      "Epoch 60, train loss: 0.177661\n",
      "Epoch 61, train loss: 0.218132\n",
      "Epoch 62, train loss: 0.165125\n",
      "Epoch 63, train loss: 0.235588\n",
      "Epoch 64, train loss: 0.159609\n",
      "Epoch 65, train loss: 0.184085\n",
      "Epoch 66, train loss: 0.187416\n",
      "Epoch 67, train loss: 0.186032\n",
      "Epoch 68, train loss: 0.175728\n",
      "Epoch 69, train loss: 0.196294\n",
      "Epoch 70, train loss: 0.209785\n",
      "Epoch 71, train loss: 0.234834\n",
      "Epoch 72, train loss: 0.203970\n",
      "Epoch 73, train loss: 0.130093\n",
      "Epoch 74, train loss: 0.367002\n",
      "Epoch 75, train loss: 0.237922\n",
      "Epoch 76, train loss: 0.211137\n",
      "Epoch 77, train loss: 0.266119\n",
      "Epoch 78, train loss: 0.201633\n",
      "Epoch 79, train loss: 0.247307\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[161. 155. 150. ...  93.  91.  89.]\n",
      " [155. 150. 147. ...  91.  89.  88.]\n",
      " [150. 147. 144. ...  89.  88.  87.]\n",
      " ...\n",
      " [270. 272. 289. ... 293. 301. 307.]\n",
      " [272. 289. 307. ... 301. 307. 306.]\n",
      " [289. 307. 313. ... 307. 306. 300.]]\n",
      "y here is\n",
      "[[ 79.  79.  79. ...  79.  79.  79.]\n",
      " [ 77.  77.  77. ...  77.  77.  77.]\n",
      " [ 76.  76.  76. ...  76.  76.  76.]\n",
      " ...\n",
      " [284. 284. 284. ... 284. 284. 284.]\n",
      " [273. 273. 273. ... 273. 273. 273.]\n",
      " [262. 262. 262. ... 262. 262. 262.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 7 continuous time series\n",
      "Data shape: (2513, 24), Train/test: 1/2512\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [163. 163. 163. ... 176. 173. 172.]\n",
      " [163. 163. 163. ... 173. 172. 173.]\n",
      " [163. 163. 161. ... 172. 173. 173.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 438/9542\n",
      "Found 23 continuous time series\n",
      "Data shape: (9982, 24), Train/test: 9980/2\n",
      "Train test ratio: 4990.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901DA350>\n",
      "Epoch 0, test loss: 0.174259\n",
      "Epoch 1, test loss: 0.174307\n",
      "Epoch 2, test loss: 0.172604\n",
      "Epoch 3, test loss: 0.175795\n",
      "Epoch 4, test loss: 0.177716\n",
      "Epoch 5, test loss: 0.172545\n",
      "Epoch 6, test loss: 0.174667\n",
      "Epoch 7, test loss: 0.172237\n",
      "Epoch 8, test loss: 0.173412\n",
      "Epoch 9, test loss: 0.173027\n",
      "Epoch 10, test loss: 0.172397\n",
      "Epoch 11, test loss: 0.172972\n",
      "Epoch 12, test loss: 0.175985\n",
      "Epoch 13, test loss: 0.175144\n",
      "Epoch 14, test loss: 0.173386\n",
      "Epoch 15, test loss: 0.176717\n",
      "Epoch 16, test loss: 0.173115\n",
      "Epoch 17, test loss: 0.173206\n",
      "Epoch 18, test loss: 0.175593\n",
      "Epoch 19, test loss: 0.177032\n",
      "Epoch 20, test loss: 0.171795\n",
      "Epoch 21, test loss: 0.173194\n",
      "Epoch 22, test loss: 0.171439\n",
      "Epoch 23, test loss: 0.174544\n",
      "Epoch 24, test loss: 0.175187\n",
      "Epoch 25, test loss: 0.173353\n",
      "Epoch 26, test loss: 0.172481\n",
      "Epoch 27, test loss: 0.172021\n",
      "Epoch 28, test loss: 0.174339\n",
      "Epoch 29, test loss: 0.172315\n",
      "Epoch 30, test loss: 0.171967\n",
      "Epoch 31, test loss: 0.173114\n",
      "Epoch 32, test loss: 0.172198\n",
      "Epoch 33, test loss: 0.175027\n",
      "Epoch 34, test loss: 0.172226\n",
      "Epoch 35, test loss: 0.171854\n",
      "Epoch 36, test loss: 0.171595\n",
      "Epoch 37, test loss: 0.172161\n",
      "Epoch 38, test loss: 0.173022\n",
      "Epoch 39, test loss: 0.176013\n",
      "Epoch 40, test loss: 0.172503\n",
      "Epoch 41, test loss: 0.172745\n",
      "Epoch 42, test loss: 0.173262\n",
      "Epoch 43, test loss: 0.174969\n",
      "Epoch 44, test loss: 0.177799\n",
      "Epoch 45, test loss: 0.172142\n",
      "Epoch 46, test loss: 0.173290\n",
      "Epoch 47, test loss: 0.177282\n",
      "Epoch 48, test loss: 0.171450\n",
      "Epoch 49, test loss: 0.171923\n",
      "Epoch 50, test loss: 0.172543\n",
      "Epoch 51, test loss: 0.176089\n",
      "Epoch 52, test loss: 0.173703\n",
      "Epoch 53, test loss: 0.172503\n",
      "Epoch 54, test loss: 0.171996\n",
      "Epoch 55, test loss: 0.171745\n",
      "Epoch 56, test loss: 0.174404\n",
      "Epoch 57, test loss: 0.175672\n",
      "Epoch 58, test loss: 0.171592\n",
      "Epoch 59, test loss: 0.171944\n",
      "Epoch 60, test loss: 0.178811\n",
      "Epoch 61, test loss: 0.173006\n",
      "Epoch 62, test loss: 0.171473\n",
      "Epoch 63, test loss: 0.173766\n",
      "Epoch 64, test loss: 0.172663\n",
      "Epoch 65, test loss: 0.171563\n",
      "Epoch 66, test loss: 0.171481\n",
      "Epoch 67, test loss: 0.171936\n",
      "Epoch 68, test loss: 0.172094\n",
      "Epoch 69, test loss: 0.173470\n",
      "Epoch 70, test loss: 0.174187\n",
      "Epoch 71, test loss: 0.172265\n",
      "Epoch 72, test loss: 0.174555\n",
      "Epoch 73, test loss: 0.173845\n",
      "Epoch 74, test loss: 0.177417\n",
      "Epoch 75, test loss: 0.172218\n",
      "Epoch 76, test loss: 0.172597\n",
      "Epoch 77, test loss: 0.172267\n",
      "Epoch 78, test loss: 0.176566\n",
      "Epoch 79, test loss: 0.172425\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901DA350>\n",
      "Epoch 0, test loss: 0.174523\n",
      "Epoch 1, test loss: 0.173149\n",
      "Epoch 2, test loss: 0.172953\n",
      "Epoch 3, test loss: 0.172975\n",
      "Epoch 4, test loss: 0.174431\n",
      "Epoch 5, test loss: 0.173895\n",
      "Epoch 6, test loss: 0.172819\n",
      "Epoch 7, test loss: 0.174690\n",
      "Epoch 8, test loss: 0.172470\n",
      "Epoch 9, test loss: 0.172255\n",
      "Epoch 10, test loss: 0.173382\n",
      "Epoch 11, test loss: 0.172458\n",
      "Epoch 12, test loss: 0.171823\n",
      "Epoch 13, test loss: 0.173297\n",
      "Epoch 14, test loss: 0.173520\n",
      "Epoch 15, test loss: 0.173038\n",
      "Epoch 16, test loss: 0.172198\n",
      "Epoch 17, test loss: 0.172444\n",
      "Epoch 18, test loss: 0.172543\n",
      "Epoch 19, test loss: 0.172753\n",
      "Epoch 20, test loss: 0.172212\n",
      "Epoch 21, test loss: 0.172512\n",
      "Epoch 22, test loss: 0.172249\n",
      "Epoch 23, test loss: 0.173078\n",
      "Epoch 24, test loss: 0.172756\n",
      "Epoch 25, test loss: 0.172183\n",
      "Epoch 26, test loss: 0.173159\n",
      "Epoch 27, test loss: 0.176002\n",
      "Epoch 28, test loss: 0.172162\n",
      "Epoch 29, test loss: 0.172073\n",
      "Epoch 30, test loss: 0.171911\n",
      "Epoch 31, test loss: 0.171836\n",
      "Epoch 32, test loss: 0.171811\n",
      "Epoch 33, test loss: 0.171788\n",
      "Epoch 34, test loss: 0.172478\n",
      "Epoch 35, test loss: 0.171877\n",
      "Epoch 36, test loss: 0.172293\n",
      "Epoch 37, test loss: 0.171659\n",
      "Epoch 38, test loss: 0.172207\n",
      "Epoch 39, test loss: 0.171842\n",
      "Epoch 40, test loss: 0.171990\n",
      "Epoch 41, test loss: 0.172608\n",
      "Epoch 42, test loss: 0.172474\n",
      "Epoch 43, test loss: 0.172807\n",
      "Epoch 44, test loss: 0.171813\n",
      "Epoch 45, test loss: 0.171510\n",
      "Epoch 46, test loss: 0.172770\n",
      "Epoch 47, test loss: 0.173748\n",
      "Epoch 48, test loss: 0.174773\n",
      "Epoch 49, test loss: 0.172046\n",
      "Epoch 50, test loss: 0.172027\n",
      "Epoch 51, test loss: 0.173345\n",
      "Epoch 52, test loss: 0.172351\n",
      "Epoch 53, test loss: 0.171960\n",
      "Epoch 54, test loss: 0.172013\n",
      "Epoch 55, test loss: 0.172857\n",
      "Epoch 56, test loss: 0.171864\n",
      "Epoch 57, test loss: 0.172190\n",
      "Epoch 58, test loss: 0.173181\n",
      "Epoch 59, test loss: 0.172950\n",
      "Epoch 60, test loss: 0.172270\n",
      "Epoch 61, test loss: 0.172474\n",
      "Epoch 62, test loss: 0.171604\n",
      "Epoch 63, test loss: 0.172735\n",
      "Epoch 64, test loss: 0.173841\n",
      "Epoch 65, test loss: 0.172971\n",
      "Epoch 66, test loss: 0.171941\n",
      "Epoch 67, test loss: 0.172076\n",
      "Epoch 68, test loss: 0.178130\n",
      "Epoch 69, test loss: 0.171842\n",
      "Epoch 70, test loss: 0.172004\n",
      "Epoch 71, test loss: 0.171938\n",
      "Epoch 72, test loss: 0.172411\n",
      "Epoch 73, test loss: 0.172158\n",
      "Epoch 74, test loss: 0.173582\n",
      "Epoch 75, test loss: 0.172292\n",
      "Epoch 76, test loss: 0.171960\n",
      "Epoch 77, test loss: 0.172058\n",
      "Epoch 78, test loss: 0.172746\n",
      "Epoch 79, test loss: 0.173793\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A2901DA350>\n",
      "Epoch 0, test loss: 0.280229\n",
      "Epoch 1, test loss: 0.202744\n",
      "Epoch 2, test loss: 0.181680\n",
      "Epoch 3, test loss: 0.179830\n",
      "Epoch 4, test loss: 0.177422\n",
      "Epoch 5, test loss: 0.176904\n",
      "Epoch 6, test loss: 0.176189\n",
      "Epoch 7, test loss: 0.175273\n",
      "Epoch 8, test loss: 0.175019\n",
      "Epoch 9, test loss: 0.174600\n",
      "Epoch 10, test loss: 0.174844\n",
      "Epoch 11, test loss: 0.174792\n",
      "Epoch 12, test loss: 0.174767\n",
      "Epoch 13, test loss: 0.174639\n",
      "Epoch 14, test loss: 0.179095\n",
      "Epoch 15, test loss: 0.175565\n",
      "Epoch 16, test loss: 0.176735\n",
      "Epoch 17, test loss: 0.173958\n",
      "Epoch 18, test loss: 0.173297\n",
      "Epoch 19, test loss: 0.174084\n",
      "Epoch 20, test loss: 0.173274\n",
      "Epoch 21, test loss: 0.173422\n",
      "Epoch 22, test loss: 0.174158\n",
      "Epoch 23, test loss: 0.174663\n",
      "Epoch 24, test loss: 0.174610\n",
      "Epoch 25, test loss: 0.175469\n",
      "Epoch 26, test loss: 0.173381\n",
      "Epoch 27, test loss: 0.174154\n",
      "Epoch 28, test loss: 0.175912\n",
      "Epoch 29, test loss: 0.173928\n",
      "Epoch 30, test loss: 0.173149\n",
      "Epoch 31, test loss: 0.175608\n",
      "Epoch 32, test loss: 0.174049\n",
      "Epoch 33, test loss: 0.174055\n",
      "Epoch 34, test loss: 0.173510\n",
      "Epoch 35, test loss: 0.173787\n",
      "Epoch 36, test loss: 0.175062\n",
      "Epoch 37, test loss: 0.173138\n",
      "Epoch 38, test loss: 0.173278\n",
      "Epoch 39, test loss: 0.173365\n",
      "Epoch 40, test loss: 0.173577\n",
      "Epoch 41, test loss: 0.174975\n",
      "Epoch 42, test loss: 0.175251\n",
      "Epoch 43, test loss: 0.173265\n",
      "Epoch 44, test loss: 0.175745\n",
      "Epoch 45, test loss: 0.175621\n",
      "Epoch 46, test loss: 0.173569\n",
      "Epoch 47, test loss: 0.174806\n",
      "Epoch 48, test loss: 0.178840\n",
      "Epoch 49, test loss: 0.173884\n",
      "Epoch 50, test loss: 0.173237\n",
      "Epoch 51, test loss: 0.174397\n",
      "Epoch 52, test loss: 0.176868\n",
      "Epoch 53, test loss: 0.173266\n",
      "Epoch 54, test loss: 0.173221\n",
      "Epoch 55, test loss: 0.174050\n",
      "Epoch 56, test loss: 0.175403\n",
      "Epoch 57, test loss: 0.176322\n",
      "Epoch 58, test loss: 0.174119\n",
      "Epoch 59, test loss: 0.173533\n",
      "Epoch 60, test loss: 0.173531\n",
      "Epoch 61, test loss: 0.175921\n",
      "Epoch 62, test loss: 0.174539\n",
      "Epoch 63, test loss: 0.174263\n",
      "Epoch 64, test loss: 0.173698\n",
      "Epoch 65, test loss: 0.174458\n",
      "Epoch 66, test loss: 0.174195\n",
      "Epoch 67, test loss: 0.176423\n",
      "Epoch 68, test loss: 0.174519\n",
      "Epoch 69, test loss: 0.174864\n",
      "Epoch 70, test loss: 0.176137\n",
      "Epoch 71, test loss: 0.173704\n",
      "Epoch 72, test loss: 0.174291\n",
      "Epoch 73, test loss: 0.175817\n",
      "Epoch 74, test loss: 0.175100\n",
      "Epoch 75, test loss: 0.174503\n",
      "Epoch 76, test loss: 0.174304\n",
      "Epoch 77, test loss: 0.173433\n",
      "Epoch 78, test loss: 0.173372\n",
      "Epoch 79, test loss: 0.174750\n",
      "Epoch 80, test loss: 0.174703\n",
      "Epoch 81, test loss: 0.173520\n",
      "Epoch 82, test loss: 0.174120\n",
      "Epoch 83, test loss: 0.173773\n",
      "Epoch 84, test loss: 0.173593\n",
      "Epoch 85, test loss: 0.174230\n",
      "Epoch 86, test loss: 0.175599\n",
      "Epoch 87, test loss: 0.174727\n",
      "Epoch 88, test loss: 0.173871\n",
      "Epoch 89, test loss: 0.174489\n",
      "Epoch 90, test loss: 0.173988\n",
      "Epoch 91, test loss: 0.174247\n",
      "Epoch 92, test loss: 0.175218\n",
      "Epoch 93, test loss: 0.175016\n",
      "Epoch 94, test loss: 0.176326\n",
      "Epoch 95, test loss: 0.173976\n",
      "Epoch 96, test loss: 0.173699\n",
      "Epoch 97, test loss: 0.174398\n",
      "Epoch 98, test loss: 0.174899\n",
      "Epoch 99, test loss: 0.173750\n",
      "Epoch 100, test loss: 0.174592\n",
      "Epoch 101, test loss: 0.174274\n",
      "Epoch 102, test loss: 0.174477\n",
      "Epoch 103, test loss: 0.173374\n",
      "Epoch 104, test loss: 0.173626\n",
      "Epoch 105, test loss: 0.174225\n",
      "Epoch 106, test loss: 0.173420\n",
      "Epoch 107, test loss: 0.176608\n",
      "Epoch 108, test loss: 0.174005\n",
      "Epoch 109, test loss: 0.174482\n",
      "Epoch 110, test loss: 0.175577\n",
      "Epoch 111, test loss: 0.176103\n",
      "Epoch 112, test loss: 0.175557\n",
      "Epoch 113, test loss: 0.173619\n",
      "Epoch 114, test loss: 0.174696\n",
      "Epoch 115, test loss: 0.173664\n",
      "Epoch 116, test loss: 0.173590\n",
      "Epoch 117, test loss: 0.177043\n",
      "Epoch 118, test loss: 0.173932\n",
      "Epoch 119, test loss: 0.173733\n",
      "Epoch 120, test loss: 0.173963\n",
      "Epoch 121, test loss: 0.173528\n",
      "Epoch 122, test loss: 0.173382\n",
      "Epoch 123, test loss: 0.173637\n",
      "Epoch 124, test loss: 0.174449\n",
      "Epoch 125, test loss: 0.174236\n",
      "Epoch 126, test loss: 0.174369\n",
      "Epoch 127, test loss: 0.174132\n",
      "Epoch 128, test loss: 0.177649\n",
      "Epoch 129, test loss: 0.173969\n",
      "Epoch 130, test loss: 0.174674\n",
      "Epoch 131, test loss: 0.173323\n",
      "Epoch 132, test loss: 0.173832\n",
      "Epoch 133, test loss: 0.174112\n",
      "Epoch 134, test loss: 0.173118\n",
      "Epoch 135, test loss: 0.173855\n",
      "Epoch 136, test loss: 0.176242\n",
      "Epoch 137, test loss: 0.175197\n",
      "Epoch 138, test loss: 0.173617\n",
      "Epoch 139, test loss: 0.173529\n",
      "Epoch 140, test loss: 0.176812\n",
      "Epoch 141, test loss: 0.173408\n",
      "Epoch 142, test loss: 0.173289\n",
      "Epoch 143, test loss: 0.174132\n",
      "Epoch 144, test loss: 0.173586\n",
      "Epoch 145, test loss: 0.173036\n",
      "Epoch 146, test loss: 0.174123\n",
      "Epoch 147, test loss: 0.175129\n",
      "Epoch 148, test loss: 0.173360\n",
      "Epoch 149, test loss: 0.176566\n",
      "Epoch 150, test loss: 0.174305\n",
      "Epoch 151, test loss: 0.173395\n",
      "Epoch 152, test loss: 0.173220\n",
      "Epoch 153, test loss: 0.173800\n",
      "Epoch 154, test loss: 0.173593\n",
      "Epoch 155, test loss: 0.174519\n",
      "Epoch 156, test loss: 0.173279\n",
      "Epoch 157, test loss: 0.173916\n",
      "Epoch 158, test loss: 0.174744\n",
      "Epoch 159, test loss: 0.174893\n",
      "Epoch 160, test loss: 0.173314\n",
      "Epoch 161, test loss: 0.173093\n",
      "Epoch 162, test loss: 0.173574\n",
      "Epoch 163, test loss: 0.174508\n",
      "Epoch 164, test loss: 0.173565\n",
      "Epoch 165, test loss: 0.173391\n",
      "Epoch 166, test loss: 0.173100\n",
      "Epoch 167, test loss: 0.173155\n",
      "Epoch 168, test loss: 0.174711\n",
      "Epoch 169, test loss: 0.174724\n",
      "Epoch 170, test loss: 0.173375\n",
      "Epoch 171, test loss: 0.173407\n",
      "Epoch 172, test loss: 0.176104\n",
      "Epoch 173, test loss: 0.175581\n",
      "Epoch 174, test loss: 0.177470\n",
      "Epoch 175, test loss: 0.173496\n",
      "Epoch 176, test loss: 0.173314\n",
      "Epoch 177, test loss: 0.174615\n",
      "Epoch 178, test loss: 0.177248\n",
      "Epoch 179, test loss: 0.174388\n",
      "Epoch 180, test loss: 0.174848\n",
      "Epoch 181, test loss: 0.174141\n",
      "Epoch 182, test loss: 0.175166\n",
      "Epoch 183, test loss: 0.180054\n",
      "Epoch 184, test loss: 0.173513\n",
      "Epoch 185, test loss: 0.173531\n",
      "Epoch 186, test loss: 0.173133\n",
      "Epoch 187, test loss: 0.174511\n",
      "Epoch 188, test loss: 0.173249\n",
      "Epoch 189, test loss: 0.178127\n",
      "Epoch 190, test loss: 0.173902\n",
      "Epoch 191, test loss: 0.173577\n",
      "Epoch 192, test loss: 0.173138\n",
      "Epoch 193, test loss: 0.172941\n",
      "Epoch 194, test loss: 0.173590\n",
      "Epoch 195, test loss: 0.173559\n",
      "Epoch 196, test loss: 0.172804\n",
      "Epoch 197, test loss: 0.172614\n",
      "Epoch 198, test loss: 0.174370\n",
      "Epoch 199, test loss: 0.177743\n",
      "Epoch 200, test loss: 0.173138\n",
      "Epoch 201, test loss: 0.175173\n",
      "Epoch 202, test loss: 0.175390\n",
      "Epoch 203, test loss: 0.173742\n",
      "Epoch 204, test loss: 0.173504\n",
      "Epoch 205, test loss: 0.174749\n",
      "Epoch 206, test loss: 0.173582\n",
      "Epoch 207, test loss: 0.175938\n",
      "Epoch 208, test loss: 0.173592\n",
      "Epoch 209, test loss: 0.173447\n",
      "Epoch 210, test loss: 0.175979\n",
      "Epoch 211, test loss: 0.173092\n",
      "Epoch 212, test loss: 0.173523\n",
      "Epoch 213, test loss: 0.173766\n",
      "Epoch 214, test loss: 0.175323\n",
      "Epoch 215, test loss: 0.173636\n",
      "Epoch 216, test loss: 0.175455\n",
      "Epoch 217, test loss: 0.173264\n",
      "Epoch 218, test loss: 0.173467\n",
      "Epoch 219, test loss: 0.173794\n",
      "Epoch 220, test loss: 0.176532\n",
      "Epoch 221, test loss: 0.176529\n",
      "Epoch 222, test loss: 0.173399\n",
      "Epoch 223, test loss: 0.173372\n",
      "Epoch 224, test loss: 0.173593\n",
      "Epoch 225, test loss: 0.174672\n",
      "Epoch 226, test loss: 0.174603\n",
      "Epoch 227, test loss: 0.172865\n",
      "Epoch 228, test loss: 0.173776\n",
      "Epoch 229, test loss: 0.172610\n",
      "Epoch 230, test loss: 0.174320\n",
      "Epoch 231, test loss: 0.174175\n",
      "Epoch 232, test loss: 0.173091\n",
      "Epoch 233, test loss: 0.174916\n",
      "Epoch 234, test loss: 0.172870\n",
      "Epoch 235, test loss: 0.173078\n",
      "Epoch 236, test loss: 0.173764\n",
      "Epoch 237, test loss: 0.175693\n",
      "Epoch 238, test loss: 0.172804\n",
      "Epoch 239, test loss: 0.173362\n",
      "Pretrain data: 19754507.0\n",
      "Building dataset, requesting data from 0 to 655\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6564/99226\n",
      "Found 655 continuous time series\n",
      "Data shape: (105792, 24), Train/test: 105790/2\n",
      "Train test ratio: 52895.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1652\n",
      "Epoch 0, train loss: 0.234262\n",
      "Epoch 1, train loss: 0.236793\n",
      "Epoch 2, train loss: 0.234443\n",
      "Epoch 3, train loss: 0.181748\n",
      "Epoch 4, train loss: 0.179864\n",
      "Epoch 5, train loss: 0.233358\n",
      "Epoch 6, train loss: 0.193061\n",
      "Epoch 7, train loss: 0.238422\n",
      "Epoch 8, train loss: 0.176518\n",
      "Epoch 9, train loss: 0.260612\n",
      "Epoch 10, train loss: 0.158138\n",
      "Epoch 11, train loss: 0.197855\n",
      "Epoch 12, train loss: 0.184995\n",
      "Epoch 13, train loss: 0.218784\n",
      "Epoch 14, train loss: 0.233525\n",
      "Epoch 15, train loss: 0.257339\n",
      "Epoch 16, train loss: 0.186347\n",
      "Epoch 17, train loss: 0.324033\n",
      "Epoch 18, train loss: 0.150931\n",
      "Epoch 19, train loss: 0.275582\n",
      "Epoch 20, train loss: 0.281755\n",
      "Epoch 21, train loss: 0.217301\n",
      "Epoch 22, train loss: 0.205035\n",
      "Epoch 23, train loss: 0.196264\n",
      "Epoch 24, train loss: 0.196408\n",
      "Epoch 25, train loss: 0.198127\n",
      "Epoch 26, train loss: 0.201738\n",
      "Epoch 27, train loss: 0.168972\n",
      "Epoch 28, train loss: 0.172348\n",
      "Epoch 29, train loss: 0.263711\n",
      "Epoch 30, train loss: 0.202479\n",
      "Epoch 31, train loss: 0.208600\n",
      "Epoch 32, train loss: 0.187919\n",
      "Epoch 33, train loss: 0.269004\n",
      "Epoch 34, train loss: 0.258261\n",
      "Epoch 35, train loss: 0.206279\n",
      "Epoch 36, train loss: 0.231899\n",
      "Epoch 37, train loss: 0.188160\n",
      "Epoch 38, train loss: 0.153303\n",
      "Epoch 39, train loss: 0.199475\n",
      "Epoch 40, train loss: 0.214755\n",
      "Epoch 41, train loss: 0.219685\n",
      "Epoch 42, train loss: 0.159368\n",
      "Epoch 43, train loss: 0.233495\n",
      "Epoch 44, train loss: 0.169172\n",
      "Epoch 45, train loss: 0.209769\n",
      "Epoch 46, train loss: 0.229892\n",
      "Epoch 47, train loss: 0.232760\n",
      "Epoch 48, train loss: 0.206295\n",
      "Epoch 49, train loss: 0.198307\n",
      "Epoch 50, train loss: 0.206938\n",
      "Epoch 51, train loss: 0.212438\n",
      "Epoch 52, train loss: 0.223867\n",
      "Epoch 53, train loss: 0.200014\n",
      "Epoch 54, train loss: 0.179616\n",
      "Epoch 55, train loss: 0.203584\n",
      "Epoch 56, train loss: 0.214139\n",
      "Epoch 57, train loss: 0.237777\n",
      "Epoch 58, train loss: 0.221407\n",
      "Epoch 59, train loss: 0.224991\n",
      "Epoch 60, train loss: 0.236974\n",
      "Epoch 61, train loss: 0.227565\n",
      "Epoch 62, train loss: 0.242715\n",
      "Epoch 63, train loss: 0.220644\n",
      "Epoch 64, train loss: 0.284748\n",
      "Epoch 65, train loss: 0.178464\n",
      "Epoch 66, train loss: 0.293655\n",
      "Epoch 67, train loss: 0.188974\n",
      "Epoch 68, train loss: 0.167611\n",
      "Epoch 69, train loss: 0.229231\n",
      "Epoch 70, train loss: 0.204764\n",
      "Epoch 71, train loss: 0.213231\n",
      "Epoch 72, train loss: 0.239793\n",
      "Epoch 73, train loss: 0.159545\n",
      "Epoch 74, train loss: 0.293942\n",
      "Epoch 75, train loss: 0.159371\n",
      "Epoch 76, train loss: 0.173902\n",
      "Epoch 77, train loss: 0.210088\n",
      "Epoch 78, train loss: 0.161894\n",
      "Epoch 79, train loss: 0.223500\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[296. 290. 284. ... 173. 173. 169.]\n",
      " [290. 284. 279. ... 173. 169. 160.]\n",
      " [284. 279. 272. ... 169. 160. 153.]\n",
      " ...\n",
      " [318. 306. 282. ... 172. 171. 178.]\n",
      " [306. 282. 275. ... 171. 178. 180.]\n",
      " [282. 275. 270. ... 178. 180. 180.]]\n",
      "y here is\n",
      "[[132. 132. 132. ... 132. 132. 132.]\n",
      " [127. 127. 127. ... 127. 127. 127.]\n",
      " [123. 123. 123. ... 123. 123. 123.]\n",
      " ...\n",
      " [169. 169. 169. ... 169. 169. 169.]\n",
      " [164. 164. 164. ... 164. 164. 164.]\n",
      " [157. 157. 157. ... 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1238, 24), Train/test: 1/1237\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 187 segments\n",
      "Building dataset, requesting data from 0 to 187\n",
      "x here is\n",
      "[[ 71.  71.  71. ...  81.  81.  83.]\n",
      " [ 71.  71.  71. ...  81.  83.  86.]\n",
      " [ 71.  71.  72. ...  83.  86.  89.]\n",
      " ...\n",
      " [ 58.  56.  58. ... 182. 177. 179.]\n",
      " [ 56.  58.  62. ... 177. 179. 185.]\n",
      " [ 58.  62.  78. ... 179. 185. 193.]]\n",
      "y here is\n",
      "[[ 87.  87.  87. ...  87.  87.  87.]\n",
      " [ 81.  81.  81. ...  81.  81.  81.]\n",
      " [ 78.  78.  78. ...  78.  78.  78.]\n",
      " ...\n",
      " [221. 221. 221. ... 221. 221. 221.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [226. 226. 226. ... 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 545/5477\n",
      "Found 187 continuous time series\n",
      "Data shape: (6024, 24), Train/test: 6022/2\n",
      "Train test ratio: 3011.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4787D8D30>\n",
      "Epoch 0, test loss: 0.217757\n",
      "Epoch 1, test loss: 0.217081\n",
      "Epoch 2, test loss: 0.222572\n",
      "Epoch 3, test loss: 0.217721\n",
      "Epoch 4, test loss: 0.216251\n",
      "Epoch 5, test loss: 0.217585\n",
      "Epoch 6, test loss: 0.217890\n",
      "Epoch 7, test loss: 0.216318\n",
      "Epoch 8, test loss: 0.217274\n",
      "Epoch 9, test loss: 0.219220\n",
      "Epoch 10, test loss: 0.217869\n",
      "Epoch 11, test loss: 0.216675\n",
      "Epoch 12, test loss: 0.215904\n",
      "Epoch 13, test loss: 0.215646\n",
      "Epoch 14, test loss: 0.216172\n",
      "Epoch 15, test loss: 0.216368\n",
      "Epoch 16, test loss: 0.215097\n",
      "Epoch 17, test loss: 0.227776\n",
      "Epoch 18, test loss: 0.218843\n",
      "Epoch 19, test loss: 0.219585\n",
      "Epoch 20, test loss: 0.216380\n",
      "Epoch 21, test loss: 0.215260\n",
      "Epoch 22, test loss: 0.217466\n",
      "Epoch 23, test loss: 0.215699\n",
      "Epoch 24, test loss: 0.216808\n",
      "Epoch 25, test loss: 0.215295\n",
      "Epoch 26, test loss: 0.215332\n",
      "Epoch 27, test loss: 0.215837\n",
      "Epoch 28, test loss: 0.218793\n",
      "Epoch 29, test loss: 0.215204\n",
      "Epoch 30, test loss: 0.215364\n",
      "Epoch 31, test loss: 0.215622\n",
      "Epoch 32, test loss: 0.216334\n",
      "Epoch 33, test loss: 0.216240\n",
      "Epoch 34, test loss: 0.215243\n",
      "Epoch 35, test loss: 0.221386\n",
      "Epoch 36, test loss: 0.214603\n",
      "Epoch 37, test loss: 0.216113\n",
      "Epoch 38, test loss: 0.216743\n",
      "Epoch 39, test loss: 0.215658\n",
      "Epoch 40, test loss: 0.216135\n",
      "Epoch 41, test loss: 0.214706\n",
      "Epoch 42, test loss: 0.217333\n",
      "Epoch 43, test loss: 0.215676\n",
      "Epoch 44, test loss: 0.214837\n",
      "Epoch 45, test loss: 0.215116\n",
      "Epoch 46, test loss: 0.215187\n",
      "Epoch 47, test loss: 0.214955\n",
      "Epoch 48, test loss: 0.214307\n",
      "Epoch 49, test loss: 0.217320\n",
      "Epoch 50, test loss: 0.216211\n",
      "Epoch 51, test loss: 0.215166\n",
      "Epoch 52, test loss: 0.218120\n",
      "Epoch 53, test loss: 0.217731\n",
      "Epoch 54, test loss: 0.215433\n",
      "Epoch 55, test loss: 0.215902\n",
      "Epoch 56, test loss: 0.219079\n",
      "Epoch 57, test loss: 0.214524\n",
      "Epoch 58, test loss: 0.217692\n",
      "Epoch 59, test loss: 0.215580\n",
      "Epoch 60, test loss: 0.214699\n",
      "Epoch 61, test loss: 0.215165\n",
      "Epoch 62, test loss: 0.217123\n",
      "Epoch 63, test loss: 0.215078\n",
      "Epoch 64, test loss: 0.214804\n",
      "Epoch 65, test loss: 0.216232\n",
      "Epoch 66, test loss: 0.214723\n",
      "Epoch 67, test loss: 0.214510\n",
      "Epoch 68, test loss: 0.215644\n",
      "Epoch 69, test loss: 0.214356\n",
      "Epoch 70, test loss: 0.213968\n",
      "Epoch 71, test loss: 0.213487\n",
      "Epoch 72, test loss: 0.213733\n",
      "Epoch 73, test loss: 0.213680\n",
      "Epoch 74, test loss: 0.214053\n",
      "Epoch 75, test loss: 0.214800\n",
      "Epoch 76, test loss: 0.214279\n",
      "Epoch 77, test loss: 0.216259\n",
      "Epoch 78, test loss: 0.213585\n",
      "Epoch 79, test loss: 0.214425\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4787D8D30>\n",
      "Epoch 0, test loss: 0.218618\n",
      "Epoch 1, test loss: 0.219692\n",
      "Epoch 2, test loss: 0.217824\n",
      "Epoch 3, test loss: 0.218311\n",
      "Epoch 4, test loss: 0.218258\n",
      "Epoch 5, test loss: 0.218040\n",
      "Epoch 6, test loss: 0.217417\n",
      "Epoch 7, test loss: 0.217508\n",
      "Epoch 8, test loss: 0.217752\n",
      "Epoch 9, test loss: 0.218911\n",
      "Epoch 10, test loss: 0.216947\n",
      "Epoch 11, test loss: 0.216719\n",
      "Epoch 12, test loss: 0.217479\n",
      "Epoch 13, test loss: 0.217915\n",
      "Epoch 14, test loss: 0.216744\n",
      "Epoch 15, test loss: 0.216426\n",
      "Epoch 16, test loss: 0.217163\n",
      "Epoch 17, test loss: 0.216964\n",
      "Epoch 18, test loss: 0.215717\n",
      "Epoch 19, test loss: 0.217925\n",
      "Epoch 20, test loss: 0.216285\n",
      "Epoch 21, test loss: 0.216072\n",
      "Epoch 22, test loss: 0.216636\n",
      "Epoch 23, test loss: 0.218569\n",
      "Epoch 24, test loss: 0.216999\n",
      "Epoch 25, test loss: 0.216743\n",
      "Epoch 26, test loss: 0.216634\n",
      "Epoch 27, test loss: 0.216797\n",
      "Epoch 28, test loss: 0.216630\n",
      "Epoch 29, test loss: 0.216954\n",
      "Epoch 30, test loss: 0.215715\n",
      "Epoch 31, test loss: 0.216080\n",
      "Epoch 32, test loss: 0.216206\n",
      "Epoch 33, test loss: 0.217801\n",
      "Epoch 34, test loss: 0.215708\n",
      "Epoch 35, test loss: 0.216789\n",
      "Epoch 36, test loss: 0.217317\n",
      "Epoch 37, test loss: 0.216129\n",
      "Epoch 38, test loss: 0.215921\n",
      "Epoch 39, test loss: 0.215878\n",
      "Epoch 40, test loss: 0.215142\n",
      "Epoch 41, test loss: 0.215649\n",
      "Epoch 42, test loss: 0.217540\n",
      "Epoch 43, test loss: 0.217107\n",
      "Epoch 44, test loss: 0.215743\n",
      "Epoch 45, test loss: 0.216333\n",
      "Epoch 46, test loss: 0.216806\n",
      "Epoch 47, test loss: 0.216790\n",
      "Epoch 48, test loss: 0.215733\n",
      "Epoch 49, test loss: 0.216563\n",
      "Epoch 50, test loss: 0.217669\n",
      "Epoch 51, test loss: 0.216107\n",
      "Epoch 52, test loss: 0.216518\n",
      "Epoch 53, test loss: 0.215900\n",
      "Epoch 54, test loss: 0.217185\n",
      "Epoch 55, test loss: 0.216216\n",
      "Epoch 56, test loss: 0.216606\n",
      "Epoch 57, test loss: 0.216315\n",
      "Epoch 58, test loss: 0.215667\n",
      "Epoch 59, test loss: 0.215184\n",
      "Epoch 60, test loss: 0.216491\n",
      "Epoch 61, test loss: 0.216241\n",
      "Epoch 62, test loss: 0.217118\n",
      "Epoch 63, test loss: 0.219988\n",
      "Epoch 64, test loss: 0.215581\n",
      "Epoch 65, test loss: 0.216307\n",
      "Epoch 66, test loss: 0.216670\n",
      "Epoch 67, test loss: 0.217671\n",
      "Epoch 68, test loss: 0.217000\n",
      "Epoch 69, test loss: 0.215035\n",
      "Epoch 70, test loss: 0.216576\n",
      "Epoch 71, test loss: 0.218479\n",
      "Epoch 72, test loss: 0.216066\n",
      "Epoch 73, test loss: 0.216902\n",
      "Epoch 74, test loss: 0.216740\n",
      "Epoch 75, test loss: 0.215815\n",
      "Epoch 76, test loss: 0.216540\n",
      "Epoch 77, test loss: 0.216359\n",
      "Epoch 78, test loss: 0.215950\n",
      "Epoch 79, test loss: 0.215767\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A4787D8D30>\n",
      "Epoch 0, test loss: 0.893613\n",
      "Epoch 1, test loss: 0.318584\n",
      "Epoch 2, test loss: 0.269847\n",
      "Epoch 3, test loss: 0.237177\n",
      "Epoch 4, test loss: 0.233006\n",
      "Epoch 5, test loss: 0.228895\n",
      "Epoch 6, test loss: 0.227165\n",
      "Epoch 7, test loss: 0.226164\n",
      "Epoch 8, test loss: 0.225356\n",
      "Epoch 9, test loss: 0.226188\n",
      "Epoch 10, test loss: 0.224405\n",
      "Epoch 11, test loss: 0.225119\n",
      "Epoch 12, test loss: 0.223249\n",
      "Epoch 13, test loss: 0.228632\n",
      "Epoch 14, test loss: 0.223117\n",
      "Epoch 15, test loss: 0.223559\n",
      "Epoch 16, test loss: 0.222680\n",
      "Epoch 17, test loss: 0.222994\n",
      "Epoch 18, test loss: 0.222248\n",
      "Epoch 19, test loss: 0.222586\n",
      "Epoch 20, test loss: 0.222075\n",
      "Epoch 21, test loss: 0.222304\n",
      "Epoch 22, test loss: 0.222143\n",
      "Epoch 23, test loss: 0.222353\n",
      "Epoch 24, test loss: 0.222267\n",
      "Epoch 25, test loss: 0.222155\n",
      "Epoch 26, test loss: 0.221779\n",
      "Epoch 27, test loss: 0.222020\n",
      "Epoch 28, test loss: 0.222195\n",
      "Epoch 29, test loss: 0.222169\n",
      "Epoch 30, test loss: 0.222875\n",
      "Epoch 31, test loss: 0.222871\n",
      "Epoch 32, test loss: 0.221327\n",
      "Epoch 33, test loss: 0.222000\n",
      "Epoch 34, test loss: 0.221936\n",
      "Epoch 35, test loss: 0.222086\n",
      "Epoch 36, test loss: 0.221262\n",
      "Epoch 37, test loss: 0.224594\n",
      "Epoch 38, test loss: 0.221826\n",
      "Epoch 39, test loss: 0.222613\n",
      "Epoch 40, test loss: 0.221596\n",
      "Epoch 41, test loss: 0.223902\n",
      "Epoch 42, test loss: 0.221570\n",
      "Epoch 43, test loss: 0.221035\n",
      "Epoch 44, test loss: 0.221135\n",
      "Epoch 45, test loss: 0.220819\n",
      "Epoch 46, test loss: 0.222927\n",
      "Epoch 47, test loss: 0.220933\n",
      "Epoch 48, test loss: 0.221565\n",
      "Epoch 49, test loss: 0.221303\n",
      "Epoch 50, test loss: 0.221060\n",
      "Epoch 51, test loss: 0.220858\n",
      "Epoch 52, test loss: 0.225809\n",
      "Epoch 53, test loss: 0.222486\n",
      "Epoch 54, test loss: 0.219784\n",
      "Epoch 55, test loss: 0.219691\n",
      "Epoch 56, test loss: 0.221565\n",
      "Epoch 57, test loss: 0.220273\n",
      "Epoch 58, test loss: 0.219526\n",
      "Epoch 59, test loss: 0.218945\n",
      "Epoch 60, test loss: 0.220656\n",
      "Epoch 61, test loss: 0.220889\n",
      "Epoch 62, test loss: 0.218666\n",
      "Epoch 63, test loss: 0.218608\n",
      "Epoch 64, test loss: 0.218296\n",
      "Epoch 65, test loss: 0.219735\n",
      "Epoch 66, test loss: 0.218716\n",
      "Epoch 67, test loss: 0.218458\n",
      "Epoch 68, test loss: 0.219848\n",
      "Epoch 69, test loss: 0.218554\n",
      "Epoch 70, test loss: 0.218272\n",
      "Epoch 71, test loss: 0.217752\n",
      "Epoch 72, test loss: 0.221068\n",
      "Epoch 73, test loss: 0.217644\n",
      "Epoch 74, test loss: 0.220113\n",
      "Epoch 75, test loss: 0.217840\n",
      "Epoch 76, test loss: 0.218237\n",
      "Epoch 77, test loss: 0.218762\n",
      "Epoch 78, test loss: 0.218565\n",
      "Epoch 79, test loss: 0.217466\n",
      "Epoch 80, test loss: 0.217540\n",
      "Epoch 81, test loss: 0.217017\n",
      "Epoch 82, test loss: 0.218804\n",
      "Epoch 83, test loss: 0.217518\n",
      "Epoch 84, test loss: 0.217833\n",
      "Epoch 85, test loss: 0.216557\n",
      "Epoch 86, test loss: 0.216777\n",
      "Epoch 87, test loss: 0.221207\n",
      "Epoch 88, test loss: 0.217186\n",
      "Epoch 89, test loss: 0.217194\n",
      "Epoch 90, test loss: 0.217050\n",
      "Epoch 91, test loss: 0.219618\n",
      "Epoch 92, test loss: 0.217519\n",
      "Epoch 93, test loss: 0.216752\n",
      "Epoch 94, test loss: 0.216460\n",
      "Epoch 95, test loss: 0.217622\n",
      "Epoch 96, test loss: 0.217980\n",
      "Epoch 97, test loss: 0.216350\n",
      "Epoch 98, test loss: 0.216599\n",
      "Epoch 99, test loss: 0.218089\n",
      "Epoch 100, test loss: 0.217623\n",
      "Epoch 101, test loss: 0.216614\n",
      "Epoch 102, test loss: 0.217753\n",
      "Epoch 103, test loss: 0.217740\n",
      "Epoch 104, test loss: 0.217538\n",
      "Epoch 105, test loss: 0.216762\n",
      "Epoch 106, test loss: 0.217442\n",
      "Epoch 107, test loss: 0.216576\n",
      "Epoch 108, test loss: 0.216148\n",
      "Epoch 109, test loss: 0.218997\n",
      "Epoch 110, test loss: 0.216585\n",
      "Epoch 111, test loss: 0.217292\n",
      "Epoch 112, test loss: 0.216260\n",
      "Epoch 113, test loss: 0.216408\n",
      "Epoch 114, test loss: 0.216187\n",
      "Epoch 115, test loss: 0.216151\n",
      "Epoch 116, test loss: 0.216298\n",
      "Epoch 117, test loss: 0.217626\n",
      "Epoch 118, test loss: 0.217903\n",
      "Epoch 119, test loss: 0.216268\n",
      "Epoch 120, test loss: 0.215674\n",
      "Epoch 121, test loss: 0.216480\n",
      "Epoch 122, test loss: 0.216118\n",
      "Epoch 123, test loss: 0.216031\n",
      "Epoch 124, test loss: 0.217247\n",
      "Epoch 125, test loss: 0.216587\n",
      "Epoch 126, test loss: 0.216422\n",
      "Epoch 127, test loss: 0.215847\n",
      "Epoch 128, test loss: 0.215656\n",
      "Epoch 129, test loss: 0.217542\n",
      "Epoch 130, test loss: 0.217275\n",
      "Epoch 131, test loss: 0.216627\n",
      "Epoch 132, test loss: 0.216499\n",
      "Epoch 133, test loss: 0.216515\n",
      "Epoch 134, test loss: 0.217398\n",
      "Epoch 135, test loss: 0.217092\n",
      "Epoch 136, test loss: 0.217166\n",
      "Epoch 137, test loss: 0.215290\n",
      "Epoch 138, test loss: 0.215682\n",
      "Epoch 139, test loss: 0.215795\n",
      "Epoch 140, test loss: 0.215893\n",
      "Epoch 141, test loss: 0.216052\n",
      "Epoch 142, test loss: 0.216443\n",
      "Epoch 143, test loss: 0.219969\n",
      "Epoch 144, test loss: 0.217514\n",
      "Epoch 145, test loss: 0.216534\n",
      "Epoch 146, test loss: 0.215727\n",
      "Epoch 147, test loss: 0.216877\n",
      "Epoch 148, test loss: 0.215854\n",
      "Epoch 149, test loss: 0.216554\n",
      "Epoch 150, test loss: 0.215949\n",
      "Epoch 151, test loss: 0.216444\n",
      "Epoch 152, test loss: 0.216051\n",
      "Epoch 153, test loss: 0.216951\n",
      "Epoch 154, test loss: 0.216166\n",
      "Epoch 155, test loss: 0.218412\n",
      "Epoch 156, test loss: 0.218108\n",
      "Epoch 157, test loss: 0.218336\n",
      "Epoch 158, test loss: 0.215859\n",
      "Epoch 159, test loss: 0.217539\n",
      "Epoch 160, test loss: 0.217295\n",
      "Epoch 161, test loss: 0.215877\n",
      "Epoch 162, test loss: 0.215830\n",
      "Epoch 163, test loss: 0.217965\n",
      "Epoch 164, test loss: 0.217671\n",
      "Epoch 165, test loss: 0.216827\n",
      "Epoch 166, test loss: 0.216128\n",
      "Epoch 167, test loss: 0.215917\n",
      "Epoch 168, test loss: 0.215974\n",
      "Epoch 169, test loss: 0.215758\n",
      "Epoch 170, test loss: 0.215601\n",
      "Epoch 171, test loss: 0.216655\n",
      "Epoch 172, test loss: 0.217031\n",
      "Epoch 173, test loss: 0.215242\n",
      "Epoch 174, test loss: 0.216117\n",
      "Epoch 175, test loss: 0.216144\n",
      "Epoch 176, test loss: 0.216481\n",
      "Epoch 177, test loss: 0.215715\n",
      "Epoch 178, test loss: 0.215812\n",
      "Epoch 179, test loss: 0.216660\n",
      "Epoch 180, test loss: 0.217096\n",
      "Epoch 181, test loss: 0.215710\n",
      "Epoch 182, test loss: 0.218947\n",
      "Epoch 183, test loss: 0.215953\n",
      "Epoch 184, test loss: 0.216916\n",
      "Epoch 185, test loss: 0.217010\n",
      "Epoch 186, test loss: 0.216361\n",
      "Epoch 187, test loss: 0.215854\n",
      "Epoch 188, test loss: 0.216219\n",
      "Epoch 189, test loss: 0.216923\n",
      "Epoch 190, test loss: 0.220101\n",
      "Epoch 191, test loss: 0.216914\n",
      "Epoch 192, test loss: 0.218119\n",
      "Epoch 193, test loss: 0.215970\n",
      "Epoch 194, test loss: 0.215708\n",
      "Epoch 195, test loss: 0.216526\n",
      "Epoch 196, test loss: 0.218133\n",
      "Epoch 197, test loss: 0.216847\n",
      "Epoch 198, test loss: 0.216401\n",
      "Epoch 199, test loss: 0.216688\n",
      "Epoch 200, test loss: 0.223120\n",
      "Epoch 201, test loss: 0.220505\n",
      "Epoch 202, test loss: 0.216217\n",
      "Epoch 203, test loss: 0.216085\n",
      "Epoch 204, test loss: 0.217928\n",
      "Epoch 205, test loss: 0.218527\n",
      "Epoch 206, test loss: 0.217659\n",
      "Epoch 207, test loss: 0.216136\n",
      "Epoch 208, test loss: 0.215979\n",
      "Epoch 209, test loss: 0.215988\n",
      "Epoch 210, test loss: 0.216809\n",
      "Epoch 211, test loss: 0.215584\n",
      "Epoch 212, test loss: 0.215959\n",
      "Epoch 213, test loss: 0.215898\n",
      "Epoch 214, test loss: 0.215434\n",
      "Epoch 215, test loss: 0.216892\n",
      "Epoch 216, test loss: 0.216061\n",
      "Epoch 217, test loss: 0.216130\n",
      "Epoch 218, test loss: 0.216401\n",
      "Epoch 219, test loss: 0.216585\n",
      "Epoch 220, test loss: 0.216351\n",
      "Epoch 221, test loss: 0.218935\n",
      "Epoch 222, test loss: 0.216271\n",
      "Epoch 223, test loss: 0.215658\n",
      "Epoch 224, test loss: 0.215813\n",
      "Epoch 225, test loss: 0.215892\n",
      "Epoch 226, test loss: 0.215974\n",
      "Epoch 227, test loss: 0.216618\n",
      "Epoch 228, test loss: 0.215862\n",
      "Epoch 229, test loss: 0.215803\n",
      "Epoch 230, test loss: 0.215601\n",
      "Epoch 231, test loss: 0.215751\n",
      "Epoch 232, test loss: 0.215685\n",
      "Epoch 233, test loss: 0.215739\n",
      "Epoch 234, test loss: 0.218639\n",
      "Epoch 235, test loss: 0.215779\n",
      "Epoch 236, test loss: 0.215892\n",
      "Epoch 237, test loss: 0.217308\n",
      "Epoch 238, test loss: 0.218740\n",
      "Epoch 239, test loss: 0.216248\n",
      "Pretrain data: 19086006.0\n",
      "Building dataset, requesting data from 0 to 782\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6954/94406\n",
      "Found 782 continuous time series\n",
      "Data shape: (101362, 24), Train/test: 101360/2\n",
      "Train test ratio: 50680.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1583\n",
      "Epoch 0, train loss: 0.235153\n",
      "Epoch 1, train loss: 0.195380\n",
      "Epoch 2, train loss: 0.198384\n",
      "Epoch 3, train loss: 0.198061\n",
      "Epoch 4, train loss: 0.201227\n",
      "Epoch 5, train loss: 0.214998\n",
      "Epoch 6, train loss: 0.161675\n",
      "Epoch 7, train loss: 0.239049\n",
      "Epoch 8, train loss: 0.223776\n",
      "Epoch 9, train loss: 0.199208\n",
      "Epoch 10, train loss: 0.242484\n",
      "Epoch 11, train loss: 0.189754\n",
      "Epoch 12, train loss: 0.216444\n",
      "Epoch 13, train loss: 0.217771\n",
      "Epoch 14, train loss: 0.297336\n",
      "Epoch 15, train loss: 0.259985\n",
      "Epoch 16, train loss: 0.191615\n",
      "Epoch 17, train loss: 0.266821\n",
      "Epoch 18, train loss: 0.149383\n",
      "Epoch 19, train loss: 0.242183\n",
      "Epoch 20, train loss: 0.181882\n",
      "Epoch 21, train loss: 0.225349\n",
      "Epoch 22, train loss: 0.245577\n",
      "Epoch 23, train loss: 0.159692\n",
      "Epoch 24, train loss: 0.226920\n",
      "Epoch 25, train loss: 0.251992\n",
      "Epoch 26, train loss: 0.142503\n",
      "Epoch 27, train loss: 0.154699\n",
      "Epoch 28, train loss: 0.182393\n",
      "Epoch 29, train loss: 0.268229\n",
      "Epoch 30, train loss: 0.216006\n",
      "Epoch 31, train loss: 0.159876\n",
      "Epoch 32, train loss: 0.208433\n",
      "Epoch 33, train loss: 0.216468\n",
      "Epoch 34, train loss: 0.224940\n",
      "Epoch 35, train loss: 0.204693\n",
      "Epoch 36, train loss: 0.232103\n",
      "Epoch 37, train loss: 0.178872\n",
      "Epoch 38, train loss: 0.244141\n",
      "Epoch 39, train loss: 0.158960\n",
      "Epoch 40, train loss: 0.212120\n",
      "Epoch 41, train loss: 0.207952\n",
      "Epoch 42, train loss: 0.266280\n",
      "Epoch 43, train loss: 0.173005\n",
      "Epoch 44, train loss: 0.229537\n",
      "Epoch 45, train loss: 0.221700\n",
      "Epoch 46, train loss: 0.203641\n",
      "Epoch 47, train loss: 0.234390\n",
      "Epoch 48, train loss: 0.186802\n",
      "Epoch 49, train loss: 0.231891\n",
      "Epoch 50, train loss: 0.227351\n",
      "Epoch 51, train loss: 0.226816\n",
      "Epoch 52, train loss: 0.211052\n",
      "Epoch 53, train loss: 0.201568\n",
      "Epoch 54, train loss: 0.184799\n",
      "Epoch 55, train loss: 0.199654\n",
      "Epoch 56, train loss: 0.175238\n",
      "Epoch 57, train loss: 0.159045\n",
      "Epoch 58, train loss: 0.181574\n",
      "Epoch 59, train loss: 0.178812\n",
      "Epoch 60, train loss: 0.204569\n",
      "Epoch 61, train loss: 0.185746\n",
      "Epoch 62, train loss: 0.168760\n",
      "Epoch 63, train loss: 0.204167\n",
      "Epoch 64, train loss: 0.231161\n",
      "Epoch 65, train loss: 0.202247\n",
      "Epoch 66, train loss: 0.224071\n",
      "Epoch 67, train loss: 0.143235\n",
      "Epoch 68, train loss: 0.225958\n",
      "Epoch 69, train loss: 0.179829\n",
      "Epoch 70, train loss: 0.175336\n",
      "Epoch 71, train loss: 0.207011\n",
      "Epoch 72, train loss: 0.165209\n",
      "Epoch 73, train loss: 0.169553\n",
      "Epoch 74, train loss: 0.267267\n",
      "Epoch 75, train loss: 0.155573\n",
      "Epoch 76, train loss: 0.146383\n",
      "Epoch 77, train loss: 0.189728\n",
      "Epoch 78, train loss: 0.194092\n",
      "Epoch 79, train loss: 0.279547\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "x here is\n",
      "[[243. 253. 262. ... 330. 318. 296.]\n",
      " [253. 262. 269. ... 318. 296. 289.]\n",
      " [262. 269. 269. ... 296. 289. 290.]\n",
      " ...\n",
      " [126. 124. 110. ...  53.  66.  90.]\n",
      " [124. 110.  97. ...  66.  90.  87.]\n",
      " [110.  97.  83. ...  90.  87.  86.]]\n",
      "y here is\n",
      "[[267. 267. 267. ... 267. 267. 267.]\n",
      " [262. 262. 262. ... 262. 262. 262.]\n",
      " [259. 259. 259. ... 259. 259. 259.]\n",
      " ...\n",
      " [ 72.  72.  72. ...  72.  72.  72.]\n",
      " [ 78.  78.  78. ...  78.  78.  78.]\n",
      " [ 79.  79.  79. ...  79.  79.  79.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (2201, 24), Train/test: 1/2200\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 60 segments\n",
      "Building dataset, requesting data from 0 to 60\n",
      "x here is\n",
      "[[ 48.  48.  53. ... 129. 130. 132.]\n",
      " [ 48.  53.  63. ... 130. 132. 134.]\n",
      " [ 53.  63.  69. ... 132. 134. 138.]\n",
      " ...\n",
      " [199. 192. 191. ... 212. 223. 227.]\n",
      " [192. 191. 194. ... 223. 227. 223.]\n",
      " [191. 194. 187. ... 227. 223. 215.]]\n",
      "y here is\n",
      "[[146. 146. 146. ... 146. 146. 146.]\n",
      " [152. 152. 152. ... 152. 152. 152.]\n",
      " [158. 158. 158. ... 158. 158. 158.]\n",
      " ...\n",
      " [213. 213. 213. ... 213. 213. 213.]\n",
      " [224. 224. 224. ... 224. 224. 224.]\n",
      " [235. 235. 235. ... 235. 235. 235.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 155/10297\n",
      "Found 60 continuous time series\n",
      "Data shape: (10454, 24), Train/test: 10452/2\n",
      "Train test ratio: 5226.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29569DA80>\n",
      "Epoch 0, test loss: 0.219857\n",
      "Epoch 1, test loss: 0.218573\n",
      "Epoch 2, test loss: 0.219269\n",
      "Epoch 3, test loss: 0.218615\n",
      "Epoch 4, test loss: 0.222997\n",
      "Epoch 5, test loss: 0.220295\n",
      "Epoch 6, test loss: 0.219583\n",
      "Epoch 7, test loss: 0.219887\n",
      "Epoch 8, test loss: 0.219682\n",
      "Epoch 9, test loss: 0.223913\n",
      "Epoch 10, test loss: 0.219981\n",
      "Epoch 11, test loss: 0.219136\n",
      "Epoch 12, test loss: 0.223121\n",
      "Epoch 13, test loss: 0.221475\n",
      "Epoch 14, test loss: 0.218583\n",
      "Epoch 15, test loss: 0.219853\n",
      "Epoch 16, test loss: 0.219239\n",
      "Epoch 17, test loss: 0.218570\n",
      "Epoch 18, test loss: 0.218989\n",
      "Epoch 19, test loss: 0.219246\n",
      "Epoch 20, test loss: 0.223362\n",
      "Epoch 21, test loss: 0.222524\n",
      "Epoch 22, test loss: 0.219561\n",
      "Epoch 23, test loss: 0.219185\n",
      "Epoch 24, test loss: 0.220832\n",
      "Epoch 25, test loss: 0.226720\n",
      "Epoch 26, test loss: 0.223776\n",
      "Epoch 27, test loss: 0.220347\n",
      "Epoch 28, test loss: 0.218480\n",
      "Epoch 29, test loss: 0.221211\n",
      "Epoch 30, test loss: 0.218607\n",
      "Epoch 31, test loss: 0.221061\n",
      "Epoch 32, test loss: 0.220291\n",
      "Epoch 33, test loss: 0.219627\n",
      "Epoch 34, test loss: 0.221912\n",
      "Epoch 35, test loss: 0.222384\n",
      "Epoch 36, test loss: 0.220701\n",
      "Epoch 37, test loss: 0.219123\n",
      "Epoch 38, test loss: 0.220426\n",
      "Epoch 39, test loss: 0.219086\n",
      "Epoch 40, test loss: 0.221438\n",
      "Epoch 41, test loss: 0.219074\n",
      "Epoch 42, test loss: 0.219055\n",
      "Epoch 43, test loss: 0.218703\n",
      "Epoch 44, test loss: 0.218458\n",
      "Epoch 45, test loss: 0.218112\n",
      "Epoch 46, test loss: 0.219032\n",
      "Epoch 47, test loss: 0.220050\n",
      "Epoch 48, test loss: 0.220593\n",
      "Epoch 49, test loss: 0.218160\n",
      "Epoch 50, test loss: 0.219576\n",
      "Epoch 51, test loss: 0.218804\n",
      "Epoch 52, test loss: 0.218616\n",
      "Epoch 53, test loss: 0.218112\n",
      "Epoch 54, test loss: 0.222215\n",
      "Epoch 55, test loss: 0.221928\n",
      "Epoch 56, test loss: 0.220071\n",
      "Epoch 57, test loss: 0.220065\n",
      "Epoch 58, test loss: 0.225984\n",
      "Epoch 59, test loss: 0.219336\n",
      "Epoch 60, test loss: 0.224820\n",
      "Epoch 61, test loss: 0.219095\n",
      "Epoch 62, test loss: 0.224597\n",
      "Epoch 63, test loss: 0.219948\n",
      "Epoch 64, test loss: 0.219356\n",
      "Epoch 65, test loss: 0.223301\n",
      "Epoch 66, test loss: 0.226253\n",
      "Epoch 67, test loss: 0.221677\n",
      "Epoch 68, test loss: 0.218200\n",
      "Epoch 69, test loss: 0.218470\n",
      "Epoch 70, test loss: 0.218799\n",
      "Epoch 71, test loss: 0.221815\n",
      "Epoch 72, test loss: 0.220821\n",
      "Epoch 73, test loss: 0.220232\n",
      "Epoch 74, test loss: 0.218805\n",
      "Epoch 75, test loss: 0.219035\n",
      "Epoch 76, test loss: 0.220736\n",
      "Epoch 77, test loss: 0.225883\n",
      "Epoch 78, test loss: 0.224927\n",
      "Epoch 79, test loss: 0.222583\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29569DA80>\n",
      "Epoch 0, test loss: 0.217037\n",
      "Epoch 1, test loss: 0.217871\n",
      "Epoch 2, test loss: 0.223730\n",
      "Epoch 3, test loss: 0.217733\n",
      "Epoch 4, test loss: 0.217374\n",
      "Epoch 5, test loss: 0.217858\n",
      "Epoch 6, test loss: 0.217979\n",
      "Epoch 7, test loss: 0.217020\n",
      "Epoch 8, test loss: 0.216350\n",
      "Epoch 9, test loss: 0.217432\n",
      "Epoch 10, test loss: 0.219234\n",
      "Epoch 11, test loss: 0.217917\n",
      "Epoch 12, test loss: 0.218560\n",
      "Epoch 13, test loss: 0.217739\n",
      "Epoch 14, test loss: 0.217520\n",
      "Epoch 15, test loss: 0.216981\n",
      "Epoch 16, test loss: 0.217595\n",
      "Epoch 17, test loss: 0.217590\n",
      "Epoch 18, test loss: 0.216573\n",
      "Epoch 19, test loss: 0.217705\n",
      "Epoch 20, test loss: 0.216692\n",
      "Epoch 21, test loss: 0.217864\n",
      "Epoch 22, test loss: 0.216659\n",
      "Epoch 23, test loss: 0.217173\n",
      "Epoch 24, test loss: 0.216612\n",
      "Epoch 25, test loss: 0.217387\n",
      "Epoch 26, test loss: 0.217894\n",
      "Epoch 27, test loss: 0.217373\n",
      "Epoch 28, test loss: 0.216745\n",
      "Epoch 29, test loss: 0.218004\n",
      "Epoch 30, test loss: 0.217871\n",
      "Epoch 31, test loss: 0.217347\n",
      "Epoch 32, test loss: 0.216965\n",
      "Epoch 33, test loss: 0.217758\n",
      "Epoch 34, test loss: 0.217494\n",
      "Epoch 35, test loss: 0.217615\n",
      "Epoch 36, test loss: 0.216939\n",
      "Epoch 37, test loss: 0.217485\n",
      "Epoch 38, test loss: 0.218160\n",
      "Epoch 39, test loss: 0.217241\n",
      "Epoch 40, test loss: 0.218780\n",
      "Epoch 41, test loss: 0.218358\n",
      "Epoch 42, test loss: 0.218371\n",
      "Epoch 43, test loss: 0.218086\n",
      "Epoch 44, test loss: 0.217727\n",
      "Epoch 45, test loss: 0.218691\n",
      "Epoch 46, test loss: 0.219596\n",
      "Epoch 47, test loss: 0.219466\n",
      "Epoch 48, test loss: 0.218634\n",
      "Epoch 49, test loss: 0.220082\n",
      "Epoch 50, test loss: 0.220556\n",
      "Epoch 51, test loss: 0.217524\n",
      "Epoch 52, test loss: 0.217575\n",
      "Epoch 53, test loss: 0.217980\n",
      "Epoch 54, test loss: 0.217955\n",
      "Epoch 55, test loss: 0.218849\n",
      "Epoch 56, test loss: 0.221395\n",
      "Epoch 57, test loss: 0.217673\n",
      "Epoch 58, test loss: 0.221429\n",
      "Epoch 59, test loss: 0.219170\n",
      "Epoch 60, test loss: 0.219376\n",
      "Epoch 61, test loss: 0.218519\n",
      "Epoch 62, test loss: 0.220179\n",
      "Epoch 63, test loss: 0.219943\n",
      "Epoch 64, test loss: 0.217291\n",
      "Epoch 65, test loss: 0.219316\n",
      "Epoch 66, test loss: 0.218200\n",
      "Epoch 67, test loss: 0.217051\n",
      "Epoch 68, test loss: 0.218661\n",
      "Epoch 69, test loss: 0.217361\n",
      "Epoch 70, test loss: 0.219596\n",
      "Epoch 71, test loss: 0.217677\n",
      "Epoch 72, test loss: 0.217349\n",
      "Epoch 73, test loss: 0.217615\n",
      "Epoch 74, test loss: 0.217419\n",
      "Epoch 75, test loss: 0.220073\n",
      "Epoch 76, test loss: 0.217798\n",
      "Epoch 77, test loss: 0.222100\n",
      "Epoch 78, test loss: 0.218254\n",
      "Epoch 79, test loss: 0.221047\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A29569DA80>\n",
      "Epoch 0, test loss: 0.324615\n",
      "Epoch 1, test loss: 0.271642\n",
      "Epoch 2, test loss: 0.236283\n",
      "Epoch 3, test loss: 0.226682\n",
      "Epoch 4, test loss: 0.223671\n",
      "Epoch 5, test loss: 0.222264\n",
      "Epoch 6, test loss: 0.222659\n",
      "Epoch 7, test loss: 0.222072\n",
      "Epoch 8, test loss: 0.220461\n",
      "Epoch 9, test loss: 0.220237\n",
      "Epoch 10, test loss: 0.220140\n",
      "Epoch 11, test loss: 0.220184\n",
      "Epoch 12, test loss: 0.226343\n",
      "Epoch 13, test loss: 0.219729\n",
      "Epoch 14, test loss: 0.219383\n",
      "Epoch 15, test loss: 0.220696\n",
      "Epoch 16, test loss: 0.218745\n",
      "Epoch 17, test loss: 0.222587\n",
      "Epoch 18, test loss: 0.218533\n",
      "Epoch 19, test loss: 0.219042\n",
      "Epoch 20, test loss: 0.218286\n",
      "Epoch 21, test loss: 0.218386\n",
      "Epoch 22, test loss: 0.219200\n",
      "Epoch 23, test loss: 0.219586\n",
      "Epoch 24, test loss: 0.220807\n",
      "Epoch 25, test loss: 0.218973\n",
      "Epoch 26, test loss: 0.219539\n",
      "Epoch 27, test loss: 0.220362\n",
      "Epoch 28, test loss: 0.218155\n",
      "Epoch 29, test loss: 0.219340\n",
      "Epoch 30, test loss: 0.219700\n",
      "Epoch 31, test loss: 0.222839\n",
      "Epoch 32, test loss: 0.218522\n",
      "Epoch 33, test loss: 0.217948\n",
      "Epoch 34, test loss: 0.217800\n",
      "Epoch 35, test loss: 0.218352\n",
      "Epoch 36, test loss: 0.217776\n",
      "Epoch 37, test loss: 0.217238\n",
      "Epoch 38, test loss: 0.217420\n",
      "Epoch 39, test loss: 0.223801\n",
      "Epoch 40, test loss: 0.218560\n",
      "Epoch 41, test loss: 0.217919\n",
      "Epoch 42, test loss: 0.217649\n",
      "Epoch 43, test loss: 0.217690\n",
      "Epoch 44, test loss: 0.223922\n",
      "Epoch 45, test loss: 0.217766\n",
      "Epoch 46, test loss: 0.218590\n",
      "Epoch 47, test loss: 0.219526\n",
      "Epoch 48, test loss: 0.217346\n",
      "Epoch 49, test loss: 0.220362\n",
      "Epoch 50, test loss: 0.222810\n",
      "Epoch 51, test loss: 0.217418\n",
      "Epoch 52, test loss: 0.221811\n",
      "Epoch 53, test loss: 0.217365\n",
      "Epoch 54, test loss: 0.218261\n",
      "Epoch 55, test loss: 0.217771\n",
      "Epoch 56, test loss: 0.218948\n",
      "Epoch 57, test loss: 0.217481\n",
      "Epoch 58, test loss: 0.217498\n",
      "Epoch 59, test loss: 0.218235\n",
      "Epoch 60, test loss: 0.216748\n",
      "Epoch 61, test loss: 0.218675\n",
      "Epoch 62, test loss: 0.219376\n",
      "Epoch 63, test loss: 0.216974\n",
      "Epoch 64, test loss: 0.220327\n",
      "Epoch 65, test loss: 0.218389\n",
      "Epoch 66, test loss: 0.219104\n",
      "Epoch 67, test loss: 0.219226\n",
      "Epoch 68, test loss: 0.218371\n",
      "Epoch 69, test loss: 0.217993\n",
      "Epoch 70, test loss: 0.217813\n",
      "Epoch 71, test loss: 0.217925\n",
      "Epoch 72, test loss: 0.217560\n",
      "Epoch 73, test loss: 0.217421\n",
      "Epoch 74, test loss: 0.217331\n",
      "Epoch 75, test loss: 0.218519\n",
      "Epoch 76, test loss: 0.217568\n",
      "Epoch 77, test loss: 0.217493\n",
      "Epoch 78, test loss: 0.217127\n",
      "Epoch 79, test loss: 0.217169\n",
      "Epoch 80, test loss: 0.218643\n",
      "Epoch 81, test loss: 0.221334\n",
      "Epoch 82, test loss: 0.219143\n",
      "Epoch 83, test loss: 0.218803\n",
      "Epoch 84, test loss: 0.217422\n",
      "Epoch 85, test loss: 0.219612\n",
      "Epoch 86, test loss: 0.217652\n",
      "Epoch 87, test loss: 0.217297\n",
      "Epoch 88, test loss: 0.216861\n",
      "Epoch 89, test loss: 0.218482\n",
      "Epoch 90, test loss: 0.222383\n",
      "Epoch 91, test loss: 0.217987\n",
      "Epoch 92, test loss: 0.218038\n",
      "Epoch 93, test loss: 0.219376\n",
      "Epoch 94, test loss: 0.217460\n",
      "Epoch 95, test loss: 0.217926\n",
      "Epoch 96, test loss: 0.218221\n",
      "Epoch 97, test loss: 0.217481\n",
      "Epoch 98, test loss: 0.217369\n",
      "Epoch 99, test loss: 0.217273\n",
      "Epoch 100, test loss: 0.218163\n",
      "Epoch 101, test loss: 0.217905\n",
      "Epoch 102, test loss: 0.216995\n",
      "Epoch 103, test loss: 0.217080\n",
      "Epoch 104, test loss: 0.218191\n",
      "Epoch 105, test loss: 0.217249\n",
      "Epoch 106, test loss: 0.217833\n",
      "Epoch 107, test loss: 0.217734\n",
      "Epoch 108, test loss: 0.218681\n",
      "Epoch 109, test loss: 0.217732\n",
      "Epoch 110, test loss: 0.217637\n",
      "Epoch 111, test loss: 0.217688\n",
      "Epoch 112, test loss: 0.219527\n",
      "Epoch 113, test loss: 0.217836\n",
      "Epoch 114, test loss: 0.219115\n",
      "Epoch 115, test loss: 0.216982\n",
      "Epoch 116, test loss: 0.217231\n",
      "Epoch 117, test loss: 0.219406\n",
      "Epoch 118, test loss: 0.217868\n",
      "Epoch 119, test loss: 0.220982\n",
      "Epoch 120, test loss: 0.217329\n",
      "Epoch 121, test loss: 0.217925\n",
      "Epoch 122, test loss: 0.217255\n",
      "Epoch 123, test loss: 0.222678\n",
      "Epoch 124, test loss: 0.217586\n",
      "Epoch 125, test loss: 0.218654\n",
      "Epoch 126, test loss: 0.219652\n",
      "Epoch 127, test loss: 0.216989\n",
      "Epoch 128, test loss: 0.220648\n",
      "Epoch 129, test loss: 0.216748\n",
      "Epoch 130, test loss: 0.217756\n",
      "Epoch 131, test loss: 0.217100\n",
      "Epoch 132, test loss: 0.218147\n",
      "Epoch 133, test loss: 0.217760\n",
      "Epoch 134, test loss: 0.219689\n",
      "Epoch 135, test loss: 0.219568\n",
      "Epoch 136, test loss: 0.217610\n",
      "Epoch 137, test loss: 0.218170\n",
      "Epoch 138, test loss: 0.218557\n",
      "Epoch 139, test loss: 0.221912\n",
      "Epoch 140, test loss: 0.219838\n",
      "Epoch 141, test loss: 0.217923\n",
      "Epoch 142, test loss: 0.218103\n",
      "Epoch 143, test loss: 0.219362\n",
      "Epoch 144, test loss: 0.218171\n",
      "Epoch 145, test loss: 0.217673\n",
      "Epoch 146, test loss: 0.218423\n",
      "Epoch 147, test loss: 0.220898\n",
      "Epoch 148, test loss: 0.219153\n",
      "Epoch 149, test loss: 0.221227\n",
      "Epoch 150, test loss: 0.220751\n",
      "Epoch 151, test loss: 0.217450\n",
      "Epoch 152, test loss: 0.218326\n",
      "Epoch 153, test loss: 0.218415\n",
      "Epoch 154, test loss: 0.217026\n",
      "Epoch 155, test loss: 0.219136\n",
      "Epoch 156, test loss: 0.218095\n",
      "Epoch 157, test loss: 0.217070\n",
      "Epoch 158, test loss: 0.219395\n",
      "Epoch 159, test loss: 0.218346\n",
      "Epoch 160, test loss: 0.219098\n",
      "Epoch 161, test loss: 0.219907\n",
      "Epoch 162, test loss: 0.217720\n",
      "Epoch 163, test loss: 0.219297\n",
      "Epoch 164, test loss: 0.217746\n",
      "Epoch 165, test loss: 0.219505\n",
      "Epoch 166, test loss: 0.218181\n",
      "Epoch 167, test loss: 0.217196\n",
      "Epoch 168, test loss: 0.217299\n",
      "Epoch 169, test loss: 0.220426\n",
      "Epoch 170, test loss: 0.223820\n",
      "Epoch 171, test loss: 0.217360\n",
      "Epoch 172, test loss: 0.218737\n",
      "Epoch 173, test loss: 0.217827\n",
      "Epoch 174, test loss: 0.217470\n",
      "Epoch 175, test loss: 0.221880\n",
      "Epoch 176, test loss: 0.217574\n",
      "Epoch 177, test loss: 0.217328\n",
      "Epoch 178, test loss: 0.218200\n",
      "Epoch 179, test loss: 0.222482\n",
      "Epoch 180, test loss: 0.218996\n",
      "Epoch 181, test loss: 0.217720\n",
      "Epoch 182, test loss: 0.217198\n",
      "Epoch 183, test loss: 0.218270\n",
      "Epoch 184, test loss: 0.217555\n",
      "Epoch 185, test loss: 0.217525\n",
      "Epoch 186, test loss: 0.218257\n",
      "Epoch 187, test loss: 0.219020\n",
      "Epoch 188, test loss: 0.217596\n",
      "Epoch 189, test loss: 0.217665\n",
      "Epoch 190, test loss: 0.217567\n",
      "Epoch 191, test loss: 0.218600\n",
      "Epoch 192, test loss: 0.218176\n",
      "Epoch 193, test loss: 0.220272\n",
      "Epoch 194, test loss: 0.218078\n",
      "Epoch 195, test loss: 0.217922\n",
      "Epoch 196, test loss: 0.218918\n",
      "Epoch 197, test loss: 0.217660\n",
      "Epoch 198, test loss: 0.219222\n",
      "Epoch 199, test loss: 0.218689\n",
      "Epoch 200, test loss: 0.219437\n",
      "Epoch 201, test loss: 0.217792\n",
      "Epoch 202, test loss: 0.218123\n",
      "Epoch 203, test loss: 0.218065\n",
      "Epoch 204, test loss: 0.217754\n",
      "Epoch 205, test loss: 0.217254\n",
      "Epoch 206, test loss: 0.219077\n",
      "Epoch 207, test loss: 0.217795\n",
      "Epoch 208, test loss: 0.218067\n",
      "Epoch 209, test loss: 0.218611\n",
      "Epoch 210, test loss: 0.218442\n",
      "Epoch 211, test loss: 0.218261\n",
      "Epoch 212, test loss: 0.219491\n",
      "Epoch 213, test loss: 0.217884\n",
      "Epoch 214, test loss: 0.223371\n",
      "Epoch 215, test loss: 0.219557\n",
      "Epoch 216, test loss: 0.217501\n",
      "Epoch 217, test loss: 0.220154\n",
      "Epoch 218, test loss: 0.218991\n",
      "Epoch 219, test loss: 0.218741\n",
      "Epoch 220, test loss: 0.220124\n",
      "Epoch 221, test loss: 0.220051\n",
      "Epoch 222, test loss: 0.217883\n",
      "Epoch 223, test loss: 0.217968\n",
      "Epoch 224, test loss: 0.218926\n",
      "Epoch 225, test loss: 0.218753\n",
      "Epoch 226, test loss: 0.217695\n",
      "Epoch 227, test loss: 0.219517\n",
      "Epoch 228, test loss: 0.218870\n",
      "Epoch 229, test loss: 0.217855\n",
      "Epoch 230, test loss: 0.217541\n",
      "Epoch 231, test loss: 0.218086\n",
      "Epoch 232, test loss: 0.221145\n",
      "Epoch 233, test loss: 0.220988\n",
      "Epoch 234, test loss: 0.220325\n",
      "Epoch 235, test loss: 0.218543\n",
      "Epoch 236, test loss: 0.217611\n",
      "Epoch 237, test loss: 0.217780\n",
      "Epoch 238, test loss: 0.219421\n",
      "Epoch 239, test loss: 0.217333\n",
      "Pretrain data: 19823659.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. ... 156. 159. 160.]\n",
      " [128. 129. 131. ... 159. 160. 160.]\n",
      " [129. 131. 133. ... 160. 160. 162.]\n",
      " ...\n",
      " [325. 332. 336. ... 248. 244. 243.]\n",
      " [332. 336. 321. ... 244. 243. 244.]\n",
      " [336. 321. 308. ... 243. 244. 239.]]\n",
      "y here is\n",
      "[[173. 173. 173. ... 173. 173. 173.]\n",
      " [176. 176. 176. ... 176. 176. 176.]\n",
      " [180. 180. 180. ... 180. 180. 180.]\n",
      " ...\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [216. 216. 216. ... 216. 216. 216.]\n",
      " [213. 213. 213. ... 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6610/95055\n",
      "Found 815 continuous time series\n",
      "Data shape: (101667, 24), Train/test: 101665/2\n",
      "Train test ratio: 50832.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 24), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 24), dtype=float32)\n",
      "line73: Shape of y: (None, 24)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1588\n",
      "Epoch 0, train loss: 0.250905\n",
      "Epoch 1, train loss: 0.197495\n",
      "Epoch 2, train loss: 0.194863\n",
      "Epoch 3, train loss: 0.228674\n",
      "Epoch 4, train loss: 0.223838\n",
      "Epoch 5, train loss: 0.242170\n",
      "Epoch 6, train loss: 0.179605\n",
      "Epoch 7, train loss: 0.209758\n",
      "Epoch 8, train loss: 0.157727\n",
      "Epoch 9, train loss: 0.205829\n",
      "Epoch 10, train loss: 0.212158\n",
      "Epoch 11, train loss: 0.192929\n",
      "Epoch 12, train loss: 0.237124\n",
      "Epoch 13, train loss: 0.187736\n",
      "Epoch 14, train loss: 0.203869\n",
      "Epoch 15, train loss: 0.199205\n",
      "Epoch 16, train loss: 0.205434\n",
      "Epoch 17, train loss: 0.225674\n",
      "Epoch 18, train loss: 0.188330\n",
      "Epoch 19, train loss: 0.229922\n",
      "Epoch 20, train loss: 0.197442\n",
      "Epoch 21, train loss: 0.184588\n",
      "Epoch 22, train loss: 0.303013\n",
      "Epoch 23, train loss: 0.204944\n",
      "Epoch 24, train loss: 0.248370\n",
      "Epoch 25, train loss: 0.173309\n",
      "Epoch 26, train loss: 0.300885\n",
      "Epoch 27, train loss: 0.201020\n",
      "Epoch 28, train loss: 0.222019\n",
      "Epoch 29, train loss: 0.235271\n",
      "Epoch 30, train loss: 0.192926\n",
      "Epoch 31, train loss: 0.203543\n",
      "Epoch 32, train loss: 0.206220\n",
      "Epoch 33, train loss: 0.241088\n",
      "Epoch 34, train loss: 0.196620\n",
      "Epoch 35, train loss: 0.186480\n",
      "Epoch 36, train loss: 0.197295\n",
      "Epoch 37, train loss: 0.218955\n",
      "Epoch 38, train loss: 0.186567\n",
      "Epoch 39, train loss: 0.234286\n",
      "Epoch 40, train loss: 0.224586\n",
      "Epoch 41, train loss: 0.221164\n",
      "Epoch 42, train loss: 0.229957\n",
      "Epoch 43, train loss: 0.316439\n",
      "Epoch 44, train loss: 0.176925\n",
      "Epoch 45, train loss: 0.256928\n",
      "Epoch 46, train loss: 0.173367\n",
      "Epoch 47, train loss: 0.190333\n",
      "Epoch 48, train loss: 0.196631\n",
      "Epoch 49, train loss: 0.160811\n",
      "Epoch 50, train loss: 0.241875\n",
      "Epoch 51, train loss: 0.239691\n",
      "Epoch 52, train loss: 0.248989\n",
      "Epoch 53, train loss: 0.200768\n",
      "Epoch 54, train loss: 0.217115\n",
      "Epoch 55, train loss: 0.214214\n",
      "Epoch 56, train loss: 0.225054\n",
      "Epoch 57, train loss: 0.194382\n",
      "Epoch 58, train loss: 0.166887\n",
      "Epoch 59, train loss: 0.220743\n",
      "Epoch 60, train loss: 0.184096\n",
      "Epoch 61, train loss: 0.191100\n",
      "Epoch 62, train loss: 0.214254\n",
      "Epoch 63, train loss: 0.260777\n",
      "Epoch 64, train loss: 0.237581\n",
      "Epoch 65, train loss: 0.239304\n",
      "Epoch 66, train loss: 0.172540\n",
      "Epoch 67, train loss: 0.203654\n",
      "Epoch 68, train loss: 0.272934\n",
      "Epoch 69, train loss: 0.195020\n",
      "Epoch 70, train loss: 0.182975\n",
      "Epoch 71, train loss: 0.174340\n",
      "Epoch 72, train loss: 0.298098\n",
      "Epoch 73, train loss: 0.240195\n",
      "Epoch 74, train loss: 0.234705\n",
      "Epoch 75, train loss: 0.256947\n",
      "Epoch 76, train loss: 0.222749\n",
      "Epoch 77, train loss: 0.203723\n",
      "Epoch 78, train loss: 0.210388\n",
      "Epoch 79, train loss: 0.199307\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[174. 173. 173. ... 163. 164. 165.]\n",
      " [173. 173. 172. ... 164. 165. 166.]\n",
      " [173. 172. 170. ... 165. 166. 166.]\n",
      " ...\n",
      " [140. 140. 141. ...  92.  88.  85.]\n",
      " [140. 141. 138. ...  88.  85.  84.]\n",
      " [141. 138. 134. ...  85.  84.  86.]]\n",
      "y here is\n",
      "[[171. 171. 171. ... 171. 171. 171.]\n",
      " [171. 171. 171. ... 171. 171. 171.]\n",
      " [172. 172. 172. ... 172. 172. 172.]\n",
      " ...\n",
      " [ 90.  90.  90. ...  90.  90.  90.]\n",
      " [ 86.  86.  86. ...  86.  86.  86.]\n",
      " [ 87.  87.  87. ...  87.  87.  87.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2540, 24), Train/test: 1/2539\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[142. 142. 142. ... 125. 125. 127.]\n",
      " [142. 142. 141. ... 125. 127. 130.]\n",
      " [142. 141. 139. ... 127. 130. 134.]\n",
      " ...\n",
      " [135. 137. 140. ... 163. 166. 168.]\n",
      " [137. 140. 143. ... 166. 168. 167.]\n",
      " [140. 143. 146. ... 168. 167. 168.]]\n",
      "y here is\n",
      "[[152. 152. 152. ... 152. 152. 152.]\n",
      " [159. 159. 159. ... 159. 159. 159.]\n",
      " [163. 163. 163. ... 163. 163. 163.]\n",
      " ...\n",
      " [173. 173. 173. ... 173. 173. 173.]\n",
      " [174. 174. 174. ... 174. 174. 174.]\n",
      " [174. 174. 174. ... 174. 174. 174.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 499/9648\n",
      "Found 27 continuous time series\n",
      "Data shape: (10149, 24), Train/test: 10147/2\n",
      "Train test ratio: 5073.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291FBA0B0>\n",
      "Epoch 0, test loss: 0.171929\n",
      "Epoch 1, test loss: 0.178199\n",
      "Epoch 2, test loss: 0.172221\n",
      "Epoch 3, test loss: 0.172840\n",
      "Epoch 4, test loss: 0.172267\n",
      "Epoch 5, test loss: 0.172323\n",
      "Epoch 6, test loss: 0.174207\n",
      "Epoch 7, test loss: 0.171927\n",
      "Epoch 8, test loss: 0.173173\n",
      "Epoch 9, test loss: 0.172888\n",
      "Epoch 10, test loss: 0.176455\n",
      "Epoch 11, test loss: 0.174552\n",
      "Epoch 12, test loss: 0.172357\n",
      "Epoch 13, test loss: 0.172074\n",
      "Epoch 14, test loss: 0.173662\n",
      "Epoch 15, test loss: 0.172848\n",
      "Epoch 16, test loss: 0.174417\n",
      "Epoch 17, test loss: 0.172531\n",
      "Epoch 18, test loss: 0.172503\n",
      "Epoch 19, test loss: 0.172827\n",
      "Epoch 20, test loss: 0.172504\n",
      "Epoch 21, test loss: 0.173062\n",
      "Epoch 22, test loss: 0.172951\n",
      "Epoch 23, test loss: 0.175767\n",
      "Epoch 24, test loss: 0.173987\n",
      "Epoch 25, test loss: 0.172950\n",
      "Epoch 26, test loss: 0.172433\n",
      "Epoch 27, test loss: 0.173191\n",
      "Epoch 28, test loss: 0.175734\n",
      "Epoch 29, test loss: 0.173882\n",
      "Epoch 30, test loss: 0.172692\n",
      "Epoch 31, test loss: 0.173144\n",
      "Epoch 32, test loss: 0.174081\n",
      "Epoch 33, test loss: 0.172739\n",
      "Epoch 34, test loss: 0.177675\n",
      "Epoch 35, test loss: 0.173781\n",
      "Epoch 36, test loss: 0.175065\n",
      "Epoch 37, test loss: 0.174816\n",
      "Epoch 38, test loss: 0.173657\n",
      "Epoch 39, test loss: 0.173430\n",
      "Epoch 40, test loss: 0.175096\n",
      "Epoch 41, test loss: 0.172849\n",
      "Epoch 42, test loss: 0.174509\n",
      "Epoch 43, test loss: 0.173399\n",
      "Epoch 44, test loss: 0.174274\n",
      "Epoch 45, test loss: 0.173106\n",
      "Epoch 46, test loss: 0.172950\n",
      "Epoch 47, test loss: 0.173065\n",
      "Epoch 48, test loss: 0.173065\n",
      "Epoch 49, test loss: 0.174572\n",
      "Epoch 50, test loss: 0.173235\n",
      "Epoch 51, test loss: 0.178158\n",
      "Epoch 52, test loss: 0.173159\n",
      "Epoch 53, test loss: 0.179717\n",
      "Epoch 54, test loss: 0.173603\n",
      "Epoch 55, test loss: 0.173593\n",
      "Epoch 56, test loss: 0.182322\n",
      "Epoch 57, test loss: 0.173030\n",
      "Epoch 58, test loss: 0.172959\n",
      "Epoch 59, test loss: 0.174918\n",
      "Epoch 60, test loss: 0.173038\n",
      "Epoch 61, test loss: 0.173726\n",
      "Epoch 62, test loss: 0.173633\n",
      "Epoch 63, test loss: 0.173157\n",
      "Epoch 64, test loss: 0.174390\n",
      "Epoch 65, test loss: 0.173782\n",
      "Epoch 66, test loss: 0.173443\n",
      "Epoch 67, test loss: 0.177325\n",
      "Epoch 68, test loss: 0.174957\n",
      "Epoch 69, test loss: 0.173189\n",
      "Epoch 70, test loss: 0.173034\n",
      "Epoch 71, test loss: 0.176935\n",
      "Epoch 72, test loss: 0.173410\n",
      "Epoch 73, test loss: 0.173591\n",
      "Epoch 74, test loss: 0.173889\n",
      "Epoch 75, test loss: 0.174368\n",
      "Epoch 76, test loss: 0.174547\n",
      "Epoch 77, test loss: 0.174084\n",
      "Epoch 78, test loss: 0.174379\n",
      "Epoch 79, test loss: 0.179825\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291FBA0B0>\n",
      "Epoch 0, test loss: 0.174092\n",
      "Epoch 1, test loss: 0.172120\n",
      "Epoch 2, test loss: 0.172481\n",
      "Epoch 3, test loss: 0.172597\n",
      "Epoch 4, test loss: 0.172849\n",
      "Epoch 5, test loss: 0.171910\n",
      "Epoch 6, test loss: 0.172325\n",
      "Epoch 7, test loss: 0.172510\n",
      "Epoch 8, test loss: 0.173254\n",
      "Epoch 9, test loss: 0.173347\n",
      "Epoch 10, test loss: 0.172941\n",
      "Epoch 11, test loss: 0.172893\n",
      "Epoch 12, test loss: 0.172406\n",
      "Epoch 13, test loss: 0.172881\n",
      "Epoch 14, test loss: 0.172627\n",
      "Epoch 15, test loss: 0.172325\n",
      "Epoch 16, test loss: 0.173288\n",
      "Epoch 17, test loss: 0.172811\n",
      "Epoch 18, test loss: 0.172831\n",
      "Epoch 19, test loss: 0.172504\n",
      "Epoch 20, test loss: 0.174043\n",
      "Epoch 21, test loss: 0.173076\n",
      "Epoch 22, test loss: 0.172156\n",
      "Epoch 23, test loss: 0.172667\n",
      "Epoch 24, test loss: 0.172353\n",
      "Epoch 25, test loss: 0.174985\n",
      "Epoch 26, test loss: 0.173675\n",
      "Epoch 27, test loss: 0.172764\n",
      "Epoch 28, test loss: 0.173024\n",
      "Epoch 29, test loss: 0.172925\n",
      "Epoch 30, test loss: 0.172812\n",
      "Epoch 31, test loss: 0.176056\n",
      "Epoch 32, test loss: 0.173154\n",
      "Epoch 33, test loss: 0.172972\n",
      "Epoch 34, test loss: 0.173350\n",
      "Epoch 35, test loss: 0.173231\n",
      "Epoch 36, test loss: 0.172949\n",
      "Epoch 37, test loss: 0.173269\n",
      "Epoch 38, test loss: 0.173827\n",
      "Epoch 39, test loss: 0.172845\n",
      "Epoch 40, test loss: 0.173043\n",
      "Epoch 41, test loss: 0.173851\n",
      "Epoch 42, test loss: 0.176586\n",
      "Epoch 43, test loss: 0.174168\n",
      "Epoch 44, test loss: 0.173748\n",
      "Epoch 45, test loss: 0.174766\n",
      "Epoch 46, test loss: 0.173514\n",
      "Epoch 47, test loss: 0.174111\n",
      "Epoch 48, test loss: 0.173739\n",
      "Epoch 49, test loss: 0.172981\n",
      "Epoch 50, test loss: 0.173260\n",
      "Epoch 51, test loss: 0.172867\n",
      "Epoch 52, test loss: 0.172791\n",
      "Epoch 53, test loss: 0.174569\n",
      "Epoch 54, test loss: 0.174641\n",
      "Epoch 55, test loss: 0.173146\n",
      "Epoch 56, test loss: 0.173519\n",
      "Epoch 57, test loss: 0.174503\n",
      "Epoch 58, test loss: 0.173090\n",
      "Epoch 59, test loss: 0.175232\n",
      "Epoch 60, test loss: 0.173000\n",
      "Epoch 61, test loss: 0.173532\n",
      "Epoch 62, test loss: 0.173459\n",
      "Epoch 63, test loss: 0.173672\n",
      "Epoch 64, test loss: 0.172960\n",
      "Epoch 65, test loss: 0.173431\n",
      "Epoch 66, test loss: 0.178719\n",
      "Epoch 67, test loss: 0.173988\n",
      "Epoch 68, test loss: 0.174350\n",
      "Epoch 69, test loss: 0.173953\n",
      "Epoch 70, test loss: 0.174268\n",
      "Epoch 71, test loss: 0.172838\n",
      "Epoch 72, test loss: 0.173200\n",
      "Epoch 73, test loss: 0.173784\n",
      "Epoch 74, test loss: 0.173779\n",
      "Epoch 75, test loss: 0.173343\n",
      "Epoch 76, test loss: 0.173838\n",
      "Epoch 77, test loss: 0.173602\n",
      "Epoch 78, test loss: 0.173194\n",
      "Epoch 79, test loss: 0.175074\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh24_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001A291FBA0B0>\n",
      "Epoch 0, test loss: 0.312518\n",
      "Epoch 1, test loss: 0.191774\n",
      "Epoch 2, test loss: 0.181504\n",
      "Epoch 3, test loss: 0.179353\n",
      "Epoch 4, test loss: 0.179061\n",
      "Epoch 5, test loss: 0.177798\n",
      "Epoch 6, test loss: 0.179368\n",
      "Epoch 7, test loss: 0.177182\n",
      "Epoch 8, test loss: 0.176643\n",
      "Epoch 9, test loss: 0.176706\n",
      "Epoch 10, test loss: 0.176835\n",
      "Epoch 11, test loss: 0.176508\n",
      "Epoch 12, test loss: 0.176843\n",
      "Epoch 13, test loss: 0.176414\n",
      "Epoch 14, test loss: 0.175991\n",
      "Epoch 15, test loss: 0.176144\n",
      "Epoch 16, test loss: 0.176674\n",
      "Epoch 17, test loss: 0.175782\n",
      "Epoch 18, test loss: 0.176068\n",
      "Epoch 19, test loss: 0.176305\n",
      "Epoch 20, test loss: 0.176080\n",
      "Epoch 21, test loss: 0.175790\n",
      "Epoch 22, test loss: 0.175669\n",
      "Epoch 23, test loss: 0.176130\n",
      "Epoch 24, test loss: 0.176558\n",
      "Epoch 25, test loss: 0.178243\n",
      "Epoch 26, test loss: 0.175672\n",
      "Epoch 27, test loss: 0.175689\n",
      "Epoch 28, test loss: 0.175324\n",
      "Epoch 29, test loss: 0.176045\n",
      "Epoch 30, test loss: 0.176334\n",
      "Epoch 31, test loss: 0.177993\n",
      "Epoch 32, test loss: 0.177011\n",
      "Epoch 33, test loss: 0.181438\n",
      "Epoch 34, test loss: 0.176819\n",
      "Epoch 35, test loss: 0.177673\n",
      "Epoch 36, test loss: 0.175597\n",
      "Epoch 37, test loss: 0.175874\n",
      "Epoch 38, test loss: 0.176986\n",
      "Epoch 39, test loss: 0.175114\n",
      "Epoch 40, test loss: 0.175951\n",
      "Epoch 41, test loss: 0.177043\n",
      "Epoch 42, test loss: 0.176304\n",
      "Epoch 43, test loss: 0.183085\n",
      "Epoch 44, test loss: 0.175667\n",
      "Epoch 45, test loss: 0.176718\n",
      "Epoch 46, test loss: 0.175444\n",
      "Epoch 47, test loss: 0.175134\n",
      "Epoch 48, test loss: 0.175209\n",
      "Epoch 49, test loss: 0.176765\n",
      "Epoch 50, test loss: 0.175542\n",
      "Epoch 51, test loss: 0.178538\n",
      "Epoch 52, test loss: 0.175364\n",
      "Epoch 53, test loss: 0.175217\n",
      "Epoch 54, test loss: 0.175020\n",
      "Epoch 55, test loss: 0.175709\n",
      "Epoch 56, test loss: 0.175396\n",
      "Epoch 57, test loss: 0.178234\n",
      "Epoch 58, test loss: 0.175344\n",
      "Epoch 59, test loss: 0.175473\n",
      "Epoch 60, test loss: 0.176199\n",
      "Epoch 61, test loss: 0.177448\n",
      "Epoch 62, test loss: 0.174415\n",
      "Epoch 63, test loss: 0.175305\n",
      "Epoch 64, test loss: 0.176278\n",
      "Epoch 65, test loss: 0.175427\n",
      "Epoch 66, test loss: 0.176224\n",
      "Epoch 67, test loss: 0.175185\n",
      "Epoch 68, test loss: 0.175506\n",
      "Epoch 69, test loss: 0.175785\n",
      "Epoch 70, test loss: 0.176103\n",
      "Epoch 71, test loss: 0.174938\n",
      "Epoch 72, test loss: 0.175758\n",
      "Epoch 73, test loss: 0.177201\n",
      "Epoch 74, test loss: 0.174641\n",
      "Epoch 75, test loss: 0.174926\n",
      "Epoch 76, test loss: 0.174578\n",
      "Epoch 77, test loss: 0.174217\n",
      "Epoch 78, test loss: 0.176246\n",
      "Epoch 79, test loss: 0.178147\n",
      "Epoch 80, test loss: 0.176014\n",
      "Epoch 81, test loss: 0.176666\n",
      "Epoch 82, test loss: 0.175666\n",
      "Epoch 83, test loss: 0.176287\n",
      "Epoch 84, test loss: 0.174924\n",
      "Epoch 85, test loss: 0.176225\n",
      "Epoch 86, test loss: 0.174829\n",
      "Epoch 87, test loss: 0.175240\n",
      "Epoch 88, test loss: 0.178870\n",
      "Epoch 89, test loss: 0.174638\n",
      "Epoch 90, test loss: 0.175418\n",
      "Epoch 91, test loss: 0.174854\n",
      "Epoch 92, test loss: 0.175466\n",
      "Epoch 93, test loss: 0.175125\n",
      "Epoch 94, test loss: 0.174815\n",
      "Epoch 95, test loss: 0.174231\n",
      "Epoch 96, test loss: 0.174311\n",
      "Epoch 97, test loss: 0.176211\n",
      "Epoch 98, test loss: 0.175549\n",
      "Epoch 99, test loss: 0.174171\n",
      "Epoch 100, test loss: 0.176755\n",
      "Epoch 101, test loss: 0.175551\n",
      "Epoch 102, test loss: 0.174545\n",
      "Epoch 103, test loss: 0.175831\n",
      "Epoch 104, test loss: 0.174532\n",
      "Epoch 105, test loss: 0.174437\n",
      "Epoch 106, test loss: 0.174400\n",
      "Epoch 107, test loss: 0.175141\n",
      "Epoch 108, test loss: 0.175948\n",
      "Epoch 109, test loss: 0.176567\n",
      "Epoch 110, test loss: 0.175738\n",
      "Epoch 111, test loss: 0.175882\n",
      "Epoch 112, test loss: 0.175745\n",
      "Epoch 113, test loss: 0.175372\n",
      "Epoch 114, test loss: 0.174520\n",
      "Epoch 115, test loss: 0.174802\n",
      "Epoch 116, test loss: 0.175281\n",
      "Epoch 117, test loss: 0.174959\n",
      "Epoch 118, test loss: 0.174293\n",
      "Epoch 119, test loss: 0.176171\n",
      "Epoch 120, test loss: 0.176174\n",
      "Epoch 121, test loss: 0.174318\n",
      "Epoch 122, test loss: 0.175058\n",
      "Epoch 123, test loss: 0.174492\n",
      "Epoch 124, test loss: 0.175708\n",
      "Epoch 125, test loss: 0.175440\n",
      "Epoch 126, test loss: 0.177012\n",
      "Epoch 127, test loss: 0.176130\n",
      "Epoch 128, test loss: 0.174998\n",
      "Epoch 129, test loss: 0.174601\n",
      "Epoch 130, test loss: 0.176235\n",
      "Epoch 131, test loss: 0.176482\n",
      "Epoch 132, test loss: 0.174352\n",
      "Epoch 133, test loss: 0.176083\n",
      "Epoch 134, test loss: 0.176840\n",
      "Epoch 135, test loss: 0.179857\n",
      "Epoch 136, test loss: 0.174770\n",
      "Epoch 137, test loss: 0.176070\n",
      "Epoch 138, test loss: 0.174409\n",
      "Epoch 139, test loss: 0.177108\n",
      "Epoch 140, test loss: 0.174895\n",
      "Epoch 141, test loss: 0.177837\n",
      "Epoch 142, test loss: 0.175216\n",
      "Epoch 143, test loss: 0.175953\n",
      "Epoch 144, test loss: 0.175564\n",
      "Epoch 145, test loss: 0.179454\n",
      "Epoch 146, test loss: 0.176142\n",
      "Epoch 147, test loss: 0.175772\n",
      "Epoch 148, test loss: 0.174691\n",
      "Epoch 149, test loss: 0.175958\n",
      "Epoch 150, test loss: 0.175650\n",
      "Epoch 151, test loss: 0.186744\n",
      "Epoch 152, test loss: 0.174371\n",
      "Epoch 153, test loss: 0.176195\n",
      "Epoch 154, test loss: 0.177084\n",
      "Epoch 155, test loss: 0.183239\n",
      "Epoch 156, test loss: 0.174934\n",
      "Epoch 157, test loss: 0.175691\n",
      "Epoch 158, test loss: 0.174288\n",
      "Epoch 159, test loss: 0.176098\n",
      "Epoch 160, test loss: 0.174499\n",
      "Epoch 161, test loss: 0.180048\n",
      "Epoch 162, test loss: 0.176129\n",
      "Epoch 163, test loss: 0.174323\n",
      "Epoch 164, test loss: 0.177049\n",
      "Epoch 165, test loss: 0.175219\n",
      "Epoch 166, test loss: 0.175245\n",
      "Epoch 167, test loss: 0.175996\n",
      "Epoch 168, test loss: 0.174671\n",
      "Epoch 169, test loss: 0.174549\n",
      "Epoch 170, test loss: 0.179952\n",
      "Epoch 171, test loss: 0.174574\n",
      "Epoch 172, test loss: 0.175886\n",
      "Epoch 173, test loss: 0.175367\n",
      "Epoch 174, test loss: 0.175597\n",
      "Epoch 175, test loss: 0.175427\n",
      "Epoch 176, test loss: 0.180300\n",
      "Epoch 177, test loss: 0.173824\n",
      "Epoch 178, test loss: 0.176669\n",
      "Epoch 179, test loss: 0.174774\n",
      "Epoch 180, test loss: 0.175242\n",
      "Epoch 181, test loss: 0.175139\n",
      "Epoch 182, test loss: 0.176615\n",
      "Epoch 183, test loss: 0.177394\n",
      "Epoch 184, test loss: 0.175514\n",
      "Epoch 185, test loss: 0.174663\n",
      "Epoch 186, test loss: 0.179187\n",
      "Epoch 187, test loss: 0.177578\n",
      "Epoch 188, test loss: 0.175360\n",
      "Epoch 189, test loss: 0.176034\n",
      "Epoch 190, test loss: 0.174799\n",
      "Epoch 191, test loss: 0.175353\n",
      "Epoch 192, test loss: 0.175674\n",
      "Epoch 193, test loss: 0.174744\n",
      "Epoch 194, test loss: 0.175597\n",
      "Epoch 195, test loss: 0.175075\n",
      "Epoch 196, test loss: 0.175268\n",
      "Epoch 197, test loss: 0.176762\n",
      "Epoch 198, test loss: 0.175016\n",
      "Epoch 199, test loss: 0.174873\n",
      "Epoch 200, test loss: 0.175771\n",
      "Epoch 201, test loss: 0.174479\n",
      "Epoch 202, test loss: 0.175014\n",
      "Epoch 203, test loss: 0.176781\n",
      "Epoch 204, test loss: 0.174748\n",
      "Epoch 205, test loss: 0.175519\n",
      "Epoch 206, test loss: 0.174765\n",
      "Epoch 207, test loss: 0.175303\n",
      "Epoch 208, test loss: 0.174816\n",
      "Epoch 209, test loss: 0.176415\n",
      "Epoch 210, test loss: 0.175256\n",
      "Epoch 211, test loss: 0.174554\n",
      "Epoch 212, test loss: 0.176403\n",
      "Epoch 213, test loss: 0.176023\n",
      "Epoch 214, test loss: 0.174819\n",
      "Epoch 215, test loss: 0.175782\n",
      "Epoch 216, test loss: 0.175255\n",
      "Epoch 217, test loss: 0.175242\n",
      "Epoch 218, test loss: 0.175189\n",
      "Epoch 219, test loss: 0.175617\n",
      "Epoch 220, test loss: 0.174944\n",
      "Epoch 221, test loss: 0.176251\n",
      "Epoch 222, test loss: 0.175404\n",
      "Epoch 223, test loss: 0.175622\n",
      "Epoch 224, test loss: 0.175900\n",
      "Epoch 225, test loss: 0.175824\n",
      "Epoch 226, test loss: 0.175847\n",
      "Epoch 227, test loss: 0.175060\n",
      "Epoch 228, test loss: 0.175539\n",
      "Epoch 229, test loss: 0.175114\n",
      "Epoch 230, test loss: 0.175211\n",
      "Epoch 231, test loss: 0.174603\n",
      "Epoch 232, test loss: 0.175562\n",
      "Epoch 233, test loss: 0.176520\n",
      "Epoch 234, test loss: 0.183452\n",
      "Epoch 235, test loss: 0.175178\n",
      "Epoch 236, test loss: 0.176236\n",
      "Epoch 237, test loss: 0.175682\n",
      "Epoch 238, test loss: 0.177967\n",
      "Epoch 239, test loss: 0.175681\n"
     ]
    }
   ],
   "source": [
    "# sh = 6\n",
    "for sh in [6, 12, 18, 24]:\n",
    "    pid_2018 = [559, 563, 570, 588, 575, 591]\n",
    "    # pid_year = {2018: pid_2018}\n",
    "    pid_2020 = [540, 552, 544, 567, 584, 596]\n",
    "    pid_year = {2018: pid_2018, 2020: pid_2020}\n",
    "\n",
    "    train_data = dict()\n",
    "    for year in list(pid_year.keys()):\n",
    "        pids = pid_year[year]\n",
    "        for pid in pids:\n",
    "            reader = DataReader(\n",
    "                \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/train/{pid}-ws-training.xml\", 5\n",
    "            )\n",
    "            train_data[pid] = reader.read()\n",
    "    # add test data of 2018 patient\n",
    "    use_2018_test = False\n",
    "    standard = False  # do not use standard\n",
    "    test_data_2018 = []\n",
    "    for pid in pid_2018:\n",
    "        reader = DataReader(\n",
    "            \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/2018/test/{pid}-ws-testing.xml\", 5\n",
    "        )\n",
    "        test_data_2018 += reader.read()\n",
    "\n",
    "    # a dumb dataset instance\n",
    "    train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/2018/train/559-ws-training.xml\", 5\n",
    "    )\n",
    "    sampling_horizon = sh\n",
    "    prediction_horizon = ph\n",
    "    scale = 0.01\n",
    "    outtype = \"Same\"\n",
    "    # train on training dataset\n",
    "    # k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "    with open(os.path.join(path, \"config.json\")) as json_file:\n",
    "        config = json.load(json_file)\n",
    "    argv = (\n",
    "        config[\"k_size\"],\n",
    "        config[\"nblock\"],\n",
    "        config[\"nn_size\"],\n",
    "        config[\"nn_layer\"],\n",
    "        config[\"learning_rate\"],\n",
    "        config[\"batch_size\"],\n",
    "        epoch,\n",
    "        config[\"beta\"],\n",
    "    )\n",
    "    l_type = config[\"loss\"]\n",
    "    # test on patients data\n",
    "    outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_{l_type}\")\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    all_errs = []\n",
    "    for year in list(pid_year.keys()):\n",
    "        pids = pid_year[year]\n",
    "        for pid in pids:\n",
    "            # only check results of 2020 patients\n",
    "            # if pid not in pid_2020:\n",
    "            #     continue\n",
    "            # 100 is dumb if set_cutpoint is used\n",
    "            train_pids = set(pid_2018 + pid_2020) - set([pid])\n",
    "            local_train_data = []\n",
    "            if use_2018_test:\n",
    "                local_train_data += test_data_2018\n",
    "            for k in train_pids:\n",
    "                local_train_data += train_data[k]\n",
    "            print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "            train_dataset.data = local_train_data\n",
    "            train_dataset.set_cutpoint = -1\n",
    "            train_dataset.reset(\n",
    "                sampling_horizon,\n",
    "                prediction_horizon,\n",
    "                scale,\n",
    "                100,\n",
    "                False,\n",
    "                outtype,\n",
    "                1,\n",
    "                standard,\n",
    "            )\n",
    "            \n",
    "            regressor(train_dataset, *argv, l_type, outdir)\n",
    "            # fine-tune on personal data\n",
    "            target_test_dataset = CGMSDataSeg(\n",
    "                \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/test/{pid}-ws-testing.xml\", 5\n",
    "                \n",
    "            )\n",
    "            target_test_dataset.set_cutpoint = 1\n",
    "            target_test_dataset.reset(\n",
    "                sampling_horizon,\n",
    "                prediction_horizon,\n",
    "                scale,\n",
    "                0.01,\n",
    "                False,\n",
    "                outtype,\n",
    "                1,\n",
    "                standard,\n",
    "            )\n",
    "            target_train_dataset = CGMSDataSeg(\n",
    "                \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/train/{pid}-ws-training.xml\", 5\n",
    "            )\n",
    "\n",
    "            target_train_dataset.set_cutpoint = -1\n",
    "            target_train_dataset.reset(\n",
    "                sampling_horizon,\n",
    "                prediction_horizon,\n",
    "                scale,\n",
    "                100,\n",
    "                False,\n",
    "                outtype,\n",
    "                1,\n",
    "                standard,\n",
    "            )\n",
    "            err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "            errs = [err]\n",
    "            transfer_res = [labels]\n",
    "            for i in range(1, 4):\n",
    "                err, labels = regressor_transfer(\n",
    "                    target_train_dataset,\n",
    "                    target_test_dataset,\n",
    "                    config[\"batch_size\"],\n",
    "                    epoch,\n",
    "                    outdir,\n",
    "                    i,\n",
    "                )\n",
    "                errs.append(err)\n",
    "                transfer_res.append(labels)\n",
    "            transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "            np.savetxt(\n",
    "                f\"{outdir}/{pid}.txt\",\n",
    "                transfer_res,\n",
    "                fmt=\"%.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f\",\n",
    "            )\n",
    "            all_errs.append([pid] + errs)\n",
    "    all_errs = np.array(all_errs)\n",
    "    np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f %.4f %.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 44 segments\n"
     ]
    }
   ],
   "source": [
    "# Before the loop\n",
    "# ATTENTION: verify the \\ or / in different system window or unix\n",
    "# read in all patients data\n",
    "pid_2018 = [559, 563, 570, 588, 575, 591]\n",
    "# pid_year = {2018: pid_2018}\n",
    "pid_2020 = [540, 552, 544, 567, 584, 596]\n",
    "pid_year = {2018: pid_2018, 2020: pid_2020}\n",
    "\n",
    "train_data = dict()\n",
    "for year in list(pid_year.keys()):\n",
    "    pids = pid_year[year]\n",
    "    for pid in pids:\n",
    "        reader = DataReader(\n",
    "            \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/train/{pid}-ws-training.xml\", 5\n",
    "        )\n",
    "        train_data[pid] = reader.read()\n",
    "# add test data of 2018 patient\n",
    "use_2018_test = False\n",
    "standard = False  # do not use standard\n",
    "test_data_2018 = []\n",
    "for pid in pid_2018:\n",
    "    reader = DataReader(\n",
    "        \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    test_data_2018 += reader.read()\n",
    "\n",
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/2018/train/559-ws-training.xml\", 5\n",
    ")\n",
    "sampling_horizon = sh\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(os.path.join(path, \"config.json\")) as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_sh{sampling_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain data: 19625082.0\n",
      "Building dataset, requesting data from 0 to 798\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7492/107922\n",
      "Found 798 continuous time series\n",
      "Data shape: (115416, 6), Train/test: 115414/2\n",
      "Train test ratio: 57707.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1803\n",
      "Epoch 0, train loss: 0.220934\n",
      "Epoch 1, train loss: 0.219956\n",
      "Epoch 2, train loss: 0.219259\n",
      "Epoch 3, train loss: 0.253998\n",
      "Epoch 4, train loss: 0.209874\n",
      "Epoch 5, train loss: 0.238242\n",
      "Epoch 6, train loss: 0.156728\n",
      "Epoch 7, train loss: 0.221577\n",
      "Epoch 8, train loss: 0.183253\n",
      "Epoch 9, train loss: 0.199774\n",
      "Epoch 10, train loss: 0.250708\n",
      "Epoch 11, train loss: 0.161821\n",
      "Epoch 12, train loss: 0.199747\n",
      "Epoch 13, train loss: 0.220074\n",
      "Epoch 14, train loss: 0.213216\n",
      "Epoch 15, train loss: 0.172695\n",
      "Epoch 16, train loss: 0.243029\n",
      "Epoch 17, train loss: 0.181885\n",
      "Epoch 18, train loss: 0.204435\n",
      "Epoch 19, train loss: 0.236377\n",
      "Epoch 20, train loss: 0.252711\n",
      "Epoch 21, train loss: 0.269029\n",
      "Epoch 22, train loss: 0.185182\n",
      "Epoch 23, train loss: 0.172205\n",
      "Epoch 24, train loss: 0.197173\n",
      "Epoch 25, train loss: 0.140589\n",
      "Epoch 26, train loss: 0.176655\n",
      "Epoch 27, train loss: 0.227117\n",
      "Epoch 28, train loss: 0.224522\n",
      "Epoch 29, train loss: 0.181894\n",
      "Epoch 30, train loss: 0.226829\n",
      "Epoch 31, train loss: 0.183960\n",
      "Epoch 32, train loss: 0.150315\n",
      "Epoch 33, train loss: 0.215099\n",
      "Epoch 34, train loss: 0.170412\n",
      "Epoch 35, train loss: 0.176841\n",
      "Epoch 36, train loss: 0.214462\n",
      "Epoch 37, train loss: 0.191614\n",
      "Epoch 38, train loss: 0.235643\n",
      "Epoch 39, train loss: 0.200707\n",
      "Epoch 40, train loss: 0.192637\n",
      "Epoch 41, train loss: 0.247859\n",
      "Epoch 42, train loss: 0.183497\n",
      "Epoch 43, train loss: 0.227553\n",
      "Epoch 44, train loss: 0.182002\n",
      "Epoch 45, train loss: 0.189038\n",
      "Epoch 46, train loss: 0.206257\n",
      "Epoch 47, train loss: 0.178814\n",
      "Epoch 48, train loss: 0.148425\n",
      "Epoch 49, train loss: 0.154329\n",
      "Epoch 50, train loss: 0.237139\n",
      "Epoch 51, train loss: 0.133635\n",
      "Epoch 52, train loss: 0.272967\n",
      "Epoch 53, train loss: 0.216939\n",
      "Epoch 54, train loss: 0.196670\n",
      "Epoch 55, train loss: 0.160733\n",
      "Epoch 56, train loss: 0.190961\n",
      "Epoch 57, train loss: 0.163831\n",
      "Epoch 58, train loss: 0.156018\n",
      "Epoch 59, train loss: 0.224200\n",
      "Epoch 60, train loss: 0.174336\n",
      "Epoch 61, train loss: 0.166942\n",
      "Epoch 62, train loss: 0.182868\n",
      "Epoch 63, train loss: 0.231617\n",
      "Epoch 64, train loss: 0.234858\n",
      "Epoch 65, train loss: 0.155411\n",
      "Epoch 66, train loss: 0.251433\n",
      "Epoch 67, train loss: 0.206356\n",
      "Epoch 68, train loss: 0.246632\n",
      "Epoch 69, train loss: 0.219166\n",
      "Epoch 70, train loss: 0.235105\n",
      "Epoch 71, train loss: 0.218756\n",
      "Epoch 72, train loss: 0.200828\n",
      "Epoch 73, train loss: 0.207087\n",
      "Epoch 74, train loss: 0.195279\n",
      "Epoch 75, train loss: 0.182063\n",
      "Epoch 76, train loss: 0.219377\n",
      "Epoch 77, train loss: 0.211115\n",
      "Epoch 78, train loss: 0.206908\n",
      "Epoch 79, train loss: 0.249011\n",
      "Epoch 80, train loss: 0.245488\n",
      "Epoch 81, train loss: 0.230051\n",
      "Epoch 82, train loss: 0.207751\n",
      "Epoch 83, train loss: 0.202502\n",
      "Epoch 84, train loss: 0.187337\n",
      "Epoch 85, train loss: 0.172174\n",
      "Epoch 86, train loss: 0.161854\n",
      "Epoch 87, train loss: 0.188569\n",
      "Epoch 88, train loss: 0.206563\n",
      "Epoch 89, train loss: 0.159455\n",
      "Epoch 90, train loss: 0.220830\n",
      "Epoch 91, train loss: 0.159336\n",
      "Epoch 92, train loss: 0.193687\n",
      "Epoch 93, train loss: 0.153158\n",
      "Epoch 94, train loss: 0.186766\n",
      "Epoch 95, train loss: 0.171244\n",
      "Epoch 96, train loss: 0.163265\n",
      "Epoch 97, train loss: 0.188460\n",
      "Epoch 98, train loss: 0.221092\n",
      "Epoch 99, train loss: 0.217128\n",
      "Epoch 100, train loss: 0.203133\n",
      "Epoch 101, train loss: 0.158128\n",
      "Epoch 102, train loss: 0.239400\n",
      "Epoch 103, train loss: 0.213955\n",
      "Epoch 104, train loss: 0.286451\n",
      "Epoch 105, train loss: 0.219828\n",
      "Epoch 106, train loss: 0.164928\n",
      "Epoch 107, train loss: 0.185486\n",
      "Epoch 108, train loss: 0.175998\n",
      "Epoch 109, train loss: 0.154579\n",
      "Epoch 110, train loss: 0.234288\n",
      "Epoch 111, train loss: 0.199934\n",
      "Epoch 112, train loss: 0.193485\n",
      "Epoch 113, train loss: 0.247161\n",
      "Epoch 114, train loss: 0.230993\n",
      "Epoch 115, train loss: 0.211659\n",
      "Epoch 116, train loss: 0.163607\n",
      "Epoch 117, train loss: 0.265563\n",
      "Epoch 118, train loss: 0.238252\n",
      "Epoch 119, train loss: 0.193103\n",
      "Epoch 120, train loss: 0.232305\n",
      "Epoch 121, train loss: 0.233368\n",
      "Epoch 122, train loss: 0.168789\n",
      "Epoch 123, train loss: 0.261133\n",
      "Epoch 124, train loss: 0.189024\n",
      "Epoch 125, train loss: 0.193610\n",
      "Epoch 126, train loss: 0.334729\n",
      "Epoch 127, train loss: 0.186130\n",
      "Epoch 128, train loss: 0.178754\n",
      "Epoch 129, train loss: 0.171751\n",
      "Epoch 130, train loss: 0.172744\n",
      "Epoch 131, train loss: 0.208351\n",
      "Epoch 132, train loss: 0.233525\n",
      "Epoch 133, train loss: 0.230178\n",
      "Epoch 134, train loss: 0.202673\n",
      "Epoch 135, train loss: 0.265477\n",
      "Epoch 136, train loss: 0.203206\n",
      "Epoch 137, train loss: 0.210295\n",
      "Epoch 138, train loss: 0.179564\n",
      "Epoch 139, train loss: 0.191522\n",
      "Epoch 140, train loss: 0.180435\n",
      "Epoch 141, train loss: 0.243052\n",
      "Epoch 142, train loss: 0.220901\n",
      "Epoch 143, train loss: 0.299641\n",
      "Epoch 144, train loss: 0.189412\n",
      "Epoch 145, train loss: 0.194579\n",
      "Epoch 146, train loss: 0.215704\n",
      "Epoch 147, train loss: 0.186831\n",
      "Epoch 148, train loss: 0.221225\n",
      "Epoch 149, train loss: 0.223561\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "x here is\n",
      "[[179. 183. 187. 191. 195. 199.]\n",
      " [183. 187. 191. 195. 199. 204.]\n",
      " [187. 191. 195. 199. 204. 209.]\n",
      " ...\n",
      " [188. 187. 186. 186. 186. 187.]\n",
      " [187. 186. 186. 186. 187. 188.]\n",
      " [186. 186. 186. 187. 188. 187.]]\n",
      "y here is\n",
      "[[215. 215. 215. 215. 215. 215.]\n",
      " [225. 225. 225. 225. 225. 225.]\n",
      " [233. 233. 233. 233. 233. 233.]\n",
      " ...\n",
      " [182. 182. 182. 182. 182. 182.]\n",
      " [180. 180. 180. 180. 180. 180.]\n",
      " [177. 177. 177. 177. 177. 177.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (2382, 6), Train/test: 1/2381\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 44 segments\n",
      "Building dataset, requesting data from 0 to 44\n",
      "x here is\n",
      "[[101.  98. 104. 112. 120. 127.]\n",
      " [ 98. 104. 112. 120. 127. 135.]\n",
      " [104. 112. 120. 127. 135. 142.]\n",
      " ...\n",
      " [148. 148. 148. 149. 150. 152.]\n",
      " [148. 148. 149. 150. 152. 155.]\n",
      " [148. 149. 150. 152. 155. 156.]]\n",
      "y here is\n",
      "[[151. 151. 151. 151. 151. 151.]\n",
      " [150. 150. 150. 150. 150. 150.]\n",
      " [124. 124. 124. 124. 124. 124.]\n",
      " ...\n",
      " [168. 168. 168. 168. 168. 168.]\n",
      " [172. 172. 172. 172. 172. 172.]\n",
      " [176. 176. 176. 176. 176. 176.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 793/9517\n",
      "Found 44 continuous time series\n",
      "Data shape: (10312, 6), Train/test: 10310/2\n",
      "Train test ratio: 5155.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F63354D6C0>\n",
      "Epoch 0, test loss: 0.186622\n",
      "Epoch 1, test loss: 0.186739\n",
      "Epoch 2, test loss: 0.186168\n",
      "Epoch 3, test loss: 0.186630\n",
      "Epoch 4, test loss: 0.187699\n",
      "Epoch 5, test loss: 0.186435\n",
      "Epoch 6, test loss: 0.192153\n",
      "Epoch 7, test loss: 0.190193\n",
      "Epoch 8, test loss: 0.187156\n",
      "Epoch 9, test loss: 0.187147\n",
      "Epoch 10, test loss: 0.188760\n",
      "Epoch 11, test loss: 0.187629\n",
      "Epoch 12, test loss: 0.192621\n",
      "Epoch 13, test loss: 0.192823\n",
      "Epoch 14, test loss: 0.188150\n",
      "Epoch 15, test loss: 0.190662\n",
      "Epoch 16, test loss: 0.189886\n",
      "Epoch 17, test loss: 0.187033\n",
      "Epoch 18, test loss: 0.194864\n",
      "Epoch 19, test loss: 0.187905\n",
      "Epoch 20, test loss: 0.188215\n",
      "Epoch 21, test loss: 0.188887\n",
      "Epoch 22, test loss: 0.187193\n",
      "Epoch 23, test loss: 0.196785\n",
      "Epoch 24, test loss: 0.206735\n",
      "Epoch 25, test loss: 0.187731\n",
      "Epoch 26, test loss: 0.187609\n",
      "Epoch 27, test loss: 0.189531\n",
      "Epoch 28, test loss: 0.187335\n",
      "Epoch 29, test loss: 0.190865\n",
      "Epoch 30, test loss: 0.186975\n",
      "Epoch 31, test loss: 0.186813\n",
      "Epoch 32, test loss: 0.187875\n",
      "Epoch 33, test loss: 0.187366\n",
      "Epoch 34, test loss: 0.187307\n",
      "Epoch 35, test loss: 0.188627\n",
      "Epoch 36, test loss: 0.187099\n",
      "Epoch 37, test loss: 0.191464\n",
      "Epoch 38, test loss: 0.191945\n",
      "Epoch 39, test loss: 0.187364\n",
      "Epoch 40, test loss: 0.187912\n",
      "Epoch 41, test loss: 0.188943\n",
      "Epoch 42, test loss: 0.190340\n",
      "Epoch 43, test loss: 0.192315\n",
      "Epoch 44, test loss: 0.189913\n",
      "Epoch 45, test loss: 0.187551\n",
      "Epoch 46, test loss: 0.187963\n",
      "Epoch 47, test loss: 0.187030\n",
      "Epoch 48, test loss: 0.188683\n",
      "Epoch 49, test loss: 0.187272\n",
      "Epoch 50, test loss: 0.187344\n",
      "Epoch 51, test loss: 0.190866\n",
      "Epoch 52, test loss: 0.188638\n",
      "Epoch 53, test loss: 0.187181\n",
      "Epoch 54, test loss: 0.186965\n",
      "Epoch 55, test loss: 0.201893\n",
      "Epoch 56, test loss: 0.191297\n",
      "Epoch 57, test loss: 0.191147\n",
      "Epoch 58, test loss: 0.188029\n",
      "Epoch 59, test loss: 0.189149\n",
      "Epoch 60, test loss: 0.186954\n",
      "Epoch 61, test loss: 0.187380\n",
      "Epoch 62, test loss: 0.187831\n",
      "Epoch 63, test loss: 0.187624\n",
      "Epoch 64, test loss: 0.188319\n",
      "Epoch 65, test loss: 0.186783\n",
      "Epoch 66, test loss: 0.189822\n",
      "Epoch 67, test loss: 0.189528\n",
      "Epoch 68, test loss: 0.187677\n",
      "Epoch 69, test loss: 0.187725\n",
      "Epoch 70, test loss: 0.203893\n",
      "Epoch 71, test loss: 0.187334\n",
      "Epoch 72, test loss: 0.188613\n",
      "Epoch 73, test loss: 0.196459\n",
      "Epoch 74, test loss: 0.186998\n",
      "Epoch 75, test loss: 0.187845\n",
      "Epoch 76, test loss: 0.189769\n",
      "Epoch 77, test loss: 0.186632\n",
      "Epoch 78, test loss: 0.197088\n",
      "Epoch 79, test loss: 0.187179\n",
      "Epoch 80, test loss: 0.188245\n",
      "Epoch 81, test loss: 0.186601\n",
      "Epoch 82, test loss: 0.192196\n",
      "Epoch 83, test loss: 0.188722\n",
      "Epoch 84, test loss: 0.188745\n",
      "Epoch 85, test loss: 0.188066\n",
      "Epoch 86, test loss: 0.188365\n",
      "Epoch 87, test loss: 0.188586\n",
      "Epoch 88, test loss: 0.186549\n",
      "Epoch 89, test loss: 0.188717\n",
      "Epoch 90, test loss: 0.187208\n",
      "Epoch 91, test loss: 0.190061\n",
      "Epoch 92, test loss: 0.193617\n",
      "Epoch 93, test loss: 0.187906\n",
      "Epoch 94, test loss: 0.190239\n",
      "Epoch 95, test loss: 0.186850\n",
      "Epoch 96, test loss: 0.189668\n",
      "Epoch 97, test loss: 0.186611\n",
      "Epoch 98, test loss: 0.189277\n",
      "Epoch 99, test loss: 0.187716\n",
      "Epoch 100, test loss: 0.189023\n",
      "Epoch 101, test loss: 0.187783\n",
      "Epoch 102, test loss: 0.192160\n",
      "Epoch 103, test loss: 0.189923\n",
      "Epoch 104, test loss: 0.189185\n",
      "Epoch 105, test loss: 0.188182\n",
      "Epoch 106, test loss: 0.187079\n",
      "Epoch 107, test loss: 0.189020\n",
      "Epoch 108, test loss: 0.186932\n",
      "Epoch 109, test loss: 0.188411\n",
      "Epoch 110, test loss: 0.187162\n",
      "Epoch 111, test loss: 0.188968\n",
      "Epoch 112, test loss: 0.187634\n",
      "Epoch 113, test loss: 0.191757\n",
      "Epoch 114, test loss: 0.188080\n",
      "Epoch 115, test loss: 0.200074\n",
      "Epoch 116, test loss: 0.190757\n",
      "Epoch 117, test loss: 0.197583\n",
      "Epoch 118, test loss: 0.191287\n",
      "Epoch 119, test loss: 0.186682\n",
      "Epoch 120, test loss: 0.188551\n",
      "Epoch 121, test loss: 0.191110\n",
      "Epoch 122, test loss: 0.187418\n",
      "Epoch 123, test loss: 0.193709\n",
      "Epoch 124, test loss: 0.187987\n",
      "Epoch 125, test loss: 0.190611\n",
      "Epoch 126, test loss: 0.190840\n",
      "Epoch 127, test loss: 0.189473\n",
      "Epoch 128, test loss: 0.190696\n",
      "Epoch 129, test loss: 0.195463\n",
      "Epoch 130, test loss: 0.188962\n",
      "Epoch 131, test loss: 0.187060\n",
      "Epoch 132, test loss: 0.196448\n",
      "Epoch 133, test loss: 0.189710\n",
      "Epoch 134, test loss: 0.190142\n",
      "Epoch 135, test loss: 0.189650\n",
      "Epoch 136, test loss: 0.187134\n",
      "Epoch 137, test loss: 0.194379\n",
      "Epoch 138, test loss: 0.195756\n",
      "Epoch 139, test loss: 0.188006\n",
      "Epoch 140, test loss: 0.187079\n",
      "Epoch 141, test loss: 0.187661\n",
      "Epoch 142, test loss: 0.190014\n",
      "Epoch 143, test loss: 0.191763\n",
      "Epoch 144, test loss: 0.191300\n",
      "Epoch 145, test loss: 0.187784\n",
      "Epoch 146, test loss: 0.188412\n",
      "Epoch 147, test loss: 0.187403\n",
      "Epoch 148, test loss: 0.187407\n",
      "Epoch 149, test loss: 0.187435\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F63354D6C0>\n",
      "Epoch 0, test loss: 0.191315\n",
      "Epoch 1, test loss: 0.187749\n",
      "Epoch 2, test loss: 0.186751\n",
      "Epoch 3, test loss: 0.189495\n",
      "Epoch 4, test loss: 0.186542\n",
      "Epoch 5, test loss: 0.187205\n",
      "Epoch 6, test loss: 0.196835\n",
      "Epoch 7, test loss: 0.186598\n",
      "Epoch 8, test loss: 0.187082\n",
      "Epoch 9, test loss: 0.189881\n",
      "Epoch 10, test loss: 0.192120\n",
      "Epoch 11, test loss: 0.187283\n",
      "Epoch 12, test loss: 0.186560\n",
      "Epoch 13, test loss: 0.186418\n",
      "Epoch 14, test loss: 0.192077\n",
      "Epoch 15, test loss: 0.190646\n",
      "Epoch 16, test loss: 0.191398\n",
      "Epoch 17, test loss: 0.189754\n",
      "Epoch 18, test loss: 0.186917\n",
      "Epoch 19, test loss: 0.187593\n",
      "Epoch 20, test loss: 0.187826\n",
      "Epoch 21, test loss: 0.191477\n",
      "Epoch 22, test loss: 0.189758\n",
      "Epoch 23, test loss: 0.188581\n",
      "Epoch 24, test loss: 0.190928\n",
      "Epoch 25, test loss: 0.188904\n",
      "Epoch 26, test loss: 0.187681\n",
      "Epoch 27, test loss: 0.187112\n",
      "Epoch 28, test loss: 0.186807\n",
      "Epoch 29, test loss: 0.186691\n",
      "Epoch 30, test loss: 0.188565\n",
      "Epoch 31, test loss: 0.188484\n",
      "Epoch 32, test loss: 0.190793\n",
      "Epoch 33, test loss: 0.187325\n",
      "Epoch 34, test loss: 0.187934\n",
      "Epoch 35, test loss: 0.189521\n",
      "Epoch 36, test loss: 0.193218\n",
      "Epoch 37, test loss: 0.189572\n",
      "Epoch 38, test loss: 0.187874\n",
      "Epoch 39, test loss: 0.189341\n",
      "Epoch 40, test loss: 0.188292\n",
      "Epoch 41, test loss: 0.198356\n",
      "Epoch 42, test loss: 0.188173\n",
      "Epoch 43, test loss: 0.188769\n",
      "Epoch 44, test loss: 0.187017\n",
      "Epoch 45, test loss: 0.186944\n",
      "Epoch 46, test loss: 0.189128\n",
      "Epoch 47, test loss: 0.189668\n",
      "Epoch 48, test loss: 0.188785\n",
      "Epoch 49, test loss: 0.186782\n",
      "Epoch 50, test loss: 0.187241\n",
      "Epoch 51, test loss: 0.187577\n",
      "Epoch 52, test loss: 0.188886\n",
      "Epoch 53, test loss: 0.190744\n",
      "Epoch 54, test loss: 0.186702\n",
      "Epoch 55, test loss: 0.188272\n",
      "Epoch 56, test loss: 0.187179\n",
      "Epoch 57, test loss: 0.187103\n",
      "Epoch 58, test loss: 0.188200\n",
      "Epoch 59, test loss: 0.188233\n",
      "Epoch 60, test loss: 0.188474\n",
      "Epoch 61, test loss: 0.191443\n",
      "Epoch 62, test loss: 0.191823\n",
      "Epoch 63, test loss: 0.188311\n",
      "Epoch 64, test loss: 0.191450\n",
      "Epoch 65, test loss: 0.191391\n",
      "Epoch 66, test loss: 0.190595\n",
      "Epoch 67, test loss: 0.187365\n",
      "Epoch 68, test loss: 0.186720\n",
      "Epoch 69, test loss: 0.187453\n",
      "Epoch 70, test loss: 0.187219\n",
      "Epoch 71, test loss: 0.190135\n",
      "Epoch 72, test loss: 0.189696\n",
      "Epoch 73, test loss: 0.186786\n",
      "Epoch 74, test loss: 0.190588\n",
      "Epoch 75, test loss: 0.188230\n",
      "Epoch 76, test loss: 0.187762\n",
      "Epoch 77, test loss: 0.188507\n",
      "Epoch 78, test loss: 0.188258\n",
      "Epoch 79, test loss: 0.190948\n",
      "Epoch 80, test loss: 0.186700\n",
      "Epoch 81, test loss: 0.187184\n",
      "Epoch 82, test loss: 0.187432\n",
      "Epoch 83, test loss: 0.188225\n",
      "Epoch 84, test loss: 0.187346\n",
      "Epoch 85, test loss: 0.189249\n",
      "Epoch 86, test loss: 0.188127\n",
      "Epoch 87, test loss: 0.188021\n",
      "Epoch 88, test loss: 0.187433\n",
      "Epoch 89, test loss: 0.188380\n",
      "Epoch 90, test loss: 0.187209\n",
      "Epoch 91, test loss: 0.188791\n",
      "Epoch 92, test loss: 0.186928\n",
      "Epoch 93, test loss: 0.196015\n",
      "Epoch 94, test loss: 0.187226\n",
      "Epoch 95, test loss: 0.189863\n",
      "Epoch 96, test loss: 0.187731\n",
      "Epoch 97, test loss: 0.188318\n",
      "Epoch 98, test loss: 0.187211\n",
      "Epoch 99, test loss: 0.186934\n",
      "Epoch 100, test loss: 0.186964\n",
      "Epoch 101, test loss: 0.186852\n",
      "Epoch 102, test loss: 0.187132\n",
      "Epoch 103, test loss: 0.189770\n",
      "Epoch 104, test loss: 0.186623\n",
      "Epoch 105, test loss: 0.187209\n",
      "Epoch 106, test loss: 0.186827\n",
      "Epoch 107, test loss: 0.187636\n",
      "Epoch 108, test loss: 0.188088\n",
      "Epoch 109, test loss: 0.188682\n",
      "Epoch 110, test loss: 0.188961\n",
      "Epoch 111, test loss: 0.189059\n",
      "Epoch 112, test loss: 0.193825\n",
      "Epoch 113, test loss: 0.191224\n",
      "Epoch 114, test loss: 0.188349\n",
      "Epoch 115, test loss: 0.188394\n",
      "Epoch 116, test loss: 0.192286\n",
      "Epoch 117, test loss: 0.190457\n",
      "Epoch 118, test loss: 0.187638\n",
      "Epoch 119, test loss: 0.186930\n",
      "Epoch 120, test loss: 0.189261\n",
      "Epoch 121, test loss: 0.186997\n",
      "Epoch 122, test loss: 0.188065\n",
      "Epoch 123, test loss: 0.187519\n",
      "Epoch 124, test loss: 0.187397\n",
      "Epoch 125, test loss: 0.190945\n",
      "Epoch 126, test loss: 0.191712\n",
      "Epoch 127, test loss: 0.188327\n",
      "Epoch 128, test loss: 0.187110\n",
      "Epoch 129, test loss: 0.188932\n",
      "Epoch 130, test loss: 0.187357\n",
      "Epoch 131, test loss: 0.193058\n",
      "Epoch 132, test loss: 0.188201\n",
      "Epoch 133, test loss: 0.187365\n",
      "Epoch 134, test loss: 0.187437\n",
      "Epoch 135, test loss: 0.189253\n",
      "Epoch 136, test loss: 0.189689\n",
      "Epoch 137, test loss: 0.188120\n",
      "Epoch 138, test loss: 0.193901\n",
      "Epoch 139, test loss: 0.186978\n",
      "Epoch 140, test loss: 0.189028\n",
      "Epoch 141, test loss: 0.187928\n",
      "Epoch 142, test loss: 0.186822\n",
      "Epoch 143, test loss: 0.189395\n",
      "Epoch 144, test loss: 0.192987\n",
      "Epoch 145, test loss: 0.190936\n",
      "Epoch 146, test loss: 0.189994\n",
      "Epoch 147, test loss: 0.187572\n",
      "Epoch 148, test loss: 0.186993\n",
      "Epoch 149, test loss: 0.195383\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F63354D6C0>\n",
      "Epoch 0, test loss: 0.242724\n",
      "Epoch 1, test loss: 0.196895\n",
      "Epoch 2, test loss: 0.194082\n",
      "Epoch 3, test loss: 0.196920\n",
      "Epoch 4, test loss: 0.193760\n",
      "Epoch 5, test loss: 0.192371\n",
      "Epoch 6, test loss: 0.192025\n",
      "Epoch 7, test loss: 0.192278\n",
      "Epoch 8, test loss: 0.197204\n",
      "Epoch 9, test loss: 0.192364\n",
      "Epoch 10, test loss: 0.193320\n",
      "Epoch 11, test loss: 0.195055\n",
      "Epoch 12, test loss: 0.196617\n",
      "Epoch 13, test loss: 0.192697\n",
      "Epoch 14, test loss: 0.199822\n",
      "Epoch 15, test loss: 0.192145\n",
      "Epoch 16, test loss: 0.192610\n",
      "Epoch 17, test loss: 0.192063\n",
      "Epoch 18, test loss: 0.191870\n",
      "Epoch 19, test loss: 0.192571\n",
      "Epoch 20, test loss: 0.193266\n",
      "Epoch 21, test loss: 0.192306\n",
      "Epoch 22, test loss: 0.199030\n",
      "Epoch 23, test loss: 0.192399\n",
      "Epoch 24, test loss: 0.191869\n",
      "Epoch 25, test loss: 0.191309\n",
      "Epoch 26, test loss: 0.192716\n",
      "Epoch 27, test loss: 0.191990\n",
      "Epoch 28, test loss: 0.191665\n",
      "Epoch 29, test loss: 0.190782\n",
      "Epoch 30, test loss: 0.190398\n",
      "Epoch 31, test loss: 0.193683\n",
      "Epoch 32, test loss: 0.192640\n",
      "Epoch 33, test loss: 0.190649\n",
      "Epoch 34, test loss: 0.195381\n",
      "Epoch 35, test loss: 0.193438\n",
      "Epoch 36, test loss: 0.190846\n",
      "Epoch 37, test loss: 0.191643\n",
      "Epoch 38, test loss: 0.195346\n",
      "Epoch 39, test loss: 0.190919\n",
      "Epoch 40, test loss: 0.190807\n",
      "Epoch 41, test loss: 0.191816\n",
      "Epoch 42, test loss: 0.193054\n",
      "Epoch 43, test loss: 0.193252\n",
      "Epoch 44, test loss: 0.193754\n",
      "Epoch 45, test loss: 0.192382\n",
      "Epoch 46, test loss: 0.192138\n",
      "Epoch 47, test loss: 0.190911\n",
      "Epoch 48, test loss: 0.190571\n",
      "Epoch 49, test loss: 0.190662\n",
      "Epoch 50, test loss: 0.193455\n",
      "Epoch 51, test loss: 0.191504\n",
      "Epoch 52, test loss: 0.191026\n",
      "Epoch 53, test loss: 0.191577\n",
      "Epoch 54, test loss: 0.191022\n",
      "Epoch 55, test loss: 0.194468\n",
      "Epoch 56, test loss: 0.193009\n",
      "Epoch 57, test loss: 0.194996\n",
      "Epoch 58, test loss: 0.203066\n",
      "Epoch 59, test loss: 0.192622\n",
      "Epoch 60, test loss: 0.195023\n",
      "Epoch 61, test loss: 0.192208\n",
      "Epoch 62, test loss: 0.191394\n",
      "Epoch 63, test loss: 0.191777\n",
      "Epoch 64, test loss: 0.191999\n",
      "Epoch 65, test loss: 0.190351\n",
      "Epoch 66, test loss: 0.191266\n",
      "Epoch 67, test loss: 0.191324\n",
      "Epoch 68, test loss: 0.191642\n",
      "Epoch 69, test loss: 0.194238\n",
      "Epoch 70, test loss: 0.192083\n",
      "Epoch 71, test loss: 0.192262\n",
      "Epoch 72, test loss: 0.190758\n",
      "Epoch 73, test loss: 0.196040\n",
      "Epoch 74, test loss: 0.194152\n",
      "Epoch 75, test loss: 0.190186\n",
      "Epoch 76, test loss: 0.190925\n",
      "Epoch 77, test loss: 0.191413\n",
      "Epoch 78, test loss: 0.190336\n",
      "Epoch 79, test loss: 0.190357\n",
      "Epoch 80, test loss: 0.195476\n",
      "Epoch 81, test loss: 0.193644\n",
      "Epoch 82, test loss: 0.202268\n",
      "Epoch 83, test loss: 0.204192\n",
      "Epoch 84, test loss: 0.192188\n",
      "Epoch 85, test loss: 0.191162\n",
      "Epoch 86, test loss: 0.190481\n",
      "Epoch 87, test loss: 0.192255\n",
      "Epoch 88, test loss: 0.198041\n",
      "Epoch 89, test loss: 0.191335\n",
      "Epoch 90, test loss: 0.189921\n",
      "Epoch 91, test loss: 0.198143\n",
      "Epoch 92, test loss: 0.194500\n",
      "Epoch 93, test loss: 0.193021\n",
      "Epoch 94, test loss: 0.191319\n",
      "Epoch 95, test loss: 0.193526\n",
      "Epoch 96, test loss: 0.192887\n",
      "Epoch 97, test loss: 0.191698\n",
      "Epoch 98, test loss: 0.199823\n",
      "Epoch 99, test loss: 0.192620\n",
      "Epoch 100, test loss: 0.193690\n",
      "Epoch 101, test loss: 0.193705\n",
      "Epoch 102, test loss: 0.196884\n",
      "Epoch 103, test loss: 0.190972\n",
      "Epoch 104, test loss: 0.191667\n",
      "Epoch 105, test loss: 0.190936\n",
      "Epoch 106, test loss: 0.192484\n",
      "Epoch 107, test loss: 0.190116\n",
      "Epoch 108, test loss: 0.195004\n",
      "Epoch 109, test loss: 0.190776\n",
      "Epoch 110, test loss: 0.192609\n",
      "Epoch 111, test loss: 0.190719\n",
      "Epoch 112, test loss: 0.197742\n",
      "Epoch 113, test loss: 0.192975\n",
      "Epoch 114, test loss: 0.190450\n",
      "Epoch 115, test loss: 0.189927\n",
      "Epoch 116, test loss: 0.191440\n",
      "Epoch 117, test loss: 0.191362\n",
      "Epoch 118, test loss: 0.191126\n",
      "Epoch 119, test loss: 0.196895\n",
      "Epoch 120, test loss: 0.190346\n",
      "Epoch 121, test loss: 0.191853\n",
      "Epoch 122, test loss: 0.190007\n",
      "Epoch 123, test loss: 0.193041\n",
      "Epoch 124, test loss: 0.193931\n",
      "Epoch 125, test loss: 0.190183\n",
      "Epoch 126, test loss: 0.199754\n",
      "Epoch 127, test loss: 0.194036\n",
      "Epoch 128, test loss: 0.190296\n",
      "Epoch 129, test loss: 0.192380\n",
      "Epoch 130, test loss: 0.198634\n",
      "Epoch 131, test loss: 0.191807\n",
      "Epoch 132, test loss: 0.191045\n",
      "Epoch 133, test loss: 0.193638\n",
      "Epoch 134, test loss: 0.196098\n",
      "Epoch 135, test loss: 0.191402\n",
      "Epoch 136, test loss: 0.189845\n",
      "Epoch 137, test loss: 0.191439\n",
      "Epoch 138, test loss: 0.190647\n",
      "Epoch 139, test loss: 0.190187\n",
      "Epoch 140, test loss: 0.194610\n",
      "Epoch 141, test loss: 0.190484\n",
      "Epoch 142, test loss: 0.190813\n",
      "Epoch 143, test loss: 0.191303\n",
      "Epoch 144, test loss: 0.189923\n",
      "Epoch 145, test loss: 0.190694\n",
      "Epoch 146, test loss: 0.190041\n",
      "Epoch 147, test loss: 0.191462\n",
      "Epoch 148, test loss: 0.190870\n",
      "Epoch 149, test loss: 0.190560\n",
      "Epoch 150, test loss: 0.192168\n",
      "Epoch 151, test loss: 0.194172\n",
      "Epoch 152, test loss: 0.194604\n",
      "Epoch 153, test loss: 0.197673\n",
      "Epoch 154, test loss: 0.200762\n",
      "Epoch 155, test loss: 0.190927\n",
      "Epoch 156, test loss: 0.196618\n",
      "Epoch 157, test loss: 0.198253\n",
      "Epoch 158, test loss: 0.190000\n",
      "Epoch 159, test loss: 0.197963\n",
      "Epoch 160, test loss: 0.194977\n",
      "Epoch 161, test loss: 0.190456\n",
      "Epoch 162, test loss: 0.191573\n",
      "Epoch 163, test loss: 0.190536\n",
      "Epoch 164, test loss: 0.190314\n",
      "Epoch 165, test loss: 0.189978\n",
      "Epoch 166, test loss: 0.190712\n",
      "Epoch 167, test loss: 0.190449\n",
      "Epoch 168, test loss: 0.195301\n",
      "Epoch 169, test loss: 0.190217\n",
      "Epoch 170, test loss: 0.190086\n",
      "Epoch 171, test loss: 0.193838\n",
      "Epoch 172, test loss: 0.190926\n",
      "Epoch 173, test loss: 0.190232\n",
      "Epoch 174, test loss: 0.192647\n",
      "Epoch 175, test loss: 0.191024\n",
      "Epoch 176, test loss: 0.198643\n",
      "Epoch 177, test loss: 0.194251\n",
      "Epoch 178, test loss: 0.192932\n",
      "Epoch 179, test loss: 0.191579\n",
      "Epoch 180, test loss: 0.190604\n",
      "Epoch 181, test loss: 0.196620\n",
      "Epoch 182, test loss: 0.193340\n",
      "Epoch 183, test loss: 0.191666\n",
      "Epoch 184, test loss: 0.190723\n",
      "Epoch 185, test loss: 0.196145\n",
      "Epoch 186, test loss: 0.195467\n",
      "Epoch 187, test loss: 0.190648\n",
      "Epoch 188, test loss: 0.190524\n",
      "Epoch 189, test loss: 0.190528\n",
      "Epoch 190, test loss: 0.191856\n",
      "Epoch 191, test loss: 0.190755\n",
      "Epoch 192, test loss: 0.194476\n",
      "Epoch 193, test loss: 0.191485\n",
      "Epoch 194, test loss: 0.198417\n",
      "Epoch 195, test loss: 0.190353\n",
      "Epoch 196, test loss: 0.193082\n",
      "Epoch 197, test loss: 0.190705\n",
      "Epoch 198, test loss: 0.197502\n",
      "Epoch 199, test loss: 0.190865\n",
      "Epoch 200, test loss: 0.190649\n",
      "Epoch 201, test loss: 0.191498\n",
      "Epoch 202, test loss: 0.190083\n",
      "Epoch 203, test loss: 0.201793\n",
      "Epoch 204, test loss: 0.196664\n",
      "Epoch 205, test loss: 0.195081\n",
      "Epoch 206, test loss: 0.190379\n",
      "Epoch 207, test loss: 0.190111\n",
      "Epoch 208, test loss: 0.192027\n",
      "Epoch 209, test loss: 0.189819\n",
      "Epoch 210, test loss: 0.192320\n",
      "Epoch 211, test loss: 0.190775\n",
      "Epoch 212, test loss: 0.191710\n",
      "Epoch 213, test loss: 0.195136\n",
      "Epoch 214, test loss: 0.190960\n",
      "Epoch 215, test loss: 0.189887\n",
      "Epoch 216, test loss: 0.190901\n",
      "Epoch 217, test loss: 0.191463\n",
      "Epoch 218, test loss: 0.193970\n",
      "Epoch 219, test loss: 0.190814\n",
      "Epoch 220, test loss: 0.191927\n",
      "Epoch 221, test loss: 0.193999\n",
      "Epoch 222, test loss: 0.194087\n",
      "Epoch 223, test loss: 0.190815\n",
      "Epoch 224, test loss: 0.189996\n",
      "Epoch 225, test loss: 0.191153\n",
      "Epoch 226, test loss: 0.190767\n",
      "Epoch 227, test loss: 0.194964\n",
      "Epoch 228, test loss: 0.190567\n",
      "Epoch 229, test loss: 0.190100\n",
      "Epoch 230, test loss: 0.191832\n",
      "Epoch 231, test loss: 0.191543\n",
      "Epoch 232, test loss: 0.200587\n",
      "Epoch 233, test loss: 0.190585\n",
      "Epoch 234, test loss: 0.194717\n",
      "Epoch 235, test loss: 0.193622\n",
      "Epoch 236, test loss: 0.190268\n",
      "Epoch 237, test loss: 0.191450\n",
      "Epoch 238, test loss: 0.190653\n",
      "Epoch 239, test loss: 0.191432\n",
      "Epoch 240, test loss: 0.192206\n",
      "Epoch 241, test loss: 0.190459\n",
      "Epoch 242, test loss: 0.190604\n",
      "Epoch 243, test loss: 0.196497\n",
      "Epoch 244, test loss: 0.190338\n",
      "Epoch 245, test loss: 0.190429\n",
      "Epoch 246, test loss: 0.190833\n",
      "Epoch 247, test loss: 0.195809\n",
      "Epoch 248, test loss: 0.192977\n",
      "Epoch 249, test loss: 0.193928\n",
      "Epoch 250, test loss: 0.196794\n",
      "Epoch 251, test loss: 0.193375\n",
      "Epoch 252, test loss: 0.190294\n",
      "Epoch 253, test loss: 0.193424\n",
      "Epoch 254, test loss: 0.192410\n",
      "Epoch 255, test loss: 0.190971\n",
      "Epoch 256, test loss: 0.192271\n",
      "Epoch 257, test loss: 0.194118\n",
      "Epoch 258, test loss: 0.191600\n",
      "Epoch 259, test loss: 0.190708\n",
      "Epoch 260, test loss: 0.192356\n",
      "Epoch 261, test loss: 0.190088\n",
      "Epoch 262, test loss: 0.191631\n",
      "Epoch 263, test loss: 0.190515\n",
      "Epoch 264, test loss: 0.190353\n",
      "Epoch 265, test loss: 0.199405\n",
      "Epoch 266, test loss: 0.191743\n",
      "Epoch 267, test loss: 0.190203\n",
      "Epoch 268, test loss: 0.190467\n",
      "Epoch 269, test loss: 0.191087\n",
      "Epoch 270, test loss: 0.189811\n",
      "Epoch 271, test loss: 0.190252\n",
      "Epoch 272, test loss: 0.191376\n",
      "Epoch 273, test loss: 0.191951\n",
      "Epoch 274, test loss: 0.194062\n",
      "Epoch 275, test loss: 0.192368\n",
      "Epoch 276, test loss: 0.190531\n",
      "Epoch 277, test loss: 0.197042\n",
      "Epoch 278, test loss: 0.191163\n",
      "Epoch 279, test loss: 0.191912\n",
      "Epoch 280, test loss: 0.191240\n",
      "Epoch 281, test loss: 0.190730\n",
      "Epoch 282, test loss: 0.190531\n",
      "Epoch 283, test loss: 0.198954\n",
      "Epoch 284, test loss: 0.200107\n",
      "Epoch 285, test loss: 0.191810\n",
      "Epoch 286, test loss: 0.190603\n",
      "Epoch 287, test loss: 0.190254\n",
      "Epoch 288, test loss: 0.191961\n",
      "Epoch 289, test loss: 0.192880\n",
      "Epoch 290, test loss: 0.190237\n",
      "Epoch 291, test loss: 0.190931\n",
      "Epoch 292, test loss: 0.193939\n",
      "Epoch 293, test loss: 0.190185\n",
      "Epoch 294, test loss: 0.189916\n",
      "Epoch 295, test loss: 0.190238\n",
      "Epoch 296, test loss: 0.193225\n",
      "Epoch 297, test loss: 0.194946\n",
      "Epoch 298, test loss: 0.192512\n",
      "Epoch 299, test loss: 0.191650\n",
      "Epoch 300, test loss: 0.191137\n",
      "Epoch 301, test loss: 0.191358\n",
      "Epoch 302, test loss: 0.197372\n",
      "Epoch 303, test loss: 0.190971\n",
      "Epoch 304, test loss: 0.190214\n",
      "Epoch 305, test loss: 0.192127\n",
      "Epoch 306, test loss: 0.195920\n",
      "Epoch 307, test loss: 0.194467\n",
      "Epoch 308, test loss: 0.190774\n",
      "Epoch 309, test loss: 0.189964\n",
      "Epoch 310, test loss: 0.192939\n",
      "Epoch 311, test loss: 0.190277\n",
      "Epoch 312, test loss: 0.193290\n",
      "Epoch 313, test loss: 0.190732\n",
      "Epoch 314, test loss: 0.190678\n",
      "Epoch 315, test loss: 0.191300\n",
      "Epoch 316, test loss: 0.190526\n",
      "Epoch 317, test loss: 0.192974\n",
      "Epoch 318, test loss: 0.194509\n",
      "Epoch 319, test loss: 0.190737\n",
      "Epoch 320, test loss: 0.190376\n",
      "Epoch 321, test loss: 0.190788\n",
      "Epoch 322, test loss: 0.193132\n",
      "Epoch 323, test loss: 0.192579\n",
      "Epoch 324, test loss: 0.192413\n",
      "Epoch 325, test loss: 0.190689\n",
      "Epoch 326, test loss: 0.194868\n",
      "Epoch 327, test loss: 0.190813\n",
      "Epoch 328, test loss: 0.189646\n",
      "Epoch 329, test loss: 0.191543\n",
      "Epoch 330, test loss: 0.193209\n",
      "Epoch 331, test loss: 0.198760\n",
      "Epoch 332, test loss: 0.190732\n",
      "Epoch 333, test loss: 0.192032\n",
      "Epoch 334, test loss: 0.190532\n",
      "Epoch 335, test loss: 0.192469\n",
      "Epoch 336, test loss: 0.190094\n",
      "Epoch 337, test loss: 0.193487\n",
      "Epoch 338, test loss: 0.191306\n",
      "Epoch 339, test loss: 0.190713\n",
      "Epoch 340, test loss: 0.191605\n",
      "Epoch 341, test loss: 0.191218\n",
      "Epoch 342, test loss: 0.190326\n",
      "Epoch 343, test loss: 0.191735\n",
      "Epoch 344, test loss: 0.190906\n",
      "Epoch 345, test loss: 0.202409\n",
      "Epoch 346, test loss: 0.191886\n",
      "Epoch 347, test loss: 0.192009\n",
      "Epoch 348, test loss: 0.194105\n",
      "Epoch 349, test loss: 0.190079\n",
      "Epoch 350, test loss: 0.190119\n",
      "Epoch 351, test loss: 0.198827\n",
      "Epoch 352, test loss: 0.192109\n",
      "Epoch 353, test loss: 0.193108\n",
      "Epoch 354, test loss: 0.189797\n",
      "Epoch 355, test loss: 0.194432\n",
      "Epoch 356, test loss: 0.190912\n",
      "Epoch 357, test loss: 0.192803\n",
      "Epoch 358, test loss: 0.192013\n",
      "Epoch 359, test loss: 0.190122\n",
      "Epoch 360, test loss: 0.194833\n",
      "Epoch 361, test loss: 0.192092\n",
      "Epoch 362, test loss: 0.189832\n",
      "Epoch 363, test loss: 0.193930\n",
      "Epoch 364, test loss: 0.193015\n",
      "Epoch 365, test loss: 0.190599\n",
      "Epoch 366, test loss: 0.191901\n",
      "Epoch 367, test loss: 0.194828\n",
      "Epoch 368, test loss: 0.191907\n",
      "Epoch 369, test loss: 0.191681\n",
      "Epoch 370, test loss: 0.192777\n",
      "Epoch 371, test loss: 0.192022\n",
      "Epoch 372, test loss: 0.192791\n",
      "Epoch 373, test loss: 0.191262\n",
      "Epoch 374, test loss: 0.191767\n",
      "Epoch 375, test loss: 0.190295\n",
      "Epoch 376, test loss: 0.190443\n",
      "Epoch 377, test loss: 0.190520\n",
      "Epoch 378, test loss: 0.190214\n",
      "Epoch 379, test loss: 0.194736\n",
      "Epoch 380, test loss: 0.192343\n",
      "Epoch 381, test loss: 0.189734\n",
      "Epoch 382, test loss: 0.194031\n",
      "Epoch 383, test loss: 0.189822\n",
      "Epoch 384, test loss: 0.190193\n",
      "Epoch 385, test loss: 0.189804\n",
      "Epoch 386, test loss: 0.190553\n",
      "Epoch 387, test loss: 0.190127\n",
      "Epoch 388, test loss: 0.190555\n",
      "Epoch 389, test loss: 0.191455\n",
      "Epoch 390, test loss: 0.193832\n",
      "Epoch 391, test loss: 0.191115\n",
      "Epoch 392, test loss: 0.191530\n",
      "Epoch 393, test loss: 0.191814\n",
      "Epoch 394, test loss: 0.195808\n",
      "Epoch 395, test loss: 0.194775\n",
      "Epoch 396, test loss: 0.192038\n",
      "Epoch 397, test loss: 0.190370\n",
      "Epoch 398, test loss: 0.192199\n",
      "Epoch 399, test loss: 0.190528\n",
      "Epoch 400, test loss: 0.189961\n",
      "Epoch 401, test loss: 0.190295\n",
      "Epoch 402, test loss: 0.190037\n",
      "Epoch 403, test loss: 0.191943\n",
      "Epoch 404, test loss: 0.191616\n",
      "Epoch 405, test loss: 0.197437\n",
      "Epoch 406, test loss: 0.196350\n",
      "Epoch 407, test loss: 0.195445\n",
      "Epoch 408, test loss: 0.191177\n",
      "Epoch 409, test loss: 0.190565\n",
      "Epoch 410, test loss: 0.197938\n",
      "Epoch 411, test loss: 0.192854\n",
      "Epoch 412, test loss: 0.191602\n",
      "Epoch 413, test loss: 0.189618\n",
      "Epoch 414, test loss: 0.193384\n",
      "Epoch 415, test loss: 0.189781\n",
      "Epoch 416, test loss: 0.190679\n",
      "Epoch 417, test loss: 0.190873\n",
      "Epoch 418, test loss: 0.190255\n",
      "Epoch 419, test loss: 0.189808\n",
      "Epoch 420, test loss: 0.190568\n",
      "Epoch 421, test loss: 0.192232\n",
      "Epoch 422, test loss: 0.197647\n",
      "Epoch 423, test loss: 0.194841\n",
      "Epoch 424, test loss: 0.190583\n",
      "Epoch 425, test loss: 0.190502\n",
      "Epoch 426, test loss: 0.199781\n",
      "Epoch 427, test loss: 0.191115\n",
      "Epoch 428, test loss: 0.195758\n",
      "Epoch 429, test loss: 0.190381\n",
      "Epoch 430, test loss: 0.193404\n",
      "Epoch 431, test loss: 0.190614\n",
      "Epoch 432, test loss: 0.190646\n",
      "Epoch 433, test loss: 0.192441\n",
      "Epoch 434, test loss: 0.190250\n",
      "Epoch 435, test loss: 0.193038\n",
      "Epoch 436, test loss: 0.191149\n",
      "Epoch 437, test loss: 0.190793\n",
      "Epoch 438, test loss: 0.193052\n",
      "Epoch 439, test loss: 0.191610\n",
      "Epoch 440, test loss: 0.193417\n",
      "Epoch 441, test loss: 0.191871\n",
      "Epoch 442, test loss: 0.196821\n",
      "Epoch 443, test loss: 0.196254\n",
      "Epoch 444, test loss: 0.189860\n",
      "Epoch 445, test loss: 0.190037\n",
      "Epoch 446, test loss: 0.192939\n",
      "Epoch 447, test loss: 0.191021\n",
      "Epoch 448, test loss: 0.192550\n",
      "Epoch 449, test loss: 0.191012\n",
      "Pretrain data: 19653653.0\n",
      "Building dataset, requesting data from 0 to 820\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7592/106252\n",
      "Found 820 continuous time series\n",
      "Data shape: (113846, 6), Train/test: 113844/2\n",
      "Train test ratio: 56922.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1778\n",
      "Epoch 0, train loss: 0.234871\n",
      "Epoch 1, train loss: 0.210654\n",
      "Epoch 2, train loss: 0.215427\n",
      "Epoch 3, train loss: 0.243612\n",
      "Epoch 4, train loss: 0.219018\n",
      "Epoch 5, train loss: 0.197512\n",
      "Epoch 6, train loss: 0.211286\n",
      "Epoch 7, train loss: 0.197209\n",
      "Epoch 8, train loss: 0.256267\n",
      "Epoch 9, train loss: 0.203223\n",
      "Epoch 10, train loss: 0.226071\n",
      "Epoch 11, train loss: 0.221969\n",
      "Epoch 12, train loss: 0.288055\n",
      "Epoch 13, train loss: 0.254012\n",
      "Epoch 14, train loss: 0.150800\n",
      "Epoch 15, train loss: 0.222245\n",
      "Epoch 16, train loss: 0.172351\n",
      "Epoch 17, train loss: 0.203544\n",
      "Epoch 18, train loss: 0.235851\n",
      "Epoch 19, train loss: 0.256045\n",
      "Epoch 20, train loss: 0.184924\n",
      "Epoch 21, train loss: 0.203123\n",
      "Epoch 22, train loss: 0.260097\n",
      "Epoch 23, train loss: 0.191982\n",
      "Epoch 24, train loss: 0.175685\n",
      "Epoch 25, train loss: 0.210132\n",
      "Epoch 26, train loss: 0.175605\n",
      "Epoch 27, train loss: 0.236641\n",
      "Epoch 28, train loss: 0.281476\n",
      "Epoch 29, train loss: 0.247051\n",
      "Epoch 30, train loss: 0.218467\n",
      "Epoch 31, train loss: 0.187888\n",
      "Epoch 32, train loss: 0.170375\n",
      "Epoch 33, train loss: 0.214733\n",
      "Epoch 34, train loss: 0.206439\n",
      "Epoch 35, train loss: 0.197819\n",
      "Epoch 36, train loss: 0.205704\n",
      "Epoch 37, train loss: 0.211462\n",
      "Epoch 38, train loss: 0.206622\n",
      "Epoch 39, train loss: 0.195602\n",
      "Epoch 40, train loss: 0.182352\n",
      "Epoch 41, train loss: 0.235056\n",
      "Epoch 42, train loss: 0.263667\n",
      "Epoch 43, train loss: 0.169325\n",
      "Epoch 44, train loss: 0.205915\n",
      "Epoch 45, train loss: 0.208173\n",
      "Epoch 46, train loss: 0.186005\n",
      "Epoch 47, train loss: 0.191613\n",
      "Epoch 48, train loss: 0.204252\n",
      "Epoch 49, train loss: 0.229283\n",
      "Epoch 50, train loss: 0.253154\n",
      "Epoch 51, train loss: 0.159677\n",
      "Epoch 52, train loss: 0.215955\n",
      "Epoch 53, train loss: 0.253081\n",
      "Epoch 54, train loss: 0.197585\n",
      "Epoch 55, train loss: 0.183093\n",
      "Epoch 56, train loss: 0.192334\n",
      "Epoch 57, train loss: 0.214455\n",
      "Epoch 58, train loss: 0.162573\n",
      "Epoch 59, train loss: 0.147505\n",
      "Epoch 60, train loss: 0.212603\n",
      "Epoch 61, train loss: 0.280131\n",
      "Epoch 62, train loss: 0.178382\n",
      "Epoch 63, train loss: 0.158101\n",
      "Epoch 64, train loss: 0.241867\n",
      "Epoch 65, train loss: 0.268902\n",
      "Epoch 66, train loss: 0.256260\n",
      "Epoch 67, train loss: 0.203517\n",
      "Epoch 68, train loss: 0.214380\n",
      "Epoch 69, train loss: 0.217827\n",
      "Epoch 70, train loss: 0.177345\n",
      "Epoch 71, train loss: 0.172045\n",
      "Epoch 72, train loss: 0.245457\n",
      "Epoch 73, train loss: 0.173795\n",
      "Epoch 74, train loss: 0.206567\n",
      "Epoch 75, train loss: 0.162516\n",
      "Epoch 76, train loss: 0.188891\n",
      "Epoch 77, train loss: 0.218394\n",
      "Epoch 78, train loss: 0.196983\n",
      "Epoch 79, train loss: 0.298253\n",
      "Epoch 80, train loss: 0.148857\n",
      "Epoch 81, train loss: 0.175811\n",
      "Epoch 82, train loss: 0.230101\n",
      "Epoch 83, train loss: 0.195616\n",
      "Epoch 84, train loss: 0.196757\n",
      "Epoch 85, train loss: 0.229101\n",
      "Epoch 86, train loss: 0.197647\n",
      "Epoch 87, train loss: 0.205411\n",
      "Epoch 88, train loss: 0.180153\n",
      "Epoch 89, train loss: 0.179838\n",
      "Epoch 90, train loss: 0.192239\n",
      "Epoch 91, train loss: 0.196317\n",
      "Epoch 92, train loss: 0.208070\n",
      "Epoch 93, train loss: 0.242711\n",
      "Epoch 94, train loss: 0.221956\n",
      "Epoch 95, train loss: 0.209670\n",
      "Epoch 96, train loss: 0.202955\n",
      "Epoch 97, train loss: 0.241677\n",
      "Epoch 98, train loss: 0.210820\n",
      "Epoch 99, train loss: 0.152446\n",
      "Epoch 100, train loss: 0.151230\n",
      "Epoch 101, train loss: 0.231592\n",
      "Epoch 102, train loss: 0.214654\n",
      "Epoch 103, train loss: 0.200171\n",
      "Epoch 104, train loss: 0.192953\n",
      "Epoch 105, train loss: 0.197335\n",
      "Epoch 106, train loss: 0.190588\n",
      "Epoch 107, train loss: 0.171308\n",
      "Epoch 108, train loss: 0.216903\n",
      "Epoch 109, train loss: 0.201801\n",
      "Epoch 110, train loss: 0.251569\n",
      "Epoch 111, train loss: 0.199133\n",
      "Epoch 112, train loss: 0.229483\n",
      "Epoch 113, train loss: 0.163716\n",
      "Epoch 114, train loss: 0.222031\n",
      "Epoch 115, train loss: 0.191362\n",
      "Epoch 116, train loss: 0.256048\n",
      "Epoch 117, train loss: 0.174767\n",
      "Epoch 118, train loss: 0.190596\n",
      "Epoch 119, train loss: 0.197130\n",
      "Epoch 120, train loss: 0.287180\n",
      "Epoch 121, train loss: 0.188488\n",
      "Epoch 122, train loss: 0.200944\n",
      "Epoch 123, train loss: 0.198611\n",
      "Epoch 124, train loss: 0.228167\n",
      "Epoch 125, train loss: 0.171475\n",
      "Epoch 126, train loss: 0.212412\n",
      "Epoch 127, train loss: 0.210489\n",
      "Epoch 128, train loss: 0.376477\n",
      "Epoch 129, train loss: 0.195325\n",
      "Epoch 130, train loss: 0.177288\n",
      "Epoch 131, train loss: 0.195095\n",
      "Epoch 132, train loss: 0.161429\n",
      "Epoch 133, train loss: 0.241138\n",
      "Epoch 134, train loss: 0.223763\n",
      "Epoch 135, train loss: 0.191697\n",
      "Epoch 136, train loss: 0.188492\n",
      "Epoch 137, train loss: 0.223141\n",
      "Epoch 138, train loss: 0.191072\n",
      "Epoch 139, train loss: 0.185332\n",
      "Epoch 140, train loss: 0.150370\n",
      "Epoch 141, train loss: 0.217931\n",
      "Epoch 142, train loss: 0.182305\n",
      "Epoch 143, train loss: 0.258527\n",
      "Epoch 144, train loss: 0.220429\n",
      "Epoch 145, train loss: 0.269658\n",
      "Epoch 146, train loss: 0.231089\n",
      "Epoch 147, train loss: 0.191983\n",
      "Epoch 148, train loss: 0.157686\n",
      "Epoch 149, train loss: 0.205541\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "x here is\n",
      "[[239. 238. 235. 233. 231. 229.]\n",
      " [238. 235. 233. 231. 229. 227.]\n",
      " [235. 233. 231. 229. 227. 222.]\n",
      " ...\n",
      " [140. 151. 154. 152. 151. 149.]\n",
      " [151. 154. 152. 151. 149. 149.]\n",
      " [154. 152. 151. 149. 149. 145.]]\n",
      "y here is\n",
      "[[208. 208. 208. 208. 208. 208.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " [204. 204. 204. 204. 204. 204.]\n",
      " ...\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [140. 140. 140. 140. 140. 140.]\n",
      " [145. 145. 145. 145. 145. 145.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (2526, 6), Train/test: 1/2525\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 22 segments\n",
      "Building dataset, requesting data from 0 to 22\n",
      "x here is\n",
      "[[219. 229. 224. 221. 215. 209.]\n",
      " [229. 224. 221. 215. 209. 203.]\n",
      " [224. 221. 215. 209. 203. 199.]\n",
      " ...\n",
      " [220. 225. 232. 237. 241. 243.]\n",
      " [225. 232. 237. 241. 243. 251.]\n",
      " [232. 237. 241. 243. 251. 257.]]\n",
      "y here is\n",
      "[[202. 202. 202. 202. 202. 202.]\n",
      " [192. 192. 192. 192. 192. 192.]\n",
      " [194. 194. 194. 194. 194. 194.]\n",
      " ...\n",
      " [250. 250. 250. 250. 250. 250.]\n",
      " [246. 246. 246. 246. 246. 246.]\n",
      " [240. 240. 240. 240. 240. 240.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 693/11187\n",
      "Found 22 continuous time series\n",
      "Data shape: (11882, 6), Train/test: 11880/2\n",
      "Train test ratio: 5940.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60DDF0490>\n",
      "Epoch 0, test loss: 0.178589\n",
      "Epoch 1, test loss: 0.179758\n",
      "Epoch 2, test loss: 0.181509\n",
      "Epoch 3, test loss: 0.178554\n",
      "Epoch 4, test loss: 0.179467\n",
      "Epoch 5, test loss: 0.181117\n",
      "Epoch 6, test loss: 0.179242\n",
      "Epoch 7, test loss: 0.179227\n",
      "Epoch 8, test loss: 0.180998\n",
      "Epoch 9, test loss: 0.180576\n",
      "Epoch 10, test loss: 0.180502\n",
      "Epoch 11, test loss: 0.181644\n",
      "Epoch 12, test loss: 0.180891\n",
      "Epoch 13, test loss: 0.181427\n",
      "Epoch 14, test loss: 0.183078\n",
      "Epoch 15, test loss: 0.182339\n",
      "Epoch 16, test loss: 0.186608\n",
      "Epoch 17, test loss: 0.179086\n",
      "Epoch 18, test loss: 0.180770\n",
      "Epoch 19, test loss: 0.182526\n",
      "Epoch 20, test loss: 0.183829\n",
      "Epoch 21, test loss: 0.185010\n",
      "Epoch 22, test loss: 0.181403\n",
      "Epoch 23, test loss: 0.182818\n",
      "Epoch 24, test loss: 0.184113\n",
      "Epoch 25, test loss: 0.182447\n",
      "Epoch 26, test loss: 0.180108\n",
      "Epoch 27, test loss: 0.182403\n",
      "Epoch 28, test loss: 0.182940\n",
      "Epoch 29, test loss: 0.180858\n",
      "Epoch 30, test loss: 0.182131\n",
      "Epoch 31, test loss: 0.184533\n",
      "Epoch 32, test loss: 0.181558\n",
      "Epoch 33, test loss: 0.186489\n",
      "Epoch 34, test loss: 0.181917\n",
      "Epoch 35, test loss: 0.180857\n",
      "Epoch 36, test loss: 0.180440\n",
      "Epoch 37, test loss: 0.180233\n",
      "Epoch 38, test loss: 0.183165\n",
      "Epoch 39, test loss: 0.179890\n",
      "Epoch 40, test loss: 0.178937\n",
      "Epoch 41, test loss: 0.180420\n",
      "Epoch 42, test loss: 0.182411\n",
      "Epoch 43, test loss: 0.180196\n",
      "Epoch 44, test loss: 0.180357\n",
      "Epoch 45, test loss: 0.186576\n",
      "Epoch 46, test loss: 0.182708\n",
      "Epoch 47, test loss: 0.183392\n",
      "Epoch 48, test loss: 0.184751\n",
      "Epoch 49, test loss: 0.180745\n",
      "Epoch 50, test loss: 0.181967\n",
      "Epoch 51, test loss: 0.180343\n",
      "Epoch 52, test loss: 0.183768\n",
      "Epoch 53, test loss: 0.182251\n",
      "Epoch 54, test loss: 0.180259\n",
      "Epoch 55, test loss: 0.180512\n",
      "Epoch 56, test loss: 0.179857\n",
      "Epoch 57, test loss: 0.183123\n",
      "Epoch 58, test loss: 0.184588\n",
      "Epoch 59, test loss: 0.181846\n",
      "Epoch 60, test loss: 0.180214\n",
      "Epoch 61, test loss: 0.184689\n",
      "Epoch 62, test loss: 0.181803\n",
      "Epoch 63, test loss: 0.180497\n",
      "Epoch 64, test loss: 0.184379\n",
      "Epoch 65, test loss: 0.180121\n",
      "Epoch 66, test loss: 0.181431\n",
      "Epoch 67, test loss: 0.181060\n",
      "Epoch 68, test loss: 0.182886\n",
      "Epoch 69, test loss: 0.182552\n",
      "Epoch 70, test loss: 0.189805\n",
      "Epoch 71, test loss: 0.186641\n",
      "Epoch 72, test loss: 0.180618\n",
      "Epoch 73, test loss: 0.179976\n",
      "Epoch 74, test loss: 0.180427\n",
      "Epoch 75, test loss: 0.179432\n",
      "Epoch 76, test loss: 0.178757\n",
      "Epoch 77, test loss: 0.180155\n",
      "Epoch 78, test loss: 0.179783\n",
      "Epoch 79, test loss: 0.178906\n",
      "Epoch 80, test loss: 0.180856\n",
      "Epoch 81, test loss: 0.179543\n",
      "Epoch 82, test loss: 0.177972\n",
      "Epoch 83, test loss: 0.182015\n",
      "Epoch 84, test loss: 0.181159\n",
      "Epoch 85, test loss: 0.182762\n",
      "Epoch 86, test loss: 0.184390\n",
      "Epoch 87, test loss: 0.178820\n",
      "Epoch 88, test loss: 0.180928\n",
      "Epoch 89, test loss: 0.180510\n",
      "Epoch 90, test loss: 0.180467\n",
      "Epoch 91, test loss: 0.179840\n",
      "Epoch 92, test loss: 0.183528\n",
      "Epoch 93, test loss: 0.180131\n",
      "Epoch 94, test loss: 0.181476\n",
      "Epoch 95, test loss: 0.183182\n",
      "Epoch 96, test loss: 0.181857\n",
      "Epoch 97, test loss: 0.183952\n",
      "Epoch 98, test loss: 0.185647\n",
      "Epoch 99, test loss: 0.180908\n",
      "Epoch 100, test loss: 0.180681\n",
      "Epoch 101, test loss: 0.180180\n",
      "Epoch 102, test loss: 0.180731\n",
      "Epoch 103, test loss: 0.179971\n",
      "Epoch 104, test loss: 0.180189\n",
      "Epoch 105, test loss: 0.183036\n",
      "Epoch 106, test loss: 0.182856\n",
      "Epoch 107, test loss: 0.184596\n",
      "Epoch 108, test loss: 0.182507\n",
      "Epoch 109, test loss: 0.183581\n",
      "Epoch 110, test loss: 0.184767\n",
      "Epoch 111, test loss: 0.183874\n",
      "Epoch 112, test loss: 0.186801\n",
      "Epoch 113, test loss: 0.182250\n",
      "Epoch 114, test loss: 0.181125\n",
      "Epoch 115, test loss: 0.179511\n",
      "Epoch 116, test loss: 0.183657\n",
      "Epoch 117, test loss: 0.184295\n",
      "Epoch 118, test loss: 0.181148\n",
      "Epoch 119, test loss: 0.181841\n",
      "Epoch 120, test loss: 0.182491\n",
      "Epoch 121, test loss: 0.184749\n",
      "Epoch 122, test loss: 0.180028\n",
      "Epoch 123, test loss: 0.181984\n",
      "Epoch 124, test loss: 0.182531\n",
      "Epoch 125, test loss: 0.187053\n",
      "Epoch 126, test loss: 0.184148\n",
      "Epoch 127, test loss: 0.181761\n",
      "Epoch 128, test loss: 0.181722\n",
      "Epoch 129, test loss: 0.181872\n",
      "Epoch 130, test loss: 0.183610\n",
      "Epoch 131, test loss: 0.182313\n",
      "Epoch 132, test loss: 0.180560\n",
      "Epoch 133, test loss: 0.181568\n",
      "Epoch 134, test loss: 0.183178\n",
      "Epoch 135, test loss: 0.188690\n",
      "Epoch 136, test loss: 0.180921\n",
      "Epoch 137, test loss: 0.182191\n",
      "Epoch 138, test loss: 0.181861\n",
      "Epoch 139, test loss: 0.181597\n",
      "Epoch 140, test loss: 0.181446\n",
      "Epoch 141, test loss: 0.186272\n",
      "Epoch 142, test loss: 0.182472\n",
      "Epoch 143, test loss: 0.194933\n",
      "Epoch 144, test loss: 0.181473\n",
      "Epoch 145, test loss: 0.183336\n",
      "Epoch 146, test loss: 0.184290\n",
      "Epoch 147, test loss: 0.181499\n",
      "Epoch 148, test loss: 0.181324\n",
      "Epoch 149, test loss: 0.189453\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60DDF0490>\n",
      "Epoch 0, test loss: 0.180966\n",
      "Epoch 1, test loss: 0.182604\n",
      "Epoch 2, test loss: 0.179595\n",
      "Epoch 3, test loss: 0.180884\n",
      "Epoch 4, test loss: 0.181129\n",
      "Epoch 5, test loss: 0.180260\n",
      "Epoch 6, test loss: 0.181506\n",
      "Epoch 7, test loss: 0.180932\n",
      "Epoch 8, test loss: 0.180906\n",
      "Epoch 9, test loss: 0.184104\n",
      "Epoch 10, test loss: 0.181012\n",
      "Epoch 11, test loss: 0.183877\n",
      "Epoch 12, test loss: 0.184875\n",
      "Epoch 13, test loss: 0.179847\n",
      "Epoch 14, test loss: 0.179162\n",
      "Epoch 15, test loss: 0.181126\n",
      "Epoch 16, test loss: 0.183155\n",
      "Epoch 17, test loss: 0.179720\n",
      "Epoch 18, test loss: 0.179530\n",
      "Epoch 19, test loss: 0.178831\n",
      "Epoch 20, test loss: 0.180783\n",
      "Epoch 21, test loss: 0.178710\n",
      "Epoch 22, test loss: 0.184392\n",
      "Epoch 23, test loss: 0.180274\n",
      "Epoch 24, test loss: 0.180251\n",
      "Epoch 25, test loss: 0.181138\n",
      "Epoch 26, test loss: 0.182103\n",
      "Epoch 27, test loss: 0.180484\n",
      "Epoch 28, test loss: 0.179541\n",
      "Epoch 29, test loss: 0.179750\n",
      "Epoch 30, test loss: 0.179920\n",
      "Epoch 31, test loss: 0.179984\n",
      "Epoch 32, test loss: 0.180222\n",
      "Epoch 33, test loss: 0.179825\n",
      "Epoch 34, test loss: 0.183566\n",
      "Epoch 35, test loss: 0.185214\n",
      "Epoch 36, test loss: 0.181160\n",
      "Epoch 37, test loss: 0.179085\n",
      "Epoch 38, test loss: 0.180075\n",
      "Epoch 39, test loss: 0.180850\n",
      "Epoch 40, test loss: 0.178576\n",
      "Epoch 41, test loss: 0.181834\n",
      "Epoch 42, test loss: 0.180484\n",
      "Epoch 43, test loss: 0.181106\n",
      "Epoch 44, test loss: 0.180452\n",
      "Epoch 45, test loss: 0.179568\n",
      "Epoch 46, test loss: 0.179567\n",
      "Epoch 47, test loss: 0.181323\n",
      "Epoch 48, test loss: 0.179988\n",
      "Epoch 49, test loss: 0.181036\n",
      "Epoch 50, test loss: 0.179345\n",
      "Epoch 51, test loss: 0.180627\n",
      "Epoch 52, test loss: 0.180073\n",
      "Epoch 53, test loss: 0.180666\n",
      "Epoch 54, test loss: 0.179382\n",
      "Epoch 55, test loss: 0.181997\n",
      "Epoch 56, test loss: 0.180200\n",
      "Epoch 57, test loss: 0.180405\n",
      "Epoch 58, test loss: 0.181166\n",
      "Epoch 59, test loss: 0.180975\n",
      "Epoch 60, test loss: 0.182759\n",
      "Epoch 61, test loss: 0.180826\n",
      "Epoch 62, test loss: 0.181277\n",
      "Epoch 63, test loss: 0.179945\n",
      "Epoch 64, test loss: 0.180218\n",
      "Epoch 65, test loss: 0.180517\n",
      "Epoch 66, test loss: 0.180055\n",
      "Epoch 67, test loss: 0.183090\n",
      "Epoch 68, test loss: 0.178764\n",
      "Epoch 69, test loss: 0.179403\n",
      "Epoch 70, test loss: 0.181357\n",
      "Epoch 71, test loss: 0.181321\n",
      "Epoch 72, test loss: 0.178865\n",
      "Epoch 73, test loss: 0.180007\n",
      "Epoch 74, test loss: 0.180185\n",
      "Epoch 75, test loss: 0.180042\n",
      "Epoch 76, test loss: 0.180306\n",
      "Epoch 77, test loss: 0.180072\n",
      "Epoch 78, test loss: 0.180603\n",
      "Epoch 79, test loss: 0.180990\n",
      "Epoch 80, test loss: 0.179952\n",
      "Epoch 81, test loss: 0.179947\n",
      "Epoch 82, test loss: 0.179624\n",
      "Epoch 83, test loss: 0.179737\n",
      "Epoch 84, test loss: 0.182455\n",
      "Epoch 85, test loss: 0.180667\n",
      "Epoch 86, test loss: 0.181390\n",
      "Epoch 87, test loss: 0.179919\n",
      "Epoch 88, test loss: 0.179955\n",
      "Epoch 89, test loss: 0.180710\n",
      "Epoch 90, test loss: 0.182583\n",
      "Epoch 91, test loss: 0.183606\n",
      "Epoch 92, test loss: 0.179988\n",
      "Epoch 93, test loss: 0.181193\n",
      "Epoch 94, test loss: 0.181194\n",
      "Epoch 95, test loss: 0.181056\n",
      "Epoch 96, test loss: 0.179976\n",
      "Epoch 97, test loss: 0.182672\n",
      "Epoch 98, test loss: 0.180402\n",
      "Epoch 99, test loss: 0.180755\n",
      "Epoch 100, test loss: 0.180349\n",
      "Epoch 101, test loss: 0.179835\n",
      "Epoch 102, test loss: 0.183953\n",
      "Epoch 103, test loss: 0.180380\n",
      "Epoch 104, test loss: 0.180713\n",
      "Epoch 105, test loss: 0.181295\n",
      "Epoch 106, test loss: 0.179964\n",
      "Epoch 107, test loss: 0.180898\n",
      "Epoch 108, test loss: 0.182170\n",
      "Epoch 109, test loss: 0.180581\n",
      "Epoch 110, test loss: 0.182527\n",
      "Epoch 111, test loss: 0.180499\n",
      "Epoch 112, test loss: 0.180380\n",
      "Epoch 113, test loss: 0.183558\n",
      "Epoch 114, test loss: 0.181690\n",
      "Epoch 115, test loss: 0.180452\n",
      "Epoch 116, test loss: 0.181521\n",
      "Epoch 117, test loss: 0.179370\n",
      "Epoch 118, test loss: 0.181364\n",
      "Epoch 119, test loss: 0.180580\n",
      "Epoch 120, test loss: 0.183323\n",
      "Epoch 121, test loss: 0.180758\n",
      "Epoch 122, test loss: 0.180266\n",
      "Epoch 123, test loss: 0.181185\n",
      "Epoch 124, test loss: 0.183328\n",
      "Epoch 125, test loss: 0.183416\n",
      "Epoch 126, test loss: 0.180853\n",
      "Epoch 127, test loss: 0.179473\n",
      "Epoch 128, test loss: 0.180756\n",
      "Epoch 129, test loss: 0.181099\n",
      "Epoch 130, test loss: 0.181309\n",
      "Epoch 131, test loss: 0.182307\n",
      "Epoch 132, test loss: 0.183524\n",
      "Epoch 133, test loss: 0.181934\n",
      "Epoch 134, test loss: 0.182422\n",
      "Epoch 135, test loss: 0.180604\n",
      "Epoch 136, test loss: 0.181237\n",
      "Epoch 137, test loss: 0.182721\n",
      "Epoch 138, test loss: 0.181332\n",
      "Epoch 139, test loss: 0.181808\n",
      "Epoch 140, test loss: 0.181360\n",
      "Epoch 141, test loss: 0.182933\n",
      "Epoch 142, test loss: 0.181467\n",
      "Epoch 143, test loss: 0.181454\n",
      "Epoch 144, test loss: 0.181983\n",
      "Epoch 145, test loss: 0.185026\n",
      "Epoch 146, test loss: 0.181610\n",
      "Epoch 147, test loss: 0.181245\n",
      "Epoch 148, test loss: 0.181668\n",
      "Epoch 149, test loss: 0.181614\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60DDF0490>\n",
      "Epoch 0, test loss: 0.196793\n",
      "Epoch 1, test loss: 0.191229\n",
      "Epoch 2, test loss: 0.189802\n",
      "Epoch 3, test loss: 0.191869\n",
      "Epoch 4, test loss: 0.193246\n",
      "Epoch 5, test loss: 0.194463\n",
      "Epoch 6, test loss: 0.198374\n",
      "Epoch 7, test loss: 0.197453\n",
      "Epoch 8, test loss: 0.195345\n",
      "Epoch 9, test loss: 0.194543\n",
      "Epoch 10, test loss: 0.194761\n",
      "Epoch 11, test loss: 0.194675\n",
      "Epoch 12, test loss: 0.193447\n",
      "Epoch 13, test loss: 0.194549\n",
      "Epoch 14, test loss: 0.195895\n",
      "Epoch 15, test loss: 0.193876\n",
      "Epoch 16, test loss: 0.195638\n",
      "Epoch 17, test loss: 0.196166\n",
      "Epoch 18, test loss: 0.193583\n",
      "Epoch 19, test loss: 0.193435\n",
      "Epoch 20, test loss: 0.199557\n",
      "Epoch 21, test loss: 0.197401\n",
      "Epoch 22, test loss: 0.194337\n",
      "Epoch 23, test loss: 0.193051\n",
      "Epoch 24, test loss: 0.192978\n",
      "Epoch 25, test loss: 0.192664\n",
      "Epoch 26, test loss: 0.193446\n",
      "Epoch 27, test loss: 0.193196\n",
      "Epoch 28, test loss: 0.192274\n",
      "Epoch 29, test loss: 0.194332\n",
      "Epoch 30, test loss: 0.191306\n",
      "Epoch 31, test loss: 0.192519\n",
      "Epoch 32, test loss: 0.196525\n",
      "Epoch 33, test loss: 0.195172\n",
      "Epoch 34, test loss: 0.191848\n",
      "Epoch 35, test loss: 0.193339\n",
      "Epoch 36, test loss: 0.192776\n",
      "Epoch 37, test loss: 0.191699\n",
      "Epoch 38, test loss: 0.193891\n",
      "Epoch 39, test loss: 0.194287\n",
      "Epoch 40, test loss: 0.192192\n",
      "Epoch 41, test loss: 0.192656\n",
      "Epoch 42, test loss: 0.193458\n",
      "Epoch 43, test loss: 0.192113\n",
      "Epoch 44, test loss: 0.192280\n",
      "Epoch 45, test loss: 0.190729\n",
      "Epoch 46, test loss: 0.191962\n",
      "Epoch 47, test loss: 0.192944\n",
      "Epoch 48, test loss: 0.191583\n",
      "Epoch 49, test loss: 0.190557\n",
      "Epoch 50, test loss: 0.191935\n",
      "Epoch 51, test loss: 0.193481\n",
      "Epoch 52, test loss: 0.193922\n",
      "Epoch 53, test loss: 0.191636\n",
      "Epoch 54, test loss: 0.191308\n",
      "Epoch 55, test loss: 0.193181\n",
      "Epoch 56, test loss: 0.191481\n",
      "Epoch 57, test loss: 0.190793\n",
      "Epoch 58, test loss: 0.193929\n",
      "Epoch 59, test loss: 0.191847\n",
      "Epoch 60, test loss: 0.191639\n",
      "Epoch 61, test loss: 0.191308\n",
      "Epoch 62, test loss: 0.190864\n",
      "Epoch 63, test loss: 0.195479\n",
      "Epoch 64, test loss: 0.191194\n",
      "Epoch 65, test loss: 0.188683\n",
      "Epoch 66, test loss: 0.189013\n",
      "Epoch 67, test loss: 0.189444\n",
      "Epoch 68, test loss: 0.188675\n",
      "Epoch 69, test loss: 0.188636\n",
      "Epoch 70, test loss: 0.190278\n",
      "Epoch 71, test loss: 0.188838\n",
      "Epoch 72, test loss: 0.187345\n",
      "Epoch 73, test loss: 0.186922\n",
      "Epoch 74, test loss: 0.189240\n",
      "Epoch 75, test loss: 0.187905\n",
      "Epoch 76, test loss: 0.186274\n",
      "Epoch 77, test loss: 0.187058\n",
      "Epoch 78, test loss: 0.190934\n",
      "Epoch 79, test loss: 0.187901\n",
      "Epoch 80, test loss: 0.188358\n",
      "Epoch 81, test loss: 0.187931\n",
      "Epoch 82, test loss: 0.191937\n",
      "Epoch 83, test loss: 0.186249\n",
      "Epoch 84, test loss: 0.187268\n",
      "Epoch 85, test loss: 0.188701\n",
      "Epoch 86, test loss: 0.187718\n",
      "Epoch 87, test loss: 0.187081\n",
      "Epoch 88, test loss: 0.188290\n",
      "Epoch 89, test loss: 0.187968\n",
      "Epoch 90, test loss: 0.188689\n",
      "Epoch 91, test loss: 0.187470\n",
      "Epoch 92, test loss: 0.191169\n",
      "Epoch 93, test loss: 0.187969\n",
      "Epoch 94, test loss: 0.189208\n",
      "Epoch 95, test loss: 0.188820\n",
      "Epoch 96, test loss: 0.188485\n",
      "Epoch 97, test loss: 0.190137\n",
      "Epoch 98, test loss: 0.189097\n",
      "Epoch 99, test loss: 0.191168\n",
      "Epoch 100, test loss: 0.189916\n",
      "Epoch 101, test loss: 0.190515\n",
      "Epoch 102, test loss: 0.190670\n",
      "Epoch 103, test loss: 0.191376\n",
      "Epoch 104, test loss: 0.193189\n",
      "Epoch 105, test loss: 0.192879\n",
      "Epoch 106, test loss: 0.189789\n",
      "Epoch 107, test loss: 0.190207\n",
      "Epoch 108, test loss: 0.189370\n",
      "Epoch 109, test loss: 0.189810\n",
      "Epoch 110, test loss: 0.189063\n",
      "Epoch 111, test loss: 0.191173\n",
      "Epoch 112, test loss: 0.194449\n",
      "Epoch 113, test loss: 0.195258\n",
      "Epoch 114, test loss: 0.189207\n",
      "Epoch 115, test loss: 0.189863\n",
      "Epoch 116, test loss: 0.190740\n",
      "Epoch 117, test loss: 0.189184\n",
      "Epoch 118, test loss: 0.189007\n",
      "Epoch 119, test loss: 0.188633\n",
      "Epoch 120, test loss: 0.191132\n",
      "Epoch 121, test loss: 0.190189\n",
      "Epoch 122, test loss: 0.190705\n",
      "Epoch 123, test loss: 0.189668\n",
      "Epoch 124, test loss: 0.190982\n",
      "Epoch 125, test loss: 0.188782\n",
      "Epoch 126, test loss: 0.189340\n",
      "Epoch 127, test loss: 0.188783\n",
      "Epoch 128, test loss: 0.186892\n",
      "Epoch 129, test loss: 0.187741\n",
      "Epoch 130, test loss: 0.186066\n",
      "Epoch 131, test loss: 0.188045\n",
      "Epoch 132, test loss: 0.187594\n",
      "Epoch 133, test loss: 0.189623\n",
      "Epoch 134, test loss: 0.188273\n",
      "Epoch 135, test loss: 0.190505\n",
      "Epoch 136, test loss: 0.186732\n",
      "Epoch 137, test loss: 0.187245\n",
      "Epoch 138, test loss: 0.186464\n",
      "Epoch 139, test loss: 0.187225\n",
      "Epoch 140, test loss: 0.186019\n",
      "Epoch 141, test loss: 0.186707\n",
      "Epoch 142, test loss: 0.186121\n",
      "Epoch 143, test loss: 0.187589\n",
      "Epoch 144, test loss: 0.186235\n",
      "Epoch 145, test loss: 0.186766\n",
      "Epoch 146, test loss: 0.186259\n",
      "Epoch 147, test loss: 0.189951\n",
      "Epoch 148, test loss: 0.191369\n",
      "Epoch 149, test loss: 0.186414\n",
      "Epoch 150, test loss: 0.188826\n",
      "Epoch 151, test loss: 0.186226\n",
      "Epoch 152, test loss: 0.186362\n",
      "Epoch 153, test loss: 0.189253\n",
      "Epoch 154, test loss: 0.187176\n",
      "Epoch 155, test loss: 0.186108\n",
      "Epoch 156, test loss: 0.188631\n",
      "Epoch 157, test loss: 0.187185\n",
      "Epoch 158, test loss: 0.186929\n",
      "Epoch 159, test loss: 0.186709\n",
      "Epoch 160, test loss: 0.186546\n",
      "Epoch 161, test loss: 0.187098\n",
      "Epoch 162, test loss: 0.190754\n",
      "Epoch 163, test loss: 0.186640\n",
      "Epoch 164, test loss: 0.187698\n",
      "Epoch 165, test loss: 0.187041\n",
      "Epoch 166, test loss: 0.186553\n",
      "Epoch 167, test loss: 0.185682\n",
      "Epoch 168, test loss: 0.185770\n",
      "Epoch 169, test loss: 0.188217\n",
      "Epoch 170, test loss: 0.186059\n",
      "Epoch 171, test loss: 0.186825\n",
      "Epoch 172, test loss: 0.186056\n",
      "Epoch 173, test loss: 0.185681\n",
      "Epoch 174, test loss: 0.186468\n",
      "Epoch 175, test loss: 0.186386\n",
      "Epoch 176, test loss: 0.186730\n",
      "Epoch 177, test loss: 0.186777\n",
      "Epoch 178, test loss: 0.186415\n",
      "Epoch 179, test loss: 0.186426\n",
      "Epoch 180, test loss: 0.186551\n",
      "Epoch 181, test loss: 0.186571\n",
      "Epoch 182, test loss: 0.185792\n",
      "Epoch 183, test loss: 0.186086\n",
      "Epoch 184, test loss: 0.185357\n",
      "Epoch 185, test loss: 0.186681\n",
      "Epoch 186, test loss: 0.185269\n",
      "Epoch 187, test loss: 0.186181\n",
      "Epoch 188, test loss: 0.186420\n",
      "Epoch 189, test loss: 0.187890\n",
      "Epoch 190, test loss: 0.185792\n",
      "Epoch 191, test loss: 0.186446\n",
      "Epoch 192, test loss: 0.186179\n",
      "Epoch 193, test loss: 0.185103\n",
      "Epoch 194, test loss: 0.185161\n",
      "Epoch 195, test loss: 0.185571\n",
      "Epoch 196, test loss: 0.187161\n",
      "Epoch 197, test loss: 0.189666\n",
      "Epoch 198, test loss: 0.185183\n",
      "Epoch 199, test loss: 0.185666\n",
      "Epoch 200, test loss: 0.187212\n",
      "Epoch 201, test loss: 0.185906\n",
      "Epoch 202, test loss: 0.186891\n",
      "Epoch 203, test loss: 0.185603\n",
      "Epoch 204, test loss: 0.188307\n",
      "Epoch 205, test loss: 0.185334\n",
      "Epoch 206, test loss: 0.189692\n",
      "Epoch 207, test loss: 0.185823\n",
      "Epoch 208, test loss: 0.187494\n",
      "Epoch 209, test loss: 0.191844\n",
      "Epoch 210, test loss: 0.188316\n",
      "Epoch 211, test loss: 0.186027\n",
      "Epoch 212, test loss: 0.185356\n",
      "Epoch 213, test loss: 0.186000\n",
      "Epoch 214, test loss: 0.186900\n",
      "Epoch 215, test loss: 0.185747\n",
      "Epoch 216, test loss: 0.186958\n",
      "Epoch 217, test loss: 0.187179\n",
      "Epoch 218, test loss: 0.187615\n",
      "Epoch 219, test loss: 0.187321\n",
      "Epoch 220, test loss: 0.186569\n",
      "Epoch 221, test loss: 0.185627\n",
      "Epoch 222, test loss: 0.188137\n",
      "Epoch 223, test loss: 0.188405\n",
      "Epoch 224, test loss: 0.185708\n",
      "Epoch 225, test loss: 0.187733\n",
      "Epoch 226, test loss: 0.186750\n",
      "Epoch 227, test loss: 0.186046\n",
      "Epoch 228, test loss: 0.185410\n",
      "Epoch 229, test loss: 0.184908\n",
      "Epoch 230, test loss: 0.185493\n",
      "Epoch 231, test loss: 0.186505\n",
      "Epoch 232, test loss: 0.185130\n",
      "Epoch 233, test loss: 0.189913\n",
      "Epoch 234, test loss: 0.185711\n",
      "Epoch 235, test loss: 0.187510\n",
      "Epoch 236, test loss: 0.185386\n",
      "Epoch 237, test loss: 0.187529\n",
      "Epoch 238, test loss: 0.187070\n",
      "Epoch 239, test loss: 0.185831\n",
      "Epoch 240, test loss: 0.185197\n",
      "Epoch 241, test loss: 0.185697\n",
      "Epoch 242, test loss: 0.185391\n",
      "Epoch 243, test loss: 0.186619\n",
      "Epoch 244, test loss: 0.185560\n",
      "Epoch 245, test loss: 0.188373\n",
      "Epoch 246, test loss: 0.185533\n",
      "Epoch 247, test loss: 0.186147\n",
      "Epoch 248, test loss: 0.187968\n",
      "Epoch 249, test loss: 0.185077\n",
      "Epoch 250, test loss: 0.187366\n",
      "Epoch 251, test loss: 0.185532\n",
      "Epoch 252, test loss: 0.184837\n",
      "Epoch 253, test loss: 0.185285\n",
      "Epoch 254, test loss: 0.187665\n",
      "Epoch 255, test loss: 0.187676\n",
      "Epoch 256, test loss: 0.186437\n",
      "Epoch 257, test loss: 0.185324\n",
      "Epoch 258, test loss: 0.185158\n",
      "Epoch 259, test loss: 0.185585\n",
      "Epoch 260, test loss: 0.185163\n",
      "Epoch 261, test loss: 0.184983\n",
      "Epoch 262, test loss: 0.184431\n",
      "Epoch 263, test loss: 0.185713\n",
      "Epoch 264, test loss: 0.186469\n",
      "Epoch 265, test loss: 0.185711\n",
      "Epoch 266, test loss: 0.185834\n",
      "Epoch 267, test loss: 0.185564\n",
      "Epoch 268, test loss: 0.184407\n",
      "Epoch 269, test loss: 0.186624\n",
      "Epoch 270, test loss: 0.185275\n",
      "Epoch 271, test loss: 0.184887\n",
      "Epoch 272, test loss: 0.184229\n",
      "Epoch 273, test loss: 0.184645\n",
      "Epoch 274, test loss: 0.186552\n",
      "Epoch 275, test loss: 0.186712\n",
      "Epoch 276, test loss: 0.185949\n",
      "Epoch 277, test loss: 0.184704\n",
      "Epoch 278, test loss: 0.184194\n",
      "Epoch 279, test loss: 0.184862\n",
      "Epoch 280, test loss: 0.185931\n",
      "Epoch 281, test loss: 0.186288\n",
      "Epoch 282, test loss: 0.185776\n",
      "Epoch 283, test loss: 0.185776\n",
      "Epoch 284, test loss: 0.185246\n",
      "Epoch 285, test loss: 0.185655\n",
      "Epoch 286, test loss: 0.185604\n",
      "Epoch 287, test loss: 0.184048\n",
      "Epoch 288, test loss: 0.184924\n",
      "Epoch 289, test loss: 0.184150\n",
      "Epoch 290, test loss: 0.185153\n",
      "Epoch 291, test loss: 0.183873\n",
      "Epoch 292, test loss: 0.187235\n",
      "Epoch 293, test loss: 0.185453\n",
      "Epoch 294, test loss: 0.183978\n",
      "Epoch 295, test loss: 0.183655\n",
      "Epoch 296, test loss: 0.184894\n",
      "Epoch 297, test loss: 0.184115\n",
      "Epoch 298, test loss: 0.185102\n",
      "Epoch 299, test loss: 0.185847\n",
      "Epoch 300, test loss: 0.187809\n",
      "Epoch 301, test loss: 0.185224\n",
      "Epoch 302, test loss: 0.186330\n",
      "Epoch 303, test loss: 0.185016\n",
      "Epoch 304, test loss: 0.187753\n",
      "Epoch 305, test loss: 0.184687\n",
      "Epoch 306, test loss: 0.184423\n",
      "Epoch 307, test loss: 0.184554\n",
      "Epoch 308, test loss: 0.186700\n",
      "Epoch 309, test loss: 0.185313\n",
      "Epoch 310, test loss: 0.184847\n",
      "Epoch 311, test loss: 0.186640\n",
      "Epoch 312, test loss: 0.185274\n",
      "Epoch 313, test loss: 0.185140\n",
      "Epoch 314, test loss: 0.185199\n",
      "Epoch 315, test loss: 0.185243\n",
      "Epoch 316, test loss: 0.185346\n",
      "Epoch 317, test loss: 0.187992\n",
      "Epoch 318, test loss: 0.185540\n",
      "Epoch 319, test loss: 0.185853\n",
      "Epoch 320, test loss: 0.185809\n",
      "Epoch 321, test loss: 0.184782\n",
      "Epoch 322, test loss: 0.186586\n",
      "Epoch 323, test loss: 0.185960\n",
      "Epoch 324, test loss: 0.186803\n",
      "Epoch 325, test loss: 0.186123\n",
      "Epoch 326, test loss: 0.185394\n",
      "Epoch 327, test loss: 0.184355\n",
      "Epoch 328, test loss: 0.187566\n",
      "Epoch 329, test loss: 0.184592\n",
      "Epoch 330, test loss: 0.186077\n",
      "Epoch 331, test loss: 0.187048\n",
      "Epoch 332, test loss: 0.185867\n",
      "Epoch 333, test loss: 0.184575\n",
      "Epoch 334, test loss: 0.183690\n",
      "Epoch 335, test loss: 0.185690\n",
      "Epoch 336, test loss: 0.185562\n",
      "Epoch 337, test loss: 0.183406\n",
      "Epoch 338, test loss: 0.186107\n",
      "Epoch 339, test loss: 0.184088\n",
      "Epoch 340, test loss: 0.185721\n",
      "Epoch 341, test loss: 0.185267\n",
      "Epoch 342, test loss: 0.184460\n",
      "Epoch 343, test loss: 0.186528\n",
      "Epoch 344, test loss: 0.186862\n",
      "Epoch 345, test loss: 0.185667\n",
      "Epoch 346, test loss: 0.183992\n",
      "Epoch 347, test loss: 0.186353\n",
      "Epoch 348, test loss: 0.187067\n",
      "Epoch 349, test loss: 0.184741\n",
      "Epoch 350, test loss: 0.190601\n",
      "Epoch 351, test loss: 0.185362\n",
      "Epoch 352, test loss: 0.185589\n",
      "Epoch 353, test loss: 0.183577\n",
      "Epoch 354, test loss: 0.183588\n",
      "Epoch 355, test loss: 0.184544\n",
      "Epoch 356, test loss: 0.184090\n",
      "Epoch 357, test loss: 0.185381\n",
      "Epoch 358, test loss: 0.184514\n",
      "Epoch 359, test loss: 0.189349\n",
      "Epoch 360, test loss: 0.184788\n",
      "Epoch 361, test loss: 0.185391\n",
      "Epoch 362, test loss: 0.184115\n",
      "Epoch 363, test loss: 0.187269\n",
      "Epoch 364, test loss: 0.186674\n",
      "Epoch 365, test loss: 0.190218\n",
      "Epoch 366, test loss: 0.184305\n",
      "Epoch 367, test loss: 0.185388\n",
      "Epoch 368, test loss: 0.185108\n",
      "Epoch 369, test loss: 0.184578\n",
      "Epoch 370, test loss: 0.186795\n",
      "Epoch 371, test loss: 0.184026\n",
      "Epoch 372, test loss: 0.184689\n",
      "Epoch 373, test loss: 0.185910\n",
      "Epoch 374, test loss: 0.183789\n",
      "Epoch 375, test loss: 0.185277\n",
      "Epoch 376, test loss: 0.183154\n",
      "Epoch 377, test loss: 0.184602\n",
      "Epoch 378, test loss: 0.184600\n",
      "Epoch 379, test loss: 0.184218\n",
      "Epoch 380, test loss: 0.183247\n",
      "Epoch 381, test loss: 0.184147\n",
      "Epoch 382, test loss: 0.185062\n",
      "Epoch 383, test loss: 0.184870\n",
      "Epoch 384, test loss: 0.186734\n",
      "Epoch 385, test loss: 0.182895\n",
      "Epoch 386, test loss: 0.185217\n",
      "Epoch 387, test loss: 0.184475\n",
      "Epoch 388, test loss: 0.183906\n",
      "Epoch 389, test loss: 0.184352\n",
      "Epoch 390, test loss: 0.184895\n",
      "Epoch 391, test loss: 0.186566\n",
      "Epoch 392, test loss: 0.184797\n",
      "Epoch 393, test loss: 0.183627\n",
      "Epoch 394, test loss: 0.185554\n",
      "Epoch 395, test loss: 0.183968\n",
      "Epoch 396, test loss: 0.183536\n",
      "Epoch 397, test loss: 0.183844\n",
      "Epoch 398, test loss: 0.184762\n",
      "Epoch 399, test loss: 0.184316\n",
      "Epoch 400, test loss: 0.184877\n",
      "Epoch 401, test loss: 0.184375\n",
      "Epoch 402, test loss: 0.183465\n",
      "Epoch 403, test loss: 0.185322\n",
      "Epoch 404, test loss: 0.183555\n",
      "Epoch 405, test loss: 0.183646\n",
      "Epoch 406, test loss: 0.184656\n",
      "Epoch 407, test loss: 0.184111\n",
      "Epoch 408, test loss: 0.184184\n",
      "Epoch 409, test loss: 0.183663\n",
      "Epoch 410, test loss: 0.184030\n",
      "Epoch 411, test loss: 0.184031\n",
      "Epoch 412, test loss: 0.184784\n",
      "Epoch 413, test loss: 0.183744\n",
      "Epoch 414, test loss: 0.183475\n",
      "Epoch 415, test loss: 0.183888\n",
      "Epoch 416, test loss: 0.185420\n",
      "Epoch 417, test loss: 0.183840\n",
      "Epoch 418, test loss: 0.184118\n",
      "Epoch 419, test loss: 0.185421\n",
      "Epoch 420, test loss: 0.184306\n",
      "Epoch 421, test loss: 0.184691\n",
      "Epoch 422, test loss: 0.183926\n",
      "Epoch 423, test loss: 0.186258\n",
      "Epoch 424, test loss: 0.190197\n",
      "Epoch 425, test loss: 0.186123\n",
      "Epoch 426, test loss: 0.188258\n",
      "Epoch 427, test loss: 0.186884\n",
      "Epoch 428, test loss: 0.185530\n",
      "Epoch 429, test loss: 0.183797\n",
      "Epoch 430, test loss: 0.183635\n",
      "Epoch 431, test loss: 0.185346\n",
      "Epoch 432, test loss: 0.183064\n",
      "Epoch 433, test loss: 0.183160\n",
      "Epoch 434, test loss: 0.185434\n",
      "Epoch 435, test loss: 0.187267\n",
      "Epoch 436, test loss: 0.189726\n",
      "Epoch 437, test loss: 0.184749\n",
      "Epoch 438, test loss: 0.184313\n",
      "Epoch 439, test loss: 0.184918\n",
      "Epoch 440, test loss: 0.184012\n",
      "Epoch 441, test loss: 0.184436\n",
      "Epoch 442, test loss: 0.183799\n",
      "Epoch 443, test loss: 0.186894\n",
      "Epoch 444, test loss: 0.184192\n",
      "Epoch 445, test loss: 0.184155\n",
      "Epoch 446, test loss: 0.183437\n",
      "Epoch 447, test loss: 0.184469\n",
      "Epoch 448, test loss: 0.184500\n",
      "Epoch 449, test loss: 0.183899\n",
      "Pretrain data: 19365644.0\n",
      "Building dataset, requesting data from 0 to 821\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7865/107110\n",
      "Found 821 continuous time series\n",
      "Data shape: (114977, 6), Train/test: 114975/2\n",
      "Train test ratio: 57487.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1796\n",
      "Epoch 0, train loss: 0.234752\n",
      "Epoch 1, train loss: 0.373092\n",
      "Epoch 2, train loss: 0.217797\n",
      "Epoch 3, train loss: 0.216796\n",
      "Epoch 4, train loss: 0.168799\n",
      "Epoch 5, train loss: 0.256278\n",
      "Epoch 6, train loss: 0.188621\n",
      "Epoch 7, train loss: 0.250606\n",
      "Epoch 8, train loss: 0.225796\n",
      "Epoch 9, train loss: 0.212564\n",
      "Epoch 10, train loss: 0.195287\n",
      "Epoch 11, train loss: 0.192563\n",
      "Epoch 12, train loss: 0.275673\n",
      "Epoch 13, train loss: 0.178714\n",
      "Epoch 14, train loss: 0.180610\n",
      "Epoch 15, train loss: 0.217219\n",
      "Epoch 16, train loss: 0.214414\n",
      "Epoch 17, train loss: 0.224247\n",
      "Epoch 18, train loss: 0.249071\n",
      "Epoch 19, train loss: 0.166662\n",
      "Epoch 20, train loss: 0.205109\n",
      "Epoch 21, train loss: 0.157367\n",
      "Epoch 22, train loss: 0.196702\n",
      "Epoch 23, train loss: 0.238348\n",
      "Epoch 24, train loss: 0.195990\n",
      "Epoch 25, train loss: 0.192891\n",
      "Epoch 26, train loss: 0.224122\n",
      "Epoch 27, train loss: 0.206353\n",
      "Epoch 28, train loss: 0.235826\n",
      "Epoch 29, train loss: 0.294640\n",
      "Epoch 30, train loss: 0.171565\n",
      "Epoch 31, train loss: 0.186498\n",
      "Epoch 32, train loss: 0.180978\n",
      "Epoch 33, train loss: 0.227936\n",
      "Epoch 34, train loss: 0.140229\n",
      "Epoch 35, train loss: 0.183553\n",
      "Epoch 36, train loss: 0.231859\n",
      "Epoch 37, train loss: 0.193211\n",
      "Epoch 38, train loss: 0.203389\n",
      "Epoch 39, train loss: 0.152064\n",
      "Epoch 40, train loss: 0.193163\n",
      "Epoch 41, train loss: 0.211822\n",
      "Epoch 42, train loss: 0.173424\n",
      "Epoch 43, train loss: 0.163330\n",
      "Epoch 44, train loss: 0.208818\n",
      "Epoch 45, train loss: 0.182770\n",
      "Epoch 46, train loss: 0.253520\n",
      "Epoch 47, train loss: 0.273923\n",
      "Epoch 48, train loss: 0.241191\n",
      "Epoch 49, train loss: 0.168770\n",
      "Epoch 50, train loss: 0.225883\n",
      "Epoch 51, train loss: 0.194370\n",
      "Epoch 52, train loss: 0.215612\n",
      "Epoch 53, train loss: 0.227824\n",
      "Epoch 54, train loss: 0.273781\n",
      "Epoch 55, train loss: 0.210827\n",
      "Epoch 56, train loss: 0.220901\n",
      "Epoch 57, train loss: 0.203678\n",
      "Epoch 58, train loss: 0.192546\n",
      "Epoch 59, train loss: 0.224723\n",
      "Epoch 60, train loss: 0.214153\n",
      "Epoch 61, train loss: 0.180370\n",
      "Epoch 62, train loss: 0.269520\n",
      "Epoch 63, train loss: 0.209090\n",
      "Epoch 64, train loss: 0.204401\n",
      "Epoch 65, train loss: 0.253770\n",
      "Epoch 66, train loss: 0.184555\n",
      "Epoch 67, train loss: 0.255702\n",
      "Epoch 68, train loss: 0.191938\n",
      "Epoch 69, train loss: 0.218817\n",
      "Epoch 70, train loss: 0.247862\n",
      "Epoch 71, train loss: 0.230735\n",
      "Epoch 72, train loss: 0.174438\n",
      "Epoch 73, train loss: 0.212501\n",
      "Epoch 74, train loss: 0.199703\n",
      "Epoch 75, train loss: 0.301306\n",
      "Epoch 76, train loss: 0.273703\n",
      "Epoch 77, train loss: 0.205884\n",
      "Epoch 78, train loss: 0.222468\n",
      "Epoch 79, train loss: 0.212017\n",
      "Epoch 80, train loss: 0.237435\n",
      "Epoch 81, train loss: 0.229000\n",
      "Epoch 82, train loss: 0.211084\n",
      "Epoch 83, train loss: 0.218320\n",
      "Epoch 84, train loss: 0.176188\n",
      "Epoch 85, train loss: 0.133872\n",
      "Epoch 86, train loss: 0.238603\n",
      "Epoch 87, train loss: 0.228829\n",
      "Epoch 88, train loss: 0.176594\n",
      "Epoch 89, train loss: 0.202413\n",
      "Epoch 90, train loss: 0.187239\n",
      "Epoch 91, train loss: 0.244860\n",
      "Epoch 92, train loss: 0.190919\n",
      "Epoch 93, train loss: 0.192572\n",
      "Epoch 94, train loss: 0.208585\n",
      "Epoch 95, train loss: 0.186604\n",
      "Epoch 96, train loss: 0.182184\n",
      "Epoch 97, train loss: 0.213428\n",
      "Epoch 98, train loss: 0.232290\n",
      "Epoch 99, train loss: 0.187828\n",
      "Epoch 100, train loss: 0.183970\n",
      "Epoch 101, train loss: 0.202749\n",
      "Epoch 102, train loss: 0.228967\n",
      "Epoch 103, train loss: 0.228297\n",
      "Epoch 104, train loss: 0.231337\n",
      "Epoch 105, train loss: 0.200641\n",
      "Epoch 106, train loss: 0.265130\n",
      "Epoch 107, train loss: 0.206415\n",
      "Epoch 108, train loss: 0.200685\n",
      "Epoch 109, train loss: 0.200669\n",
      "Epoch 110, train loss: 0.195160\n",
      "Epoch 111, train loss: 0.211629\n",
      "Epoch 112, train loss: 0.224675\n",
      "Epoch 113, train loss: 0.234128\n",
      "Epoch 114, train loss: 0.198832\n",
      "Epoch 115, train loss: 0.185156\n",
      "Epoch 116, train loss: 0.234108\n",
      "Epoch 117, train loss: 0.192491\n",
      "Epoch 118, train loss: 0.152582\n",
      "Epoch 119, train loss: 0.178830\n",
      "Epoch 120, train loss: 0.195496\n",
      "Epoch 121, train loss: 0.184739\n",
      "Epoch 122, train loss: 0.191617\n",
      "Epoch 123, train loss: 0.206335\n",
      "Epoch 124, train loss: 0.245060\n",
      "Epoch 125, train loss: 0.242387\n",
      "Epoch 126, train loss: 0.190117\n",
      "Epoch 127, train loss: 0.191284\n",
      "Epoch 128, train loss: 0.263961\n",
      "Epoch 129, train loss: 0.161203\n",
      "Epoch 130, train loss: 0.173402\n",
      "Epoch 131, train loss: 0.220786\n",
      "Epoch 132, train loss: 0.227454\n",
      "Epoch 133, train loss: 0.177069\n",
      "Epoch 134, train loss: 0.210321\n",
      "Epoch 135, train loss: 0.194849\n",
      "Epoch 136, train loss: 0.202291\n",
      "Epoch 137, train loss: 0.249914\n",
      "Epoch 138, train loss: 0.204653\n",
      "Epoch 139, train loss: 0.180104\n",
      "Epoch 140, train loss: 0.210083\n",
      "Epoch 141, train loss: 0.212506\n",
      "Epoch 142, train loss: 0.197211\n",
      "Epoch 143, train loss: 0.196076\n",
      "Epoch 144, train loss: 0.264769\n",
      "Epoch 145, train loss: 0.190638\n",
      "Epoch 146, train loss: 0.191145\n",
      "Epoch 147, train loss: 0.202467\n",
      "Epoch 148, train loss: 0.166165\n",
      "Epoch 149, train loss: 0.178094\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "x here is\n",
      "[[135. 143. 152. 159. 166. 172.]\n",
      " [143. 152. 159. 166. 172. 178.]\n",
      " [152. 159. 166. 172. 178. 184.]\n",
      " ...\n",
      " [203. 205. 208. 204. 202. 201.]\n",
      " [205. 208. 204. 202. 201. 201.]\n",
      " [208. 204. 202. 201. 201. 201.]]\n",
      "y here is\n",
      "[[203. 203. 203. 203. 203. 203.]\n",
      " [204. 204. 204. 204. 204. 204.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " ...\n",
      " [212. 212. 212. 212. 212. 212.]\n",
      " [218. 218. 218. 218. 218. 218.]\n",
      " [224. 224. 224. 224. 224. 224.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (2635, 6), Train/test: 1/2634\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 21 segments\n",
      "Building dataset, requesting data from 0 to 21\n",
      "x here is\n",
      "[[101. 100. 100.  99.  98.  98.]\n",
      " [100. 100.  99.  98.  98.  95.]\n",
      " [100.  99.  98.  98.  95.  94.]\n",
      " ...\n",
      " [ 71.  73.  74.  74.  76.  79.]\n",
      " [ 73.  74.  74.  76.  79.  87.]\n",
      " [ 74.  74.  76.  79.  87.  95.]]\n",
      "y here is\n",
      "[[ 86.  86.  86.  86.  86.  86.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " ...\n",
      " [120. 120. 120. 120. 120. 120.]\n",
      " [123. 123. 123. 123. 123. 123.]\n",
      " [128. 128. 128. 128. 128. 128.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 420/10329\n",
      "Found 21 continuous time series\n",
      "Data shape: (10751, 6), Train/test: 10749/2\n",
      "Train test ratio: 5374.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61089D810>\n",
      "Epoch 0, test loss: 0.158507\n",
      "Epoch 1, test loss: 0.176601\n",
      "Epoch 2, test loss: 0.158987\n",
      "Epoch 3, test loss: 0.158678\n",
      "Epoch 4, test loss: 0.159260\n",
      "Epoch 5, test loss: 0.160818\n",
      "Epoch 6, test loss: 0.161522\n",
      "Epoch 7, test loss: 0.164812\n",
      "Epoch 8, test loss: 0.162011\n",
      "Epoch 9, test loss: 0.158941\n",
      "Epoch 10, test loss: 0.159017\n",
      "Epoch 11, test loss: 0.159527\n",
      "Epoch 12, test loss: 0.165368\n",
      "Epoch 13, test loss: 0.163526\n",
      "Epoch 14, test loss: 0.165853\n",
      "Epoch 15, test loss: 0.159190\n",
      "Epoch 16, test loss: 0.161643\n",
      "Epoch 17, test loss: 0.160992\n",
      "Epoch 18, test loss: 0.158882\n",
      "Epoch 19, test loss: 0.159870\n",
      "Epoch 20, test loss: 0.168668\n",
      "Epoch 21, test loss: 0.161000\n",
      "Epoch 22, test loss: 0.158261\n",
      "Epoch 23, test loss: 0.163090\n",
      "Epoch 24, test loss: 0.158187\n",
      "Epoch 25, test loss: 0.162342\n",
      "Epoch 26, test loss: 0.166718\n",
      "Epoch 27, test loss: 0.158543\n",
      "Epoch 28, test loss: 0.161241\n",
      "Epoch 29, test loss: 0.159908\n",
      "Epoch 30, test loss: 0.162706\n",
      "Epoch 31, test loss: 0.164127\n",
      "Epoch 32, test loss: 0.159469\n",
      "Epoch 33, test loss: 0.158456\n",
      "Epoch 34, test loss: 0.159160\n",
      "Epoch 35, test loss: 0.159411\n",
      "Epoch 36, test loss: 0.163974\n",
      "Epoch 37, test loss: 0.161020\n",
      "Epoch 38, test loss: 0.160626\n",
      "Epoch 39, test loss: 0.159461\n",
      "Epoch 40, test loss: 0.160152\n",
      "Epoch 41, test loss: 0.158118\n",
      "Epoch 42, test loss: 0.172852\n",
      "Epoch 43, test loss: 0.158600\n",
      "Epoch 44, test loss: 0.158288\n",
      "Epoch 45, test loss: 0.169612\n",
      "Epoch 46, test loss: 0.159065\n",
      "Epoch 47, test loss: 0.159861\n",
      "Epoch 48, test loss: 0.160529\n",
      "Epoch 49, test loss: 0.158520\n",
      "Epoch 50, test loss: 0.159228\n",
      "Epoch 51, test loss: 0.161134\n",
      "Epoch 52, test loss: 0.164175\n",
      "Epoch 53, test loss: 0.160083\n",
      "Epoch 54, test loss: 0.158841\n",
      "Epoch 55, test loss: 0.158525\n",
      "Epoch 56, test loss: 0.158962\n",
      "Epoch 57, test loss: 0.160991\n",
      "Epoch 58, test loss: 0.158734\n",
      "Epoch 59, test loss: 0.166687\n",
      "Epoch 60, test loss: 0.158694\n",
      "Epoch 61, test loss: 0.161594\n",
      "Epoch 62, test loss: 0.165674\n",
      "Epoch 63, test loss: 0.158981\n",
      "Epoch 64, test loss: 0.160941\n",
      "Epoch 65, test loss: 0.167124\n",
      "Epoch 66, test loss: 0.164819\n",
      "Epoch 67, test loss: 0.161151\n",
      "Epoch 68, test loss: 0.160784\n",
      "Epoch 69, test loss: 0.158884\n",
      "Epoch 70, test loss: 0.166100\n",
      "Epoch 71, test loss: 0.158816\n",
      "Epoch 72, test loss: 0.162615\n",
      "Epoch 73, test loss: 0.162121\n",
      "Epoch 74, test loss: 0.163629\n",
      "Epoch 75, test loss: 0.163081\n",
      "Epoch 76, test loss: 0.160314\n",
      "Epoch 77, test loss: 0.159856\n",
      "Epoch 78, test loss: 0.166080\n",
      "Epoch 79, test loss: 0.166061\n",
      "Epoch 80, test loss: 0.168018\n",
      "Epoch 81, test loss: 0.158798\n",
      "Epoch 82, test loss: 0.161562\n",
      "Epoch 83, test loss: 0.158909\n",
      "Epoch 84, test loss: 0.160832\n",
      "Epoch 85, test loss: 0.159761\n",
      "Epoch 86, test loss: 0.164381\n",
      "Epoch 87, test loss: 0.160600\n",
      "Epoch 88, test loss: 0.158794\n",
      "Epoch 89, test loss: 0.159201\n",
      "Epoch 90, test loss: 0.159521\n",
      "Epoch 91, test loss: 0.160144\n",
      "Epoch 92, test loss: 0.161304\n",
      "Epoch 93, test loss: 0.159666\n",
      "Epoch 94, test loss: 0.161282\n",
      "Epoch 95, test loss: 0.160707\n",
      "Epoch 96, test loss: 0.162867\n",
      "Epoch 97, test loss: 0.161193\n",
      "Epoch 98, test loss: 0.159045\n",
      "Epoch 99, test loss: 0.162264\n",
      "Epoch 100, test loss: 0.160226\n",
      "Epoch 101, test loss: 0.158838\n",
      "Epoch 102, test loss: 0.159505\n",
      "Epoch 103, test loss: 0.160426\n",
      "Epoch 104, test loss: 0.159311\n",
      "Epoch 105, test loss: 0.158970\n",
      "Epoch 106, test loss: 0.159699\n",
      "Epoch 107, test loss: 0.162957\n",
      "Epoch 108, test loss: 0.160970\n",
      "Epoch 109, test loss: 0.161006\n",
      "Epoch 110, test loss: 0.160914\n",
      "Epoch 111, test loss: 0.158957\n",
      "Epoch 112, test loss: 0.160558\n",
      "Epoch 113, test loss: 0.160300\n",
      "Epoch 114, test loss: 0.160026\n",
      "Epoch 115, test loss: 0.161116\n",
      "Epoch 116, test loss: 0.162312\n",
      "Epoch 117, test loss: 0.159434\n",
      "Epoch 118, test loss: 0.159631\n",
      "Epoch 119, test loss: 0.162689\n",
      "Epoch 120, test loss: 0.159728\n",
      "Epoch 121, test loss: 0.160634\n",
      "Epoch 122, test loss: 0.161218\n",
      "Epoch 123, test loss: 0.159474\n",
      "Epoch 124, test loss: 0.159533\n",
      "Epoch 125, test loss: 0.167552\n",
      "Epoch 126, test loss: 0.159054\n",
      "Epoch 127, test loss: 0.162164\n",
      "Epoch 128, test loss: 0.159955\n",
      "Epoch 129, test loss: 0.165668\n",
      "Epoch 130, test loss: 0.159552\n",
      "Epoch 131, test loss: 0.162054\n",
      "Epoch 132, test loss: 0.161032\n",
      "Epoch 133, test loss: 0.163145\n",
      "Epoch 134, test loss: 0.162393\n",
      "Epoch 135, test loss: 0.159151\n",
      "Epoch 136, test loss: 0.163614\n",
      "Epoch 137, test loss: 0.169184\n",
      "Epoch 138, test loss: 0.160214\n",
      "Epoch 139, test loss: 0.161052\n",
      "Epoch 140, test loss: 0.163779\n",
      "Epoch 141, test loss: 0.159025\n",
      "Epoch 142, test loss: 0.163943\n",
      "Epoch 143, test loss: 0.159720\n",
      "Epoch 144, test loss: 0.167428\n",
      "Epoch 145, test loss: 0.161366\n",
      "Epoch 146, test loss: 0.161121\n",
      "Epoch 147, test loss: 0.159772\n",
      "Epoch 148, test loss: 0.158706\n",
      "Epoch 149, test loss: 0.158889\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61089D810>\n",
      "Epoch 0, test loss: 0.158881\n",
      "Epoch 1, test loss: 0.158617\n",
      "Epoch 2, test loss: 0.158650\n",
      "Epoch 3, test loss: 0.172077\n",
      "Epoch 4, test loss: 0.160862\n",
      "Epoch 5, test loss: 0.168930\n",
      "Epoch 6, test loss: 0.159447\n",
      "Epoch 7, test loss: 0.161703\n",
      "Epoch 8, test loss: 0.159106\n",
      "Epoch 9, test loss: 0.161097\n",
      "Epoch 10, test loss: 0.159780\n",
      "Epoch 11, test loss: 0.159965\n",
      "Epoch 12, test loss: 0.161122\n",
      "Epoch 13, test loss: 0.159616\n",
      "Epoch 14, test loss: 0.158945\n",
      "Epoch 15, test loss: 0.159970\n",
      "Epoch 16, test loss: 0.159879\n",
      "Epoch 17, test loss: 0.160111\n",
      "Epoch 18, test loss: 0.159732\n",
      "Epoch 19, test loss: 0.159560\n",
      "Epoch 20, test loss: 0.159703\n",
      "Epoch 21, test loss: 0.159400\n",
      "Epoch 22, test loss: 0.162142\n",
      "Epoch 23, test loss: 0.160607\n",
      "Epoch 24, test loss: 0.160603\n",
      "Epoch 25, test loss: 0.158829\n",
      "Epoch 26, test loss: 0.165784\n",
      "Epoch 27, test loss: 0.162252\n",
      "Epoch 28, test loss: 0.161397\n",
      "Epoch 29, test loss: 0.159442\n",
      "Epoch 30, test loss: 0.162550\n",
      "Epoch 31, test loss: 0.159089\n",
      "Epoch 32, test loss: 0.159825\n",
      "Epoch 33, test loss: 0.160612\n",
      "Epoch 34, test loss: 0.169192\n",
      "Epoch 35, test loss: 0.159520\n",
      "Epoch 36, test loss: 0.160559\n",
      "Epoch 37, test loss: 0.159084\n",
      "Epoch 38, test loss: 0.163214\n",
      "Epoch 39, test loss: 0.159373\n",
      "Epoch 40, test loss: 0.164496\n",
      "Epoch 41, test loss: 0.166359\n",
      "Epoch 42, test loss: 0.161183\n",
      "Epoch 43, test loss: 0.159117\n",
      "Epoch 44, test loss: 0.161494\n",
      "Epoch 45, test loss: 0.159002\n",
      "Epoch 46, test loss: 0.161697\n",
      "Epoch 47, test loss: 0.159941\n",
      "Epoch 48, test loss: 0.160036\n",
      "Epoch 49, test loss: 0.160715\n",
      "Epoch 50, test loss: 0.159842\n",
      "Epoch 51, test loss: 0.161240\n",
      "Epoch 52, test loss: 0.159870\n",
      "Epoch 53, test loss: 0.159349\n",
      "Epoch 54, test loss: 0.165874\n",
      "Epoch 55, test loss: 0.165007\n",
      "Epoch 56, test loss: 0.162519\n",
      "Epoch 57, test loss: 0.167980\n",
      "Epoch 58, test loss: 0.161117\n",
      "Epoch 59, test loss: 0.159371\n",
      "Epoch 60, test loss: 0.160345\n",
      "Epoch 61, test loss: 0.163465\n",
      "Epoch 62, test loss: 0.159359\n",
      "Epoch 63, test loss: 0.160776\n",
      "Epoch 64, test loss: 0.160012\n",
      "Epoch 65, test loss: 0.160553\n",
      "Epoch 66, test loss: 0.163254\n",
      "Epoch 67, test loss: 0.159380\n",
      "Epoch 68, test loss: 0.159270\n",
      "Epoch 69, test loss: 0.164089\n",
      "Epoch 70, test loss: 0.159248\n",
      "Epoch 71, test loss: 0.160275\n",
      "Epoch 72, test loss: 0.159948\n",
      "Epoch 73, test loss: 0.159478\n",
      "Epoch 74, test loss: 0.160456\n",
      "Epoch 75, test loss: 0.160222\n",
      "Epoch 76, test loss: 0.160878\n",
      "Epoch 77, test loss: 0.159960\n",
      "Epoch 78, test loss: 0.159771\n",
      "Epoch 79, test loss: 0.159930\n",
      "Epoch 80, test loss: 0.159537\n",
      "Epoch 81, test loss: 0.159849\n",
      "Epoch 82, test loss: 0.159579\n",
      "Epoch 83, test loss: 0.159965\n",
      "Epoch 84, test loss: 0.159814\n",
      "Epoch 85, test loss: 0.164262\n",
      "Epoch 86, test loss: 0.160744\n",
      "Epoch 87, test loss: 0.160041\n",
      "Epoch 88, test loss: 0.161350\n",
      "Epoch 89, test loss: 0.173048\n",
      "Epoch 90, test loss: 0.160631\n",
      "Epoch 91, test loss: 0.159586\n",
      "Epoch 92, test loss: 0.159964\n",
      "Epoch 93, test loss: 0.162890\n",
      "Epoch 94, test loss: 0.164278\n",
      "Epoch 95, test loss: 0.159328\n",
      "Epoch 96, test loss: 0.166327\n",
      "Epoch 97, test loss: 0.159854\n",
      "Epoch 98, test loss: 0.160183\n",
      "Epoch 99, test loss: 0.161142\n",
      "Epoch 100, test loss: 0.167362\n",
      "Epoch 101, test loss: 0.160298\n",
      "Epoch 102, test loss: 0.167840\n",
      "Epoch 103, test loss: 0.160154\n",
      "Epoch 104, test loss: 0.160469\n",
      "Epoch 105, test loss: 0.161450\n",
      "Epoch 106, test loss: 0.161901\n",
      "Epoch 107, test loss: 0.165080\n",
      "Epoch 108, test loss: 0.160649\n",
      "Epoch 109, test loss: 0.165013\n",
      "Epoch 110, test loss: 0.160073\n",
      "Epoch 111, test loss: 0.160298\n",
      "Epoch 112, test loss: 0.159422\n",
      "Epoch 113, test loss: 0.163009\n",
      "Epoch 114, test loss: 0.160040\n",
      "Epoch 115, test loss: 0.160913\n",
      "Epoch 116, test loss: 0.161180\n",
      "Epoch 117, test loss: 0.160427\n",
      "Epoch 118, test loss: 0.161433\n",
      "Epoch 119, test loss: 0.172412\n",
      "Epoch 120, test loss: 0.159689\n",
      "Epoch 121, test loss: 0.165134\n",
      "Epoch 122, test loss: 0.163220\n",
      "Epoch 123, test loss: 0.161950\n",
      "Epoch 124, test loss: 0.160803\n",
      "Epoch 125, test loss: 0.159446\n",
      "Epoch 126, test loss: 0.164151\n",
      "Epoch 127, test loss: 0.160743\n",
      "Epoch 128, test loss: 0.159854\n",
      "Epoch 129, test loss: 0.161555\n",
      "Epoch 130, test loss: 0.160094\n",
      "Epoch 131, test loss: 0.166552\n",
      "Epoch 132, test loss: 0.160958\n",
      "Epoch 133, test loss: 0.159824\n",
      "Epoch 134, test loss: 0.159546\n",
      "Epoch 135, test loss: 0.160729\n",
      "Epoch 136, test loss: 0.162321\n",
      "Epoch 137, test loss: 0.162325\n",
      "Epoch 138, test loss: 0.161601\n",
      "Epoch 139, test loss: 0.164823\n",
      "Epoch 140, test loss: 0.162968\n",
      "Epoch 141, test loss: 0.161188\n",
      "Epoch 142, test loss: 0.160289\n",
      "Epoch 143, test loss: 0.159633\n",
      "Epoch 144, test loss: 0.160138\n",
      "Epoch 145, test loss: 0.159929\n",
      "Epoch 146, test loss: 0.161646\n",
      "Epoch 147, test loss: 0.161042\n",
      "Epoch 148, test loss: 0.161727\n",
      "Epoch 149, test loss: 0.162264\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61089D810>\n",
      "Epoch 0, test loss: 0.182350\n",
      "Epoch 1, test loss: 0.171654\n",
      "Epoch 2, test loss: 0.163259\n",
      "Epoch 3, test loss: 0.162963\n",
      "Epoch 4, test loss: 0.162754\n",
      "Epoch 5, test loss: 0.163040\n",
      "Epoch 6, test loss: 0.163372\n",
      "Epoch 7, test loss: 0.165783\n",
      "Epoch 8, test loss: 0.163699\n",
      "Epoch 9, test loss: 0.165139\n",
      "Epoch 10, test loss: 0.162149\n",
      "Epoch 11, test loss: 0.162408\n",
      "Epoch 12, test loss: 0.171202\n",
      "Epoch 13, test loss: 0.161878\n",
      "Epoch 14, test loss: 0.161985\n",
      "Epoch 15, test loss: 0.162345\n",
      "Epoch 16, test loss: 0.164464\n",
      "Epoch 17, test loss: 0.162121\n",
      "Epoch 18, test loss: 0.162913\n",
      "Epoch 19, test loss: 0.162305\n",
      "Epoch 20, test loss: 0.162012\n",
      "Epoch 21, test loss: 0.161711\n",
      "Epoch 22, test loss: 0.161787\n",
      "Epoch 23, test loss: 0.163018\n",
      "Epoch 24, test loss: 0.163476\n",
      "Epoch 25, test loss: 0.164756\n",
      "Epoch 26, test loss: 0.161838\n",
      "Epoch 27, test loss: 0.160826\n",
      "Epoch 28, test loss: 0.160686\n",
      "Epoch 29, test loss: 0.160920\n",
      "Epoch 30, test loss: 0.160538\n",
      "Epoch 31, test loss: 0.160462\n",
      "Epoch 32, test loss: 0.160464\n",
      "Epoch 33, test loss: 0.162308\n",
      "Epoch 34, test loss: 0.161684\n",
      "Epoch 35, test loss: 0.160610\n",
      "Epoch 36, test loss: 0.162316\n",
      "Epoch 37, test loss: 0.160371\n",
      "Epoch 38, test loss: 0.160002\n",
      "Epoch 39, test loss: 0.162859\n",
      "Epoch 40, test loss: 0.160639\n",
      "Epoch 41, test loss: 0.166258\n",
      "Epoch 42, test loss: 0.168197\n",
      "Epoch 43, test loss: 0.160314\n",
      "Epoch 44, test loss: 0.164525\n",
      "Epoch 45, test loss: 0.159806\n",
      "Epoch 46, test loss: 0.159665\n",
      "Epoch 47, test loss: 0.159871\n",
      "Epoch 48, test loss: 0.168162\n",
      "Epoch 49, test loss: 0.160142\n",
      "Epoch 50, test loss: 0.161263\n",
      "Epoch 51, test loss: 0.159632\n",
      "Epoch 52, test loss: 0.163949\n",
      "Epoch 53, test loss: 0.164684\n",
      "Epoch 54, test loss: 0.159703\n",
      "Epoch 55, test loss: 0.159727\n",
      "Epoch 56, test loss: 0.161090\n",
      "Epoch 57, test loss: 0.160677\n",
      "Epoch 58, test loss: 0.161454\n",
      "Epoch 59, test loss: 0.160018\n",
      "Epoch 60, test loss: 0.159279\n",
      "Epoch 61, test loss: 0.159336\n",
      "Epoch 62, test loss: 0.159418\n",
      "Epoch 63, test loss: 0.159536\n",
      "Epoch 64, test loss: 0.160208\n",
      "Epoch 65, test loss: 0.159644\n",
      "Epoch 66, test loss: 0.161175\n",
      "Epoch 67, test loss: 0.159216\n",
      "Epoch 68, test loss: 0.160207\n",
      "Epoch 69, test loss: 0.171849\n",
      "Epoch 70, test loss: 0.159423\n",
      "Epoch 71, test loss: 0.158938\n",
      "Epoch 72, test loss: 0.162634\n",
      "Epoch 73, test loss: 0.166019\n",
      "Epoch 74, test loss: 0.166808\n",
      "Epoch 75, test loss: 0.159114\n",
      "Epoch 76, test loss: 0.161804\n",
      "Epoch 77, test loss: 0.164809\n",
      "Epoch 78, test loss: 0.159410\n",
      "Epoch 79, test loss: 0.159295\n",
      "Epoch 80, test loss: 0.159278\n",
      "Epoch 81, test loss: 0.159217\n",
      "Epoch 82, test loss: 0.158994\n",
      "Epoch 83, test loss: 0.160689\n",
      "Epoch 84, test loss: 0.161840\n",
      "Epoch 85, test loss: 0.161889\n",
      "Epoch 86, test loss: 0.160956\n",
      "Epoch 87, test loss: 0.160935\n",
      "Epoch 88, test loss: 0.160125\n",
      "Epoch 89, test loss: 0.159557\n",
      "Epoch 90, test loss: 0.161235\n",
      "Epoch 91, test loss: 0.158799\n",
      "Epoch 92, test loss: 0.160369\n",
      "Epoch 93, test loss: 0.159143\n",
      "Epoch 94, test loss: 0.162586\n",
      "Epoch 95, test loss: 0.159125\n",
      "Epoch 96, test loss: 0.160003\n",
      "Epoch 97, test loss: 0.167150\n",
      "Epoch 98, test loss: 0.160530\n",
      "Epoch 99, test loss: 0.159100\n",
      "Epoch 100, test loss: 0.159344\n",
      "Epoch 101, test loss: 0.160584\n",
      "Epoch 102, test loss: 0.159458\n",
      "Epoch 103, test loss: 0.165094\n",
      "Epoch 104, test loss: 0.159798\n",
      "Epoch 105, test loss: 0.159015\n",
      "Epoch 106, test loss: 0.159417\n",
      "Epoch 107, test loss: 0.158972\n",
      "Epoch 108, test loss: 0.159276\n",
      "Epoch 109, test loss: 0.159356\n",
      "Epoch 110, test loss: 0.162810\n",
      "Epoch 111, test loss: 0.159347\n",
      "Epoch 112, test loss: 0.165411\n",
      "Epoch 113, test loss: 0.159214\n",
      "Epoch 114, test loss: 0.159373\n",
      "Epoch 115, test loss: 0.159501\n",
      "Epoch 116, test loss: 0.164642\n",
      "Epoch 117, test loss: 0.165112\n",
      "Epoch 118, test loss: 0.159784\n",
      "Epoch 119, test loss: 0.160356\n",
      "Epoch 120, test loss: 0.161050\n",
      "Epoch 121, test loss: 0.163946\n",
      "Epoch 122, test loss: 0.163713\n",
      "Epoch 123, test loss: 0.159434\n",
      "Epoch 124, test loss: 0.160547\n",
      "Epoch 125, test loss: 0.160395\n",
      "Epoch 126, test loss: 0.159905\n",
      "Epoch 127, test loss: 0.160458\n",
      "Epoch 128, test loss: 0.162114\n",
      "Epoch 129, test loss: 0.159550\n",
      "Epoch 130, test loss: 0.159436\n",
      "Epoch 131, test loss: 0.161088\n",
      "Epoch 132, test loss: 0.159603\n",
      "Epoch 133, test loss: 0.160521\n",
      "Epoch 134, test loss: 0.162605\n",
      "Epoch 135, test loss: 0.163626\n",
      "Epoch 136, test loss: 0.160391\n",
      "Epoch 137, test loss: 0.159646\n",
      "Epoch 138, test loss: 0.159735\n",
      "Epoch 139, test loss: 0.165159\n",
      "Epoch 140, test loss: 0.160370\n",
      "Epoch 141, test loss: 0.159598\n",
      "Epoch 142, test loss: 0.159676\n",
      "Epoch 143, test loss: 0.163929\n",
      "Epoch 144, test loss: 0.161943\n",
      "Epoch 145, test loss: 0.163766\n",
      "Epoch 146, test loss: 0.160829\n",
      "Epoch 147, test loss: 0.160179\n",
      "Epoch 148, test loss: 0.162798\n",
      "Epoch 149, test loss: 0.161425\n",
      "Epoch 150, test loss: 0.161278\n",
      "Epoch 151, test loss: 0.160502\n",
      "Epoch 152, test loss: 0.163682\n",
      "Epoch 153, test loss: 0.161443\n",
      "Epoch 154, test loss: 0.160204\n",
      "Epoch 155, test loss: 0.179041\n",
      "Epoch 156, test loss: 0.160666\n",
      "Epoch 157, test loss: 0.159959\n",
      "Epoch 158, test loss: 0.160258\n",
      "Epoch 159, test loss: 0.160105\n",
      "Epoch 160, test loss: 0.160057\n",
      "Epoch 161, test loss: 0.172327\n",
      "Epoch 162, test loss: 0.172401\n",
      "Epoch 163, test loss: 0.160613\n",
      "Epoch 164, test loss: 0.161805\n",
      "Epoch 165, test loss: 0.161148\n",
      "Epoch 166, test loss: 0.163708\n",
      "Epoch 167, test loss: 0.159840\n",
      "Epoch 168, test loss: 0.161928\n",
      "Epoch 169, test loss: 0.160579\n",
      "Epoch 170, test loss: 0.160090\n",
      "Epoch 171, test loss: 0.165442\n",
      "Epoch 172, test loss: 0.161123\n",
      "Epoch 173, test loss: 0.160880\n",
      "Epoch 174, test loss: 0.160534\n",
      "Epoch 175, test loss: 0.159837\n",
      "Epoch 176, test loss: 0.161194\n",
      "Epoch 177, test loss: 0.160166\n",
      "Epoch 178, test loss: 0.160068\n",
      "Epoch 179, test loss: 0.159906\n",
      "Epoch 180, test loss: 0.162073\n",
      "Epoch 181, test loss: 0.160013\n",
      "Epoch 182, test loss: 0.167554\n",
      "Epoch 183, test loss: 0.164499\n",
      "Epoch 184, test loss: 0.165313\n",
      "Epoch 185, test loss: 0.160408\n",
      "Epoch 186, test loss: 0.164523\n",
      "Epoch 187, test loss: 0.166219\n",
      "Epoch 188, test loss: 0.161077\n",
      "Epoch 189, test loss: 0.160605\n",
      "Epoch 190, test loss: 0.167907\n",
      "Epoch 191, test loss: 0.162070\n",
      "Epoch 192, test loss: 0.160153\n",
      "Epoch 193, test loss: 0.163476\n",
      "Epoch 194, test loss: 0.168841\n",
      "Epoch 195, test loss: 0.170564\n",
      "Epoch 196, test loss: 0.160002\n",
      "Epoch 197, test loss: 0.162805\n",
      "Epoch 198, test loss: 0.160104\n",
      "Epoch 199, test loss: 0.165231\n",
      "Epoch 200, test loss: 0.161887\n",
      "Epoch 201, test loss: 0.161619\n",
      "Epoch 202, test loss: 0.159966\n",
      "Epoch 203, test loss: 0.166573\n",
      "Epoch 204, test loss: 0.161851\n",
      "Epoch 205, test loss: 0.162326\n",
      "Epoch 206, test loss: 0.163583\n",
      "Epoch 207, test loss: 0.162323\n",
      "Epoch 208, test loss: 0.159939\n",
      "Epoch 209, test loss: 0.161193\n",
      "Epoch 210, test loss: 0.162330\n",
      "Epoch 211, test loss: 0.159989\n",
      "Epoch 212, test loss: 0.160782\n",
      "Epoch 213, test loss: 0.160551\n",
      "Epoch 214, test loss: 0.165910\n",
      "Epoch 215, test loss: 0.166291\n",
      "Epoch 216, test loss: 0.162757\n",
      "Epoch 217, test loss: 0.162739\n",
      "Epoch 218, test loss: 0.161155\n",
      "Epoch 219, test loss: 0.164292\n",
      "Epoch 220, test loss: 0.160489\n",
      "Epoch 221, test loss: 0.168037\n",
      "Epoch 222, test loss: 0.170353\n",
      "Epoch 223, test loss: 0.162259\n",
      "Epoch 224, test loss: 0.165389\n",
      "Epoch 225, test loss: 0.159658\n",
      "Epoch 226, test loss: 0.159765\n",
      "Epoch 227, test loss: 0.160317\n",
      "Epoch 228, test loss: 0.161360\n",
      "Epoch 229, test loss: 0.161946\n",
      "Epoch 230, test loss: 0.160373\n",
      "Epoch 231, test loss: 0.165768\n",
      "Epoch 232, test loss: 0.171005\n",
      "Epoch 233, test loss: 0.160046\n",
      "Epoch 234, test loss: 0.163079\n",
      "Epoch 235, test loss: 0.161747\n",
      "Epoch 236, test loss: 0.164187\n",
      "Epoch 237, test loss: 0.160978\n",
      "Epoch 238, test loss: 0.159813\n",
      "Epoch 239, test loss: 0.160519\n",
      "Epoch 240, test loss: 0.160008\n",
      "Epoch 241, test loss: 0.161665\n",
      "Epoch 242, test loss: 0.160956\n",
      "Epoch 243, test loss: 0.161462\n",
      "Epoch 244, test loss: 0.163265\n",
      "Epoch 245, test loss: 0.159892\n",
      "Epoch 246, test loss: 0.166045\n",
      "Epoch 247, test loss: 0.160245\n",
      "Epoch 248, test loss: 0.164650\n",
      "Epoch 249, test loss: 0.160379\n",
      "Epoch 250, test loss: 0.160898\n",
      "Epoch 251, test loss: 0.160297\n",
      "Epoch 252, test loss: 0.162867\n",
      "Epoch 253, test loss: 0.161947\n",
      "Epoch 254, test loss: 0.160447\n",
      "Epoch 255, test loss: 0.161234\n",
      "Epoch 256, test loss: 0.169090\n",
      "Epoch 257, test loss: 0.160719\n",
      "Epoch 258, test loss: 0.163196\n",
      "Epoch 259, test loss: 0.160562\n",
      "Epoch 260, test loss: 0.163459\n",
      "Epoch 261, test loss: 0.160010\n",
      "Epoch 262, test loss: 0.165737\n",
      "Epoch 263, test loss: 0.162655\n",
      "Epoch 264, test loss: 0.160859\n",
      "Epoch 265, test loss: 0.159783\n",
      "Epoch 266, test loss: 0.162371\n",
      "Epoch 267, test loss: 0.161496\n",
      "Epoch 268, test loss: 0.166432\n",
      "Epoch 269, test loss: 0.160372\n",
      "Epoch 270, test loss: 0.160324\n",
      "Epoch 271, test loss: 0.160658\n",
      "Epoch 272, test loss: 0.160110\n",
      "Epoch 273, test loss: 0.160056\n",
      "Epoch 274, test loss: 0.160967\n",
      "Epoch 275, test loss: 0.160936\n",
      "Epoch 276, test loss: 0.160359\n",
      "Epoch 277, test loss: 0.161441\n",
      "Epoch 278, test loss: 0.162487\n",
      "Epoch 279, test loss: 0.161930\n",
      "Epoch 280, test loss: 0.160837\n",
      "Epoch 281, test loss: 0.160171\n",
      "Epoch 282, test loss: 0.161184\n",
      "Epoch 283, test loss: 0.161738\n",
      "Epoch 284, test loss: 0.162198\n",
      "Epoch 285, test loss: 0.162013\n",
      "Epoch 286, test loss: 0.162037\n",
      "Epoch 287, test loss: 0.161649\n",
      "Epoch 288, test loss: 0.166068\n",
      "Epoch 289, test loss: 0.161652\n",
      "Epoch 290, test loss: 0.160736\n",
      "Epoch 291, test loss: 0.160069\n",
      "Epoch 292, test loss: 0.162012\n",
      "Epoch 293, test loss: 0.160874\n",
      "Epoch 294, test loss: 0.163752\n",
      "Epoch 295, test loss: 0.160918\n",
      "Epoch 296, test loss: 0.159901\n",
      "Epoch 297, test loss: 0.166112\n",
      "Epoch 298, test loss: 0.162172\n",
      "Epoch 299, test loss: 0.163633\n",
      "Epoch 300, test loss: 0.162929\n",
      "Epoch 301, test loss: 0.161697\n",
      "Epoch 302, test loss: 0.162920\n",
      "Epoch 303, test loss: 0.160943\n",
      "Epoch 304, test loss: 0.164615\n",
      "Epoch 305, test loss: 0.161964\n",
      "Epoch 306, test loss: 0.160627\n",
      "Epoch 307, test loss: 0.160361\n",
      "Epoch 308, test loss: 0.160827\n",
      "Epoch 309, test loss: 0.163217\n",
      "Epoch 310, test loss: 0.164615\n",
      "Epoch 311, test loss: 0.163211\n",
      "Epoch 312, test loss: 0.160261\n",
      "Epoch 313, test loss: 0.160273\n",
      "Epoch 314, test loss: 0.161447\n",
      "Epoch 315, test loss: 0.160185\n",
      "Epoch 316, test loss: 0.164562\n",
      "Epoch 317, test loss: 0.161860\n",
      "Epoch 318, test loss: 0.161054\n",
      "Epoch 319, test loss: 0.162872\n",
      "Epoch 320, test loss: 0.161034\n",
      "Epoch 321, test loss: 0.161137\n",
      "Epoch 322, test loss: 0.169377\n",
      "Epoch 323, test loss: 0.160236\n",
      "Epoch 324, test loss: 0.160083\n",
      "Epoch 325, test loss: 0.161855\n",
      "Epoch 326, test loss: 0.167004\n",
      "Epoch 327, test loss: 0.166939\n",
      "Epoch 328, test loss: 0.160396\n",
      "Epoch 329, test loss: 0.164519\n",
      "Epoch 330, test loss: 0.161013\n",
      "Epoch 331, test loss: 0.161881\n",
      "Epoch 332, test loss: 0.163045\n",
      "Epoch 333, test loss: 0.162186\n",
      "Epoch 334, test loss: 0.160894\n",
      "Epoch 335, test loss: 0.160443\n",
      "Epoch 336, test loss: 0.160326\n",
      "Epoch 337, test loss: 0.164984\n",
      "Epoch 338, test loss: 0.160652\n",
      "Epoch 339, test loss: 0.160955\n",
      "Epoch 340, test loss: 0.164589\n",
      "Epoch 341, test loss: 0.160014\n",
      "Epoch 342, test loss: 0.160182\n",
      "Epoch 343, test loss: 0.159977\n",
      "Epoch 344, test loss: 0.161247\n",
      "Epoch 345, test loss: 0.164421\n",
      "Epoch 346, test loss: 0.161939\n",
      "Epoch 347, test loss: 0.167156\n",
      "Epoch 348, test loss: 0.160531\n",
      "Epoch 349, test loss: 0.160155\n",
      "Epoch 350, test loss: 0.160623\n",
      "Epoch 351, test loss: 0.162306\n",
      "Epoch 352, test loss: 0.168607\n",
      "Epoch 353, test loss: 0.160222\n",
      "Epoch 354, test loss: 0.160714\n",
      "Epoch 355, test loss: 0.161024\n",
      "Epoch 356, test loss: 0.160905\n",
      "Epoch 357, test loss: 0.160778\n",
      "Epoch 358, test loss: 0.161021\n",
      "Epoch 359, test loss: 0.160618\n",
      "Epoch 360, test loss: 0.166716\n",
      "Epoch 361, test loss: 0.162228\n",
      "Epoch 362, test loss: 0.169183\n",
      "Epoch 363, test loss: 0.160439\n",
      "Epoch 364, test loss: 0.161857\n",
      "Epoch 365, test loss: 0.161211\n",
      "Epoch 366, test loss: 0.160532\n",
      "Epoch 367, test loss: 0.161797\n",
      "Epoch 368, test loss: 0.161376\n",
      "Epoch 369, test loss: 0.160055\n",
      "Epoch 370, test loss: 0.160496\n",
      "Epoch 371, test loss: 0.160079\n",
      "Epoch 372, test loss: 0.160029\n",
      "Epoch 373, test loss: 0.160473\n",
      "Epoch 374, test loss: 0.160717\n",
      "Epoch 375, test loss: 0.160519\n",
      "Epoch 376, test loss: 0.160740\n",
      "Epoch 377, test loss: 0.160472\n",
      "Epoch 378, test loss: 0.167430\n",
      "Epoch 379, test loss: 0.164192\n",
      "Epoch 380, test loss: 0.162841\n",
      "Epoch 381, test loss: 0.166393\n",
      "Epoch 382, test loss: 0.159923\n",
      "Epoch 383, test loss: 0.160216\n",
      "Epoch 384, test loss: 0.162356\n",
      "Epoch 385, test loss: 0.161583\n",
      "Epoch 386, test loss: 0.163883\n",
      "Epoch 387, test loss: 0.165380\n",
      "Epoch 388, test loss: 0.162579\n",
      "Epoch 389, test loss: 0.160513\n",
      "Epoch 390, test loss: 0.163057\n",
      "Epoch 391, test loss: 0.169517\n",
      "Epoch 392, test loss: 0.164519\n",
      "Epoch 393, test loss: 0.160517\n",
      "Epoch 394, test loss: 0.160654\n",
      "Epoch 395, test loss: 0.160311\n",
      "Epoch 396, test loss: 0.161466\n",
      "Epoch 397, test loss: 0.163136\n",
      "Epoch 398, test loss: 0.160179\n",
      "Epoch 399, test loss: 0.160238\n",
      "Epoch 400, test loss: 0.169661\n",
      "Epoch 401, test loss: 0.160372\n",
      "Epoch 402, test loss: 0.160770\n",
      "Epoch 403, test loss: 0.162029\n",
      "Epoch 404, test loss: 0.161687\n",
      "Epoch 405, test loss: 0.162843\n",
      "Epoch 406, test loss: 0.160925\n",
      "Epoch 407, test loss: 0.163419\n",
      "Epoch 408, test loss: 0.160401\n",
      "Epoch 409, test loss: 0.162520\n",
      "Epoch 410, test loss: 0.162154\n",
      "Epoch 411, test loss: 0.164120\n",
      "Epoch 412, test loss: 0.168214\n",
      "Epoch 413, test loss: 0.160263\n",
      "Epoch 414, test loss: 0.160397\n",
      "Epoch 415, test loss: 0.160162\n",
      "Epoch 416, test loss: 0.160452\n",
      "Epoch 417, test loss: 0.172324\n",
      "Epoch 418, test loss: 0.163198\n",
      "Epoch 419, test loss: 0.160808\n",
      "Epoch 420, test loss: 0.162187\n",
      "Epoch 421, test loss: 0.160560\n",
      "Epoch 422, test loss: 0.161892\n",
      "Epoch 423, test loss: 0.160240\n",
      "Epoch 424, test loss: 0.159954\n",
      "Epoch 425, test loss: 0.160202\n",
      "Epoch 426, test loss: 0.160406\n",
      "Epoch 427, test loss: 0.160771\n",
      "Epoch 428, test loss: 0.160457\n",
      "Epoch 429, test loss: 0.160927\n",
      "Epoch 430, test loss: 0.162913\n",
      "Epoch 431, test loss: 0.170732\n",
      "Epoch 432, test loss: 0.168815\n",
      "Epoch 433, test loss: 0.161192\n",
      "Epoch 434, test loss: 0.163469\n",
      "Epoch 435, test loss: 0.163723\n",
      "Epoch 436, test loss: 0.163032\n",
      "Epoch 437, test loss: 0.165520\n",
      "Epoch 438, test loss: 0.162935\n",
      "Epoch 439, test loss: 0.168039\n",
      "Epoch 440, test loss: 0.165099\n",
      "Epoch 441, test loss: 0.160292\n",
      "Epoch 442, test loss: 0.159983\n",
      "Epoch 443, test loss: 0.161822\n",
      "Epoch 444, test loss: 0.160373\n",
      "Epoch 445, test loss: 0.168568\n",
      "Epoch 446, test loss: 0.164045\n",
      "Epoch 447, test loss: 0.160315\n",
      "Epoch 448, test loss: 0.166595\n",
      "Epoch 449, test loss: 0.160504\n",
      "Pretrain data: 19339240.0\n",
      "Building dataset, requesting data from 0 to 831\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7998/105206\n",
      "Found 831 continuous time series\n",
      "Data shape: (113206, 6), Train/test: 113204/2\n",
      "Train test ratio: 56602.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1768\n",
      "Epoch 0, train loss: 0.254647\n",
      "Epoch 1, train loss: 0.159288\n",
      "Epoch 2, train loss: 0.209302\n",
      "Epoch 3, train loss: 0.194718\n",
      "Epoch 4, train loss: 0.226054\n",
      "Epoch 5, train loss: 0.202642\n",
      "Epoch 6, train loss: 0.195532\n",
      "Epoch 7, train loss: 0.185053\n",
      "Epoch 8, train loss: 0.210912\n",
      "Epoch 9, train loss: 0.191826\n",
      "Epoch 10, train loss: 0.198692\n",
      "Epoch 11, train loss: 0.180318\n",
      "Epoch 12, train loss: 0.162952\n",
      "Epoch 13, train loss: 0.222167\n",
      "Epoch 14, train loss: 0.211659\n",
      "Epoch 15, train loss: 0.203464\n",
      "Epoch 16, train loss: 0.209218\n",
      "Epoch 17, train loss: 0.196588\n",
      "Epoch 18, train loss: 0.226428\n",
      "Epoch 19, train loss: 0.209041\n",
      "Epoch 20, train loss: 0.193350\n",
      "Epoch 21, train loss: 0.177564\n",
      "Epoch 22, train loss: 0.294842\n",
      "Epoch 23, train loss: 0.224337\n",
      "Epoch 24, train loss: 0.229795\n",
      "Epoch 25, train loss: 0.265062\n",
      "Epoch 26, train loss: 0.227810\n",
      "Epoch 27, train loss: 0.163657\n",
      "Epoch 28, train loss: 0.190765\n",
      "Epoch 29, train loss: 0.218471\n",
      "Epoch 30, train loss: 0.246576\n",
      "Epoch 31, train loss: 0.188664\n",
      "Epoch 32, train loss: 0.173903\n",
      "Epoch 33, train loss: 0.204208\n",
      "Epoch 34, train loss: 0.195351\n",
      "Epoch 35, train loss: 0.356412\n",
      "Epoch 36, train loss: 0.198322\n",
      "Epoch 37, train loss: 0.267583\n",
      "Epoch 38, train loss: 0.200013\n",
      "Epoch 39, train loss: 0.193090\n",
      "Epoch 40, train loss: 0.215248\n",
      "Epoch 41, train loss: 0.231086\n",
      "Epoch 42, train loss: 0.231585\n",
      "Epoch 43, train loss: 0.173904\n",
      "Epoch 44, train loss: 0.244687\n",
      "Epoch 45, train loss: 0.184375\n",
      "Epoch 46, train loss: 0.172031\n",
      "Epoch 47, train loss: 0.205006\n",
      "Epoch 48, train loss: 0.226888\n",
      "Epoch 49, train loss: 0.237775\n",
      "Epoch 50, train loss: 0.198352\n",
      "Epoch 51, train loss: 0.218240\n",
      "Epoch 52, train loss: 0.314344\n",
      "Epoch 53, train loss: 0.183259\n",
      "Epoch 54, train loss: 0.221794\n",
      "Epoch 55, train loss: 0.220962\n",
      "Epoch 56, train loss: 0.228726\n",
      "Epoch 57, train loss: 0.172001\n",
      "Epoch 58, train loss: 0.168040\n",
      "Epoch 59, train loss: 0.214646\n",
      "Epoch 60, train loss: 0.221089\n",
      "Epoch 61, train loss: 0.205460\n",
      "Epoch 62, train loss: 0.185208\n",
      "Epoch 63, train loss: 0.195741\n",
      "Epoch 64, train loss: 0.215189\n",
      "Epoch 65, train loss: 0.228684\n",
      "Epoch 66, train loss: 0.229408\n",
      "Epoch 67, train loss: 0.173139\n",
      "Epoch 68, train loss: 0.172125\n",
      "Epoch 69, train loss: 0.194044\n",
      "Epoch 70, train loss: 0.236080\n",
      "Epoch 71, train loss: 0.210558\n",
      "Epoch 72, train loss: 0.196061\n",
      "Epoch 73, train loss: 0.191344\n",
      "Epoch 74, train loss: 0.203636\n",
      "Epoch 75, train loss: 0.220680\n",
      "Epoch 76, train loss: 0.222540\n",
      "Epoch 77, train loss: 0.171391\n",
      "Epoch 78, train loss: 0.244237\n",
      "Epoch 79, train loss: 0.227171\n",
      "Epoch 80, train loss: 0.150983\n",
      "Epoch 81, train loss: 0.236827\n",
      "Epoch 82, train loss: 0.161863\n",
      "Epoch 83, train loss: 0.202834\n",
      "Epoch 84, train loss: 0.153473\n",
      "Epoch 85, train loss: 0.269015\n",
      "Epoch 86, train loss: 0.224580\n",
      "Epoch 87, train loss: 0.158983\n",
      "Epoch 88, train loss: 0.172175\n",
      "Epoch 89, train loss: 0.217984\n",
      "Epoch 90, train loss: 0.224323\n",
      "Epoch 91, train loss: 0.227873\n",
      "Epoch 92, train loss: 0.159607\n",
      "Epoch 93, train loss: 0.172573\n",
      "Epoch 94, train loss: 0.181829\n",
      "Epoch 95, train loss: 0.217101\n",
      "Epoch 96, train loss: 0.204401\n",
      "Epoch 97, train loss: 0.208575\n",
      "Epoch 98, train loss: 0.212832\n",
      "Epoch 99, train loss: 0.228066\n",
      "Epoch 100, train loss: 0.239877\n",
      "Epoch 101, train loss: 0.206007\n",
      "Epoch 102, train loss: 0.179038\n",
      "Epoch 103, train loss: 0.164232\n",
      "Epoch 104, train loss: 0.181015\n",
      "Epoch 105, train loss: 0.237658\n",
      "Epoch 106, train loss: 0.181057\n",
      "Epoch 107, train loss: 0.221193\n",
      "Epoch 108, train loss: 0.210252\n",
      "Epoch 109, train loss: 0.187559\n",
      "Epoch 110, train loss: 0.235726\n",
      "Epoch 111, train loss: 0.242970\n",
      "Epoch 112, train loss: 0.204352\n",
      "Epoch 113, train loss: 0.196474\n",
      "Epoch 114, train loss: 0.247402\n",
      "Epoch 115, train loss: 0.169227\n",
      "Epoch 116, train loss: 0.205828\n",
      "Epoch 117, train loss: 0.229936\n",
      "Epoch 118, train loss: 0.160954\n",
      "Epoch 119, train loss: 0.217007\n",
      "Epoch 120, train loss: 0.227077\n",
      "Epoch 121, train loss: 0.270056\n",
      "Epoch 122, train loss: 0.193315\n",
      "Epoch 123, train loss: 0.203481\n",
      "Epoch 124, train loss: 0.177234\n",
      "Epoch 125, train loss: 0.183443\n",
      "Epoch 126, train loss: 0.192574\n",
      "Epoch 127, train loss: 0.187711\n",
      "Epoch 128, train loss: 0.201594\n",
      "Epoch 129, train loss: 0.196418\n",
      "Epoch 130, train loss: 0.191728\n",
      "Epoch 131, train loss: 0.175158\n",
      "Epoch 132, train loss: 0.138556\n",
      "Epoch 133, train loss: 0.221391\n",
      "Epoch 134, train loss: 0.201287\n",
      "Epoch 135, train loss: 0.298671\n",
      "Epoch 136, train loss: 0.181481\n",
      "Epoch 137, train loss: 0.249048\n",
      "Epoch 138, train loss: 0.170033\n",
      "Epoch 139, train loss: 0.152113\n",
      "Epoch 140, train loss: 0.198568\n",
      "Epoch 141, train loss: 0.174996\n",
      "Epoch 142, train loss: 0.168548\n",
      "Epoch 143, train loss: 0.194639\n",
      "Epoch 144, train loss: 0.161576\n",
      "Epoch 145, train loss: 0.233349\n",
      "Epoch 146, train loss: 0.205828\n",
      "Epoch 147, train loss: 0.197504\n",
      "Epoch 148, train loss: 0.196974\n",
      "Epoch 149, train loss: 0.160109\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "x here is\n",
      "[[127. 123. 118. 112. 108. 106.]\n",
      " [123. 118. 112. 108. 106. 103.]\n",
      " [118. 112. 108. 106. 103.  98.]\n",
      " ...\n",
      " [238. 243. 250. 254. 263. 280.]\n",
      " [243. 250. 254. 263. 280. 288.]\n",
      " [250. 254. 263. 280. 288. 301.]]\n",
      "y here is\n",
      "[[ 89.  89.  89.  89.  89.  89.]\n",
      " [ 87.  87.  87.  87.  87.  87.]\n",
      " [ 85.  85.  85.  85.  85.  85.]\n",
      " ...\n",
      " [307. 307. 307. 307. 307. 307.]\n",
      " [311. 311. 311. 311. 311. 311.]\n",
      " [321. 321. 321. 321. 321. 321.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (2758, 6), Train/test: 1/2757\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[116. 117. 119. 116. 111. 110.]\n",
      " [117. 119. 116. 111. 110. 111.]\n",
      " [119. 116. 111. 110. 111. 113.]\n",
      " ...\n",
      " [198. 191. 186. 180. 175. 171.]\n",
      " [191. 186. 180. 175. 171. 168.]\n",
      " [186. 180. 175. 171. 168. 162.]]\n",
      "y here is\n",
      "[[126. 126. 126. 126. 126. 126.]\n",
      " [131. 131. 131. 131. 131. 131.]\n",
      " [136. 136. 136. 136. 136. 136.]\n",
      " ...\n",
      " [140. 140. 140. 140. 140. 140.]\n",
      " [137. 137. 137. 137. 137. 137.]\n",
      " [132. 132. 132. 132. 132. 132.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 287/12233\n",
      "Found 11 continuous time series\n",
      "Data shape: (12522, 6), Train/test: 12520/2\n",
      "Train test ratio: 6260.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61388BA30>\n",
      "Epoch 0, test loss: 0.187571\n",
      "Epoch 1, test loss: 0.184564\n",
      "Epoch 2, test loss: 0.188866\n",
      "Epoch 3, test loss: 0.185358\n",
      "Epoch 4, test loss: 0.187114\n",
      "Epoch 5, test loss: 0.189867\n",
      "Epoch 6, test loss: 0.187911\n",
      "Epoch 7, test loss: 0.184896\n",
      "Epoch 8, test loss: 0.185466\n",
      "Epoch 9, test loss: 0.184244\n",
      "Epoch 10, test loss: 0.184991\n",
      "Epoch 11, test loss: 0.188931\n",
      "Epoch 12, test loss: 0.188595\n",
      "Epoch 13, test loss: 0.184105\n",
      "Epoch 14, test loss: 0.184621\n",
      "Epoch 15, test loss: 0.186756\n",
      "Epoch 16, test loss: 0.185097\n",
      "Epoch 17, test loss: 0.185344\n",
      "Epoch 18, test loss: 0.187041\n",
      "Epoch 19, test loss: 0.184599\n",
      "Epoch 20, test loss: 0.185131\n",
      "Epoch 21, test loss: 0.190504\n",
      "Epoch 22, test loss: 0.184366\n",
      "Epoch 23, test loss: 0.186093\n",
      "Epoch 24, test loss: 0.185348\n",
      "Epoch 25, test loss: 0.185081\n",
      "Epoch 26, test loss: 0.184849\n",
      "Epoch 27, test loss: 0.185851\n",
      "Epoch 28, test loss: 0.188016\n",
      "Epoch 29, test loss: 0.184619\n",
      "Epoch 30, test loss: 0.188109\n",
      "Epoch 31, test loss: 0.184742\n",
      "Epoch 32, test loss: 0.188271\n",
      "Epoch 33, test loss: 0.184861\n",
      "Epoch 34, test loss: 0.184171\n",
      "Epoch 35, test loss: 0.185105\n",
      "Epoch 36, test loss: 0.184846\n",
      "Epoch 37, test loss: 0.184429\n",
      "Epoch 38, test loss: 0.184978\n",
      "Epoch 39, test loss: 0.188796\n",
      "Epoch 40, test loss: 0.187152\n",
      "Epoch 41, test loss: 0.184269\n",
      "Epoch 42, test loss: 0.186520\n",
      "Epoch 43, test loss: 0.184824\n",
      "Epoch 44, test loss: 0.187453\n",
      "Epoch 45, test loss: 0.192821\n",
      "Epoch 46, test loss: 0.185928\n",
      "Epoch 47, test loss: 0.189309\n",
      "Epoch 48, test loss: 0.189647\n",
      "Epoch 49, test loss: 0.187228\n",
      "Epoch 50, test loss: 0.185274\n",
      "Epoch 51, test loss: 0.184491\n",
      "Epoch 52, test loss: 0.185144\n",
      "Epoch 53, test loss: 0.186493\n",
      "Epoch 54, test loss: 0.185492\n",
      "Epoch 55, test loss: 0.191782\n",
      "Epoch 56, test loss: 0.185138\n",
      "Epoch 57, test loss: 0.184339\n",
      "Epoch 58, test loss: 0.198400\n",
      "Epoch 59, test loss: 0.190774\n",
      "Epoch 60, test loss: 0.186558\n",
      "Epoch 61, test loss: 0.184108\n",
      "Epoch 62, test loss: 0.185236\n",
      "Epoch 63, test loss: 0.187461\n",
      "Epoch 64, test loss: 0.192317\n",
      "Epoch 65, test loss: 0.186393\n",
      "Epoch 66, test loss: 0.185224\n",
      "Epoch 67, test loss: 0.184590\n",
      "Epoch 68, test loss: 0.185314\n",
      "Epoch 69, test loss: 0.184300\n",
      "Epoch 70, test loss: 0.184421\n",
      "Epoch 71, test loss: 0.185501\n",
      "Epoch 72, test loss: 0.185247\n",
      "Epoch 73, test loss: 0.186771\n",
      "Epoch 74, test loss: 0.184752\n",
      "Epoch 75, test loss: 0.189743\n",
      "Epoch 76, test loss: 0.191524\n",
      "Epoch 77, test loss: 0.184485\n",
      "Epoch 78, test loss: 0.184129\n",
      "Epoch 79, test loss: 0.185223\n",
      "Epoch 80, test loss: 0.186836\n",
      "Epoch 81, test loss: 0.185447\n",
      "Epoch 82, test loss: 0.185666\n",
      "Epoch 83, test loss: 0.184858\n",
      "Epoch 84, test loss: 0.184032\n",
      "Epoch 85, test loss: 0.184869\n",
      "Epoch 86, test loss: 0.192309\n",
      "Epoch 87, test loss: 0.184605\n",
      "Epoch 88, test loss: 0.184436\n",
      "Epoch 89, test loss: 0.184686\n",
      "Epoch 90, test loss: 0.185705\n",
      "Epoch 91, test loss: 0.184409\n",
      "Epoch 92, test loss: 0.186396\n",
      "Epoch 93, test loss: 0.186793\n",
      "Epoch 94, test loss: 0.185227\n",
      "Epoch 95, test loss: 0.187306\n",
      "Epoch 96, test loss: 0.185208\n",
      "Epoch 97, test loss: 0.185168\n",
      "Epoch 98, test loss: 0.186010\n",
      "Epoch 99, test loss: 0.184860\n",
      "Epoch 100, test loss: 0.184813\n",
      "Epoch 101, test loss: 0.188156\n",
      "Epoch 102, test loss: 0.190610\n",
      "Epoch 103, test loss: 0.184504\n",
      "Epoch 104, test loss: 0.184190\n",
      "Epoch 105, test loss: 0.185612\n",
      "Epoch 106, test loss: 0.190498\n",
      "Epoch 107, test loss: 0.187665\n",
      "Epoch 108, test loss: 0.184304\n",
      "Epoch 109, test loss: 0.184692\n",
      "Epoch 110, test loss: 0.186573\n",
      "Epoch 111, test loss: 0.186145\n",
      "Epoch 112, test loss: 0.185695\n",
      "Epoch 113, test loss: 0.185041\n",
      "Epoch 114, test loss: 0.184861\n",
      "Epoch 115, test loss: 0.187442\n",
      "Epoch 116, test loss: 0.185143\n",
      "Epoch 117, test loss: 0.185726\n",
      "Epoch 118, test loss: 0.184741\n",
      "Epoch 119, test loss: 0.185294\n",
      "Epoch 120, test loss: 0.190736\n",
      "Epoch 121, test loss: 0.186960\n",
      "Epoch 122, test loss: 0.186128\n",
      "Epoch 123, test loss: 0.191502\n",
      "Epoch 124, test loss: 0.185583\n",
      "Epoch 125, test loss: 0.184840\n",
      "Epoch 126, test loss: 0.185276\n",
      "Epoch 127, test loss: 0.185853\n",
      "Epoch 128, test loss: 0.185016\n",
      "Epoch 129, test loss: 0.185677\n",
      "Epoch 130, test loss: 0.184605\n",
      "Epoch 131, test loss: 0.186959\n",
      "Epoch 132, test loss: 0.184763\n",
      "Epoch 133, test loss: 0.186661\n",
      "Epoch 134, test loss: 0.187285\n",
      "Epoch 135, test loss: 0.184512\n",
      "Epoch 136, test loss: 0.186756\n",
      "Epoch 137, test loss: 0.184254\n",
      "Epoch 138, test loss: 0.185687\n",
      "Epoch 139, test loss: 0.184763\n",
      "Epoch 140, test loss: 0.184952\n",
      "Epoch 141, test loss: 0.189055\n",
      "Epoch 142, test loss: 0.184477\n",
      "Epoch 143, test loss: 0.186169\n",
      "Epoch 144, test loss: 0.185772\n",
      "Epoch 145, test loss: 0.184333\n",
      "Epoch 146, test loss: 0.190981\n",
      "Epoch 147, test loss: 0.184435\n",
      "Epoch 148, test loss: 0.185152\n",
      "Epoch 149, test loss: 0.185347\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61388BA30>\n",
      "Epoch 0, test loss: 0.184353\n",
      "Epoch 1, test loss: 0.186062\n",
      "Epoch 2, test loss: 0.184121\n",
      "Epoch 3, test loss: 0.184136\n",
      "Epoch 4, test loss: 0.186327\n",
      "Epoch 5, test loss: 0.186707\n",
      "Epoch 6, test loss: 0.186885\n",
      "Epoch 7, test loss: 0.184426\n",
      "Epoch 8, test loss: 0.184933\n",
      "Epoch 9, test loss: 0.185564\n",
      "Epoch 10, test loss: 0.187654\n",
      "Epoch 11, test loss: 0.186250\n",
      "Epoch 12, test loss: 0.184273\n",
      "Epoch 13, test loss: 0.187941\n",
      "Epoch 14, test loss: 0.184333\n",
      "Epoch 15, test loss: 0.186189\n",
      "Epoch 16, test loss: 0.185831\n",
      "Epoch 17, test loss: 0.184641\n",
      "Epoch 18, test loss: 0.190200\n",
      "Epoch 19, test loss: 0.184198\n",
      "Epoch 20, test loss: 0.185304\n",
      "Epoch 21, test loss: 0.185193\n",
      "Epoch 22, test loss: 0.184758\n",
      "Epoch 23, test loss: 0.184755\n",
      "Epoch 24, test loss: 0.185238\n",
      "Epoch 25, test loss: 0.184801\n",
      "Epoch 26, test loss: 0.184719\n",
      "Epoch 27, test loss: 0.185756\n",
      "Epoch 28, test loss: 0.184326\n",
      "Epoch 29, test loss: 0.184630\n",
      "Epoch 30, test loss: 0.185286\n",
      "Epoch 31, test loss: 0.184514\n",
      "Epoch 32, test loss: 0.187159\n",
      "Epoch 33, test loss: 0.185597\n",
      "Epoch 34, test loss: 0.186307\n",
      "Epoch 35, test loss: 0.185654\n",
      "Epoch 36, test loss: 0.184452\n",
      "Epoch 37, test loss: 0.189581\n",
      "Epoch 38, test loss: 0.184824\n",
      "Epoch 39, test loss: 0.185733\n",
      "Epoch 40, test loss: 0.184820\n",
      "Epoch 41, test loss: 0.186301\n",
      "Epoch 42, test loss: 0.185976\n",
      "Epoch 43, test loss: 0.185560\n",
      "Epoch 44, test loss: 0.186067\n",
      "Epoch 45, test loss: 0.187044\n",
      "Epoch 46, test loss: 0.185647\n",
      "Epoch 47, test loss: 0.190130\n",
      "Epoch 48, test loss: 0.186582\n",
      "Epoch 49, test loss: 0.186503\n",
      "Epoch 50, test loss: 0.190289\n",
      "Epoch 51, test loss: 0.186587\n",
      "Epoch 52, test loss: 0.184924\n",
      "Epoch 53, test loss: 0.185936\n",
      "Epoch 54, test loss: 0.185783\n",
      "Epoch 55, test loss: 0.187275\n",
      "Epoch 56, test loss: 0.184595\n",
      "Epoch 57, test loss: 0.187432\n",
      "Epoch 58, test loss: 0.184922\n",
      "Epoch 59, test loss: 0.185417\n",
      "Epoch 60, test loss: 0.185575\n",
      "Epoch 61, test loss: 0.184372\n",
      "Epoch 62, test loss: 0.185868\n",
      "Epoch 63, test loss: 0.184265\n",
      "Epoch 64, test loss: 0.184609\n",
      "Epoch 65, test loss: 0.184786\n",
      "Epoch 66, test loss: 0.187632\n",
      "Epoch 67, test loss: 0.184734\n",
      "Epoch 68, test loss: 0.187447\n",
      "Epoch 69, test loss: 0.187648\n",
      "Epoch 70, test loss: 0.191540\n",
      "Epoch 71, test loss: 0.186464\n",
      "Epoch 72, test loss: 0.185646\n",
      "Epoch 73, test loss: 0.186143\n",
      "Epoch 74, test loss: 0.185136\n",
      "Epoch 75, test loss: 0.188668\n",
      "Epoch 76, test loss: 0.185737\n",
      "Epoch 77, test loss: 0.184900\n",
      "Epoch 78, test loss: 0.188516\n",
      "Epoch 79, test loss: 0.185548\n",
      "Epoch 80, test loss: 0.185035\n",
      "Epoch 81, test loss: 0.185734\n",
      "Epoch 82, test loss: 0.185207\n",
      "Epoch 83, test loss: 0.186811\n",
      "Epoch 84, test loss: 0.185382\n",
      "Epoch 85, test loss: 0.189496\n",
      "Epoch 86, test loss: 0.185215\n",
      "Epoch 87, test loss: 0.187222\n",
      "Epoch 88, test loss: 0.184915\n",
      "Epoch 89, test loss: 0.191642\n",
      "Epoch 90, test loss: 0.186882\n",
      "Epoch 91, test loss: 0.185375\n",
      "Epoch 92, test loss: 0.184725\n",
      "Epoch 93, test loss: 0.184034\n",
      "Epoch 94, test loss: 0.184802\n",
      "Epoch 95, test loss: 0.184303\n",
      "Epoch 96, test loss: 0.188582\n",
      "Epoch 97, test loss: 0.190093\n",
      "Epoch 98, test loss: 0.184495\n",
      "Epoch 99, test loss: 0.186462\n",
      "Epoch 100, test loss: 0.185701\n",
      "Epoch 101, test loss: 0.184641\n",
      "Epoch 102, test loss: 0.186343\n",
      "Epoch 103, test loss: 0.188244\n",
      "Epoch 104, test loss: 0.185703\n",
      "Epoch 105, test loss: 0.184454\n",
      "Epoch 106, test loss: 0.184541\n",
      "Epoch 107, test loss: 0.185054\n",
      "Epoch 108, test loss: 0.185265\n",
      "Epoch 109, test loss: 0.185677\n",
      "Epoch 110, test loss: 0.186375\n",
      "Epoch 111, test loss: 0.187290\n",
      "Epoch 112, test loss: 0.187628\n",
      "Epoch 113, test loss: 0.184909\n",
      "Epoch 114, test loss: 0.186711\n",
      "Epoch 115, test loss: 0.185913\n",
      "Epoch 116, test loss: 0.184067\n",
      "Epoch 117, test loss: 0.187262\n",
      "Epoch 118, test loss: 0.185039\n",
      "Epoch 119, test loss: 0.188176\n",
      "Epoch 120, test loss: 0.185716\n",
      "Epoch 121, test loss: 0.186948\n",
      "Epoch 122, test loss: 0.185612\n",
      "Epoch 123, test loss: 0.188442\n",
      "Epoch 124, test loss: 0.185709\n",
      "Epoch 125, test loss: 0.185254\n",
      "Epoch 126, test loss: 0.187100\n",
      "Epoch 127, test loss: 0.185645\n",
      "Epoch 128, test loss: 0.190283\n",
      "Epoch 129, test loss: 0.188575\n",
      "Epoch 130, test loss: 0.187390\n",
      "Epoch 131, test loss: 0.185361\n",
      "Epoch 132, test loss: 0.185569\n",
      "Epoch 133, test loss: 0.185892\n",
      "Epoch 134, test loss: 0.186951\n",
      "Epoch 135, test loss: 0.184842\n",
      "Epoch 136, test loss: 0.185608\n",
      "Epoch 137, test loss: 0.185171\n",
      "Epoch 138, test loss: 0.186492\n",
      "Epoch 139, test loss: 0.185456\n",
      "Epoch 140, test loss: 0.185842\n",
      "Epoch 141, test loss: 0.185045\n",
      "Epoch 142, test loss: 0.186367\n",
      "Epoch 143, test loss: 0.186002\n",
      "Epoch 144, test loss: 0.185445\n",
      "Epoch 145, test loss: 0.184704\n",
      "Epoch 146, test loss: 0.188504\n",
      "Epoch 147, test loss: 0.187463\n",
      "Epoch 148, test loss: 0.184965\n",
      "Epoch 149, test loss: 0.186673\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61388BA30>\n",
      "Epoch 0, test loss: 0.216601\n",
      "Epoch 1, test loss: 0.196598\n",
      "Epoch 2, test loss: 0.194112\n",
      "Epoch 3, test loss: 0.189733\n",
      "Epoch 4, test loss: 0.188592\n",
      "Epoch 5, test loss: 0.190849\n",
      "Epoch 6, test loss: 0.187837\n",
      "Epoch 7, test loss: 0.190158\n",
      "Epoch 8, test loss: 0.189687\n",
      "Epoch 9, test loss: 0.187083\n",
      "Epoch 10, test loss: 0.187111\n",
      "Epoch 11, test loss: 0.187432\n",
      "Epoch 12, test loss: 0.191419\n",
      "Epoch 13, test loss: 0.187035\n",
      "Epoch 14, test loss: 0.187610\n",
      "Epoch 15, test loss: 0.186549\n",
      "Epoch 16, test loss: 0.186819\n",
      "Epoch 17, test loss: 0.187542\n",
      "Epoch 18, test loss: 0.190695\n",
      "Epoch 19, test loss: 0.188897\n",
      "Epoch 20, test loss: 0.190999\n",
      "Epoch 21, test loss: 0.187151\n",
      "Epoch 22, test loss: 0.186069\n",
      "Epoch 23, test loss: 0.188013\n",
      "Epoch 24, test loss: 0.188444\n",
      "Epoch 25, test loss: 0.187280\n",
      "Epoch 26, test loss: 0.190438\n",
      "Epoch 27, test loss: 0.187006\n",
      "Epoch 28, test loss: 0.186564\n",
      "Epoch 29, test loss: 0.186256\n",
      "Epoch 30, test loss: 0.186648\n",
      "Epoch 31, test loss: 0.191621\n",
      "Epoch 32, test loss: 0.187025\n",
      "Epoch 33, test loss: 0.188774\n",
      "Epoch 34, test loss: 0.187077\n",
      "Epoch 35, test loss: 0.187028\n",
      "Epoch 36, test loss: 0.187293\n",
      "Epoch 37, test loss: 0.187190\n",
      "Epoch 38, test loss: 0.186445\n",
      "Epoch 39, test loss: 0.186229\n",
      "Epoch 40, test loss: 0.186049\n",
      "Epoch 41, test loss: 0.187620\n",
      "Epoch 42, test loss: 0.186580\n",
      "Epoch 43, test loss: 0.187061\n",
      "Epoch 44, test loss: 0.186751\n",
      "Epoch 45, test loss: 0.186518\n",
      "Epoch 46, test loss: 0.186763\n",
      "Epoch 47, test loss: 0.186098\n",
      "Epoch 48, test loss: 0.192121\n",
      "Epoch 49, test loss: 0.185845\n",
      "Epoch 50, test loss: 0.187533\n",
      "Epoch 51, test loss: 0.186342\n",
      "Epoch 52, test loss: 0.186016\n",
      "Epoch 53, test loss: 0.185477\n",
      "Epoch 54, test loss: 0.188440\n",
      "Epoch 55, test loss: 0.188384\n",
      "Epoch 56, test loss: 0.188685\n",
      "Epoch 57, test loss: 0.186112\n",
      "Epoch 58, test loss: 0.186305\n",
      "Epoch 59, test loss: 0.185841\n",
      "Epoch 60, test loss: 0.188091\n",
      "Epoch 61, test loss: 0.185901\n",
      "Epoch 62, test loss: 0.186148\n",
      "Epoch 63, test loss: 0.185054\n",
      "Epoch 64, test loss: 0.186864\n",
      "Epoch 65, test loss: 0.187763\n",
      "Epoch 66, test loss: 0.186986\n",
      "Epoch 67, test loss: 0.185805\n",
      "Epoch 68, test loss: 0.191871\n",
      "Epoch 69, test loss: 0.188396\n",
      "Epoch 70, test loss: 0.190956\n",
      "Epoch 71, test loss: 0.188188\n",
      "Epoch 72, test loss: 0.187492\n",
      "Epoch 73, test loss: 0.187891\n",
      "Epoch 74, test loss: 0.185467\n",
      "Epoch 75, test loss: 0.186875\n",
      "Epoch 76, test loss: 0.185952\n",
      "Epoch 77, test loss: 0.185138\n",
      "Epoch 78, test loss: 0.185888\n",
      "Epoch 79, test loss: 0.185429\n",
      "Epoch 80, test loss: 0.186914\n",
      "Epoch 81, test loss: 0.185067\n",
      "Epoch 82, test loss: 0.190917\n",
      "Epoch 83, test loss: 0.190169\n",
      "Epoch 84, test loss: 0.185782\n",
      "Epoch 85, test loss: 0.187524\n",
      "Epoch 86, test loss: 0.184943\n",
      "Epoch 87, test loss: 0.185118\n",
      "Epoch 88, test loss: 0.187435\n",
      "Epoch 89, test loss: 0.186380\n",
      "Epoch 90, test loss: 0.187580\n",
      "Epoch 91, test loss: 0.186078\n",
      "Epoch 92, test loss: 0.185432\n",
      "Epoch 93, test loss: 0.188404\n",
      "Epoch 94, test loss: 0.191563\n",
      "Epoch 95, test loss: 0.185138\n",
      "Epoch 96, test loss: 0.187171\n",
      "Epoch 97, test loss: 0.189407\n",
      "Epoch 98, test loss: 0.188551\n",
      "Epoch 99, test loss: 0.185441\n",
      "Epoch 100, test loss: 0.186275\n",
      "Epoch 101, test loss: 0.185317\n",
      "Epoch 102, test loss: 0.189119\n",
      "Epoch 103, test loss: 0.190227\n",
      "Epoch 104, test loss: 0.190719\n",
      "Epoch 105, test loss: 0.185590\n",
      "Epoch 106, test loss: 0.185794\n",
      "Epoch 107, test loss: 0.187701\n",
      "Epoch 108, test loss: 0.185076\n",
      "Epoch 109, test loss: 0.185829\n",
      "Epoch 110, test loss: 0.194267\n",
      "Epoch 111, test loss: 0.188378\n",
      "Epoch 112, test loss: 0.185304\n",
      "Epoch 113, test loss: 0.186081\n",
      "Epoch 114, test loss: 0.192016\n",
      "Epoch 115, test loss: 0.187354\n",
      "Epoch 116, test loss: 0.185286\n",
      "Epoch 117, test loss: 0.188042\n",
      "Epoch 118, test loss: 0.186403\n",
      "Epoch 119, test loss: 0.186143\n",
      "Epoch 120, test loss: 0.185284\n",
      "Epoch 121, test loss: 0.186861\n",
      "Epoch 122, test loss: 0.186351\n",
      "Epoch 123, test loss: 0.187301\n",
      "Epoch 124, test loss: 0.191845\n",
      "Epoch 125, test loss: 0.192174\n",
      "Epoch 126, test loss: 0.187419\n",
      "Epoch 127, test loss: 0.186764\n",
      "Epoch 128, test loss: 0.185835\n",
      "Epoch 129, test loss: 0.185541\n",
      "Epoch 130, test loss: 0.185958\n",
      "Epoch 131, test loss: 0.185260\n",
      "Epoch 132, test loss: 0.185516\n",
      "Epoch 133, test loss: 0.189734\n",
      "Epoch 134, test loss: 0.187611\n",
      "Epoch 135, test loss: 0.192605\n",
      "Epoch 136, test loss: 0.188106\n",
      "Epoch 137, test loss: 0.190661\n",
      "Epoch 138, test loss: 0.186735\n",
      "Epoch 139, test loss: 0.185876\n",
      "Epoch 140, test loss: 0.186818\n",
      "Epoch 141, test loss: 0.185079\n",
      "Epoch 142, test loss: 0.186754\n",
      "Epoch 143, test loss: 0.187412\n",
      "Epoch 144, test loss: 0.185506\n",
      "Epoch 145, test loss: 0.190745\n",
      "Epoch 146, test loss: 0.186874\n",
      "Epoch 147, test loss: 0.185798\n",
      "Epoch 148, test loss: 0.185250\n",
      "Epoch 149, test loss: 0.185018\n",
      "Epoch 150, test loss: 0.185090\n",
      "Epoch 151, test loss: 0.187397\n",
      "Epoch 152, test loss: 0.186304\n",
      "Epoch 153, test loss: 0.188861\n",
      "Epoch 154, test loss: 0.187117\n",
      "Epoch 155, test loss: 0.192946\n",
      "Epoch 156, test loss: 0.187199\n",
      "Epoch 157, test loss: 0.185358\n",
      "Epoch 158, test loss: 0.185410\n",
      "Epoch 159, test loss: 0.185135\n",
      "Epoch 160, test loss: 0.186803\n",
      "Epoch 161, test loss: 0.185093\n",
      "Epoch 162, test loss: 0.189603\n",
      "Epoch 163, test loss: 0.185916\n",
      "Epoch 164, test loss: 0.186055\n",
      "Epoch 165, test loss: 0.185956\n",
      "Epoch 166, test loss: 0.188545\n",
      "Epoch 167, test loss: 0.185226\n",
      "Epoch 168, test loss: 0.184783\n",
      "Epoch 169, test loss: 0.186101\n",
      "Epoch 170, test loss: 0.186176\n",
      "Epoch 171, test loss: 0.184966\n",
      "Epoch 172, test loss: 0.187681\n",
      "Epoch 173, test loss: 0.186994\n",
      "Epoch 174, test loss: 0.185247\n",
      "Epoch 175, test loss: 0.185456\n",
      "Epoch 176, test loss: 0.186724\n",
      "Epoch 177, test loss: 0.185458\n",
      "Epoch 178, test loss: 0.188443\n",
      "Epoch 179, test loss: 0.186491\n",
      "Epoch 180, test loss: 0.188091\n",
      "Epoch 181, test loss: 0.185324\n",
      "Epoch 182, test loss: 0.186771\n",
      "Epoch 183, test loss: 0.186435\n",
      "Epoch 184, test loss: 0.186191\n",
      "Epoch 185, test loss: 0.189825\n",
      "Epoch 186, test loss: 0.184925\n",
      "Epoch 187, test loss: 0.186164\n",
      "Epoch 188, test loss: 0.185232\n",
      "Epoch 189, test loss: 0.185513\n",
      "Epoch 190, test loss: 0.185169\n",
      "Epoch 191, test loss: 0.185728\n",
      "Epoch 192, test loss: 0.184603\n",
      "Epoch 193, test loss: 0.185532\n",
      "Epoch 194, test loss: 0.185856\n",
      "Epoch 195, test loss: 0.184772\n",
      "Epoch 196, test loss: 0.191778\n",
      "Epoch 197, test loss: 0.186307\n",
      "Epoch 198, test loss: 0.187499\n",
      "Epoch 199, test loss: 0.185210\n",
      "Epoch 200, test loss: 0.187239\n",
      "Epoch 201, test loss: 0.187855\n",
      "Epoch 202, test loss: 0.188066\n",
      "Epoch 203, test loss: 0.189745\n",
      "Epoch 204, test loss: 0.184630\n",
      "Epoch 205, test loss: 0.185982\n",
      "Epoch 206, test loss: 0.186551\n",
      "Epoch 207, test loss: 0.185184\n",
      "Epoch 208, test loss: 0.184800\n",
      "Epoch 209, test loss: 0.184987\n",
      "Epoch 210, test loss: 0.184946\n",
      "Epoch 211, test loss: 0.185747\n",
      "Epoch 212, test loss: 0.185542\n",
      "Epoch 213, test loss: 0.187416\n",
      "Epoch 214, test loss: 0.184880\n",
      "Epoch 215, test loss: 0.192885\n",
      "Epoch 216, test loss: 0.185882\n",
      "Epoch 217, test loss: 0.186164\n",
      "Epoch 218, test loss: 0.186084\n",
      "Epoch 219, test loss: 0.185198\n",
      "Epoch 220, test loss: 0.184895\n",
      "Epoch 221, test loss: 0.185772\n",
      "Epoch 222, test loss: 0.185755\n",
      "Epoch 223, test loss: 0.186748\n",
      "Epoch 224, test loss: 0.185558\n",
      "Epoch 225, test loss: 0.186589\n",
      "Epoch 226, test loss: 0.186017\n",
      "Epoch 227, test loss: 0.184801\n",
      "Epoch 228, test loss: 0.186990\n",
      "Epoch 229, test loss: 0.185054\n",
      "Epoch 230, test loss: 0.188235\n",
      "Epoch 231, test loss: 0.185941\n",
      "Epoch 232, test loss: 0.186801\n",
      "Epoch 233, test loss: 0.189571\n",
      "Epoch 234, test loss: 0.188641\n",
      "Epoch 235, test loss: 0.185113\n",
      "Epoch 236, test loss: 0.184989\n",
      "Epoch 237, test loss: 0.187917\n",
      "Epoch 238, test loss: 0.184889\n",
      "Epoch 239, test loss: 0.189036\n",
      "Epoch 240, test loss: 0.185592\n",
      "Epoch 241, test loss: 0.186222\n",
      "Epoch 242, test loss: 0.184697\n",
      "Epoch 243, test loss: 0.184716\n",
      "Epoch 244, test loss: 0.187618\n",
      "Epoch 245, test loss: 0.184728\n",
      "Epoch 246, test loss: 0.186345\n",
      "Epoch 247, test loss: 0.187144\n",
      "Epoch 248, test loss: 0.184813\n",
      "Epoch 249, test loss: 0.187708\n",
      "Epoch 250, test loss: 0.185632\n",
      "Epoch 251, test loss: 0.187540\n",
      "Epoch 252, test loss: 0.185125\n",
      "Epoch 253, test loss: 0.194238\n",
      "Epoch 254, test loss: 0.185868\n",
      "Epoch 255, test loss: 0.186523\n",
      "Epoch 256, test loss: 0.184850\n",
      "Epoch 257, test loss: 0.185104\n",
      "Epoch 258, test loss: 0.186743\n",
      "Epoch 259, test loss: 0.186801\n",
      "Epoch 260, test loss: 0.185404\n",
      "Epoch 261, test loss: 0.185254\n",
      "Epoch 262, test loss: 0.186563\n",
      "Epoch 263, test loss: 0.185529\n",
      "Epoch 264, test loss: 0.185053\n",
      "Epoch 265, test loss: 0.184937\n",
      "Epoch 266, test loss: 0.184908\n",
      "Epoch 267, test loss: 0.184988\n",
      "Epoch 268, test loss: 0.185066\n",
      "Epoch 269, test loss: 0.187843\n",
      "Epoch 270, test loss: 0.187259\n",
      "Epoch 271, test loss: 0.194032\n",
      "Epoch 272, test loss: 0.185100\n",
      "Epoch 273, test loss: 0.187727\n",
      "Epoch 274, test loss: 0.187976\n",
      "Epoch 275, test loss: 0.188794\n",
      "Epoch 276, test loss: 0.185059\n",
      "Epoch 277, test loss: 0.186008\n",
      "Epoch 278, test loss: 0.189762\n",
      "Epoch 279, test loss: 0.191241\n",
      "Epoch 280, test loss: 0.187449\n",
      "Epoch 281, test loss: 0.185198\n",
      "Epoch 282, test loss: 0.186675\n",
      "Epoch 283, test loss: 0.184966\n",
      "Epoch 284, test loss: 0.185348\n",
      "Epoch 285, test loss: 0.191688\n",
      "Epoch 286, test loss: 0.184873\n",
      "Epoch 287, test loss: 0.188359\n",
      "Epoch 288, test loss: 0.184910\n",
      "Epoch 289, test loss: 0.187127\n",
      "Epoch 290, test loss: 0.186355\n",
      "Epoch 291, test loss: 0.187303\n",
      "Epoch 292, test loss: 0.188220\n",
      "Epoch 293, test loss: 0.186307\n",
      "Epoch 294, test loss: 0.189092\n",
      "Epoch 295, test loss: 0.186856\n",
      "Epoch 296, test loss: 0.191573\n",
      "Epoch 297, test loss: 0.187870\n",
      "Epoch 298, test loss: 0.186935\n",
      "Epoch 299, test loss: 0.189852\n",
      "Epoch 300, test loss: 0.184935\n",
      "Epoch 301, test loss: 0.185535\n",
      "Epoch 302, test loss: 0.186229\n",
      "Epoch 303, test loss: 0.185044\n",
      "Epoch 304, test loss: 0.187373\n",
      "Epoch 305, test loss: 0.185008\n",
      "Epoch 306, test loss: 0.184930\n",
      "Epoch 307, test loss: 0.186463\n",
      "Epoch 308, test loss: 0.197239\n",
      "Epoch 309, test loss: 0.187379\n",
      "Epoch 310, test loss: 0.185911\n",
      "Epoch 311, test loss: 0.186522\n",
      "Epoch 312, test loss: 0.189590\n",
      "Epoch 313, test loss: 0.184992\n",
      "Epoch 314, test loss: 0.185059\n",
      "Epoch 315, test loss: 0.187938\n",
      "Epoch 316, test loss: 0.184959\n",
      "Epoch 317, test loss: 0.186798\n",
      "Epoch 318, test loss: 0.185358\n",
      "Epoch 319, test loss: 0.187371\n",
      "Epoch 320, test loss: 0.186615\n",
      "Epoch 321, test loss: 0.186327\n",
      "Epoch 322, test loss: 0.186644\n",
      "Epoch 323, test loss: 0.184982\n",
      "Epoch 324, test loss: 0.189360\n",
      "Epoch 325, test loss: 0.185330\n",
      "Epoch 326, test loss: 0.185733\n",
      "Epoch 327, test loss: 0.187726\n",
      "Epoch 328, test loss: 0.186009\n",
      "Epoch 329, test loss: 0.184984\n",
      "Epoch 330, test loss: 0.184810\n",
      "Epoch 331, test loss: 0.185026\n",
      "Epoch 332, test loss: 0.188038\n",
      "Epoch 333, test loss: 0.184637\n",
      "Epoch 334, test loss: 0.185796\n",
      "Epoch 335, test loss: 0.184759\n",
      "Epoch 336, test loss: 0.187380\n",
      "Epoch 337, test loss: 0.188736\n",
      "Epoch 338, test loss: 0.184650\n",
      "Epoch 339, test loss: 0.189900\n",
      "Epoch 340, test loss: 0.186268\n",
      "Epoch 341, test loss: 0.184911\n",
      "Epoch 342, test loss: 0.184779\n",
      "Epoch 343, test loss: 0.185357\n",
      "Epoch 344, test loss: 0.191816\n",
      "Epoch 345, test loss: 0.184650\n",
      "Epoch 346, test loss: 0.193808\n",
      "Epoch 347, test loss: 0.187855\n",
      "Epoch 348, test loss: 0.187530\n",
      "Epoch 349, test loss: 0.193358\n",
      "Epoch 350, test loss: 0.185362\n",
      "Epoch 351, test loss: 0.189632\n",
      "Epoch 352, test loss: 0.195221\n",
      "Epoch 353, test loss: 0.186124\n",
      "Epoch 354, test loss: 0.187405\n",
      "Epoch 355, test loss: 0.185064\n",
      "Epoch 356, test loss: 0.193107\n",
      "Epoch 357, test loss: 0.185665\n",
      "Epoch 358, test loss: 0.185589\n",
      "Epoch 359, test loss: 0.185271\n",
      "Epoch 360, test loss: 0.185543\n",
      "Epoch 361, test loss: 0.185769\n",
      "Epoch 362, test loss: 0.184692\n",
      "Epoch 363, test loss: 0.184835\n",
      "Epoch 364, test loss: 0.187609\n",
      "Epoch 365, test loss: 0.185733\n",
      "Epoch 366, test loss: 0.185169\n",
      "Epoch 367, test loss: 0.187372\n",
      "Epoch 368, test loss: 0.186286\n",
      "Epoch 369, test loss: 0.186583\n",
      "Epoch 370, test loss: 0.185098\n",
      "Epoch 371, test loss: 0.188241\n",
      "Epoch 372, test loss: 0.185747\n",
      "Epoch 373, test loss: 0.186453\n",
      "Epoch 374, test loss: 0.190178\n",
      "Epoch 375, test loss: 0.185933\n",
      "Epoch 376, test loss: 0.188861\n",
      "Epoch 377, test loss: 0.184851\n",
      "Epoch 378, test loss: 0.187902\n",
      "Epoch 379, test loss: 0.186397\n",
      "Epoch 380, test loss: 0.185380\n",
      "Epoch 381, test loss: 0.185200\n",
      "Epoch 382, test loss: 0.184911\n",
      "Epoch 383, test loss: 0.185381\n",
      "Epoch 384, test loss: 0.185671\n",
      "Epoch 385, test loss: 0.185095\n",
      "Epoch 386, test loss: 0.187625\n",
      "Epoch 387, test loss: 0.187135\n",
      "Epoch 388, test loss: 0.184704\n",
      "Epoch 389, test loss: 0.185462\n",
      "Epoch 390, test loss: 0.187033\n",
      "Epoch 391, test loss: 0.186342\n",
      "Epoch 392, test loss: 0.185521\n",
      "Epoch 393, test loss: 0.185108\n",
      "Epoch 394, test loss: 0.187336\n",
      "Epoch 395, test loss: 0.187754\n",
      "Epoch 396, test loss: 0.185519\n",
      "Epoch 397, test loss: 0.186279\n",
      "Epoch 398, test loss: 0.185142\n",
      "Epoch 399, test loss: 0.192528\n",
      "Epoch 400, test loss: 0.186331\n",
      "Epoch 401, test loss: 0.188251\n",
      "Epoch 402, test loss: 0.184956\n",
      "Epoch 403, test loss: 0.187615\n",
      "Epoch 404, test loss: 0.188780\n",
      "Epoch 405, test loss: 0.185039\n",
      "Epoch 406, test loss: 0.184800\n",
      "Epoch 407, test loss: 0.185822\n",
      "Epoch 408, test loss: 0.188659\n",
      "Epoch 409, test loss: 0.186217\n",
      "Epoch 410, test loss: 0.185295\n",
      "Epoch 411, test loss: 0.185336\n",
      "Epoch 412, test loss: 0.196646\n",
      "Epoch 413, test loss: 0.184948\n",
      "Epoch 414, test loss: 0.190092\n",
      "Epoch 415, test loss: 0.187001\n",
      "Epoch 416, test loss: 0.187103\n",
      "Epoch 417, test loss: 0.184878\n",
      "Epoch 418, test loss: 0.190397\n",
      "Epoch 419, test loss: 0.191075\n",
      "Epoch 420, test loss: 0.186955\n",
      "Epoch 421, test loss: 0.185257\n",
      "Epoch 422, test loss: 0.187322\n",
      "Epoch 423, test loss: 0.185635\n",
      "Epoch 424, test loss: 0.186593\n",
      "Epoch 425, test loss: 0.185601\n",
      "Epoch 426, test loss: 0.187591\n",
      "Epoch 427, test loss: 0.189262\n",
      "Epoch 428, test loss: 0.185196\n",
      "Epoch 429, test loss: 0.185140\n",
      "Epoch 430, test loss: 0.186126\n",
      "Epoch 431, test loss: 0.185437\n",
      "Epoch 432, test loss: 0.192796\n",
      "Epoch 433, test loss: 0.189308\n",
      "Epoch 434, test loss: 0.184749\n",
      "Epoch 435, test loss: 0.185693\n",
      "Epoch 436, test loss: 0.185034\n",
      "Epoch 437, test loss: 0.185581\n",
      "Epoch 438, test loss: 0.186045\n",
      "Epoch 439, test loss: 0.185034\n",
      "Epoch 440, test loss: 0.187548\n",
      "Epoch 441, test loss: 0.184817\n",
      "Epoch 442, test loss: 0.186739\n",
      "Epoch 443, test loss: 0.185471\n",
      "Epoch 444, test loss: 0.187576\n",
      "Epoch 445, test loss: 0.185446\n",
      "Epoch 446, test loss: 0.189485\n",
      "Epoch 447, test loss: 0.186010\n",
      "Epoch 448, test loss: 0.185643\n",
      "Epoch 449, test loss: 0.188135\n",
      "Pretrain data: 19742408.0\n",
      "Building dataset, requesting data from 0 to 769\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [266. 263. 259. 254. 250. 254.]\n",
      " [263. 259. 254. 250. 254. 261.]\n",
      " [259. 254. 250. 254. 261. 267.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [258. 258. 258. 258. 258. 258.]\n",
      " [257. 257. 257. 257. 257. 257.]\n",
      " [255. 255. 255. 255. 255. 255.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6845/107816\n",
      "Found 769 continuous time series\n",
      "Data shape: (114663, 6), Train/test: 114661/2\n",
      "Train test ratio: 57330.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1791\n",
      "Epoch 0, train loss: 0.196157\n",
      "Epoch 1, train loss: 0.215080\n",
      "Epoch 2, train loss: 0.230540\n",
      "Epoch 3, train loss: 0.194758\n",
      "Epoch 4, train loss: 0.240800\n",
      "Epoch 5, train loss: 0.227339\n",
      "Epoch 6, train loss: 0.175520\n",
      "Epoch 7, train loss: 0.201335\n",
      "Epoch 8, train loss: 0.265179\n",
      "Epoch 9, train loss: 0.227220\n",
      "Epoch 10, train loss: 0.264597\n",
      "Epoch 11, train loss: 0.240575\n",
      "Epoch 12, train loss: 0.187467\n",
      "Epoch 13, train loss: 0.186703\n",
      "Epoch 14, train loss: 0.239036\n",
      "Epoch 15, train loss: 0.292165\n",
      "Epoch 16, train loss: 0.206044\n",
      "Epoch 17, train loss: 0.198114\n",
      "Epoch 18, train loss: 0.195451\n",
      "Epoch 19, train loss: 0.169298\n",
      "Epoch 20, train loss: 0.255445\n",
      "Epoch 21, train loss: 0.219258\n",
      "Epoch 22, train loss: 0.196353\n",
      "Epoch 23, train loss: 0.162922\n",
      "Epoch 24, train loss: 0.181407\n",
      "Epoch 25, train loss: 0.354349\n",
      "Epoch 26, train loss: 0.175802\n",
      "Epoch 27, train loss: 0.204265\n",
      "Epoch 28, train loss: 0.193625\n",
      "Epoch 29, train loss: 0.177848\n",
      "Epoch 30, train loss: 0.176866\n",
      "Epoch 31, train loss: 0.229456\n",
      "Epoch 32, train loss: 0.260767\n",
      "Epoch 33, train loss: 0.212854\n",
      "Epoch 34, train loss: 0.206693\n",
      "Epoch 35, train loss: 0.197006\n",
      "Epoch 36, train loss: 0.178081\n",
      "Epoch 37, train loss: 0.163167\n",
      "Epoch 38, train loss: 0.232581\n",
      "Epoch 39, train loss: 0.236146\n",
      "Epoch 40, train loss: 0.185610\n",
      "Epoch 41, train loss: 0.202267\n",
      "Epoch 42, train loss: 0.183650\n",
      "Epoch 43, train loss: 0.188774\n",
      "Epoch 44, train loss: 0.220227\n",
      "Epoch 45, train loss: 0.203305\n",
      "Epoch 46, train loss: 0.201184\n",
      "Epoch 47, train loss: 0.171119\n",
      "Epoch 48, train loss: 0.237010\n",
      "Epoch 49, train loss: 0.204626\n",
      "Epoch 50, train loss: 0.149798\n",
      "Epoch 51, train loss: 0.295417\n",
      "Epoch 52, train loss: 0.202270\n",
      "Epoch 53, train loss: 0.194687\n",
      "Epoch 54, train loss: 0.185982\n",
      "Epoch 55, train loss: 0.186631\n",
      "Epoch 56, train loss: 0.235150\n",
      "Epoch 57, train loss: 0.313984\n",
      "Epoch 58, train loss: 0.198660\n",
      "Epoch 59, train loss: 0.187256\n",
      "Epoch 60, train loss: 0.212619\n",
      "Epoch 61, train loss: 0.200747\n",
      "Epoch 62, train loss: 0.226879\n",
      "Epoch 63, train loss: 0.225518\n",
      "Epoch 64, train loss: 0.203465\n",
      "Epoch 65, train loss: 0.214334\n",
      "Epoch 66, train loss: 0.253275\n",
      "Epoch 67, train loss: 0.235451\n",
      "Epoch 68, train loss: 0.221195\n",
      "Epoch 69, train loss: 0.185040\n",
      "Epoch 70, train loss: 0.194837\n",
      "Epoch 71, train loss: 0.206228\n",
      "Epoch 72, train loss: 0.165233\n",
      "Epoch 73, train loss: 0.191462\n",
      "Epoch 74, train loss: 0.206191\n",
      "Epoch 75, train loss: 0.201556\n",
      "Epoch 76, train loss: 0.200296\n",
      "Epoch 77, train loss: 0.167374\n",
      "Epoch 78, train loss: 0.246894\n",
      "Epoch 79, train loss: 0.134786\n",
      "Epoch 80, train loss: 0.236537\n",
      "Epoch 81, train loss: 0.246163\n",
      "Epoch 82, train loss: 0.160400\n",
      "Epoch 83, train loss: 0.187031\n",
      "Epoch 84, train loss: 0.165644\n",
      "Epoch 85, train loss: 0.164645\n",
      "Epoch 86, train loss: 0.245081\n",
      "Epoch 87, train loss: 0.250624\n",
      "Epoch 88, train loss: 0.175535\n",
      "Epoch 89, train loss: 0.205984\n",
      "Epoch 90, train loss: 0.180576\n",
      "Epoch 91, train loss: 0.208173\n",
      "Epoch 92, train loss: 0.265164\n",
      "Epoch 93, train loss: 0.237685\n",
      "Epoch 94, train loss: 0.186059\n",
      "Epoch 95, train loss: 0.254364\n",
      "Epoch 96, train loss: 0.214785\n",
      "Epoch 97, train loss: 0.171527\n",
      "Epoch 98, train loss: 0.192785\n",
      "Epoch 99, train loss: 0.197014\n",
      "Epoch 100, train loss: 0.194375\n",
      "Epoch 101, train loss: 0.231988\n",
      "Epoch 102, train loss: 0.196829\n",
      "Epoch 103, train loss: 0.200052\n",
      "Epoch 104, train loss: 0.237046\n",
      "Epoch 105, train loss: 0.175937\n",
      "Epoch 106, train loss: 0.164375\n",
      "Epoch 107, train loss: 0.192987\n",
      "Epoch 108, train loss: 0.225859\n",
      "Epoch 109, train loss: 0.263924\n",
      "Epoch 110, train loss: 0.182549\n",
      "Epoch 111, train loss: 0.140522\n",
      "Epoch 112, train loss: 0.252946\n",
      "Epoch 113, train loss: 0.190367\n",
      "Epoch 114, train loss: 0.217521\n",
      "Epoch 115, train loss: 0.192452\n",
      "Epoch 116, train loss: 0.223714\n",
      "Epoch 117, train loss: 0.240045\n",
      "Epoch 118, train loss: 0.123587\n",
      "Epoch 119, train loss: 0.195193\n",
      "Epoch 120, train loss: 0.188202\n",
      "Epoch 121, train loss: 0.229587\n",
      "Epoch 122, train loss: 0.248438\n",
      "Epoch 123, train loss: 0.197321\n",
      "Epoch 124, train loss: 0.215887\n",
      "Epoch 125, train loss: 0.190919\n",
      "Epoch 126, train loss: 0.361267\n",
      "Epoch 127, train loss: 0.241573\n",
      "Epoch 128, train loss: 0.194235\n",
      "Epoch 129, train loss: 0.209257\n",
      "Epoch 130, train loss: 0.157797\n",
      "Epoch 131, train loss: 0.257473\n",
      "Epoch 132, train loss: 0.189071\n",
      "Epoch 133, train loss: 0.194175\n",
      "Epoch 134, train loss: 0.201024\n",
      "Epoch 135, train loss: 0.171882\n",
      "Epoch 136, train loss: 0.182876\n",
      "Epoch 137, train loss: 0.236625\n",
      "Epoch 138, train loss: 0.167836\n",
      "Epoch 139, train loss: 0.216123\n",
      "Epoch 140, train loss: 0.160216\n",
      "Epoch 141, train loss: 0.218092\n",
      "Epoch 142, train loss: 0.200370\n",
      "Epoch 143, train loss: 0.174605\n",
      "Epoch 144, train loss: 0.193237\n",
      "Epoch 145, train loss: 0.215476\n",
      "Epoch 146, train loss: 0.176240\n",
      "Epoch 147, train loss: 0.177065\n",
      "Epoch 148, train loss: 0.229541\n",
      "Epoch 149, train loss: 0.217839\n",
      "Reading 11 segments\n",
      "Building dataset, requesting data from 0 to 11\n",
      "x here is\n",
      "[[214. 217. 217. 212. 209. 208.]\n",
      " [217. 217. 212. 209. 208. 209.]\n",
      " [217. 212. 209. 208. 209. 209.]\n",
      " ...\n",
      " [128. 124. 126. 128. 130. 124.]\n",
      " [124. 126. 128. 130. 124. 120.]\n",
      " [126. 128. 130. 124. 120. 117.]]\n",
      "y here is\n",
      "[[190. 190. 190. 190. 190. 190.]\n",
      " [183. 183. 183. 183. 183. 183.]\n",
      " [179. 179. 179. 179. 179. 179.]\n",
      " ...\n",
      " [129. 129. 129. 129. 129. 129.]\n",
      " [139. 139. 139. 139. 139. 139.]\n",
      " [157. 157. 157. 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 11 continuous time series\n",
      "Data shape: (2469, 6), Train/test: 1/2468\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 73 segments\n",
      "Building dataset, requesting data from 0 to 73\n",
      "x here is\n",
      "[[128. 123. 120. 124. 121. 120.]\n",
      " [123. 120. 124. 121. 120. 121.]\n",
      " [120. 124. 121. 120. 121. 121.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[118. 118. 118. 118. 118. 118.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1440/9623\n",
      "Found 73 continuous time series\n",
      "Data shape: (11065, 6), Train/test: 11063/2\n",
      "Train test ratio: 5531.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610889960>\n",
      "Epoch 0, test loss: 0.220589\n",
      "Epoch 1, test loss: 0.221171\n",
      "Epoch 2, test loss: 0.219888\n",
      "Epoch 3, test loss: 0.221601\n",
      "Epoch 4, test loss: 0.221674\n",
      "Epoch 5, test loss: 0.223400\n",
      "Epoch 6, test loss: 0.222486\n",
      "Epoch 7, test loss: 0.220845\n",
      "Epoch 8, test loss: 0.221107\n",
      "Epoch 9, test loss: 0.221507\n",
      "Epoch 10, test loss: 0.220637\n",
      "Epoch 11, test loss: 0.220361\n",
      "Epoch 12, test loss: 0.221121\n",
      "Epoch 13, test loss: 0.222247\n",
      "Epoch 14, test loss: 0.224392\n",
      "Epoch 15, test loss: 0.222198\n",
      "Epoch 16, test loss: 0.221425\n",
      "Epoch 17, test loss: 0.220376\n",
      "Epoch 18, test loss: 0.223472\n",
      "Epoch 19, test loss: 0.220980\n",
      "Epoch 20, test loss: 0.225123\n",
      "Epoch 21, test loss: 0.220775\n",
      "Epoch 22, test loss: 0.222292\n",
      "Epoch 23, test loss: 0.220279\n",
      "Epoch 24, test loss: 0.223226\n",
      "Epoch 25, test loss: 0.220318\n",
      "Epoch 26, test loss: 0.219767\n",
      "Epoch 27, test loss: 0.221232\n",
      "Epoch 28, test loss: 0.221210\n",
      "Epoch 29, test loss: 0.220930\n",
      "Epoch 30, test loss: 0.227179\n",
      "Epoch 31, test loss: 0.222285\n",
      "Epoch 32, test loss: 0.220283\n",
      "Epoch 33, test loss: 0.221785\n",
      "Epoch 34, test loss: 0.226338\n",
      "Epoch 35, test loss: 0.219985\n",
      "Epoch 36, test loss: 0.220751\n",
      "Epoch 37, test loss: 0.220145\n",
      "Epoch 38, test loss: 0.220680\n",
      "Epoch 39, test loss: 0.220356\n",
      "Epoch 40, test loss: 0.220322\n",
      "Epoch 41, test loss: 0.220232\n",
      "Epoch 42, test loss: 0.221648\n",
      "Epoch 43, test loss: 0.224000\n",
      "Epoch 44, test loss: 0.223176\n",
      "Epoch 45, test loss: 0.221292\n",
      "Epoch 46, test loss: 0.221452\n",
      "Epoch 47, test loss: 0.221092\n",
      "Epoch 48, test loss: 0.221041\n",
      "Epoch 49, test loss: 0.221470\n",
      "Epoch 50, test loss: 0.223741\n",
      "Epoch 51, test loss: 0.224824\n",
      "Epoch 52, test loss: 0.220901\n",
      "Epoch 53, test loss: 0.220168\n",
      "Epoch 54, test loss: 0.221658\n",
      "Epoch 55, test loss: 0.220170\n",
      "Epoch 56, test loss: 0.221733\n",
      "Epoch 57, test loss: 0.220560\n",
      "Epoch 58, test loss: 0.221974\n",
      "Epoch 59, test loss: 0.220908\n",
      "Epoch 60, test loss: 0.219520\n",
      "Epoch 61, test loss: 0.222127\n",
      "Epoch 62, test loss: 0.221770\n",
      "Epoch 63, test loss: 0.226825\n",
      "Epoch 64, test loss: 0.227604\n",
      "Epoch 65, test loss: 0.227478\n",
      "Epoch 66, test loss: 0.221534\n",
      "Epoch 67, test loss: 0.221219\n",
      "Epoch 68, test loss: 0.220265\n",
      "Epoch 69, test loss: 0.222118\n",
      "Epoch 70, test loss: 0.223995\n",
      "Epoch 71, test loss: 0.223843\n",
      "Epoch 72, test loss: 0.223591\n",
      "Epoch 73, test loss: 0.222725\n",
      "Epoch 74, test loss: 0.221282\n",
      "Epoch 75, test loss: 0.221882\n",
      "Epoch 76, test loss: 0.220938\n",
      "Epoch 77, test loss: 0.225786\n",
      "Epoch 78, test loss: 0.220058\n",
      "Epoch 79, test loss: 0.220057\n",
      "Epoch 80, test loss: 0.222124\n",
      "Epoch 81, test loss: 0.220829\n",
      "Epoch 82, test loss: 0.220331\n",
      "Epoch 83, test loss: 0.220842\n",
      "Epoch 84, test loss: 0.222075\n",
      "Epoch 85, test loss: 0.221428\n",
      "Epoch 86, test loss: 0.224993\n",
      "Epoch 87, test loss: 0.223049\n",
      "Epoch 88, test loss: 0.223168\n",
      "Epoch 89, test loss: 0.222236\n",
      "Epoch 90, test loss: 0.221913\n",
      "Epoch 91, test loss: 0.223873\n",
      "Epoch 92, test loss: 0.220246\n",
      "Epoch 93, test loss: 0.222349\n",
      "Epoch 94, test loss: 0.221860\n",
      "Epoch 95, test loss: 0.221233\n",
      "Epoch 96, test loss: 0.221352\n",
      "Epoch 97, test loss: 0.220912\n",
      "Epoch 98, test loss: 0.222328\n",
      "Epoch 99, test loss: 0.224217\n",
      "Epoch 100, test loss: 0.221603\n",
      "Epoch 101, test loss: 0.222443\n",
      "Epoch 102, test loss: 0.221445\n",
      "Epoch 103, test loss: 0.224201\n",
      "Epoch 104, test loss: 0.221702\n",
      "Epoch 105, test loss: 0.226078\n",
      "Epoch 106, test loss: 0.221940\n",
      "Epoch 107, test loss: 0.222693\n",
      "Epoch 108, test loss: 0.221659\n",
      "Epoch 109, test loss: 0.220738\n",
      "Epoch 110, test loss: 0.223744\n",
      "Epoch 111, test loss: 0.221775\n",
      "Epoch 112, test loss: 0.221086\n",
      "Epoch 113, test loss: 0.222763\n",
      "Epoch 114, test loss: 0.223671\n",
      "Epoch 115, test loss: 0.223264\n",
      "Epoch 116, test loss: 0.221874\n",
      "Epoch 117, test loss: 0.222452\n",
      "Epoch 118, test loss: 0.223417\n",
      "Epoch 119, test loss: 0.224224\n",
      "Epoch 120, test loss: 0.224935\n",
      "Epoch 121, test loss: 0.221377\n",
      "Epoch 122, test loss: 0.221391\n",
      "Epoch 123, test loss: 0.221475\n",
      "Epoch 124, test loss: 0.221970\n",
      "Epoch 125, test loss: 0.221775\n",
      "Epoch 126, test loss: 0.222738\n",
      "Epoch 127, test loss: 0.223875\n",
      "Epoch 128, test loss: 0.221538\n",
      "Epoch 129, test loss: 0.221270\n",
      "Epoch 130, test loss: 0.222884\n",
      "Epoch 131, test loss: 0.223631\n",
      "Epoch 132, test loss: 0.222906\n",
      "Epoch 133, test loss: 0.225291\n",
      "Epoch 134, test loss: 0.223985\n",
      "Epoch 135, test loss: 0.223414\n",
      "Epoch 136, test loss: 0.223526\n",
      "Epoch 137, test loss: 0.224688\n",
      "Epoch 138, test loss: 0.222268\n",
      "Epoch 139, test loss: 0.224734\n",
      "Epoch 140, test loss: 0.222450\n",
      "Epoch 141, test loss: 0.228613\n",
      "Epoch 142, test loss: 0.222718\n",
      "Epoch 143, test loss: 0.221595\n",
      "Epoch 144, test loss: 0.222091\n",
      "Epoch 145, test loss: 0.224579\n",
      "Epoch 146, test loss: 0.222583\n",
      "Epoch 147, test loss: 0.223657\n",
      "Epoch 148, test loss: 0.225306\n",
      "Epoch 149, test loss: 0.222155\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610889960>\n",
      "Epoch 0, test loss: 0.219725\n",
      "Epoch 1, test loss: 0.219740\n",
      "Epoch 2, test loss: 0.221045\n",
      "Epoch 3, test loss: 0.223553\n",
      "Epoch 4, test loss: 0.222664\n",
      "Epoch 5, test loss: 0.220206\n",
      "Epoch 6, test loss: 0.220526\n",
      "Epoch 7, test loss: 0.221193\n",
      "Epoch 8, test loss: 0.221295\n",
      "Epoch 9, test loss: 0.220041\n",
      "Epoch 10, test loss: 0.221195\n",
      "Epoch 11, test loss: 0.221114\n",
      "Epoch 12, test loss: 0.220968\n",
      "Epoch 13, test loss: 0.223059\n",
      "Epoch 14, test loss: 0.222165\n",
      "Epoch 15, test loss: 0.220920\n",
      "Epoch 16, test loss: 0.222023\n",
      "Epoch 17, test loss: 0.220257\n",
      "Epoch 18, test loss: 0.220823\n",
      "Epoch 19, test loss: 0.222172\n",
      "Epoch 20, test loss: 0.221512\n",
      "Epoch 21, test loss: 0.221540\n",
      "Epoch 22, test loss: 0.222412\n",
      "Epoch 23, test loss: 0.221437\n",
      "Epoch 24, test loss: 0.220725\n",
      "Epoch 25, test loss: 0.221349\n",
      "Epoch 26, test loss: 0.223072\n",
      "Epoch 27, test loss: 0.224744\n",
      "Epoch 28, test loss: 0.221560\n",
      "Epoch 29, test loss: 0.220176\n",
      "Epoch 30, test loss: 0.221702\n",
      "Epoch 31, test loss: 0.221474\n",
      "Epoch 32, test loss: 0.221968\n",
      "Epoch 33, test loss: 0.219946\n",
      "Epoch 34, test loss: 0.221415\n",
      "Epoch 35, test loss: 0.221259\n",
      "Epoch 36, test loss: 0.222841\n",
      "Epoch 37, test loss: 0.220740\n",
      "Epoch 38, test loss: 0.221352\n",
      "Epoch 39, test loss: 0.220795\n",
      "Epoch 40, test loss: 0.223485\n",
      "Epoch 41, test loss: 0.221414\n",
      "Epoch 42, test loss: 0.221100\n",
      "Epoch 43, test loss: 0.221600\n",
      "Epoch 44, test loss: 0.221184\n",
      "Epoch 45, test loss: 0.221809\n",
      "Epoch 46, test loss: 0.221121\n",
      "Epoch 47, test loss: 0.220388\n",
      "Epoch 48, test loss: 0.221680\n",
      "Epoch 49, test loss: 0.221744\n",
      "Epoch 50, test loss: 0.220629\n",
      "Epoch 51, test loss: 0.221336\n",
      "Epoch 52, test loss: 0.221675\n",
      "Epoch 53, test loss: 0.220904\n",
      "Epoch 54, test loss: 0.220572\n",
      "Epoch 55, test loss: 0.223041\n",
      "Epoch 56, test loss: 0.220011\n",
      "Epoch 57, test loss: 0.221478\n",
      "Epoch 58, test loss: 0.220958\n",
      "Epoch 59, test loss: 0.220736\n",
      "Epoch 60, test loss: 0.221364\n",
      "Epoch 61, test loss: 0.220160\n",
      "Epoch 62, test loss: 0.220886\n",
      "Epoch 63, test loss: 0.220201\n",
      "Epoch 64, test loss: 0.220902\n",
      "Epoch 65, test loss: 0.219939\n",
      "Epoch 66, test loss: 0.221858\n",
      "Epoch 67, test loss: 0.220446\n",
      "Epoch 68, test loss: 0.220804\n",
      "Epoch 69, test loss: 0.222234\n",
      "Epoch 70, test loss: 0.220040\n",
      "Epoch 71, test loss: 0.219216\n",
      "Epoch 72, test loss: 0.221042\n",
      "Epoch 73, test loss: 0.219985\n",
      "Epoch 74, test loss: 0.220023\n",
      "Epoch 75, test loss: 0.221223\n",
      "Epoch 76, test loss: 0.221740\n",
      "Epoch 77, test loss: 0.219501\n",
      "Epoch 78, test loss: 0.220397\n",
      "Epoch 79, test loss: 0.223415\n",
      "Epoch 80, test loss: 0.221528\n",
      "Epoch 81, test loss: 0.221499\n",
      "Epoch 82, test loss: 0.222177\n",
      "Epoch 83, test loss: 0.223018\n",
      "Epoch 84, test loss: 0.219634\n",
      "Epoch 85, test loss: 0.220388\n",
      "Epoch 86, test loss: 0.220104\n",
      "Epoch 87, test loss: 0.220683\n",
      "Epoch 88, test loss: 0.221483\n",
      "Epoch 89, test loss: 0.220080\n",
      "Epoch 90, test loss: 0.219894\n",
      "Epoch 91, test loss: 0.221597\n",
      "Epoch 92, test loss: 0.221041\n",
      "Epoch 93, test loss: 0.221454\n",
      "Epoch 94, test loss: 0.220028\n",
      "Epoch 95, test loss: 0.221031\n",
      "Epoch 96, test loss: 0.221040\n",
      "Epoch 97, test loss: 0.219674\n",
      "Epoch 98, test loss: 0.222449\n",
      "Epoch 99, test loss: 0.221403\n",
      "Epoch 100, test loss: 0.224821\n",
      "Epoch 101, test loss: 0.220394\n",
      "Epoch 102, test loss: 0.219957\n",
      "Epoch 103, test loss: 0.221686\n",
      "Epoch 104, test loss: 0.220213\n",
      "Epoch 105, test loss: 0.221096\n",
      "Epoch 106, test loss: 0.222104\n",
      "Epoch 107, test loss: 0.221590\n",
      "Epoch 108, test loss: 0.225882\n",
      "Epoch 109, test loss: 0.221107\n",
      "Epoch 110, test loss: 0.221179\n",
      "Epoch 111, test loss: 0.220612\n",
      "Epoch 112, test loss: 0.220444\n",
      "Epoch 113, test loss: 0.222405\n",
      "Epoch 114, test loss: 0.220353\n",
      "Epoch 115, test loss: 0.223620\n",
      "Epoch 116, test loss: 0.220688\n",
      "Epoch 117, test loss: 0.220518\n",
      "Epoch 118, test loss: 0.220287\n",
      "Epoch 119, test loss: 0.220102\n",
      "Epoch 120, test loss: 0.220927\n",
      "Epoch 121, test loss: 0.220834\n",
      "Epoch 122, test loss: 0.221643\n",
      "Epoch 123, test loss: 0.220039\n",
      "Epoch 124, test loss: 0.219613\n",
      "Epoch 125, test loss: 0.221317\n",
      "Epoch 126, test loss: 0.219885\n",
      "Epoch 127, test loss: 0.220376\n",
      "Epoch 128, test loss: 0.221270\n",
      "Epoch 129, test loss: 0.220667\n",
      "Epoch 130, test loss: 0.220270\n",
      "Epoch 131, test loss: 0.221236\n",
      "Epoch 132, test loss: 0.221503\n",
      "Epoch 133, test loss: 0.220553\n",
      "Epoch 134, test loss: 0.221255\n",
      "Epoch 135, test loss: 0.221021\n",
      "Epoch 136, test loss: 0.221176\n",
      "Epoch 137, test loss: 0.220873\n",
      "Epoch 138, test loss: 0.220912\n",
      "Epoch 139, test loss: 0.220512\n",
      "Epoch 140, test loss: 0.220453\n",
      "Epoch 141, test loss: 0.220712\n",
      "Epoch 142, test loss: 0.222036\n",
      "Epoch 143, test loss: 0.220462\n",
      "Epoch 144, test loss: 0.224422\n",
      "Epoch 145, test loss: 0.220696\n",
      "Epoch 146, test loss: 0.220019\n",
      "Epoch 147, test loss: 0.220117\n",
      "Epoch 148, test loss: 0.219865\n",
      "Epoch 149, test loss: 0.220013\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610889960>\n",
      "Epoch 0, test loss: 0.270973\n",
      "Epoch 1, test loss: 0.254680\n",
      "Epoch 2, test loss: 0.252541\n",
      "Epoch 3, test loss: 0.251469\n",
      "Epoch 4, test loss: 0.250728\n",
      "Epoch 5, test loss: 0.248423\n",
      "Epoch 6, test loss: 0.247473\n",
      "Epoch 7, test loss: 0.247297\n",
      "Epoch 8, test loss: 0.246946\n",
      "Epoch 9, test loss: 0.244058\n",
      "Epoch 10, test loss: 0.242245\n",
      "Epoch 11, test loss: 0.242183\n",
      "Epoch 12, test loss: 0.241775\n",
      "Epoch 13, test loss: 0.242046\n",
      "Epoch 14, test loss: 0.241799\n",
      "Epoch 15, test loss: 0.241845\n",
      "Epoch 16, test loss: 0.239856\n",
      "Epoch 17, test loss: 0.240464\n",
      "Epoch 18, test loss: 0.239260\n",
      "Epoch 19, test loss: 0.242660\n",
      "Epoch 20, test loss: 0.237795\n",
      "Epoch 21, test loss: 0.236812\n",
      "Epoch 22, test loss: 0.235881\n",
      "Epoch 23, test loss: 0.235164\n",
      "Epoch 24, test loss: 0.234259\n",
      "Epoch 25, test loss: 0.234166\n",
      "Epoch 26, test loss: 0.232866\n",
      "Epoch 27, test loss: 0.234349\n",
      "Epoch 28, test loss: 0.233059\n",
      "Epoch 29, test loss: 0.233435\n",
      "Epoch 30, test loss: 0.233142\n",
      "Epoch 31, test loss: 0.234108\n",
      "Epoch 32, test loss: 0.232872\n",
      "Epoch 33, test loss: 0.232987\n",
      "Epoch 34, test loss: 0.231601\n",
      "Epoch 35, test loss: 0.236638\n",
      "Epoch 36, test loss: 0.232117\n",
      "Epoch 37, test loss: 0.233884\n",
      "Epoch 38, test loss: 0.231985\n",
      "Epoch 39, test loss: 0.233461\n",
      "Epoch 40, test loss: 0.232338\n",
      "Epoch 41, test loss: 0.233888\n",
      "Epoch 42, test loss: 0.234345\n",
      "Epoch 43, test loss: 0.232409\n",
      "Epoch 44, test loss: 0.231276\n",
      "Epoch 45, test loss: 0.232732\n",
      "Epoch 46, test loss: 0.232510\n",
      "Epoch 47, test loss: 0.231942\n",
      "Epoch 48, test loss: 0.232794\n",
      "Epoch 49, test loss: 0.231390\n",
      "Epoch 50, test loss: 0.232960\n",
      "Epoch 51, test loss: 0.231896\n",
      "Epoch 52, test loss: 0.231830\n",
      "Epoch 53, test loss: 0.236312\n",
      "Epoch 54, test loss: 0.232474\n",
      "Epoch 55, test loss: 0.230810\n",
      "Epoch 56, test loss: 0.232235\n",
      "Epoch 57, test loss: 0.233557\n",
      "Epoch 58, test loss: 0.233682\n",
      "Epoch 59, test loss: 0.234551\n",
      "Epoch 60, test loss: 0.233357\n",
      "Epoch 61, test loss: 0.232933\n",
      "Epoch 62, test loss: 0.232988\n",
      "Epoch 63, test loss: 0.231161\n",
      "Epoch 64, test loss: 0.231490\n",
      "Epoch 65, test loss: 0.231529\n",
      "Epoch 66, test loss: 0.231989\n",
      "Epoch 67, test loss: 0.234126\n",
      "Epoch 68, test loss: 0.232822\n",
      "Epoch 69, test loss: 0.232556\n",
      "Epoch 70, test loss: 0.232352\n",
      "Epoch 71, test loss: 0.231662\n",
      "Epoch 72, test loss: 0.231595\n",
      "Epoch 73, test loss: 0.231953\n",
      "Epoch 74, test loss: 0.232224\n",
      "Epoch 75, test loss: 0.231765\n",
      "Epoch 76, test loss: 0.231854\n",
      "Epoch 77, test loss: 0.232993\n",
      "Epoch 78, test loss: 0.233266\n",
      "Epoch 79, test loss: 0.232442\n",
      "Epoch 80, test loss: 0.233101\n",
      "Epoch 81, test loss: 0.231949\n",
      "Epoch 82, test loss: 0.233779\n",
      "Epoch 83, test loss: 0.232955\n",
      "Epoch 84, test loss: 0.232750\n",
      "Epoch 85, test loss: 0.236088\n",
      "Epoch 86, test loss: 0.234696\n",
      "Epoch 87, test loss: 0.232566\n",
      "Epoch 88, test loss: 0.234425\n",
      "Epoch 89, test loss: 0.233648\n",
      "Epoch 90, test loss: 0.233742\n",
      "Epoch 91, test loss: 0.233220\n",
      "Epoch 92, test loss: 0.231961\n",
      "Epoch 93, test loss: 0.231160\n",
      "Epoch 94, test loss: 0.231236\n",
      "Epoch 95, test loss: 0.231020\n",
      "Epoch 96, test loss: 0.231917\n",
      "Epoch 97, test loss: 0.231690\n",
      "Epoch 98, test loss: 0.231598\n",
      "Epoch 99, test loss: 0.231758\n",
      "Epoch 100, test loss: 0.234004\n",
      "Epoch 101, test loss: 0.234767\n",
      "Epoch 102, test loss: 0.233041\n",
      "Epoch 103, test loss: 0.233797\n",
      "Epoch 104, test loss: 0.232393\n",
      "Epoch 105, test loss: 0.231890\n",
      "Epoch 106, test loss: 0.231969\n",
      "Epoch 107, test loss: 0.232742\n",
      "Epoch 108, test loss: 0.231131\n",
      "Epoch 109, test loss: 0.233841\n",
      "Epoch 110, test loss: 0.233971\n",
      "Epoch 111, test loss: 0.231678\n",
      "Epoch 112, test loss: 0.231873\n",
      "Epoch 113, test loss: 0.232925\n",
      "Epoch 114, test loss: 0.231843\n",
      "Epoch 115, test loss: 0.231800\n",
      "Epoch 116, test loss: 0.231615\n",
      "Epoch 117, test loss: 0.232178\n",
      "Epoch 118, test loss: 0.231833\n",
      "Epoch 119, test loss: 0.231959\n",
      "Epoch 120, test loss: 0.232482\n",
      "Epoch 121, test loss: 0.233219\n",
      "Epoch 122, test loss: 0.232905\n",
      "Epoch 123, test loss: 0.232817\n",
      "Epoch 124, test loss: 0.232202\n",
      "Epoch 125, test loss: 0.232333\n",
      "Epoch 126, test loss: 0.230618\n",
      "Epoch 127, test loss: 0.232611\n",
      "Epoch 128, test loss: 0.234661\n",
      "Epoch 129, test loss: 0.231550\n",
      "Epoch 130, test loss: 0.232115\n",
      "Epoch 131, test loss: 0.232271\n",
      "Epoch 132, test loss: 0.232800\n",
      "Epoch 133, test loss: 0.232166\n",
      "Epoch 134, test loss: 0.231408\n",
      "Epoch 135, test loss: 0.230891\n",
      "Epoch 136, test loss: 0.231916\n",
      "Epoch 137, test loss: 0.230609\n",
      "Epoch 138, test loss: 0.231648\n",
      "Epoch 139, test loss: 0.234449\n",
      "Epoch 140, test loss: 0.231597\n",
      "Epoch 141, test loss: 0.232753\n",
      "Epoch 142, test loss: 0.230888\n",
      "Epoch 143, test loss: 0.231531\n",
      "Epoch 144, test loss: 0.231484\n",
      "Epoch 145, test loss: 0.232702\n",
      "Epoch 146, test loss: 0.230881\n",
      "Epoch 147, test loss: 0.231276\n",
      "Epoch 148, test loss: 0.231151\n",
      "Epoch 149, test loss: 0.231039\n",
      "Epoch 150, test loss: 0.230220\n",
      "Epoch 151, test loss: 0.231852\n",
      "Epoch 152, test loss: 0.232177\n",
      "Epoch 153, test loss: 0.231620\n",
      "Epoch 154, test loss: 0.231312\n",
      "Epoch 155, test loss: 0.231001\n",
      "Epoch 156, test loss: 0.233316\n",
      "Epoch 157, test loss: 0.231978\n",
      "Epoch 158, test loss: 0.231761\n",
      "Epoch 159, test loss: 0.233121\n",
      "Epoch 160, test loss: 0.231774\n",
      "Epoch 161, test loss: 0.231960\n",
      "Epoch 162, test loss: 0.232976\n",
      "Epoch 163, test loss: 0.230811\n",
      "Epoch 164, test loss: 0.231454\n",
      "Epoch 165, test loss: 0.232042\n",
      "Epoch 166, test loss: 0.230828\n",
      "Epoch 167, test loss: 0.232233\n",
      "Epoch 168, test loss: 0.231378\n",
      "Epoch 169, test loss: 0.232519\n",
      "Epoch 170, test loss: 0.232312\n",
      "Epoch 171, test loss: 0.230887\n",
      "Epoch 172, test loss: 0.232850\n",
      "Epoch 173, test loss: 0.230600\n",
      "Epoch 174, test loss: 0.232028\n",
      "Epoch 175, test loss: 0.230813\n",
      "Epoch 176, test loss: 0.231250\n",
      "Epoch 177, test loss: 0.231643\n",
      "Epoch 178, test loss: 0.231959\n",
      "Epoch 179, test loss: 0.230696\n",
      "Epoch 180, test loss: 0.231816\n",
      "Epoch 181, test loss: 0.231826\n",
      "Epoch 182, test loss: 0.231347\n",
      "Epoch 183, test loss: 0.230986\n",
      "Epoch 184, test loss: 0.231212\n",
      "Epoch 185, test loss: 0.230271\n",
      "Epoch 186, test loss: 0.230500\n",
      "Epoch 187, test loss: 0.230727\n",
      "Epoch 188, test loss: 0.231868\n",
      "Epoch 189, test loss: 0.231235\n",
      "Epoch 190, test loss: 0.229091\n",
      "Epoch 191, test loss: 0.229895\n",
      "Epoch 192, test loss: 0.229295\n",
      "Epoch 193, test loss: 0.229322\n",
      "Epoch 194, test loss: 0.228434\n",
      "Epoch 195, test loss: 0.228991\n",
      "Epoch 196, test loss: 0.228617\n",
      "Epoch 197, test loss: 0.230242\n",
      "Epoch 198, test loss: 0.229057\n",
      "Epoch 199, test loss: 0.229915\n",
      "Epoch 200, test loss: 0.230399\n",
      "Epoch 201, test loss: 0.228596\n",
      "Epoch 202, test loss: 0.228933\n",
      "Epoch 203, test loss: 0.229535\n",
      "Epoch 204, test loss: 0.228262\n",
      "Epoch 205, test loss: 0.229633\n",
      "Epoch 206, test loss: 0.227877\n",
      "Epoch 207, test loss: 0.227717\n",
      "Epoch 208, test loss: 0.228302\n",
      "Epoch 209, test loss: 0.226898\n",
      "Epoch 210, test loss: 0.228340\n",
      "Epoch 211, test loss: 0.228646\n",
      "Epoch 212, test loss: 0.228231\n",
      "Epoch 213, test loss: 0.227439\n",
      "Epoch 214, test loss: 0.227083\n",
      "Epoch 215, test loss: 0.226983\n",
      "Epoch 216, test loss: 0.227655\n",
      "Epoch 217, test loss: 0.226631\n",
      "Epoch 218, test loss: 0.230389\n",
      "Epoch 219, test loss: 0.227804\n",
      "Epoch 220, test loss: 0.226914\n",
      "Epoch 221, test loss: 0.226793\n",
      "Epoch 222, test loss: 0.225999\n",
      "Epoch 223, test loss: 0.226129\n",
      "Epoch 224, test loss: 0.226427\n",
      "Epoch 225, test loss: 0.229484\n",
      "Epoch 226, test loss: 0.225502\n",
      "Epoch 227, test loss: 0.225710\n",
      "Epoch 228, test loss: 0.225624\n",
      "Epoch 229, test loss: 0.226381\n",
      "Epoch 230, test loss: 0.225589\n",
      "Epoch 231, test loss: 0.226549\n",
      "Epoch 232, test loss: 0.226315\n",
      "Epoch 233, test loss: 0.226347\n",
      "Epoch 234, test loss: 0.225545\n",
      "Epoch 235, test loss: 0.226602\n",
      "Epoch 236, test loss: 0.227396\n",
      "Epoch 237, test loss: 0.225776\n",
      "Epoch 238, test loss: 0.225596\n",
      "Epoch 239, test loss: 0.224961\n",
      "Epoch 240, test loss: 0.225187\n",
      "Epoch 241, test loss: 0.226377\n",
      "Epoch 242, test loss: 0.226258\n",
      "Epoch 243, test loss: 0.225299\n",
      "Epoch 244, test loss: 0.227786\n",
      "Epoch 245, test loss: 0.225606\n",
      "Epoch 246, test loss: 0.225098\n",
      "Epoch 247, test loss: 0.224814\n",
      "Epoch 248, test loss: 0.227919\n",
      "Epoch 249, test loss: 0.225368\n",
      "Epoch 250, test loss: 0.226832\n",
      "Epoch 251, test loss: 0.225595\n",
      "Epoch 252, test loss: 0.224897\n",
      "Epoch 253, test loss: 0.226775\n",
      "Epoch 254, test loss: 0.225760\n",
      "Epoch 255, test loss: 0.225510\n",
      "Epoch 256, test loss: 0.224613\n",
      "Epoch 257, test loss: 0.226451\n",
      "Epoch 258, test loss: 0.224100\n",
      "Epoch 259, test loss: 0.224826\n",
      "Epoch 260, test loss: 0.225070\n",
      "Epoch 261, test loss: 0.227251\n",
      "Epoch 262, test loss: 0.225366\n",
      "Epoch 263, test loss: 0.225284\n",
      "Epoch 264, test loss: 0.224400\n",
      "Epoch 265, test loss: 0.225296\n",
      "Epoch 266, test loss: 0.226712\n",
      "Epoch 267, test loss: 0.224790\n",
      "Epoch 268, test loss: 0.225421\n",
      "Epoch 269, test loss: 0.224767\n",
      "Epoch 270, test loss: 0.224786\n",
      "Epoch 271, test loss: 0.224686\n",
      "Epoch 272, test loss: 0.226593\n",
      "Epoch 273, test loss: 0.224508\n",
      "Epoch 274, test loss: 0.225913\n",
      "Epoch 275, test loss: 0.223863\n",
      "Epoch 276, test loss: 0.224192\n",
      "Epoch 277, test loss: 0.224498\n",
      "Epoch 278, test loss: 0.224866\n",
      "Epoch 279, test loss: 0.224636\n",
      "Epoch 280, test loss: 0.224756\n",
      "Epoch 281, test loss: 0.225924\n",
      "Epoch 282, test loss: 0.223962\n",
      "Epoch 283, test loss: 0.224501\n",
      "Epoch 284, test loss: 0.224374\n",
      "Epoch 285, test loss: 0.228001\n",
      "Epoch 286, test loss: 0.224099\n",
      "Epoch 287, test loss: 0.224194\n",
      "Epoch 288, test loss: 0.225168\n",
      "Epoch 289, test loss: 0.223869\n",
      "Epoch 290, test loss: 0.223681\n",
      "Epoch 291, test loss: 0.226004\n",
      "Epoch 292, test loss: 0.224789\n",
      "Epoch 293, test loss: 0.224019\n",
      "Epoch 294, test loss: 0.225760\n",
      "Epoch 295, test loss: 0.224436\n",
      "Epoch 296, test loss: 0.224786\n",
      "Epoch 297, test loss: 0.224213\n",
      "Epoch 298, test loss: 0.223799\n",
      "Epoch 299, test loss: 0.223413\n",
      "Epoch 300, test loss: 0.224278\n",
      "Epoch 301, test loss: 0.223600\n",
      "Epoch 302, test loss: 0.223872\n",
      "Epoch 303, test loss: 0.223345\n",
      "Epoch 304, test loss: 0.225276\n",
      "Epoch 305, test loss: 0.223392\n",
      "Epoch 306, test loss: 0.224677\n",
      "Epoch 307, test loss: 0.224791\n",
      "Epoch 308, test loss: 0.224679\n",
      "Epoch 309, test loss: 0.224788\n",
      "Epoch 310, test loss: 0.227690\n",
      "Epoch 311, test loss: 0.225102\n",
      "Epoch 312, test loss: 0.226085\n",
      "Epoch 313, test loss: 0.224645\n",
      "Epoch 314, test loss: 0.224768\n",
      "Epoch 315, test loss: 0.224161\n",
      "Epoch 316, test loss: 0.223616\n",
      "Epoch 317, test loss: 0.222754\n",
      "Epoch 318, test loss: 0.223466\n",
      "Epoch 319, test loss: 0.224415\n",
      "Epoch 320, test loss: 0.224973\n",
      "Epoch 321, test loss: 0.224927\n",
      "Epoch 322, test loss: 0.222955\n",
      "Epoch 323, test loss: 0.223475\n",
      "Epoch 324, test loss: 0.224433\n",
      "Epoch 325, test loss: 0.225136\n",
      "Epoch 326, test loss: 0.223692\n",
      "Epoch 327, test loss: 0.224499\n",
      "Epoch 328, test loss: 0.224554\n",
      "Epoch 329, test loss: 0.224149\n",
      "Epoch 330, test loss: 0.225541\n",
      "Epoch 331, test loss: 0.223528\n",
      "Epoch 332, test loss: 0.224879\n",
      "Epoch 333, test loss: 0.224220\n",
      "Epoch 334, test loss: 0.225538\n",
      "Epoch 335, test loss: 0.224117\n",
      "Epoch 336, test loss: 0.224875\n",
      "Epoch 337, test loss: 0.224019\n",
      "Epoch 338, test loss: 0.222857\n",
      "Epoch 339, test loss: 0.223513\n",
      "Epoch 340, test loss: 0.223861\n",
      "Epoch 341, test loss: 0.223742\n",
      "Epoch 342, test loss: 0.225364\n",
      "Epoch 343, test loss: 0.223293\n",
      "Epoch 344, test loss: 0.222997\n",
      "Epoch 345, test loss: 0.223613\n",
      "Epoch 346, test loss: 0.225252\n",
      "Epoch 347, test loss: 0.223693\n",
      "Epoch 348, test loss: 0.227372\n",
      "Epoch 349, test loss: 0.223659\n",
      "Epoch 350, test loss: 0.226724\n",
      "Epoch 351, test loss: 0.224497\n",
      "Epoch 352, test loss: 0.224718\n",
      "Epoch 353, test loss: 0.224691\n",
      "Epoch 354, test loss: 0.226381\n",
      "Epoch 355, test loss: 0.225134\n",
      "Epoch 356, test loss: 0.225825\n",
      "Epoch 357, test loss: 0.223533\n",
      "Epoch 358, test loss: 0.223589\n",
      "Epoch 359, test loss: 0.224992\n",
      "Epoch 360, test loss: 0.226154\n",
      "Epoch 361, test loss: 0.223564\n",
      "Epoch 362, test loss: 0.223593\n",
      "Epoch 363, test loss: 0.224646\n",
      "Epoch 364, test loss: 0.224542\n",
      "Epoch 365, test loss: 0.224546\n",
      "Epoch 366, test loss: 0.224880\n",
      "Epoch 367, test loss: 0.224036\n",
      "Epoch 368, test loss: 0.223197\n",
      "Epoch 369, test loss: 0.224925\n",
      "Epoch 370, test loss: 0.225057\n",
      "Epoch 371, test loss: 0.227409\n",
      "Epoch 372, test loss: 0.224679\n",
      "Epoch 373, test loss: 0.223824\n",
      "Epoch 374, test loss: 0.228024\n",
      "Epoch 375, test loss: 0.223985\n",
      "Epoch 376, test loss: 0.225402\n",
      "Epoch 377, test loss: 0.227046\n",
      "Epoch 378, test loss: 0.223231\n",
      "Epoch 379, test loss: 0.227621\n",
      "Epoch 380, test loss: 0.225966\n",
      "Epoch 381, test loss: 0.224164\n",
      "Epoch 382, test loss: 0.223789\n",
      "Epoch 383, test loss: 0.223995\n",
      "Epoch 384, test loss: 0.224965\n",
      "Epoch 385, test loss: 0.224350\n",
      "Epoch 386, test loss: 0.227084\n",
      "Epoch 387, test loss: 0.225069\n",
      "Epoch 388, test loss: 0.224107\n",
      "Epoch 389, test loss: 0.224401\n",
      "Epoch 390, test loss: 0.228376\n",
      "Epoch 391, test loss: 0.226144\n",
      "Epoch 392, test loss: 0.225283\n",
      "Epoch 393, test loss: 0.224379\n",
      "Epoch 394, test loss: 0.222887\n",
      "Epoch 395, test loss: 0.224193\n",
      "Epoch 396, test loss: 0.226498\n",
      "Epoch 397, test loss: 0.224783\n",
      "Epoch 398, test loss: 0.224663\n",
      "Epoch 399, test loss: 0.223149\n",
      "Epoch 400, test loss: 0.223223\n",
      "Epoch 401, test loss: 0.224168\n",
      "Epoch 402, test loss: 0.223634\n",
      "Epoch 403, test loss: 0.223761\n",
      "Epoch 404, test loss: 0.224306\n",
      "Epoch 405, test loss: 0.223937\n",
      "Epoch 406, test loss: 0.226601\n",
      "Epoch 407, test loss: 0.223525\n",
      "Epoch 408, test loss: 0.224135\n",
      "Epoch 409, test loss: 0.224781\n",
      "Epoch 410, test loss: 0.225495\n",
      "Epoch 411, test loss: 0.226920\n",
      "Epoch 412, test loss: 0.223568\n",
      "Epoch 413, test loss: 0.223957\n",
      "Epoch 414, test loss: 0.224440\n",
      "Epoch 415, test loss: 0.224709\n",
      "Epoch 416, test loss: 0.224587\n",
      "Epoch 417, test loss: 0.223456\n",
      "Epoch 418, test loss: 0.225710\n",
      "Epoch 419, test loss: 0.224019\n",
      "Epoch 420, test loss: 0.225546\n",
      "Epoch 421, test loss: 0.224073\n",
      "Epoch 422, test loss: 0.226839\n",
      "Epoch 423, test loss: 0.224720\n",
      "Epoch 424, test loss: 0.224011\n",
      "Epoch 425, test loss: 0.222791\n",
      "Epoch 426, test loss: 0.223696\n",
      "Epoch 427, test loss: 0.223627\n",
      "Epoch 428, test loss: 0.224999\n",
      "Epoch 429, test loss: 0.224362\n",
      "Epoch 430, test loss: 0.225769\n",
      "Epoch 431, test loss: 0.224286\n",
      "Epoch 432, test loss: 0.223921\n",
      "Epoch 433, test loss: 0.226839\n",
      "Epoch 434, test loss: 0.223773\n",
      "Epoch 435, test loss: 0.224399\n",
      "Epoch 436, test loss: 0.224287\n",
      "Epoch 437, test loss: 0.224349\n",
      "Epoch 438, test loss: 0.224505\n",
      "Epoch 439, test loss: 0.224513\n",
      "Epoch 440, test loss: 0.225391\n",
      "Epoch 441, test loss: 0.225925\n",
      "Epoch 442, test loss: 0.226794\n",
      "Epoch 443, test loss: 0.227705\n",
      "Epoch 444, test loss: 0.222998\n",
      "Epoch 445, test loss: 0.222943\n",
      "Epoch 446, test loss: 0.224817\n",
      "Epoch 447, test loss: 0.223124\n",
      "Epoch 448, test loss: 0.223928\n",
      "Epoch 449, test loss: 0.223727\n",
      "Pretrain data: 19732321.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7547/107625\n",
      "Found 815 continuous time series\n",
      "Data shape: (115174, 6), Train/test: 115172/2\n",
      "Train test ratio: 57586.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1799\n",
      "Epoch 0, train loss: 0.155701\n",
      "Epoch 1, train loss: 0.245683\n",
      "Epoch 2, train loss: 0.199120\n",
      "Epoch 3, train loss: 0.270633\n",
      "Epoch 4, train loss: 0.166781\n",
      "Epoch 5, train loss: 0.191743\n",
      "Epoch 6, train loss: 0.180517\n",
      "Epoch 7, train loss: 0.242003\n",
      "Epoch 8, train loss: 0.243887\n",
      "Epoch 9, train loss: 0.207780\n",
      "Epoch 10, train loss: 0.190263\n",
      "Epoch 11, train loss: 0.204889\n",
      "Epoch 12, train loss: 0.326803\n",
      "Epoch 13, train loss: 0.247688\n",
      "Epoch 14, train loss: 0.163234\n",
      "Epoch 15, train loss: 0.246148\n",
      "Epoch 16, train loss: 0.165913\n",
      "Epoch 17, train loss: 0.216406\n",
      "Epoch 18, train loss: 0.230754\n",
      "Epoch 19, train loss: 0.244677\n",
      "Epoch 20, train loss: 0.167071\n",
      "Epoch 21, train loss: 0.172551\n",
      "Epoch 22, train loss: 0.226562\n",
      "Epoch 23, train loss: 0.210625\n",
      "Epoch 24, train loss: 0.152542\n",
      "Epoch 25, train loss: 0.166157\n",
      "Epoch 26, train loss: 0.186403\n",
      "Epoch 27, train loss: 0.165860\n",
      "Epoch 28, train loss: 0.187796\n",
      "Epoch 29, train loss: 0.289001\n",
      "Epoch 30, train loss: 0.214963\n",
      "Epoch 31, train loss: 0.176947\n",
      "Epoch 32, train loss: 0.184319\n",
      "Epoch 33, train loss: 0.263623\n",
      "Epoch 34, train loss: 0.185904\n",
      "Epoch 35, train loss: 0.171159\n",
      "Epoch 36, train loss: 0.165857\n",
      "Epoch 37, train loss: 0.250132\n",
      "Epoch 38, train loss: 0.222264\n",
      "Epoch 39, train loss: 0.210690\n",
      "Epoch 40, train loss: 0.168503\n",
      "Epoch 41, train loss: 0.249337\n",
      "Epoch 42, train loss: 0.198812\n",
      "Epoch 43, train loss: 0.248443\n",
      "Epoch 44, train loss: 0.217324\n",
      "Epoch 45, train loss: 0.208652\n",
      "Epoch 46, train loss: 0.199111\n",
      "Epoch 47, train loss: 0.256904\n",
      "Epoch 48, train loss: 0.201267\n",
      "Epoch 49, train loss: 0.218586\n",
      "Epoch 50, train loss: 0.176298\n",
      "Epoch 51, train loss: 0.230268\n",
      "Epoch 52, train loss: 0.163441\n",
      "Epoch 53, train loss: 0.223941\n",
      "Epoch 54, train loss: 0.170999\n",
      "Epoch 55, train loss: 0.228417\n",
      "Epoch 56, train loss: 0.175912\n",
      "Epoch 57, train loss: 0.278288\n",
      "Epoch 58, train loss: 0.182505\n",
      "Epoch 59, train loss: 0.245334\n",
      "Epoch 60, train loss: 0.298324\n",
      "Epoch 61, train loss: 0.223699\n",
      "Epoch 62, train loss: 0.182509\n",
      "Epoch 63, train loss: 0.259474\n",
      "Epoch 64, train loss: 0.170862\n",
      "Epoch 65, train loss: 0.187684\n",
      "Epoch 66, train loss: 0.192940\n",
      "Epoch 67, train loss: 0.197550\n",
      "Epoch 68, train loss: 0.260751\n",
      "Epoch 69, train loss: 0.284341\n",
      "Epoch 70, train loss: 0.257808\n",
      "Epoch 71, train loss: 0.189569\n",
      "Epoch 72, train loss: 0.224959\n",
      "Epoch 73, train loss: 0.254330\n",
      "Epoch 74, train loss: 0.222822\n",
      "Epoch 75, train loss: 0.259438\n",
      "Epoch 76, train loss: 0.183789\n",
      "Epoch 77, train loss: 0.200662\n",
      "Epoch 78, train loss: 0.190987\n",
      "Epoch 79, train loss: 0.240736\n",
      "Epoch 80, train loss: 0.185261\n",
      "Epoch 81, train loss: 0.266377\n",
      "Epoch 82, train loss: 0.144665\n",
      "Epoch 83, train loss: 0.206166\n",
      "Epoch 84, train loss: 0.191641\n",
      "Epoch 85, train loss: 0.211859\n",
      "Epoch 86, train loss: 0.180942\n",
      "Epoch 87, train loss: 0.221935\n",
      "Epoch 88, train loss: 0.184962\n",
      "Epoch 89, train loss: 0.209549\n",
      "Epoch 90, train loss: 0.227567\n",
      "Epoch 91, train loss: 0.217607\n",
      "Epoch 92, train loss: 0.220175\n",
      "Epoch 93, train loss: 0.241121\n",
      "Epoch 94, train loss: 0.224143\n",
      "Epoch 95, train loss: 0.261711\n",
      "Epoch 96, train loss: 0.195065\n",
      "Epoch 97, train loss: 0.202766\n",
      "Epoch 98, train loss: 0.178914\n",
      "Epoch 99, train loss: 0.199330\n",
      "Epoch 100, train loss: 0.166834\n",
      "Epoch 101, train loss: 0.217349\n",
      "Epoch 102, train loss: 0.168063\n",
      "Epoch 103, train loss: 0.222082\n",
      "Epoch 104, train loss: 0.276994\n",
      "Epoch 105, train loss: 0.202654\n",
      "Epoch 106, train loss: 0.229463\n",
      "Epoch 107, train loss: 0.180028\n",
      "Epoch 108, train loss: 0.200495\n",
      "Epoch 109, train loss: 0.189158\n",
      "Epoch 110, train loss: 0.206155\n",
      "Epoch 111, train loss: 0.219572\n",
      "Epoch 112, train loss: 0.217171\n",
      "Epoch 113, train loss: 0.186572\n",
      "Epoch 114, train loss: 0.282653\n",
      "Epoch 115, train loss: 0.216426\n",
      "Epoch 116, train loss: 0.235690\n",
      "Epoch 117, train loss: 0.174807\n",
      "Epoch 118, train loss: 0.200521\n",
      "Epoch 119, train loss: 0.243996\n",
      "Epoch 120, train loss: 0.186302\n",
      "Epoch 121, train loss: 0.167236\n",
      "Epoch 122, train loss: 0.168210\n",
      "Epoch 123, train loss: 0.273927\n",
      "Epoch 124, train loss: 0.216502\n",
      "Epoch 125, train loss: 0.227872\n",
      "Epoch 126, train loss: 0.260200\n",
      "Epoch 127, train loss: 0.221064\n",
      "Epoch 128, train loss: 0.193838\n",
      "Epoch 129, train loss: 0.191774\n",
      "Epoch 130, train loss: 0.169447\n",
      "Epoch 131, train loss: 0.189853\n",
      "Epoch 132, train loss: 0.174629\n",
      "Epoch 133, train loss: 0.182470\n",
      "Epoch 134, train loss: 0.223240\n",
      "Epoch 135, train loss: 0.225125\n",
      "Epoch 136, train loss: 0.186321\n",
      "Epoch 137, train loss: 0.193986\n",
      "Epoch 138, train loss: 0.284706\n",
      "Epoch 139, train loss: 0.208688\n",
      "Epoch 140, train loss: 0.170582\n",
      "Epoch 141, train loss: 0.197773\n",
      "Epoch 142, train loss: 0.217779\n",
      "Epoch 143, train loss: 0.140548\n",
      "Epoch 144, train loss: 0.194223\n",
      "Epoch 145, train loss: 0.185254\n",
      "Epoch 146, train loss: 0.197533\n",
      "Epoch 147, train loss: 0.303756\n",
      "Epoch 148, train loss: 0.176115\n",
      "Epoch 149, train loss: 0.205875\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "x here is\n",
      "[[283. 282. 281. 277. 267. 258.]\n",
      " [282. 281. 277. 267. 258. 251.]\n",
      " [281. 277. 267. 258. 251. 237.]\n",
      " ...\n",
      " [164. 163. 160. 159. 159. 156.]\n",
      " [163. 160. 159. 159. 156. 154.]\n",
      " [160. 159. 159. 156. 154. 152.]]\n",
      "y here is\n",
      "[[201. 201. 201. 201. 201. 201.]\n",
      " [195. 195. 195. 195. 195. 195.]\n",
      " [189. 189. 189. 189. 189. 189.]\n",
      " ...\n",
      " [151. 151. 151. 151. 151. 151.]\n",
      " [149. 149. 149. 149. 149. 149.]\n",
      " [144. 144. 144. 144. 144. 144.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (2705, 6), Train/test: 1/2704\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[160. 158. 160. 166. 175. 182.]\n",
      " [158. 160. 166. 175. 182. 188.]\n",
      " [160. 166. 175. 182. 188. 192.]\n",
      " ...\n",
      " [227. 223. 219. 211. 202. 196.]\n",
      " [223. 219. 211. 202. 196. 199.]\n",
      " [219. 211. 202. 196. 199. 224.]]\n",
      "y here is\n",
      "[[218. 218. 218. 218. 218. 218.]\n",
      " [226. 226. 226. 226. 226. 226.]\n",
      " [231. 231. 231. 231. 231. 231.]\n",
      " ...\n",
      " [268. 268. 268. 268. 268. 268.]\n",
      " [301. 301. 301. 301. 301. 301.]\n",
      " [290. 290. 290. 290. 290. 290.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 738/9814\n",
      "Found 27 continuous time series\n",
      "Data shape: (10554, 6), Train/test: 10552/2\n",
      "Train test ratio: 5276.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61086DE10>\n",
      "Epoch 0, test loss: 0.207293\n",
      "Epoch 1, test loss: 0.206404\n",
      "Epoch 2, test loss: 0.208148\n",
      "Epoch 3, test loss: 0.208418\n",
      "Epoch 4, test loss: 0.207911\n",
      "Epoch 5, test loss: 0.206739\n",
      "Epoch 6, test loss: 0.206685\n",
      "Epoch 7, test loss: 0.206635\n",
      "Epoch 8, test loss: 0.206946\n",
      "Epoch 9, test loss: 0.211750\n",
      "Epoch 10, test loss: 0.209248\n",
      "Epoch 11, test loss: 0.207171\n",
      "Epoch 12, test loss: 0.208627\n",
      "Epoch 13, test loss: 0.207805\n",
      "Epoch 14, test loss: 0.207567\n",
      "Epoch 15, test loss: 0.209006\n",
      "Epoch 16, test loss: 0.210779\n",
      "Epoch 17, test loss: 0.207636\n",
      "Epoch 18, test loss: 0.210628\n",
      "Epoch 19, test loss: 0.207491\n",
      "Epoch 20, test loss: 0.207468\n",
      "Epoch 21, test loss: 0.209059\n",
      "Epoch 22, test loss: 0.211887\n",
      "Epoch 23, test loss: 0.207738\n",
      "Epoch 24, test loss: 0.208106\n",
      "Epoch 25, test loss: 0.208375\n",
      "Epoch 26, test loss: 0.208348\n",
      "Epoch 27, test loss: 0.208054\n",
      "Epoch 28, test loss: 0.207639\n",
      "Epoch 29, test loss: 0.208377\n",
      "Epoch 30, test loss: 0.207428\n",
      "Epoch 31, test loss: 0.207794\n",
      "Epoch 32, test loss: 0.209675\n",
      "Epoch 33, test loss: 0.207891\n",
      "Epoch 34, test loss: 0.207844\n",
      "Epoch 35, test loss: 0.207493\n",
      "Epoch 36, test loss: 0.208957\n",
      "Epoch 37, test loss: 0.207716\n",
      "Epoch 38, test loss: 0.208801\n",
      "Epoch 39, test loss: 0.207871\n",
      "Epoch 40, test loss: 0.209515\n",
      "Epoch 41, test loss: 0.208431\n",
      "Epoch 42, test loss: 0.210397\n",
      "Epoch 43, test loss: 0.208042\n",
      "Epoch 44, test loss: 0.208729\n",
      "Epoch 45, test loss: 0.207674\n",
      "Epoch 46, test loss: 0.207762\n",
      "Epoch 47, test loss: 0.208644\n",
      "Epoch 48, test loss: 0.207927\n",
      "Epoch 49, test loss: 0.207738\n",
      "Epoch 50, test loss: 0.208789\n",
      "Epoch 51, test loss: 0.208899\n",
      "Epoch 52, test loss: 0.207662\n",
      "Epoch 53, test loss: 0.208439\n",
      "Epoch 54, test loss: 0.207907\n",
      "Epoch 55, test loss: 0.207617\n",
      "Epoch 56, test loss: 0.210542\n",
      "Epoch 57, test loss: 0.210059\n",
      "Epoch 58, test loss: 0.208102\n",
      "Epoch 59, test loss: 0.208857\n",
      "Epoch 60, test loss: 0.210944\n",
      "Epoch 61, test loss: 0.208952\n",
      "Epoch 62, test loss: 0.208019\n",
      "Epoch 63, test loss: 0.212102\n",
      "Epoch 64, test loss: 0.209250\n",
      "Epoch 65, test loss: 0.207952\n",
      "Epoch 66, test loss: 0.209725\n",
      "Epoch 67, test loss: 0.209695\n",
      "Epoch 68, test loss: 0.208020\n",
      "Epoch 69, test loss: 0.207834\n",
      "Epoch 70, test loss: 0.207910\n",
      "Epoch 71, test loss: 0.208596\n",
      "Epoch 72, test loss: 0.208031\n",
      "Epoch 73, test loss: 0.208354\n",
      "Epoch 74, test loss: 0.208266\n",
      "Epoch 75, test loss: 0.209412\n",
      "Epoch 76, test loss: 0.208935\n",
      "Epoch 77, test loss: 0.208026\n",
      "Epoch 78, test loss: 0.207860\n",
      "Epoch 79, test loss: 0.208081\n",
      "Epoch 80, test loss: 0.208635\n",
      "Epoch 81, test loss: 0.207876\n",
      "Epoch 82, test loss: 0.207844\n",
      "Epoch 83, test loss: 0.208521\n",
      "Epoch 84, test loss: 0.207691\n",
      "Epoch 85, test loss: 0.209807\n",
      "Epoch 86, test loss: 0.208990\n",
      "Epoch 87, test loss: 0.208469\n",
      "Epoch 88, test loss: 0.208339\n",
      "Epoch 89, test loss: 0.209219\n",
      "Epoch 90, test loss: 0.208226\n",
      "Epoch 91, test loss: 0.208901\n",
      "Epoch 92, test loss: 0.207813\n",
      "Epoch 93, test loss: 0.208015\n",
      "Epoch 94, test loss: 0.208520\n",
      "Epoch 95, test loss: 0.207768\n",
      "Epoch 96, test loss: 0.211027\n",
      "Epoch 97, test loss: 0.209693\n",
      "Epoch 98, test loss: 0.208627\n",
      "Epoch 99, test loss: 0.208933\n",
      "Epoch 100, test loss: 0.209938\n",
      "Epoch 101, test loss: 0.208480\n",
      "Epoch 102, test loss: 0.208665\n",
      "Epoch 103, test loss: 0.210003\n",
      "Epoch 104, test loss: 0.209639\n",
      "Epoch 105, test loss: 0.208074\n",
      "Epoch 106, test loss: 0.211442\n",
      "Epoch 107, test loss: 0.208625\n",
      "Epoch 108, test loss: 0.208190\n",
      "Epoch 109, test loss: 0.208035\n",
      "Epoch 110, test loss: 0.209067\n",
      "Epoch 111, test loss: 0.210324\n",
      "Epoch 112, test loss: 0.211572\n",
      "Epoch 113, test loss: 0.207786\n",
      "Epoch 114, test loss: 0.208356\n",
      "Epoch 115, test loss: 0.209775\n",
      "Epoch 116, test loss: 0.209152\n",
      "Epoch 117, test loss: 0.208319\n",
      "Epoch 118, test loss: 0.208837\n",
      "Epoch 119, test loss: 0.207792\n",
      "Epoch 120, test loss: 0.208530\n",
      "Epoch 121, test loss: 0.208272\n",
      "Epoch 122, test loss: 0.213009\n",
      "Epoch 123, test loss: 0.210747\n",
      "Epoch 124, test loss: 0.207678\n",
      "Epoch 125, test loss: 0.209421\n",
      "Epoch 126, test loss: 0.208388\n",
      "Epoch 127, test loss: 0.209074\n",
      "Epoch 128, test loss: 0.208011\n",
      "Epoch 129, test loss: 0.209363\n",
      "Epoch 130, test loss: 0.208230\n",
      "Epoch 131, test loss: 0.208377\n",
      "Epoch 132, test loss: 0.217791\n",
      "Epoch 133, test loss: 0.210536\n",
      "Epoch 134, test loss: 0.213680\n",
      "Epoch 135, test loss: 0.207958\n",
      "Epoch 136, test loss: 0.209050\n",
      "Epoch 137, test loss: 0.208587\n",
      "Epoch 138, test loss: 0.208827\n",
      "Epoch 139, test loss: 0.209405\n",
      "Epoch 140, test loss: 0.209533\n",
      "Epoch 141, test loss: 0.208537\n",
      "Epoch 142, test loss: 0.209975\n",
      "Epoch 143, test loss: 0.208043\n",
      "Epoch 144, test loss: 0.207876\n",
      "Epoch 145, test loss: 0.207394\n",
      "Epoch 146, test loss: 0.208026\n",
      "Epoch 147, test loss: 0.208170\n",
      "Epoch 148, test loss: 0.208029\n",
      "Epoch 149, test loss: 0.208885\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61086DE10>\n",
      "Epoch 0, test loss: 0.205906\n",
      "Epoch 1, test loss: 0.206563\n",
      "Epoch 2, test loss: 0.206227\n",
      "Epoch 3, test loss: 0.205918\n",
      "Epoch 4, test loss: 0.208074\n",
      "Epoch 5, test loss: 0.206404\n",
      "Epoch 6, test loss: 0.206181\n",
      "Epoch 7, test loss: 0.205750\n",
      "Epoch 8, test loss: 0.205933\n",
      "Epoch 9, test loss: 0.207659\n",
      "Epoch 10, test loss: 0.206367\n",
      "Epoch 11, test loss: 0.208191\n",
      "Epoch 12, test loss: 0.207359\n",
      "Epoch 13, test loss: 0.205854\n",
      "Epoch 14, test loss: 0.206628\n",
      "Epoch 15, test loss: 0.208847\n",
      "Epoch 16, test loss: 0.206647\n",
      "Epoch 17, test loss: 0.206797\n",
      "Epoch 18, test loss: 0.206363\n",
      "Epoch 19, test loss: 0.207137\n",
      "Epoch 20, test loss: 0.206188\n",
      "Epoch 21, test loss: 0.207043\n",
      "Epoch 22, test loss: 0.207314\n",
      "Epoch 23, test loss: 0.206805\n",
      "Epoch 24, test loss: 0.206546\n",
      "Epoch 25, test loss: 0.206365\n",
      "Epoch 26, test loss: 0.206660\n",
      "Epoch 27, test loss: 0.208241\n",
      "Epoch 28, test loss: 0.206881\n",
      "Epoch 29, test loss: 0.208515\n",
      "Epoch 30, test loss: 0.207162\n",
      "Epoch 31, test loss: 0.211971\n",
      "Epoch 32, test loss: 0.206867\n",
      "Epoch 33, test loss: 0.206815\n",
      "Epoch 34, test loss: 0.206365\n",
      "Epoch 35, test loss: 0.206640\n",
      "Epoch 36, test loss: 0.207597\n",
      "Epoch 37, test loss: 0.207980\n",
      "Epoch 38, test loss: 0.206859\n",
      "Epoch 39, test loss: 0.206409\n",
      "Epoch 40, test loss: 0.206667\n",
      "Epoch 41, test loss: 0.206629\n",
      "Epoch 42, test loss: 0.207164\n",
      "Epoch 43, test loss: 0.206703\n",
      "Epoch 44, test loss: 0.207042\n",
      "Epoch 45, test loss: 0.206251\n",
      "Epoch 46, test loss: 0.208444\n",
      "Epoch 47, test loss: 0.207713\n",
      "Epoch 48, test loss: 0.206854\n",
      "Epoch 49, test loss: 0.207018\n",
      "Epoch 50, test loss: 0.206469\n",
      "Epoch 51, test loss: 0.207093\n",
      "Epoch 52, test loss: 0.207535\n",
      "Epoch 53, test loss: 0.207061\n",
      "Epoch 54, test loss: 0.208235\n",
      "Epoch 55, test loss: 0.208021\n",
      "Epoch 56, test loss: 0.206764\n",
      "Epoch 57, test loss: 0.207448\n",
      "Epoch 58, test loss: 0.206757\n",
      "Epoch 59, test loss: 0.208176\n",
      "Epoch 60, test loss: 0.207566\n",
      "Epoch 61, test loss: 0.208759\n",
      "Epoch 62, test loss: 0.206994\n",
      "Epoch 63, test loss: 0.207802\n",
      "Epoch 64, test loss: 0.207542\n",
      "Epoch 65, test loss: 0.206412\n",
      "Epoch 66, test loss: 0.207846\n",
      "Epoch 67, test loss: 0.206442\n",
      "Epoch 68, test loss: 0.207678\n",
      "Epoch 69, test loss: 0.207617\n",
      "Epoch 70, test loss: 0.206884\n",
      "Epoch 71, test loss: 0.207237\n",
      "Epoch 72, test loss: 0.206686\n",
      "Epoch 73, test loss: 0.206605\n",
      "Epoch 74, test loss: 0.206995\n",
      "Epoch 75, test loss: 0.206938\n",
      "Epoch 76, test loss: 0.206669\n",
      "Epoch 77, test loss: 0.207802\n",
      "Epoch 78, test loss: 0.207228\n",
      "Epoch 79, test loss: 0.206865\n",
      "Epoch 80, test loss: 0.206427\n",
      "Epoch 81, test loss: 0.207356\n",
      "Epoch 82, test loss: 0.206640\n",
      "Epoch 83, test loss: 0.207346\n",
      "Epoch 84, test loss: 0.206669\n",
      "Epoch 85, test loss: 0.208064\n",
      "Epoch 86, test loss: 0.206478\n",
      "Epoch 87, test loss: 0.207055\n",
      "Epoch 88, test loss: 0.206667\n",
      "Epoch 89, test loss: 0.207614\n",
      "Epoch 90, test loss: 0.208008\n",
      "Epoch 91, test loss: 0.206811\n",
      "Epoch 92, test loss: 0.207016\n",
      "Epoch 93, test loss: 0.206760\n",
      "Epoch 94, test loss: 0.209590\n",
      "Epoch 95, test loss: 0.207526\n",
      "Epoch 96, test loss: 0.207041\n",
      "Epoch 97, test loss: 0.206657\n",
      "Epoch 98, test loss: 0.207115\n",
      "Epoch 99, test loss: 0.212244\n",
      "Epoch 100, test loss: 0.206491\n",
      "Epoch 101, test loss: 0.208018\n",
      "Epoch 102, test loss: 0.206669\n",
      "Epoch 103, test loss: 0.207201\n",
      "Epoch 104, test loss: 0.206645\n",
      "Epoch 105, test loss: 0.206466\n",
      "Epoch 106, test loss: 0.206931\n",
      "Epoch 107, test loss: 0.206754\n",
      "Epoch 108, test loss: 0.206686\n",
      "Epoch 109, test loss: 0.207417\n",
      "Epoch 110, test loss: 0.208429\n",
      "Epoch 111, test loss: 0.206465\n",
      "Epoch 112, test loss: 0.208619\n",
      "Epoch 113, test loss: 0.207141\n",
      "Epoch 114, test loss: 0.207751\n",
      "Epoch 115, test loss: 0.207326\n",
      "Epoch 116, test loss: 0.207967\n",
      "Epoch 117, test loss: 0.209401\n",
      "Epoch 118, test loss: 0.207531\n",
      "Epoch 119, test loss: 0.206684\n",
      "Epoch 120, test loss: 0.206899\n",
      "Epoch 121, test loss: 0.207074\n",
      "Epoch 122, test loss: 0.209955\n",
      "Epoch 123, test loss: 0.207042\n",
      "Epoch 124, test loss: 0.208652\n",
      "Epoch 125, test loss: 0.206719\n",
      "Epoch 126, test loss: 0.208250\n",
      "Epoch 127, test loss: 0.207175\n",
      "Epoch 128, test loss: 0.207077\n",
      "Epoch 129, test loss: 0.207361\n",
      "Epoch 130, test loss: 0.206836\n",
      "Epoch 131, test loss: 0.206434\n",
      "Epoch 132, test loss: 0.206314\n",
      "Epoch 133, test loss: 0.207271\n",
      "Epoch 134, test loss: 0.207160\n",
      "Epoch 135, test loss: 0.206486\n",
      "Epoch 136, test loss: 0.207103\n",
      "Epoch 137, test loss: 0.207527\n",
      "Epoch 138, test loss: 0.207552\n",
      "Epoch 139, test loss: 0.206660\n",
      "Epoch 140, test loss: 0.207171\n",
      "Epoch 141, test loss: 0.206262\n",
      "Epoch 142, test loss: 0.206907\n",
      "Epoch 143, test loss: 0.206667\n",
      "Epoch 144, test loss: 0.207426\n",
      "Epoch 145, test loss: 0.206843\n",
      "Epoch 146, test loss: 0.207091\n",
      "Epoch 147, test loss: 0.207053\n",
      "Epoch 148, test loss: 0.207593\n",
      "Epoch 149, test loss: 0.209751\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F61086DE10>\n",
      "Epoch 0, test loss: 0.291349\n",
      "Epoch 1, test loss: 0.223232\n",
      "Epoch 2, test loss: 0.217296\n",
      "Epoch 3, test loss: 0.213744\n",
      "Epoch 4, test loss: 0.211406\n",
      "Epoch 5, test loss: 0.211024\n",
      "Epoch 6, test loss: 0.209593\n",
      "Epoch 7, test loss: 0.209067\n",
      "Epoch 8, test loss: 0.209071\n",
      "Epoch 9, test loss: 0.208980\n",
      "Epoch 10, test loss: 0.208581\n",
      "Epoch 11, test loss: 0.208495\n",
      "Epoch 12, test loss: 0.208814\n",
      "Epoch 13, test loss: 0.210723\n",
      "Epoch 14, test loss: 0.208264\n",
      "Epoch 15, test loss: 0.208921\n",
      "Epoch 16, test loss: 0.208802\n",
      "Epoch 17, test loss: 0.209296\n",
      "Epoch 18, test loss: 0.208905\n",
      "Epoch 19, test loss: 0.208881\n",
      "Epoch 20, test loss: 0.208789\n",
      "Epoch 21, test loss: 0.209511\n",
      "Epoch 22, test loss: 0.211885\n",
      "Epoch 23, test loss: 0.208306\n",
      "Epoch 24, test loss: 0.209022\n",
      "Epoch 25, test loss: 0.208237\n",
      "Epoch 26, test loss: 0.208335\n",
      "Epoch 27, test loss: 0.208159\n",
      "Epoch 28, test loss: 0.208325\n",
      "Epoch 29, test loss: 0.208629\n",
      "Epoch 30, test loss: 0.210793\n",
      "Epoch 31, test loss: 0.207748\n",
      "Epoch 32, test loss: 0.209419\n",
      "Epoch 33, test loss: 0.208490\n",
      "Epoch 34, test loss: 0.208283\n",
      "Epoch 35, test loss: 0.208734\n",
      "Epoch 36, test loss: 0.208874\n",
      "Epoch 37, test loss: 0.211129\n",
      "Epoch 38, test loss: 0.214078\n",
      "Epoch 39, test loss: 0.207281\n",
      "Epoch 40, test loss: 0.207908\n",
      "Epoch 41, test loss: 0.207423\n",
      "Epoch 42, test loss: 0.207472\n",
      "Epoch 43, test loss: 0.208620\n",
      "Epoch 44, test loss: 0.208681\n",
      "Epoch 45, test loss: 0.207674\n",
      "Epoch 46, test loss: 0.207936\n",
      "Epoch 47, test loss: 0.207736\n",
      "Epoch 48, test loss: 0.207609\n",
      "Epoch 49, test loss: 0.207221\n",
      "Epoch 50, test loss: 0.207774\n",
      "Epoch 51, test loss: 0.207925\n",
      "Epoch 52, test loss: 0.207347\n",
      "Epoch 53, test loss: 0.207777\n",
      "Epoch 54, test loss: 0.207141\n",
      "Epoch 55, test loss: 0.207200\n",
      "Epoch 56, test loss: 0.209838\n",
      "Epoch 57, test loss: 0.207074\n",
      "Epoch 58, test loss: 0.209257\n",
      "Epoch 59, test loss: 0.207500\n",
      "Epoch 60, test loss: 0.207303\n",
      "Epoch 61, test loss: 0.207703\n",
      "Epoch 62, test loss: 0.207143\n",
      "Epoch 63, test loss: 0.209343\n",
      "Epoch 64, test loss: 0.207892\n",
      "Epoch 65, test loss: 0.207104\n",
      "Epoch 66, test loss: 0.210240\n",
      "Epoch 67, test loss: 0.207332\n",
      "Epoch 68, test loss: 0.207383\n",
      "Epoch 69, test loss: 0.213830\n",
      "Epoch 70, test loss: 0.207073\n",
      "Epoch 71, test loss: 0.206946\n",
      "Epoch 72, test loss: 0.207316\n",
      "Epoch 73, test loss: 0.207416\n",
      "Epoch 74, test loss: 0.207000\n",
      "Epoch 75, test loss: 0.207516\n",
      "Epoch 76, test loss: 0.213732\n",
      "Epoch 77, test loss: 0.207311\n",
      "Epoch 78, test loss: 0.207862\n",
      "Epoch 79, test loss: 0.207191\n",
      "Epoch 80, test loss: 0.206998\n",
      "Epoch 81, test loss: 0.209543\n",
      "Epoch 82, test loss: 0.211460\n",
      "Epoch 83, test loss: 0.208054\n",
      "Epoch 84, test loss: 0.208644\n",
      "Epoch 85, test loss: 0.208716\n",
      "Epoch 86, test loss: 0.209805\n",
      "Epoch 87, test loss: 0.209858\n",
      "Epoch 88, test loss: 0.207351\n",
      "Epoch 89, test loss: 0.209056\n",
      "Epoch 90, test loss: 0.207635\n",
      "Epoch 91, test loss: 0.207574\n",
      "Epoch 92, test loss: 0.208253\n",
      "Epoch 93, test loss: 0.208131\n",
      "Epoch 94, test loss: 0.207220\n",
      "Epoch 95, test loss: 0.207713\n",
      "Epoch 96, test loss: 0.209099\n",
      "Epoch 97, test loss: 0.207650\n",
      "Epoch 98, test loss: 0.207221\n",
      "Epoch 99, test loss: 0.209202\n",
      "Epoch 100, test loss: 0.207448\n",
      "Epoch 101, test loss: 0.208890\n",
      "Epoch 102, test loss: 0.209063\n",
      "Epoch 103, test loss: 0.209286\n",
      "Epoch 104, test loss: 0.208200\n",
      "Epoch 105, test loss: 0.207269\n",
      "Epoch 106, test loss: 0.207551\n",
      "Epoch 107, test loss: 0.209928\n",
      "Epoch 108, test loss: 0.206948\n",
      "Epoch 109, test loss: 0.207394\n",
      "Epoch 110, test loss: 0.206899\n",
      "Epoch 111, test loss: 0.207959\n",
      "Epoch 112, test loss: 0.209324\n",
      "Epoch 113, test loss: 0.208364\n",
      "Epoch 114, test loss: 0.207067\n",
      "Epoch 115, test loss: 0.208117\n",
      "Epoch 116, test loss: 0.207083\n",
      "Epoch 117, test loss: 0.208449\n",
      "Epoch 118, test loss: 0.207777\n",
      "Epoch 119, test loss: 0.208084\n",
      "Epoch 120, test loss: 0.207394\n",
      "Epoch 121, test loss: 0.207618\n",
      "Epoch 122, test loss: 0.207977\n",
      "Epoch 123, test loss: 0.211480\n",
      "Epoch 124, test loss: 0.210307\n",
      "Epoch 125, test loss: 0.207337\n",
      "Epoch 126, test loss: 0.208189\n",
      "Epoch 127, test loss: 0.207807\n",
      "Epoch 128, test loss: 0.209791\n",
      "Epoch 129, test loss: 0.208036\n",
      "Epoch 130, test loss: 0.208273\n",
      "Epoch 131, test loss: 0.210267\n",
      "Epoch 132, test loss: 0.207407\n",
      "Epoch 133, test loss: 0.208976\n",
      "Epoch 134, test loss: 0.208021\n",
      "Epoch 135, test loss: 0.207613\n",
      "Epoch 136, test loss: 0.207833\n",
      "Epoch 137, test loss: 0.209552\n",
      "Epoch 138, test loss: 0.208015\n",
      "Epoch 139, test loss: 0.207546\n",
      "Epoch 140, test loss: 0.208603\n",
      "Epoch 141, test loss: 0.208428\n",
      "Epoch 142, test loss: 0.208180\n",
      "Epoch 143, test loss: 0.207217\n",
      "Epoch 144, test loss: 0.207857\n",
      "Epoch 145, test loss: 0.208157\n",
      "Epoch 146, test loss: 0.207347\n",
      "Epoch 147, test loss: 0.213078\n",
      "Epoch 148, test loss: 0.207600\n",
      "Epoch 149, test loss: 0.208266\n",
      "Epoch 150, test loss: 0.210745\n",
      "Epoch 151, test loss: 0.207782\n",
      "Epoch 152, test loss: 0.207272\n",
      "Epoch 153, test loss: 0.207345\n",
      "Epoch 154, test loss: 0.207464\n",
      "Epoch 155, test loss: 0.207913\n",
      "Epoch 156, test loss: 0.209230\n",
      "Epoch 157, test loss: 0.210754\n",
      "Epoch 158, test loss: 0.207921\n",
      "Epoch 159, test loss: 0.208373\n",
      "Epoch 160, test loss: 0.207337\n",
      "Epoch 161, test loss: 0.210620\n",
      "Epoch 162, test loss: 0.207468\n",
      "Epoch 163, test loss: 0.209081\n",
      "Epoch 164, test loss: 0.207594\n",
      "Epoch 165, test loss: 0.207406\n",
      "Epoch 166, test loss: 0.211335\n",
      "Epoch 167, test loss: 0.208745\n",
      "Epoch 168, test loss: 0.207676\n",
      "Epoch 169, test loss: 0.208365\n",
      "Epoch 170, test loss: 0.208587\n",
      "Epoch 171, test loss: 0.207431\n",
      "Epoch 172, test loss: 0.212079\n",
      "Epoch 173, test loss: 0.207906\n",
      "Epoch 174, test loss: 0.207390\n",
      "Epoch 175, test loss: 0.207164\n",
      "Epoch 176, test loss: 0.208789\n",
      "Epoch 177, test loss: 0.207107\n",
      "Epoch 178, test loss: 0.208780\n",
      "Epoch 179, test loss: 0.207622\n",
      "Epoch 180, test loss: 0.207516\n",
      "Epoch 181, test loss: 0.208485\n",
      "Epoch 182, test loss: 0.208986\n",
      "Epoch 183, test loss: 0.207011\n",
      "Epoch 184, test loss: 0.208840\n",
      "Epoch 185, test loss: 0.208960\n",
      "Epoch 186, test loss: 0.209269\n",
      "Epoch 187, test loss: 0.208609\n",
      "Epoch 188, test loss: 0.208452\n",
      "Epoch 189, test loss: 0.207841\n",
      "Epoch 190, test loss: 0.207402\n",
      "Epoch 191, test loss: 0.213341\n",
      "Epoch 192, test loss: 0.209684\n",
      "Epoch 193, test loss: 0.210599\n",
      "Epoch 194, test loss: 0.207399\n",
      "Epoch 195, test loss: 0.211517\n",
      "Epoch 196, test loss: 0.207600\n",
      "Epoch 197, test loss: 0.207568\n",
      "Epoch 198, test loss: 0.207961\n",
      "Epoch 199, test loss: 0.207718\n",
      "Epoch 200, test loss: 0.207133\n",
      "Epoch 201, test loss: 0.208299\n",
      "Epoch 202, test loss: 0.209252\n",
      "Epoch 203, test loss: 0.208310\n",
      "Epoch 204, test loss: 0.212425\n",
      "Epoch 205, test loss: 0.207561\n",
      "Epoch 206, test loss: 0.210215\n",
      "Epoch 207, test loss: 0.207638\n",
      "Epoch 208, test loss: 0.210897\n",
      "Epoch 209, test loss: 0.212932\n",
      "Epoch 210, test loss: 0.207233\n",
      "Epoch 211, test loss: 0.207511\n",
      "Epoch 212, test loss: 0.207321\n",
      "Epoch 213, test loss: 0.207613\n",
      "Epoch 214, test loss: 0.208631\n",
      "Epoch 215, test loss: 0.208356\n",
      "Epoch 216, test loss: 0.208736\n",
      "Epoch 217, test loss: 0.207217\n",
      "Epoch 218, test loss: 0.207543\n",
      "Epoch 219, test loss: 0.207882\n",
      "Epoch 220, test loss: 0.207923\n",
      "Epoch 221, test loss: 0.208264\n",
      "Epoch 222, test loss: 0.207581\n",
      "Epoch 223, test loss: 0.207845\n",
      "Epoch 224, test loss: 0.207052\n",
      "Epoch 225, test loss: 0.208809\n",
      "Epoch 226, test loss: 0.209133\n",
      "Epoch 227, test loss: 0.208213\n",
      "Epoch 228, test loss: 0.207247\n",
      "Epoch 229, test loss: 0.208201\n",
      "Epoch 230, test loss: 0.208335\n",
      "Epoch 231, test loss: 0.208989\n",
      "Epoch 232, test loss: 0.207086\n",
      "Epoch 233, test loss: 0.207809\n",
      "Epoch 234, test loss: 0.211307\n",
      "Epoch 235, test loss: 0.208235\n",
      "Epoch 236, test loss: 0.208398\n",
      "Epoch 237, test loss: 0.209737\n",
      "Epoch 238, test loss: 0.207020\n",
      "Epoch 239, test loss: 0.210009\n",
      "Epoch 240, test loss: 0.207592\n",
      "Epoch 241, test loss: 0.208029\n",
      "Epoch 242, test loss: 0.207374\n",
      "Epoch 243, test loss: 0.207425\n",
      "Epoch 244, test loss: 0.209067\n",
      "Epoch 245, test loss: 0.207335\n",
      "Epoch 246, test loss: 0.208079\n",
      "Epoch 247, test loss: 0.207395\n",
      "Epoch 248, test loss: 0.207980\n",
      "Epoch 249, test loss: 0.207313\n",
      "Epoch 250, test loss: 0.207403\n",
      "Epoch 251, test loss: 0.209038\n",
      "Epoch 252, test loss: 0.207212\n",
      "Epoch 253, test loss: 0.207427\n",
      "Epoch 254, test loss: 0.207026\n",
      "Epoch 255, test loss: 0.208363\n",
      "Epoch 256, test loss: 0.207430\n",
      "Epoch 257, test loss: 0.208324\n",
      "Epoch 258, test loss: 0.207265\n",
      "Epoch 259, test loss: 0.207278\n",
      "Epoch 260, test loss: 0.209492\n",
      "Epoch 261, test loss: 0.207565\n",
      "Epoch 262, test loss: 0.207772\n",
      "Epoch 263, test loss: 0.208039\n",
      "Epoch 264, test loss: 0.207355\n",
      "Epoch 265, test loss: 0.207651\n",
      "Epoch 266, test loss: 0.209782\n",
      "Epoch 267, test loss: 0.209905\n",
      "Epoch 268, test loss: 0.207950\n",
      "Epoch 269, test loss: 0.209649\n",
      "Epoch 270, test loss: 0.208279\n",
      "Epoch 271, test loss: 0.208285\n",
      "Epoch 272, test loss: 0.208579\n",
      "Epoch 273, test loss: 0.207072\n",
      "Epoch 274, test loss: 0.207146\n",
      "Epoch 275, test loss: 0.207199\n",
      "Epoch 276, test loss: 0.207514\n",
      "Epoch 277, test loss: 0.207518\n",
      "Epoch 278, test loss: 0.208048\n",
      "Epoch 279, test loss: 0.207615\n",
      "Epoch 280, test loss: 0.207222\n",
      "Epoch 281, test loss: 0.207999\n",
      "Epoch 282, test loss: 0.206423\n",
      "Epoch 283, test loss: 0.207094\n",
      "Epoch 284, test loss: 0.207170\n",
      "Epoch 285, test loss: 0.207432\n",
      "Epoch 286, test loss: 0.208144\n",
      "Epoch 287, test loss: 0.208348\n",
      "Epoch 288, test loss: 0.207049\n",
      "Epoch 289, test loss: 0.207828\n",
      "Epoch 290, test loss: 0.207218\n",
      "Epoch 291, test loss: 0.207804\n",
      "Epoch 292, test loss: 0.208993\n",
      "Epoch 293, test loss: 0.208093\n",
      "Epoch 294, test loss: 0.207222\n",
      "Epoch 295, test loss: 0.207774\n",
      "Epoch 296, test loss: 0.207908\n",
      "Epoch 297, test loss: 0.207437\n",
      "Epoch 298, test loss: 0.207581\n",
      "Epoch 299, test loss: 0.207686\n",
      "Epoch 300, test loss: 0.207371\n",
      "Epoch 301, test loss: 0.209200\n",
      "Epoch 302, test loss: 0.209334\n",
      "Epoch 303, test loss: 0.211851\n",
      "Epoch 304, test loss: 0.207588\n",
      "Epoch 305, test loss: 0.207104\n",
      "Epoch 306, test loss: 0.208344\n",
      "Epoch 307, test loss: 0.207643\n",
      "Epoch 308, test loss: 0.207506\n",
      "Epoch 309, test loss: 0.208743\n",
      "Epoch 310, test loss: 0.207365\n",
      "Epoch 311, test loss: 0.209166\n",
      "Epoch 312, test loss: 0.214454\n",
      "Epoch 313, test loss: 0.207859\n",
      "Epoch 314, test loss: 0.207371\n",
      "Epoch 315, test loss: 0.207880\n",
      "Epoch 316, test loss: 0.207605\n",
      "Epoch 317, test loss: 0.206836\n",
      "Epoch 318, test loss: 0.209924\n",
      "Epoch 319, test loss: 0.207473\n",
      "Epoch 320, test loss: 0.206666\n",
      "Epoch 321, test loss: 0.207381\n",
      "Epoch 322, test loss: 0.206900\n",
      "Epoch 323, test loss: 0.209116\n",
      "Epoch 324, test loss: 0.207582\n",
      "Epoch 325, test loss: 0.208341\n",
      "Epoch 326, test loss: 0.207592\n",
      "Epoch 327, test loss: 0.207318\n",
      "Epoch 328, test loss: 0.209182\n",
      "Epoch 329, test loss: 0.209352\n",
      "Epoch 330, test loss: 0.207805\n",
      "Epoch 331, test loss: 0.207695\n",
      "Epoch 332, test loss: 0.207711\n",
      "Epoch 333, test loss: 0.208734\n",
      "Epoch 334, test loss: 0.207625\n",
      "Epoch 335, test loss: 0.207507\n",
      "Epoch 336, test loss: 0.206943\n",
      "Epoch 337, test loss: 0.208856\n",
      "Epoch 338, test loss: 0.207360\n",
      "Epoch 339, test loss: 0.209762\n",
      "Epoch 340, test loss: 0.210338\n",
      "Epoch 341, test loss: 0.207669\n",
      "Epoch 342, test loss: 0.207621\n",
      "Epoch 343, test loss: 0.207356\n",
      "Epoch 344, test loss: 0.207274\n",
      "Epoch 345, test loss: 0.209788\n",
      "Epoch 346, test loss: 0.207563\n",
      "Epoch 347, test loss: 0.207071\n",
      "Epoch 348, test loss: 0.209942\n",
      "Epoch 349, test loss: 0.210429\n",
      "Epoch 350, test loss: 0.207555\n",
      "Epoch 351, test loss: 0.207926\n",
      "Epoch 352, test loss: 0.208219\n",
      "Epoch 353, test loss: 0.207787\n",
      "Epoch 354, test loss: 0.206944\n",
      "Epoch 355, test loss: 0.207184\n",
      "Epoch 356, test loss: 0.208726\n",
      "Epoch 357, test loss: 0.208311\n",
      "Epoch 358, test loss: 0.208874\n",
      "Epoch 359, test loss: 0.207603\n",
      "Epoch 360, test loss: 0.209263\n",
      "Epoch 361, test loss: 0.207539\n",
      "Epoch 362, test loss: 0.207653\n",
      "Epoch 363, test loss: 0.207904\n",
      "Epoch 364, test loss: 0.206921\n",
      "Epoch 365, test loss: 0.207111\n",
      "Epoch 366, test loss: 0.207387\n",
      "Epoch 367, test loss: 0.209963\n",
      "Epoch 368, test loss: 0.208225\n",
      "Epoch 369, test loss: 0.207316\n",
      "Epoch 370, test loss: 0.206999\n",
      "Epoch 371, test loss: 0.210148\n",
      "Epoch 372, test loss: 0.207475\n",
      "Epoch 373, test loss: 0.210605\n",
      "Epoch 374, test loss: 0.207811\n",
      "Epoch 375, test loss: 0.207957\n",
      "Epoch 376, test loss: 0.208349\n",
      "Epoch 377, test loss: 0.208791\n",
      "Epoch 378, test loss: 0.211173\n",
      "Epoch 379, test loss: 0.207403\n",
      "Epoch 380, test loss: 0.207230\n",
      "Epoch 381, test loss: 0.207532\n",
      "Epoch 382, test loss: 0.208251\n",
      "Epoch 383, test loss: 0.208276\n",
      "Epoch 384, test loss: 0.208046\n",
      "Epoch 385, test loss: 0.207174\n",
      "Epoch 386, test loss: 0.210258\n",
      "Epoch 387, test loss: 0.210139\n",
      "Epoch 388, test loss: 0.208488\n",
      "Epoch 389, test loss: 0.209453\n",
      "Epoch 390, test loss: 0.208330\n",
      "Epoch 391, test loss: 0.207546\n",
      "Epoch 392, test loss: 0.207105\n",
      "Epoch 393, test loss: 0.211513\n",
      "Epoch 394, test loss: 0.207705\n",
      "Epoch 395, test loss: 0.209276\n",
      "Epoch 396, test loss: 0.208589\n",
      "Epoch 397, test loss: 0.207466\n",
      "Epoch 398, test loss: 0.209327\n",
      "Epoch 399, test loss: 0.206994\n",
      "Epoch 400, test loss: 0.208937\n",
      "Epoch 401, test loss: 0.207044\n",
      "Epoch 402, test loss: 0.210157\n",
      "Epoch 403, test loss: 0.207737\n",
      "Epoch 404, test loss: 0.208506\n",
      "Epoch 405, test loss: 0.209106\n",
      "Epoch 406, test loss: 0.208567\n",
      "Epoch 407, test loss: 0.210900\n",
      "Epoch 408, test loss: 0.209721\n",
      "Epoch 409, test loss: 0.210146\n",
      "Epoch 410, test loss: 0.208388\n",
      "Epoch 411, test loss: 0.207237\n",
      "Epoch 412, test loss: 0.208999\n",
      "Epoch 413, test loss: 0.208939\n",
      "Epoch 414, test loss: 0.207786\n",
      "Epoch 415, test loss: 0.208874\n",
      "Epoch 416, test loss: 0.209054\n",
      "Epoch 417, test loss: 0.208461\n",
      "Epoch 418, test loss: 0.208320\n",
      "Epoch 419, test loss: 0.207891\n",
      "Epoch 420, test loss: 0.207719\n",
      "Epoch 421, test loss: 0.210241\n",
      "Epoch 422, test loss: 0.210806\n",
      "Epoch 423, test loss: 0.214317\n",
      "Epoch 424, test loss: 0.208626\n",
      "Epoch 425, test loss: 0.207538\n",
      "Epoch 426, test loss: 0.208380\n",
      "Epoch 427, test loss: 0.208066\n",
      "Epoch 428, test loss: 0.209520\n",
      "Epoch 429, test loss: 0.208283\n",
      "Epoch 430, test loss: 0.209233\n",
      "Epoch 431, test loss: 0.207655\n",
      "Epoch 432, test loss: 0.207880\n",
      "Epoch 433, test loss: 0.208240\n",
      "Epoch 434, test loss: 0.208378\n",
      "Epoch 435, test loss: 0.211055\n",
      "Epoch 436, test loss: 0.208187\n",
      "Epoch 437, test loss: 0.209834\n",
      "Epoch 438, test loss: 0.208937\n",
      "Epoch 439, test loss: 0.209249\n",
      "Epoch 440, test loss: 0.208303\n",
      "Epoch 441, test loss: 0.208275\n",
      "Epoch 442, test loss: 0.208254\n",
      "Epoch 443, test loss: 0.209818\n",
      "Epoch 444, test loss: 0.208267\n",
      "Epoch 445, test loss: 0.208065\n",
      "Epoch 446, test loss: 0.208938\n",
      "Epoch 447, test loss: 0.207518\n",
      "Epoch 448, test loss: 0.208231\n",
      "Epoch 449, test loss: 0.207747\n",
      "Pretrain data: 19786775.0\n",
      "Building dataset, requesting data from 0 to 665\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6955/108716\n",
      "Found 665 continuous time series\n",
      "Data shape: (115673, 6), Train/test: 115671/2\n",
      "Train test ratio: 57835.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1807\n",
      "Epoch 0, train loss: 0.205378\n",
      "Epoch 1, train loss: 0.213924\n",
      "Epoch 2, train loss: 0.185624\n",
      "Epoch 3, train loss: 0.254104\n",
      "Epoch 4, train loss: 0.202587\n",
      "Epoch 5, train loss: 0.263518\n",
      "Epoch 6, train loss: 0.178700\n",
      "Epoch 7, train loss: 0.189598\n",
      "Epoch 8, train loss: 0.220982\n",
      "Epoch 9, train loss: 0.171762\n",
      "Epoch 10, train loss: 0.232864\n",
      "Epoch 11, train loss: 0.207989\n",
      "Epoch 12, train loss: 0.262802\n",
      "Epoch 13, train loss: 0.165412\n",
      "Epoch 14, train loss: 0.248333\n",
      "Epoch 15, train loss: 0.256653\n",
      "Epoch 16, train loss: 0.264987\n",
      "Epoch 17, train loss: 0.285871\n",
      "Epoch 18, train loss: 0.186072\n",
      "Epoch 19, train loss: 0.202296\n",
      "Epoch 20, train loss: 0.202729\n",
      "Epoch 21, train loss: 0.182131\n",
      "Epoch 22, train loss: 0.241389\n",
      "Epoch 23, train loss: 0.218461\n",
      "Epoch 24, train loss: 0.186408\n",
      "Epoch 25, train loss: 0.220494\n",
      "Epoch 26, train loss: 0.202992\n",
      "Epoch 27, train loss: 0.194860\n",
      "Epoch 28, train loss: 0.214227\n",
      "Epoch 29, train loss: 0.177812\n",
      "Epoch 30, train loss: 0.295023\n",
      "Epoch 31, train loss: 0.209116\n",
      "Epoch 32, train loss: 0.284161\n",
      "Epoch 33, train loss: 0.230202\n",
      "Epoch 34, train loss: 0.204186\n",
      "Epoch 35, train loss: 0.182030\n",
      "Epoch 36, train loss: 0.181698\n",
      "Epoch 37, train loss: 0.299354\n",
      "Epoch 38, train loss: 0.215220\n",
      "Epoch 39, train loss: 0.198068\n",
      "Epoch 40, train loss: 0.187734\n",
      "Epoch 41, train loss: 0.188595\n",
      "Epoch 42, train loss: 0.172585\n",
      "Epoch 43, train loss: 0.211882\n",
      "Epoch 44, train loss: 0.159323\n",
      "Epoch 45, train loss: 0.222708\n",
      "Epoch 46, train loss: 0.160407\n",
      "Epoch 47, train loss: 0.148016\n",
      "Epoch 48, train loss: 0.186713\n",
      "Epoch 49, train loss: 0.202851\n",
      "Epoch 50, train loss: 0.171798\n",
      "Epoch 51, train loss: 0.158194\n",
      "Epoch 52, train loss: 0.176355\n",
      "Epoch 53, train loss: 0.205126\n",
      "Epoch 54, train loss: 0.230324\n",
      "Epoch 55, train loss: 0.189266\n",
      "Epoch 56, train loss: 0.234203\n",
      "Epoch 57, train loss: 0.192115\n",
      "Epoch 58, train loss: 0.199435\n",
      "Epoch 59, train loss: 0.248137\n",
      "Epoch 60, train loss: 0.189951\n",
      "Epoch 61, train loss: 0.232816\n",
      "Epoch 62, train loss: 0.164043\n",
      "Epoch 63, train loss: 0.201111\n",
      "Epoch 64, train loss: 0.255650\n",
      "Epoch 65, train loss: 0.192325\n",
      "Epoch 66, train loss: 0.144991\n",
      "Epoch 67, train loss: 0.195045\n",
      "Epoch 68, train loss: 0.191529\n",
      "Epoch 69, train loss: 0.158490\n",
      "Epoch 70, train loss: 0.222907\n",
      "Epoch 71, train loss: 0.191308\n",
      "Epoch 72, train loss: 0.264216\n",
      "Epoch 73, train loss: 0.177062\n",
      "Epoch 74, train loss: 0.156625\n",
      "Epoch 75, train loss: 0.193222\n",
      "Epoch 76, train loss: 0.168299\n",
      "Epoch 77, train loss: 0.200251\n",
      "Epoch 78, train loss: 0.184775\n",
      "Epoch 79, train loss: 0.279066\n",
      "Epoch 80, train loss: 0.235083\n",
      "Epoch 81, train loss: 0.181788\n",
      "Epoch 82, train loss: 0.260373\n",
      "Epoch 83, train loss: 0.235691\n",
      "Epoch 84, train loss: 0.251803\n",
      "Epoch 85, train loss: 0.179866\n",
      "Epoch 86, train loss: 0.215807\n",
      "Epoch 87, train loss: 0.163074\n",
      "Epoch 88, train loss: 0.219696\n",
      "Epoch 89, train loss: 0.182617\n",
      "Epoch 90, train loss: 0.257809\n",
      "Epoch 91, train loss: 0.223590\n",
      "Epoch 92, train loss: 0.210538\n",
      "Epoch 93, train loss: 0.186913\n",
      "Epoch 94, train loss: 0.197839\n",
      "Epoch 95, train loss: 0.165156\n",
      "Epoch 96, train loss: 0.295133\n",
      "Epoch 97, train loss: 0.154527\n",
      "Epoch 98, train loss: 0.161331\n",
      "Epoch 99, train loss: 0.233508\n",
      "Epoch 100, train loss: 0.176339\n",
      "Epoch 101, train loss: 0.183464\n",
      "Epoch 102, train loss: 0.195363\n",
      "Epoch 103, train loss: 0.244841\n",
      "Epoch 104, train loss: 0.167907\n",
      "Epoch 105, train loss: 0.210534\n",
      "Epoch 106, train loss: 0.171130\n",
      "Epoch 107, train loss: 0.213116\n",
      "Epoch 108, train loss: 0.220954\n",
      "Epoch 109, train loss: 0.217457\n",
      "Epoch 110, train loss: 0.199839\n",
      "Epoch 111, train loss: 0.182097\n",
      "Epoch 112, train loss: 0.219327\n",
      "Epoch 113, train loss: 0.268647\n",
      "Epoch 114, train loss: 0.252758\n",
      "Epoch 115, train loss: 0.369587\n",
      "Epoch 116, train loss: 0.187615\n",
      "Epoch 117, train loss: 0.158471\n",
      "Epoch 118, train loss: 0.178363\n",
      "Epoch 119, train loss: 0.187357\n",
      "Epoch 120, train loss: 0.164496\n",
      "Epoch 121, train loss: 0.204209\n",
      "Epoch 122, train loss: 0.227272\n",
      "Epoch 123, train loss: 0.192058\n",
      "Epoch 124, train loss: 0.250490\n",
      "Epoch 125, train loss: 0.228322\n",
      "Epoch 126, train loss: 0.205777\n",
      "Epoch 127, train loss: 0.189692\n",
      "Epoch 128, train loss: 0.192413\n",
      "Epoch 129, train loss: 0.211049\n",
      "Epoch 130, train loss: 0.225133\n",
      "Epoch 131, train loss: 0.224330\n",
      "Epoch 132, train loss: 0.209141\n",
      "Epoch 133, train loss: 0.216037\n",
      "Epoch 134, train loss: 0.234895\n",
      "Epoch 135, train loss: 0.187185\n",
      "Epoch 136, train loss: 0.187220\n",
      "Epoch 137, train loss: 0.193956\n",
      "Epoch 138, train loss: 0.204606\n",
      "Epoch 139, train loss: 0.195023\n",
      "Epoch 140, train loss: 0.169982\n",
      "Epoch 141, train loss: 0.201405\n",
      "Epoch 142, train loss: 0.224834\n",
      "Epoch 143, train loss: 0.216266\n",
      "Epoch 144, train loss: 0.192366\n",
      "Epoch 145, train loss: 0.216588\n",
      "Epoch 146, train loss: 0.213916\n",
      "Epoch 147, train loss: 0.175813\n",
      "Epoch 148, train loss: 0.183096\n",
      "Epoch 149, train loss: 0.204337\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "x here is\n",
      "[[254. 250. 249. 247. 242. 235.]\n",
      " [250. 249. 247. 242. 235. 229.]\n",
      " [249. 247. 242. 235. 229. 224.]\n",
      " ...\n",
      " [118. 122. 130. 143. 159. 177.]\n",
      " [122. 130. 143. 159. 177. 181.]\n",
      " [130. 143. 159. 177. 181. 205.]]\n",
      "y here is\n",
      "[[212. 212. 212. 212. 212. 212.]\n",
      " [209. 209. 209. 209. 209. 209.]\n",
      " [205. 205. 205. 205. 205. 205.]\n",
      " ...\n",
      " [254. 254. 254. 254. 254. 254.]\n",
      " [248. 248. 248. 248. 248. 248.]\n",
      " [242. 242. 242. 242. 242. 242.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 45 continuous time series\n",
      "Data shape: (2401, 6), Train/test: 1/2400\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "x here is\n",
      "[[ 76.  72.  68.  65.  63.  66.]\n",
      " [ 72.  68.  65.  63.  66.  71.]\n",
      " [ 68.  65.  63.  66.  71.  78.]\n",
      " ...\n",
      " [266. 263. 259. 254. 250. 254.]\n",
      " [263. 259. 254. 250. 254. 261.]\n",
      " [259. 254. 250. 254. 261. 267.]]\n",
      "y here is\n",
      "[[121. 121. 121. 121. 121. 121.]\n",
      " [131. 131. 131. 131. 131. 131.]\n",
      " [137. 137. 137. 137. 137. 137.]\n",
      " ...\n",
      " [258. 258. 258. 258. 258. 258.]\n",
      " [257. 257. 257. 257. 257. 257.]\n",
      " [255. 255. 255. 255. 255. 255.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1330/8723\n",
      "Found 177 continuous time series\n",
      "Data shape: (10055, 6), Train/test: 10053/2\n",
      "Train test ratio: 5026.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F495D50>\n",
      "Epoch 0, test loss: 0.218675\n",
      "Epoch 1, test loss: 0.214800\n",
      "Epoch 2, test loss: 0.214588\n",
      "Epoch 3, test loss: 0.213822\n",
      "Epoch 4, test loss: 0.214458\n",
      "Epoch 5, test loss: 0.215652\n",
      "Epoch 6, test loss: 0.215165\n",
      "Epoch 7, test loss: 0.215081\n",
      "Epoch 8, test loss: 0.213973\n",
      "Epoch 9, test loss: 0.214458\n",
      "Epoch 10, test loss: 0.213997\n",
      "Epoch 11, test loss: 0.215514\n",
      "Epoch 12, test loss: 0.215366\n",
      "Epoch 13, test loss: 0.213807\n",
      "Epoch 14, test loss: 0.214422\n",
      "Epoch 15, test loss: 0.214806\n",
      "Epoch 16, test loss: 0.215430\n",
      "Epoch 17, test loss: 0.215526\n",
      "Epoch 18, test loss: 0.214913\n",
      "Epoch 19, test loss: 0.219059\n",
      "Epoch 20, test loss: 0.215647\n",
      "Epoch 21, test loss: 0.214869\n",
      "Epoch 22, test loss: 0.218487\n",
      "Epoch 23, test loss: 0.214978\n",
      "Epoch 24, test loss: 0.214773\n",
      "Epoch 25, test loss: 0.214395\n",
      "Epoch 26, test loss: 0.214572\n",
      "Epoch 27, test loss: 0.213931\n",
      "Epoch 28, test loss: 0.215338\n",
      "Epoch 29, test loss: 0.214382\n",
      "Epoch 30, test loss: 0.214699\n",
      "Epoch 31, test loss: 0.216051\n",
      "Epoch 32, test loss: 0.214169\n",
      "Epoch 33, test loss: 0.214995\n",
      "Epoch 34, test loss: 0.213872\n",
      "Epoch 35, test loss: 0.216089\n",
      "Epoch 36, test loss: 0.214593\n",
      "Epoch 37, test loss: 0.219017\n",
      "Epoch 38, test loss: 0.214664\n",
      "Epoch 39, test loss: 0.215129\n",
      "Epoch 40, test loss: 0.214383\n",
      "Epoch 41, test loss: 0.215152\n",
      "Epoch 42, test loss: 0.214269\n",
      "Epoch 43, test loss: 0.215905\n",
      "Epoch 44, test loss: 0.221535\n",
      "Epoch 45, test loss: 0.223143\n",
      "Epoch 46, test loss: 0.216224\n",
      "Epoch 47, test loss: 0.213469\n",
      "Epoch 48, test loss: 0.214192\n",
      "Epoch 49, test loss: 0.219023\n",
      "Epoch 50, test loss: 0.213690\n",
      "Epoch 51, test loss: 0.216857\n",
      "Epoch 52, test loss: 0.214728\n",
      "Epoch 53, test loss: 0.214206\n",
      "Epoch 54, test loss: 0.215057\n",
      "Epoch 55, test loss: 0.219306\n",
      "Epoch 56, test loss: 0.218737\n",
      "Epoch 57, test loss: 0.218935\n",
      "Epoch 58, test loss: 0.213797\n",
      "Epoch 59, test loss: 0.216459\n",
      "Epoch 60, test loss: 0.218347\n",
      "Epoch 61, test loss: 0.213633\n",
      "Epoch 62, test loss: 0.214667\n",
      "Epoch 63, test loss: 0.217816\n",
      "Epoch 64, test loss: 0.216479\n",
      "Epoch 65, test loss: 0.213633\n",
      "Epoch 66, test loss: 0.215441\n",
      "Epoch 67, test loss: 0.214243\n",
      "Epoch 68, test loss: 0.214357\n",
      "Epoch 69, test loss: 0.214352\n",
      "Epoch 70, test loss: 0.214556\n",
      "Epoch 71, test loss: 0.213594\n",
      "Epoch 72, test loss: 0.216561\n",
      "Epoch 73, test loss: 0.213985\n",
      "Epoch 74, test loss: 0.215803\n",
      "Epoch 75, test loss: 0.217226\n",
      "Epoch 76, test loss: 0.213825\n",
      "Epoch 77, test loss: 0.218526\n",
      "Epoch 78, test loss: 0.213856\n",
      "Epoch 79, test loss: 0.215045\n",
      "Epoch 80, test loss: 0.215084\n",
      "Epoch 81, test loss: 0.214949\n",
      "Epoch 82, test loss: 0.214724\n",
      "Epoch 83, test loss: 0.213817\n",
      "Epoch 84, test loss: 0.213601\n",
      "Epoch 85, test loss: 0.215119\n",
      "Epoch 86, test loss: 0.214455\n",
      "Epoch 87, test loss: 0.216203\n",
      "Epoch 88, test loss: 0.218517\n",
      "Epoch 89, test loss: 0.215711\n",
      "Epoch 90, test loss: 0.217865\n",
      "Epoch 91, test loss: 0.217762\n",
      "Epoch 92, test loss: 0.219220\n",
      "Epoch 93, test loss: 0.223524\n",
      "Epoch 94, test loss: 0.214373\n",
      "Epoch 95, test loss: 0.214012\n",
      "Epoch 96, test loss: 0.214631\n",
      "Epoch 97, test loss: 0.217555\n",
      "Epoch 98, test loss: 0.214321\n",
      "Epoch 99, test loss: 0.214807\n",
      "Epoch 100, test loss: 0.216083\n",
      "Epoch 101, test loss: 0.213921\n",
      "Epoch 102, test loss: 0.213946\n",
      "Epoch 103, test loss: 0.216284\n",
      "Epoch 104, test loss: 0.214244\n",
      "Epoch 105, test loss: 0.214877\n",
      "Epoch 106, test loss: 0.216937\n",
      "Epoch 107, test loss: 0.216509\n",
      "Epoch 108, test loss: 0.214203\n",
      "Epoch 109, test loss: 0.215566\n",
      "Epoch 110, test loss: 0.221978\n",
      "Epoch 111, test loss: 0.217059\n",
      "Epoch 112, test loss: 0.214283\n",
      "Epoch 113, test loss: 0.216967\n",
      "Epoch 114, test loss: 0.214824\n",
      "Epoch 115, test loss: 0.218249\n",
      "Epoch 116, test loss: 0.214459\n",
      "Epoch 117, test loss: 0.214725\n",
      "Epoch 118, test loss: 0.215431\n",
      "Epoch 119, test loss: 0.214476\n",
      "Epoch 120, test loss: 0.218289\n",
      "Epoch 121, test loss: 0.214355\n",
      "Epoch 122, test loss: 0.214683\n",
      "Epoch 123, test loss: 0.214069\n",
      "Epoch 124, test loss: 0.213936\n",
      "Epoch 125, test loss: 0.214199\n",
      "Epoch 126, test loss: 0.214540\n",
      "Epoch 127, test loss: 0.214529\n",
      "Epoch 128, test loss: 0.215767\n",
      "Epoch 129, test loss: 0.214793\n",
      "Epoch 130, test loss: 0.220431\n",
      "Epoch 131, test loss: 0.215049\n",
      "Epoch 132, test loss: 0.220485\n",
      "Epoch 133, test loss: 0.214905\n",
      "Epoch 134, test loss: 0.223160\n",
      "Epoch 135, test loss: 0.217055\n",
      "Epoch 136, test loss: 0.216410\n",
      "Epoch 137, test loss: 0.214495\n",
      "Epoch 138, test loss: 0.217378\n",
      "Epoch 139, test loss: 0.214688\n",
      "Epoch 140, test loss: 0.214443\n",
      "Epoch 141, test loss: 0.215030\n",
      "Epoch 142, test loss: 0.214702\n",
      "Epoch 143, test loss: 0.217593\n",
      "Epoch 144, test loss: 0.214110\n",
      "Epoch 145, test loss: 0.214372\n",
      "Epoch 146, test loss: 0.218790\n",
      "Epoch 147, test loss: 0.215450\n",
      "Epoch 148, test loss: 0.216937\n",
      "Epoch 149, test loss: 0.214445\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F495D50>\n",
      "Epoch 0, test loss: 0.214591\n",
      "Epoch 1, test loss: 0.215365\n",
      "Epoch 2, test loss: 0.214480\n",
      "Epoch 3, test loss: 0.216043\n",
      "Epoch 4, test loss: 0.214531\n",
      "Epoch 5, test loss: 0.214090\n",
      "Epoch 6, test loss: 0.214896\n",
      "Epoch 7, test loss: 0.214912\n",
      "Epoch 8, test loss: 0.214474\n",
      "Epoch 9, test loss: 0.217594\n",
      "Epoch 10, test loss: 0.214798\n",
      "Epoch 11, test loss: 0.214787\n",
      "Epoch 12, test loss: 0.216087\n",
      "Epoch 13, test loss: 0.215452\n",
      "Epoch 14, test loss: 0.218379\n",
      "Epoch 15, test loss: 0.215485\n",
      "Epoch 16, test loss: 0.215134\n",
      "Epoch 17, test loss: 0.215400\n",
      "Epoch 18, test loss: 0.218392\n",
      "Epoch 19, test loss: 0.215914\n",
      "Epoch 20, test loss: 0.215494\n",
      "Epoch 21, test loss: 0.216238\n",
      "Epoch 22, test loss: 0.216409\n",
      "Epoch 23, test loss: 0.221051\n",
      "Epoch 24, test loss: 0.214506\n",
      "Epoch 25, test loss: 0.216091\n",
      "Epoch 26, test loss: 0.215489\n",
      "Epoch 27, test loss: 0.216930\n",
      "Epoch 28, test loss: 0.215283\n",
      "Epoch 29, test loss: 0.215548\n",
      "Epoch 30, test loss: 0.215349\n",
      "Epoch 31, test loss: 0.214364\n",
      "Epoch 32, test loss: 0.214958\n",
      "Epoch 33, test loss: 0.214610\n",
      "Epoch 34, test loss: 0.214975\n",
      "Epoch 35, test loss: 0.219268\n",
      "Epoch 36, test loss: 0.214733\n",
      "Epoch 37, test loss: 0.216513\n",
      "Epoch 38, test loss: 0.220564\n",
      "Epoch 39, test loss: 0.214613\n",
      "Epoch 40, test loss: 0.215488\n",
      "Epoch 41, test loss: 0.217825\n",
      "Epoch 42, test loss: 0.214394\n",
      "Epoch 43, test loss: 0.218327\n",
      "Epoch 44, test loss: 0.214820\n",
      "Epoch 45, test loss: 0.215238\n",
      "Epoch 46, test loss: 0.215120\n",
      "Epoch 47, test loss: 0.216467\n",
      "Epoch 48, test loss: 0.214516\n",
      "Epoch 49, test loss: 0.214956\n",
      "Epoch 50, test loss: 0.214447\n",
      "Epoch 51, test loss: 0.218388\n",
      "Epoch 52, test loss: 0.214699\n",
      "Epoch 53, test loss: 0.215463\n",
      "Epoch 54, test loss: 0.217342\n",
      "Epoch 55, test loss: 0.214312\n",
      "Epoch 56, test loss: 0.214407\n",
      "Epoch 57, test loss: 0.220591\n",
      "Epoch 58, test loss: 0.217532\n",
      "Epoch 59, test loss: 0.214643\n",
      "Epoch 60, test loss: 0.214537\n",
      "Epoch 61, test loss: 0.215425\n",
      "Epoch 62, test loss: 0.216824\n",
      "Epoch 63, test loss: 0.214422\n",
      "Epoch 64, test loss: 0.221298\n",
      "Epoch 65, test loss: 0.214436\n",
      "Epoch 66, test loss: 0.216195\n",
      "Epoch 67, test loss: 0.214313\n",
      "Epoch 68, test loss: 0.214006\n",
      "Epoch 69, test loss: 0.217831\n",
      "Epoch 70, test loss: 0.216598\n",
      "Epoch 71, test loss: 0.214435\n",
      "Epoch 72, test loss: 0.217549\n",
      "Epoch 73, test loss: 0.214405\n",
      "Epoch 74, test loss: 0.216153\n",
      "Epoch 75, test loss: 0.213624\n",
      "Epoch 76, test loss: 0.214121\n",
      "Epoch 77, test loss: 0.214720\n",
      "Epoch 78, test loss: 0.217664\n",
      "Epoch 79, test loss: 0.214080\n",
      "Epoch 80, test loss: 0.214779\n",
      "Epoch 81, test loss: 0.214075\n",
      "Epoch 82, test loss: 0.215268\n",
      "Epoch 83, test loss: 0.215851\n",
      "Epoch 84, test loss: 0.214845\n",
      "Epoch 85, test loss: 0.214645\n",
      "Epoch 86, test loss: 0.213844\n",
      "Epoch 87, test loss: 0.218019\n",
      "Epoch 88, test loss: 0.214812\n",
      "Epoch 89, test loss: 0.215691\n",
      "Epoch 90, test loss: 0.214240\n",
      "Epoch 91, test loss: 0.214633\n",
      "Epoch 92, test loss: 0.215042\n",
      "Epoch 93, test loss: 0.213701\n",
      "Epoch 94, test loss: 0.214097\n",
      "Epoch 95, test loss: 0.217611\n",
      "Epoch 96, test loss: 0.214592\n",
      "Epoch 97, test loss: 0.214536\n",
      "Epoch 98, test loss: 0.214214\n",
      "Epoch 99, test loss: 0.215122\n",
      "Epoch 100, test loss: 0.214515\n",
      "Epoch 101, test loss: 0.214370\n",
      "Epoch 102, test loss: 0.213937\n",
      "Epoch 103, test loss: 0.217799\n",
      "Epoch 104, test loss: 0.214257\n",
      "Epoch 105, test loss: 0.214927\n",
      "Epoch 106, test loss: 0.214269\n",
      "Epoch 107, test loss: 0.214695\n",
      "Epoch 108, test loss: 0.213600\n",
      "Epoch 109, test loss: 0.216807\n",
      "Epoch 110, test loss: 0.214621\n",
      "Epoch 111, test loss: 0.215368\n",
      "Epoch 112, test loss: 0.214678\n",
      "Epoch 113, test loss: 0.215188\n",
      "Epoch 114, test loss: 0.214323\n",
      "Epoch 115, test loss: 0.213663\n",
      "Epoch 116, test loss: 0.213992\n",
      "Epoch 117, test loss: 0.214249\n",
      "Epoch 118, test loss: 0.214811\n",
      "Epoch 119, test loss: 0.217600\n",
      "Epoch 120, test loss: 0.214289\n",
      "Epoch 121, test loss: 0.214189\n",
      "Epoch 122, test loss: 0.215420\n",
      "Epoch 123, test loss: 0.218221\n",
      "Epoch 124, test loss: 0.214840\n",
      "Epoch 125, test loss: 0.214436\n",
      "Epoch 126, test loss: 0.214399\n",
      "Epoch 127, test loss: 0.221709\n",
      "Epoch 128, test loss: 0.216318\n",
      "Epoch 129, test loss: 0.215464\n",
      "Epoch 130, test loss: 0.215029\n",
      "Epoch 131, test loss: 0.218491\n",
      "Epoch 132, test loss: 0.214736\n",
      "Epoch 133, test loss: 0.213640\n",
      "Epoch 134, test loss: 0.216073\n",
      "Epoch 135, test loss: 0.216971\n",
      "Epoch 136, test loss: 0.214119\n",
      "Epoch 137, test loss: 0.215791\n",
      "Epoch 138, test loss: 0.215076\n",
      "Epoch 139, test loss: 0.213826\n",
      "Epoch 140, test loss: 0.218504\n",
      "Epoch 141, test loss: 0.214532\n",
      "Epoch 142, test loss: 0.213959\n",
      "Epoch 143, test loss: 0.216906\n",
      "Epoch 144, test loss: 0.214010\n",
      "Epoch 145, test loss: 0.214098\n",
      "Epoch 146, test loss: 0.216276\n",
      "Epoch 147, test loss: 0.214370\n",
      "Epoch 148, test loss: 0.213878\n",
      "Epoch 149, test loss: 0.216677\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F495D50>\n",
      "Epoch 0, test loss: 0.289467\n",
      "Epoch 1, test loss: 0.237665\n",
      "Epoch 2, test loss: 0.230094\n",
      "Epoch 3, test loss: 0.225784\n",
      "Epoch 4, test loss: 0.225045\n",
      "Epoch 5, test loss: 0.225412\n",
      "Epoch 6, test loss: 0.224300\n",
      "Epoch 7, test loss: 0.224199\n",
      "Epoch 8, test loss: 0.222571\n",
      "Epoch 9, test loss: 0.223361\n",
      "Epoch 10, test loss: 0.223909\n",
      "Epoch 11, test loss: 0.222680\n",
      "Epoch 12, test loss: 0.223989\n",
      "Epoch 13, test loss: 0.222644\n",
      "Epoch 14, test loss: 0.222802\n",
      "Epoch 15, test loss: 0.223988\n",
      "Epoch 16, test loss: 0.222083\n",
      "Epoch 17, test loss: 0.222214\n",
      "Epoch 18, test loss: 0.222018\n",
      "Epoch 19, test loss: 0.221291\n",
      "Epoch 20, test loss: 0.221139\n",
      "Epoch 21, test loss: 0.226512\n",
      "Epoch 22, test loss: 0.223976\n",
      "Epoch 23, test loss: 0.219941\n",
      "Epoch 24, test loss: 0.222343\n",
      "Epoch 25, test loss: 0.221176\n",
      "Epoch 26, test loss: 0.220593\n",
      "Epoch 27, test loss: 0.220493\n",
      "Epoch 28, test loss: 0.223102\n",
      "Epoch 29, test loss: 0.219021\n",
      "Epoch 30, test loss: 0.219561\n",
      "Epoch 31, test loss: 0.221418\n",
      "Epoch 32, test loss: 0.218612\n",
      "Epoch 33, test loss: 0.221125\n",
      "Epoch 34, test loss: 0.219794\n",
      "Epoch 35, test loss: 0.218037\n",
      "Epoch 36, test loss: 0.220576\n",
      "Epoch 37, test loss: 0.219575\n",
      "Epoch 38, test loss: 0.218947\n",
      "Epoch 39, test loss: 0.225254\n",
      "Epoch 40, test loss: 0.217251\n",
      "Epoch 41, test loss: 0.218950\n",
      "Epoch 42, test loss: 0.217468\n",
      "Epoch 43, test loss: 0.217270\n",
      "Epoch 44, test loss: 0.219090\n",
      "Epoch 45, test loss: 0.217798\n",
      "Epoch 46, test loss: 0.217098\n",
      "Epoch 47, test loss: 0.217746\n",
      "Epoch 48, test loss: 0.219177\n",
      "Epoch 49, test loss: 0.218000\n",
      "Epoch 50, test loss: 0.220157\n",
      "Epoch 51, test loss: 0.219538\n",
      "Epoch 52, test loss: 0.219225\n",
      "Epoch 53, test loss: 0.218928\n",
      "Epoch 54, test loss: 0.219223\n",
      "Epoch 55, test loss: 0.221860\n",
      "Epoch 56, test loss: 0.217490\n",
      "Epoch 57, test loss: 0.218938\n",
      "Epoch 58, test loss: 0.217014\n",
      "Epoch 59, test loss: 0.219957\n",
      "Epoch 60, test loss: 0.217733\n",
      "Epoch 61, test loss: 0.217912\n",
      "Epoch 62, test loss: 0.219604\n",
      "Epoch 63, test loss: 0.219705\n",
      "Epoch 64, test loss: 0.216872\n",
      "Epoch 65, test loss: 0.217951\n",
      "Epoch 66, test loss: 0.217134\n",
      "Epoch 67, test loss: 0.216881\n",
      "Epoch 68, test loss: 0.216800\n",
      "Epoch 69, test loss: 0.216298\n",
      "Epoch 70, test loss: 0.217546\n",
      "Epoch 71, test loss: 0.218140\n",
      "Epoch 72, test loss: 0.219012\n",
      "Epoch 73, test loss: 0.217541\n",
      "Epoch 74, test loss: 0.218595\n",
      "Epoch 75, test loss: 0.216795\n",
      "Epoch 76, test loss: 0.217291\n",
      "Epoch 77, test loss: 0.217486\n",
      "Epoch 78, test loss: 0.216219\n",
      "Epoch 79, test loss: 0.216447\n",
      "Epoch 80, test loss: 0.217912\n",
      "Epoch 81, test loss: 0.221102\n",
      "Epoch 82, test loss: 0.216819\n",
      "Epoch 83, test loss: 0.216990\n",
      "Epoch 84, test loss: 0.216288\n",
      "Epoch 85, test loss: 0.219108\n",
      "Epoch 86, test loss: 0.217425\n",
      "Epoch 87, test loss: 0.218319\n",
      "Epoch 88, test loss: 0.224225\n",
      "Epoch 89, test loss: 0.219966\n",
      "Epoch 90, test loss: 0.216226\n",
      "Epoch 91, test loss: 0.216461\n",
      "Epoch 92, test loss: 0.217962\n",
      "Epoch 93, test loss: 0.219170\n",
      "Epoch 94, test loss: 0.223843\n",
      "Epoch 95, test loss: 0.216051\n",
      "Epoch 96, test loss: 0.215834\n",
      "Epoch 97, test loss: 0.215773\n",
      "Epoch 98, test loss: 0.216039\n",
      "Epoch 99, test loss: 0.220202\n",
      "Epoch 100, test loss: 0.216939\n",
      "Epoch 101, test loss: 0.216366\n",
      "Epoch 102, test loss: 0.220192\n",
      "Epoch 103, test loss: 0.216366\n",
      "Epoch 104, test loss: 0.218966\n",
      "Epoch 105, test loss: 0.219703\n",
      "Epoch 106, test loss: 0.215927\n",
      "Epoch 107, test loss: 0.216341\n",
      "Epoch 108, test loss: 0.216156\n",
      "Epoch 109, test loss: 0.218268\n",
      "Epoch 110, test loss: 0.216585\n",
      "Epoch 111, test loss: 0.218904\n",
      "Epoch 112, test loss: 0.216510\n",
      "Epoch 113, test loss: 0.219082\n",
      "Epoch 114, test loss: 0.219441\n",
      "Epoch 115, test loss: 0.223486\n",
      "Epoch 116, test loss: 0.217174\n",
      "Epoch 117, test loss: 0.218113\n",
      "Epoch 118, test loss: 0.216684\n",
      "Epoch 119, test loss: 0.217840\n",
      "Epoch 120, test loss: 0.219699\n",
      "Epoch 121, test loss: 0.220932\n",
      "Epoch 122, test loss: 0.219201\n",
      "Epoch 123, test loss: 0.217341\n",
      "Epoch 124, test loss: 0.220433\n",
      "Epoch 125, test loss: 0.216581\n",
      "Epoch 126, test loss: 0.217259\n",
      "Epoch 127, test loss: 0.219415\n",
      "Epoch 128, test loss: 0.216189\n",
      "Epoch 129, test loss: 0.216960\n",
      "Epoch 130, test loss: 0.216939\n",
      "Epoch 131, test loss: 0.216497\n",
      "Epoch 132, test loss: 0.217835\n",
      "Epoch 133, test loss: 0.219645\n",
      "Epoch 134, test loss: 0.217826\n",
      "Epoch 135, test loss: 0.218831\n",
      "Epoch 136, test loss: 0.221795\n",
      "Epoch 137, test loss: 0.217193\n",
      "Epoch 138, test loss: 0.218160\n",
      "Epoch 139, test loss: 0.217924\n",
      "Epoch 140, test loss: 0.216030\n",
      "Epoch 141, test loss: 0.216847\n",
      "Epoch 142, test loss: 0.216119\n",
      "Epoch 143, test loss: 0.217307\n",
      "Epoch 144, test loss: 0.217149\n",
      "Epoch 145, test loss: 0.226564\n",
      "Epoch 146, test loss: 0.218094\n",
      "Epoch 147, test loss: 0.218337\n",
      "Epoch 148, test loss: 0.216165\n",
      "Epoch 149, test loss: 0.216311\n",
      "Epoch 150, test loss: 0.216483\n",
      "Epoch 151, test loss: 0.225856\n",
      "Epoch 152, test loss: 0.216624\n",
      "Epoch 153, test loss: 0.219344\n",
      "Epoch 154, test loss: 0.219773\n",
      "Epoch 155, test loss: 0.217874\n",
      "Epoch 156, test loss: 0.218184\n",
      "Epoch 157, test loss: 0.216784\n",
      "Epoch 158, test loss: 0.216486\n",
      "Epoch 159, test loss: 0.218410\n",
      "Epoch 160, test loss: 0.217373\n",
      "Epoch 161, test loss: 0.218031\n",
      "Epoch 162, test loss: 0.216520\n",
      "Epoch 163, test loss: 0.216375\n",
      "Epoch 164, test loss: 0.216423\n",
      "Epoch 165, test loss: 0.219613\n",
      "Epoch 166, test loss: 0.216677\n",
      "Epoch 167, test loss: 0.216821\n",
      "Epoch 168, test loss: 0.217013\n",
      "Epoch 169, test loss: 0.216403\n",
      "Epoch 170, test loss: 0.217321\n",
      "Epoch 171, test loss: 0.221863\n",
      "Epoch 172, test loss: 0.217244\n",
      "Epoch 173, test loss: 0.219798\n",
      "Epoch 174, test loss: 0.216416\n",
      "Epoch 175, test loss: 0.218988\n",
      "Epoch 176, test loss: 0.217541\n",
      "Epoch 177, test loss: 0.216679\n",
      "Epoch 178, test loss: 0.217478\n",
      "Epoch 179, test loss: 0.216471\n",
      "Epoch 180, test loss: 0.217770\n",
      "Epoch 181, test loss: 0.216128\n",
      "Epoch 182, test loss: 0.221193\n",
      "Epoch 183, test loss: 0.217838\n",
      "Epoch 184, test loss: 0.216615\n",
      "Epoch 185, test loss: 0.218121\n",
      "Epoch 186, test loss: 0.217311\n",
      "Epoch 187, test loss: 0.216155\n",
      "Epoch 188, test loss: 0.221025\n",
      "Epoch 189, test loss: 0.216564\n",
      "Epoch 190, test loss: 0.221055\n",
      "Epoch 191, test loss: 0.216482\n",
      "Epoch 192, test loss: 0.218196\n",
      "Epoch 193, test loss: 0.218144\n",
      "Epoch 194, test loss: 0.216942\n",
      "Epoch 195, test loss: 0.216471\n",
      "Epoch 196, test loss: 0.220084\n",
      "Epoch 197, test loss: 0.217083\n",
      "Epoch 198, test loss: 0.222081\n",
      "Epoch 199, test loss: 0.216740\n",
      "Epoch 200, test loss: 0.216359\n",
      "Epoch 201, test loss: 0.216185\n",
      "Epoch 202, test loss: 0.216425\n",
      "Epoch 203, test loss: 0.220617\n",
      "Epoch 204, test loss: 0.224670\n",
      "Epoch 205, test loss: 0.216483\n",
      "Epoch 206, test loss: 0.217937\n",
      "Epoch 207, test loss: 0.216349\n",
      "Epoch 208, test loss: 0.217480\n",
      "Epoch 209, test loss: 0.218686\n",
      "Epoch 210, test loss: 0.217541\n",
      "Epoch 211, test loss: 0.216294\n",
      "Epoch 212, test loss: 0.218830\n",
      "Epoch 213, test loss: 0.216930\n",
      "Epoch 214, test loss: 0.216514\n",
      "Epoch 215, test loss: 0.216568\n",
      "Epoch 216, test loss: 0.217699\n",
      "Epoch 217, test loss: 0.217656\n",
      "Epoch 218, test loss: 0.219151\n",
      "Epoch 219, test loss: 0.217099\n",
      "Epoch 220, test loss: 0.219185\n",
      "Epoch 221, test loss: 0.216886\n",
      "Epoch 222, test loss: 0.218890\n",
      "Epoch 223, test loss: 0.218312\n",
      "Epoch 224, test loss: 0.218211\n",
      "Epoch 225, test loss: 0.221611\n",
      "Epoch 226, test loss: 0.216532\n",
      "Epoch 227, test loss: 0.219970\n",
      "Epoch 228, test loss: 0.216369\n",
      "Epoch 229, test loss: 0.218228\n",
      "Epoch 230, test loss: 0.217428\n",
      "Epoch 231, test loss: 0.216269\n",
      "Epoch 232, test loss: 0.220763\n",
      "Epoch 233, test loss: 0.215900\n",
      "Epoch 234, test loss: 0.217982\n",
      "Epoch 235, test loss: 0.216152\n",
      "Epoch 236, test loss: 0.218188\n",
      "Epoch 237, test loss: 0.216476\n",
      "Epoch 238, test loss: 0.216041\n",
      "Epoch 239, test loss: 0.217326\n",
      "Epoch 240, test loss: 0.218784\n",
      "Epoch 241, test loss: 0.218216\n",
      "Epoch 242, test loss: 0.218776\n",
      "Epoch 243, test loss: 0.220585\n",
      "Epoch 244, test loss: 0.216605\n",
      "Epoch 245, test loss: 0.215774\n",
      "Epoch 246, test loss: 0.216513\n",
      "Epoch 247, test loss: 0.218324\n",
      "Epoch 248, test loss: 0.216511\n",
      "Epoch 249, test loss: 0.221396\n",
      "Epoch 250, test loss: 0.217706\n",
      "Epoch 251, test loss: 0.216579\n",
      "Epoch 252, test loss: 0.215914\n",
      "Epoch 253, test loss: 0.217464\n",
      "Epoch 254, test loss: 0.216625\n",
      "Epoch 255, test loss: 0.220823\n",
      "Epoch 256, test loss: 0.216180\n",
      "Epoch 257, test loss: 0.215650\n",
      "Epoch 258, test loss: 0.217266\n",
      "Epoch 259, test loss: 0.217270\n",
      "Epoch 260, test loss: 0.215854\n",
      "Epoch 261, test loss: 0.219610\n",
      "Epoch 262, test loss: 0.219382\n",
      "Epoch 263, test loss: 0.215869\n",
      "Epoch 264, test loss: 0.217308\n",
      "Epoch 265, test loss: 0.216489\n",
      "Epoch 266, test loss: 0.217215\n",
      "Epoch 267, test loss: 0.218870\n",
      "Epoch 268, test loss: 0.219256\n",
      "Epoch 269, test loss: 0.218357\n",
      "Epoch 270, test loss: 0.218855\n",
      "Epoch 271, test loss: 0.225318\n",
      "Epoch 272, test loss: 0.218342\n",
      "Epoch 273, test loss: 0.218249\n",
      "Epoch 274, test loss: 0.217880\n",
      "Epoch 275, test loss: 0.216230\n",
      "Epoch 276, test loss: 0.224750\n",
      "Epoch 277, test loss: 0.216148\n",
      "Epoch 278, test loss: 0.217396\n",
      "Epoch 279, test loss: 0.215704\n",
      "Epoch 280, test loss: 0.215777\n",
      "Epoch 281, test loss: 0.216268\n",
      "Epoch 282, test loss: 0.216854\n",
      "Epoch 283, test loss: 0.215873\n",
      "Epoch 284, test loss: 0.216533\n",
      "Epoch 285, test loss: 0.218197\n",
      "Epoch 286, test loss: 0.216103\n",
      "Epoch 287, test loss: 0.218458\n",
      "Epoch 288, test loss: 0.215601\n",
      "Epoch 289, test loss: 0.216893\n",
      "Epoch 290, test loss: 0.216538\n",
      "Epoch 291, test loss: 0.216143\n",
      "Epoch 292, test loss: 0.215796\n",
      "Epoch 293, test loss: 0.219291\n",
      "Epoch 294, test loss: 0.216927\n",
      "Epoch 295, test loss: 0.216529\n",
      "Epoch 296, test loss: 0.218468\n",
      "Epoch 297, test loss: 0.222180\n",
      "Epoch 298, test loss: 0.222374\n",
      "Epoch 299, test loss: 0.216763\n",
      "Epoch 300, test loss: 0.216713\n",
      "Epoch 301, test loss: 0.225955\n",
      "Epoch 302, test loss: 0.218898\n",
      "Epoch 303, test loss: 0.215817\n",
      "Epoch 304, test loss: 0.215582\n",
      "Epoch 305, test loss: 0.216336\n",
      "Epoch 306, test loss: 0.217076\n",
      "Epoch 307, test loss: 0.215688\n",
      "Epoch 308, test loss: 0.216020\n",
      "Epoch 309, test loss: 0.218424\n",
      "Epoch 310, test loss: 0.215640\n",
      "Epoch 311, test loss: 0.216509\n",
      "Epoch 312, test loss: 0.215825\n",
      "Epoch 313, test loss: 0.218844\n",
      "Epoch 314, test loss: 0.224240\n",
      "Epoch 315, test loss: 0.217237\n",
      "Epoch 316, test loss: 0.216405\n",
      "Epoch 317, test loss: 0.218396\n",
      "Epoch 318, test loss: 0.219171\n",
      "Epoch 319, test loss: 0.217034\n",
      "Epoch 320, test loss: 0.217004\n",
      "Epoch 321, test loss: 0.216181\n",
      "Epoch 322, test loss: 0.215911\n",
      "Epoch 323, test loss: 0.217358\n",
      "Epoch 324, test loss: 0.216302\n",
      "Epoch 325, test loss: 0.218961\n",
      "Epoch 326, test loss: 0.218350\n",
      "Epoch 327, test loss: 0.215945\n",
      "Epoch 328, test loss: 0.216319\n",
      "Epoch 329, test loss: 0.215905\n",
      "Epoch 330, test loss: 0.216190\n",
      "Epoch 331, test loss: 0.216685\n",
      "Epoch 332, test loss: 0.215968\n",
      "Epoch 333, test loss: 0.218106\n",
      "Epoch 334, test loss: 0.218742\n",
      "Epoch 335, test loss: 0.219752\n",
      "Epoch 336, test loss: 0.217563\n",
      "Epoch 337, test loss: 0.217231\n",
      "Epoch 338, test loss: 0.217597\n",
      "Epoch 339, test loss: 0.215571\n",
      "Epoch 340, test loss: 0.215972\n",
      "Epoch 341, test loss: 0.218981\n",
      "Epoch 342, test loss: 0.216074\n",
      "Epoch 343, test loss: 0.223273\n",
      "Epoch 344, test loss: 0.219033\n",
      "Epoch 345, test loss: 0.216212\n",
      "Epoch 346, test loss: 0.218174\n",
      "Epoch 347, test loss: 0.215780\n",
      "Epoch 348, test loss: 0.215892\n",
      "Epoch 349, test loss: 0.216767\n",
      "Epoch 350, test loss: 0.215582\n",
      "Epoch 351, test loss: 0.218243\n",
      "Epoch 352, test loss: 0.216360\n",
      "Epoch 353, test loss: 0.216998\n",
      "Epoch 354, test loss: 0.216227\n",
      "Epoch 355, test loss: 0.216371\n",
      "Epoch 356, test loss: 0.217466\n",
      "Epoch 357, test loss: 0.217726\n",
      "Epoch 358, test loss: 0.216637\n",
      "Epoch 359, test loss: 0.215892\n",
      "Epoch 360, test loss: 0.219907\n",
      "Epoch 361, test loss: 0.216178\n",
      "Epoch 362, test loss: 0.219176\n",
      "Epoch 363, test loss: 0.217860\n",
      "Epoch 364, test loss: 0.216582\n",
      "Epoch 365, test loss: 0.216614\n",
      "Epoch 366, test loss: 0.219794\n",
      "Epoch 367, test loss: 0.216819\n",
      "Epoch 368, test loss: 0.216138\n",
      "Epoch 369, test loss: 0.216174\n",
      "Epoch 370, test loss: 0.217198\n",
      "Epoch 371, test loss: 0.216547\n",
      "Epoch 372, test loss: 0.219713\n",
      "Epoch 373, test loss: 0.217418\n",
      "Epoch 374, test loss: 0.215790\n",
      "Epoch 375, test loss: 0.217322\n",
      "Epoch 376, test loss: 0.215584\n",
      "Epoch 377, test loss: 0.215943\n",
      "Epoch 378, test loss: 0.216647\n",
      "Epoch 379, test loss: 0.217349\n",
      "Epoch 380, test loss: 0.221066\n",
      "Epoch 381, test loss: 0.217372\n",
      "Epoch 382, test loss: 0.218203\n",
      "Epoch 383, test loss: 0.219852\n",
      "Epoch 384, test loss: 0.216357\n",
      "Epoch 385, test loss: 0.215429\n",
      "Epoch 386, test loss: 0.219005\n",
      "Epoch 387, test loss: 0.217794\n",
      "Epoch 388, test loss: 0.216961\n",
      "Epoch 389, test loss: 0.221914\n",
      "Epoch 390, test loss: 0.216668\n",
      "Epoch 391, test loss: 0.216201\n",
      "Epoch 392, test loss: 0.219172\n",
      "Epoch 393, test loss: 0.215378\n",
      "Epoch 394, test loss: 0.217871\n",
      "Epoch 395, test loss: 0.219545\n",
      "Epoch 396, test loss: 0.215831\n",
      "Epoch 397, test loss: 0.223320\n",
      "Epoch 398, test loss: 0.218194\n",
      "Epoch 399, test loss: 0.216988\n",
      "Epoch 400, test loss: 0.219062\n",
      "Epoch 401, test loss: 0.215408\n",
      "Epoch 402, test loss: 0.217345\n",
      "Epoch 403, test loss: 0.217340\n",
      "Epoch 404, test loss: 0.220257\n",
      "Epoch 405, test loss: 0.215244\n",
      "Epoch 406, test loss: 0.216312\n",
      "Epoch 407, test loss: 0.217852\n",
      "Epoch 408, test loss: 0.215867\n",
      "Epoch 409, test loss: 0.218135\n",
      "Epoch 410, test loss: 0.225641\n",
      "Epoch 411, test loss: 0.216202\n",
      "Epoch 412, test loss: 0.215284\n",
      "Epoch 413, test loss: 0.221141\n",
      "Epoch 414, test loss: 0.217789\n",
      "Epoch 415, test loss: 0.216451\n",
      "Epoch 416, test loss: 0.216465\n",
      "Epoch 417, test loss: 0.217548\n",
      "Epoch 418, test loss: 0.215551\n",
      "Epoch 419, test loss: 0.225526\n",
      "Epoch 420, test loss: 0.215309\n",
      "Epoch 421, test loss: 0.217742\n",
      "Epoch 422, test loss: 0.227489\n",
      "Epoch 423, test loss: 0.217540\n",
      "Epoch 424, test loss: 0.216814\n",
      "Epoch 425, test loss: 0.220564\n",
      "Epoch 426, test loss: 0.215576\n",
      "Epoch 427, test loss: 0.216080\n",
      "Epoch 428, test loss: 0.215671\n",
      "Epoch 429, test loss: 0.215852\n",
      "Epoch 430, test loss: 0.220115\n",
      "Epoch 431, test loss: 0.216113\n",
      "Epoch 432, test loss: 0.216861\n",
      "Epoch 433, test loss: 0.215634\n",
      "Epoch 434, test loss: 0.218822\n",
      "Epoch 435, test loss: 0.216365\n",
      "Epoch 436, test loss: 0.217518\n",
      "Epoch 437, test loss: 0.215516\n",
      "Epoch 438, test loss: 0.215896\n",
      "Epoch 439, test loss: 0.219933\n",
      "Epoch 440, test loss: 0.215487\n",
      "Epoch 441, test loss: 0.217149\n",
      "Epoch 442, test loss: 0.216212\n",
      "Epoch 443, test loss: 0.218552\n",
      "Epoch 444, test loss: 0.215450\n",
      "Epoch 445, test loss: 0.217165\n",
      "Epoch 446, test loss: 0.215588\n",
      "Epoch 447, test loss: 0.216910\n",
      "Epoch 448, test loss: 0.216028\n",
      "Epoch 449, test loss: 0.216789\n",
      "Pretrain data: 20092707.0\n",
      "Building dataset, requesting data from 0 to 672\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7734/110770\n",
      "Found 672 continuous time series\n",
      "Data shape: (118506, 6), Train/test: 118504/2\n",
      "Train test ratio: 59252.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1851\n",
      "Epoch 0, train loss: 0.217789\n",
      "Epoch 1, train loss: 0.296364\n",
      "Epoch 2, train loss: 0.255340\n",
      "Epoch 3, train loss: 0.240552\n",
      "Epoch 4, train loss: 0.228153\n",
      "Epoch 5, train loss: 0.178239\n",
      "Epoch 6, train loss: 0.201578\n",
      "Epoch 7, train loss: 0.213524\n",
      "Epoch 8, train loss: 0.214279\n",
      "Epoch 9, train loss: 0.200864\n",
      "Epoch 10, train loss: 0.179090\n",
      "Epoch 11, train loss: 0.176866\n",
      "Epoch 12, train loss: 0.206879\n",
      "Epoch 13, train loss: 0.196616\n",
      "Epoch 14, train loss: 0.231880\n",
      "Epoch 15, train loss: 0.287342\n",
      "Epoch 16, train loss: 0.252612\n",
      "Epoch 17, train loss: 0.198787\n",
      "Epoch 18, train loss: 0.212479\n",
      "Epoch 19, train loss: 0.201834\n",
      "Epoch 20, train loss: 0.202447\n",
      "Epoch 21, train loss: 0.188552\n",
      "Epoch 22, train loss: 0.210661\n",
      "Epoch 23, train loss: 0.220342\n",
      "Epoch 24, train loss: 0.200447\n",
      "Epoch 25, train loss: 0.173663\n",
      "Epoch 26, train loss: 0.203442\n",
      "Epoch 27, train loss: 0.206292\n",
      "Epoch 28, train loss: 0.173521\n",
      "Epoch 29, train loss: 0.189745\n",
      "Epoch 30, train loss: 0.195134\n",
      "Epoch 31, train loss: 0.219678\n",
      "Epoch 32, train loss: 0.197609\n",
      "Epoch 33, train loss: 0.358192\n",
      "Epoch 34, train loss: 0.147628\n",
      "Epoch 35, train loss: 0.221815\n",
      "Epoch 36, train loss: 0.239407\n",
      "Epoch 37, train loss: 0.213877\n",
      "Epoch 38, train loss: 0.220283\n",
      "Epoch 39, train loss: 0.164315\n",
      "Epoch 40, train loss: 0.214402\n",
      "Epoch 41, train loss: 0.238046\n",
      "Epoch 42, train loss: 0.228938\n",
      "Epoch 43, train loss: 0.243916\n",
      "Epoch 44, train loss: 0.209252\n",
      "Epoch 45, train loss: 0.206475\n",
      "Epoch 46, train loss: 0.209763\n",
      "Epoch 47, train loss: 0.167235\n",
      "Epoch 48, train loss: 0.198131\n",
      "Epoch 49, train loss: 0.178873\n",
      "Epoch 50, train loss: 0.210355\n",
      "Epoch 51, train loss: 0.221967\n",
      "Epoch 52, train loss: 0.214169\n",
      "Epoch 53, train loss: 0.192798\n",
      "Epoch 54, train loss: 0.227742\n",
      "Epoch 55, train loss: 0.204768\n",
      "Epoch 56, train loss: 0.174127\n",
      "Epoch 57, train loss: 0.193587\n",
      "Epoch 58, train loss: 0.210694\n",
      "Epoch 59, train loss: 0.265109\n",
      "Epoch 60, train loss: 0.160084\n",
      "Epoch 61, train loss: 0.183209\n",
      "Epoch 62, train loss: 0.187174\n",
      "Epoch 63, train loss: 0.180305\n",
      "Epoch 64, train loss: 0.217743\n",
      "Epoch 65, train loss: 0.291766\n",
      "Epoch 66, train loss: 0.236830\n",
      "Epoch 67, train loss: 0.228680\n",
      "Epoch 68, train loss: 0.234869\n",
      "Epoch 69, train loss: 0.214215\n",
      "Epoch 70, train loss: 0.259094\n",
      "Epoch 71, train loss: 0.180625\n",
      "Epoch 72, train loss: 0.196607\n",
      "Epoch 73, train loss: 0.182798\n",
      "Epoch 74, train loss: 0.272785\n",
      "Epoch 75, train loss: 0.262870\n",
      "Epoch 76, train loss: 0.178947\n",
      "Epoch 77, train loss: 0.263256\n",
      "Epoch 78, train loss: 0.190515\n",
      "Epoch 79, train loss: 0.412916\n",
      "Epoch 80, train loss: 0.177426\n",
      "Epoch 81, train loss: 0.221852\n",
      "Epoch 82, train loss: 0.189210\n",
      "Epoch 83, train loss: 0.291300\n",
      "Epoch 84, train loss: 0.256174\n",
      "Epoch 85, train loss: 0.262574\n",
      "Epoch 86, train loss: 0.178802\n",
      "Epoch 87, train loss: 0.296460\n",
      "Epoch 88, train loss: 0.204445\n",
      "Epoch 89, train loss: 0.174947\n",
      "Epoch 90, train loss: 0.202741\n",
      "Epoch 91, train loss: 0.225780\n",
      "Epoch 92, train loss: 0.239221\n",
      "Epoch 93, train loss: 0.194270\n",
      "Epoch 94, train loss: 0.233882\n",
      "Epoch 95, train loss: 0.190537\n",
      "Epoch 96, train loss: 0.232877\n",
      "Epoch 97, train loss: 0.189546\n",
      "Epoch 98, train loss: 0.240736\n",
      "Epoch 99, train loss: 0.278220\n",
      "Epoch 100, train loss: 0.205556\n",
      "Epoch 101, train loss: 0.139979\n",
      "Epoch 102, train loss: 0.180152\n",
      "Epoch 103, train loss: 0.165110\n",
      "Epoch 104, train loss: 0.184178\n",
      "Epoch 105, train loss: 0.182135\n",
      "Epoch 106, train loss: 0.200753\n",
      "Epoch 107, train loss: 0.186483\n",
      "Epoch 108, train loss: 0.187468\n",
      "Epoch 109, train loss: 0.221587\n",
      "Epoch 110, train loss: 0.210475\n",
      "Epoch 111, train loss: 0.232091\n",
      "Epoch 112, train loss: 0.174118\n",
      "Epoch 113, train loss: 0.245302\n",
      "Epoch 114, train loss: 0.255532\n",
      "Epoch 115, train loss: 0.163500\n",
      "Epoch 116, train loss: 0.222206\n",
      "Epoch 117, train loss: 0.221110\n",
      "Epoch 118, train loss: 0.250652\n",
      "Epoch 119, train loss: 0.165165\n",
      "Epoch 120, train loss: 0.187403\n",
      "Epoch 121, train loss: 0.248262\n",
      "Epoch 122, train loss: 0.163203\n",
      "Epoch 123, train loss: 0.182893\n",
      "Epoch 124, train loss: 0.144402\n",
      "Epoch 125, train loss: 0.215468\n",
      "Epoch 126, train loss: 0.241617\n",
      "Epoch 127, train loss: 0.212618\n",
      "Epoch 128, train loss: 0.168153\n",
      "Epoch 129, train loss: 0.211635\n",
      "Epoch 130, train loss: 0.227535\n",
      "Epoch 131, train loss: 0.194223\n",
      "Epoch 132, train loss: 0.192460\n",
      "Epoch 133, train loss: 0.217151\n",
      "Epoch 134, train loss: 0.233669\n",
      "Epoch 135, train loss: 0.212161\n",
      "Epoch 136, train loss: 0.222620\n",
      "Epoch 137, train loss: 0.221122\n",
      "Epoch 138, train loss: 0.191691\n",
      "Epoch 139, train loss: 0.190134\n",
      "Epoch 140, train loss: 0.182569\n",
      "Epoch 141, train loss: 0.327782\n",
      "Epoch 142, train loss: 0.145530\n",
      "Epoch 143, train loss: 0.171993\n",
      "Epoch 144, train loss: 0.220396\n",
      "Epoch 145, train loss: 0.217464\n",
      "Epoch 146, train loss: 0.189349\n",
      "Epoch 147, train loss: 0.208200\n",
      "Epoch 148, train loss: 0.220771\n",
      "Epoch 149, train loss: 0.213930\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[181. 180. 178. 178. 176. 173.]\n",
      " [180. 178. 178. 176. 173. 168.]\n",
      " [178. 178. 176. 173. 168. 163.]\n",
      " ...\n",
      " [293. 289. 287. 283. 276. 269.]\n",
      " [289. 287. 283. 276. 269. 261.]\n",
      " [287. 283. 276. 269. 261. 252.]]\n",
      "y here is\n",
      "[[154. 154. 154. 154. 154. 154.]\n",
      " [153. 153. 153. 153. 153. 153.]\n",
      " [151. 151. 151. 151. 151. 151.]\n",
      " ...\n",
      " [229. 229. 229. 229. 229. 229.]\n",
      " [224. 224. 224. 224. 224. 224.]\n",
      " [215. 215. 215. 215. 215. 215.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1891, 6), Train/test: 1/1890\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 170 segments\n",
      "Building dataset, requesting data from 0 to 170\n",
      "x here is\n",
      "[[ 95.  86.  81.  81.  82.  82.]\n",
      " [ 86.  81.  81.  82.  82.  84.]\n",
      " [ 81.  81.  82.  82.  84.  88.]\n",
      " ...\n",
      " [277. 261. 245. 232. 222. 221.]\n",
      " [261. 245. 232. 222. 221. 222.]\n",
      " [245. 232. 222. 221. 222. 223.]]\n",
      "y here is\n",
      "[[ 97.  97.  97.  97.  97.  97.]\n",
      " [ 97.  97.  97.  97.  97.  97.]\n",
      " [ 95.  95.  95.  95.  95.  95.]\n",
      " ...\n",
      " [225. 225. 225. 225. 225. 225.]\n",
      " [227. 227. 227. 227. 227. 227.]\n",
      " [226. 226. 226. 226. 226. 226.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 551/6669\n",
      "Found 170 continuous time series\n",
      "Data shape: (7222, 6), Train/test: 7220/2\n",
      "Train test ratio: 3610.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60CD871C0>\n",
      "Epoch 0, test loss: 0.162173\n",
      "Epoch 1, test loss: 0.162309\n",
      "Epoch 2, test loss: 0.162467\n",
      "Epoch 3, test loss: 0.164522\n",
      "Epoch 4, test loss: 0.162587\n",
      "Epoch 5, test loss: 0.162559\n",
      "Epoch 6, test loss: 0.162954\n",
      "Epoch 7, test loss: 0.162864\n",
      "Epoch 8, test loss: 0.164833\n",
      "Epoch 9, test loss: 0.163228\n",
      "Epoch 10, test loss: 0.163158\n",
      "Epoch 11, test loss: 0.163259\n",
      "Epoch 12, test loss: 0.163259\n",
      "Epoch 13, test loss: 0.165437\n",
      "Epoch 14, test loss: 0.163289\n",
      "Epoch 15, test loss: 0.163392\n",
      "Epoch 16, test loss: 0.163482\n",
      "Epoch 17, test loss: 0.163517\n",
      "Epoch 18, test loss: 0.164212\n",
      "Epoch 19, test loss: 0.165516\n",
      "Epoch 20, test loss: 0.164980\n",
      "Epoch 21, test loss: 0.163633\n",
      "Epoch 22, test loss: 0.165922\n",
      "Epoch 23, test loss: 0.167481\n",
      "Epoch 24, test loss: 0.166138\n",
      "Epoch 25, test loss: 0.163472\n",
      "Epoch 26, test loss: 0.163481\n",
      "Epoch 27, test loss: 0.165116\n",
      "Epoch 28, test loss: 0.167370\n",
      "Epoch 29, test loss: 0.164747\n",
      "Epoch 30, test loss: 0.163855\n",
      "Epoch 31, test loss: 0.164244\n",
      "Epoch 32, test loss: 0.168308\n",
      "Epoch 33, test loss: 0.163902\n",
      "Epoch 34, test loss: 0.163805\n",
      "Epoch 35, test loss: 0.164138\n",
      "Epoch 36, test loss: 0.170459\n",
      "Epoch 37, test loss: 0.163794\n",
      "Epoch 38, test loss: 0.165127\n",
      "Epoch 39, test loss: 0.164157\n",
      "Epoch 40, test loss: 0.164275\n",
      "Epoch 41, test loss: 0.164425\n",
      "Epoch 42, test loss: 0.164790\n",
      "Epoch 43, test loss: 0.164816\n",
      "Epoch 44, test loss: 0.165093\n",
      "Epoch 45, test loss: 0.164522\n",
      "Epoch 46, test loss: 0.164392\n",
      "Epoch 47, test loss: 0.164450\n",
      "Epoch 48, test loss: 0.164963\n",
      "Epoch 49, test loss: 0.165027\n",
      "Epoch 50, test loss: 0.164423\n",
      "Epoch 51, test loss: 0.165310\n",
      "Epoch 52, test loss: 0.166935\n",
      "Epoch 53, test loss: 0.166909\n",
      "Epoch 54, test loss: 0.164631\n",
      "Epoch 55, test loss: 0.165546\n",
      "Epoch 56, test loss: 0.164486\n",
      "Epoch 57, test loss: 0.165326\n",
      "Epoch 58, test loss: 0.165846\n",
      "Epoch 59, test loss: 0.164714\n",
      "Epoch 60, test loss: 0.167042\n",
      "Epoch 61, test loss: 0.164622\n",
      "Epoch 62, test loss: 0.164715\n",
      "Epoch 63, test loss: 0.164283\n",
      "Epoch 64, test loss: 0.170974\n",
      "Epoch 65, test loss: 0.165528\n",
      "Epoch 66, test loss: 0.164929\n",
      "Epoch 67, test loss: 0.164897\n",
      "Epoch 68, test loss: 0.166403\n",
      "Epoch 69, test loss: 0.170057\n",
      "Epoch 70, test loss: 0.169786\n",
      "Epoch 71, test loss: 0.164590\n",
      "Epoch 72, test loss: 0.164619\n",
      "Epoch 73, test loss: 0.165777\n",
      "Epoch 74, test loss: 0.164480\n",
      "Epoch 75, test loss: 0.169220\n",
      "Epoch 76, test loss: 0.164856\n",
      "Epoch 77, test loss: 0.165027\n",
      "Epoch 78, test loss: 0.164445\n",
      "Epoch 79, test loss: 0.164402\n",
      "Epoch 80, test loss: 0.164731\n",
      "Epoch 81, test loss: 0.164341\n",
      "Epoch 82, test loss: 0.166213\n",
      "Epoch 83, test loss: 0.164901\n",
      "Epoch 84, test loss: 0.164461\n",
      "Epoch 85, test loss: 0.166889\n",
      "Epoch 86, test loss: 0.164529\n",
      "Epoch 87, test loss: 0.165470\n",
      "Epoch 88, test loss: 0.167358\n",
      "Epoch 89, test loss: 0.164607\n",
      "Epoch 90, test loss: 0.178920\n",
      "Epoch 91, test loss: 0.165743\n",
      "Epoch 92, test loss: 0.164786\n",
      "Epoch 93, test loss: 0.167654\n",
      "Epoch 94, test loss: 0.164321\n",
      "Epoch 95, test loss: 0.165104\n",
      "Epoch 96, test loss: 0.166970\n",
      "Epoch 97, test loss: 0.165242\n",
      "Epoch 98, test loss: 0.164852\n",
      "Epoch 99, test loss: 0.164766\n",
      "Epoch 100, test loss: 0.171807\n",
      "Epoch 101, test loss: 0.164252\n",
      "Epoch 102, test loss: 0.167133\n",
      "Epoch 103, test loss: 0.164601\n",
      "Epoch 104, test loss: 0.165588\n",
      "Epoch 105, test loss: 0.165385\n",
      "Epoch 106, test loss: 0.166109\n",
      "Epoch 107, test loss: 0.165253\n",
      "Epoch 108, test loss: 0.165535\n",
      "Epoch 109, test loss: 0.165663\n",
      "Epoch 110, test loss: 0.167595\n",
      "Epoch 111, test loss: 0.166470\n",
      "Epoch 112, test loss: 0.164766\n",
      "Epoch 113, test loss: 0.164562\n",
      "Epoch 114, test loss: 0.164438\n",
      "Epoch 115, test loss: 0.165367\n",
      "Epoch 116, test loss: 0.165371\n",
      "Epoch 117, test loss: 0.165350\n",
      "Epoch 118, test loss: 0.167075\n",
      "Epoch 119, test loss: 0.165216\n",
      "Epoch 120, test loss: 0.164711\n",
      "Epoch 121, test loss: 0.164546\n",
      "Epoch 122, test loss: 0.166665\n",
      "Epoch 123, test loss: 0.165534\n",
      "Epoch 124, test loss: 0.164662\n",
      "Epoch 125, test loss: 0.166299\n",
      "Epoch 126, test loss: 0.164803\n",
      "Epoch 127, test loss: 0.166020\n",
      "Epoch 128, test loss: 0.164843\n",
      "Epoch 129, test loss: 0.167162\n",
      "Epoch 130, test loss: 0.164623\n",
      "Epoch 131, test loss: 0.166877\n",
      "Epoch 132, test loss: 0.165242\n",
      "Epoch 133, test loss: 0.164569\n",
      "Epoch 134, test loss: 0.166088\n",
      "Epoch 135, test loss: 0.165395\n",
      "Epoch 136, test loss: 0.164827\n",
      "Epoch 137, test loss: 0.165049\n",
      "Epoch 138, test loss: 0.164450\n",
      "Epoch 139, test loss: 0.165724\n",
      "Epoch 140, test loss: 0.166551\n",
      "Epoch 141, test loss: 0.164837\n",
      "Epoch 142, test loss: 0.164578\n",
      "Epoch 143, test loss: 0.165407\n",
      "Epoch 144, test loss: 0.165787\n",
      "Epoch 145, test loss: 0.164789\n",
      "Epoch 146, test loss: 0.165079\n",
      "Epoch 147, test loss: 0.165964\n",
      "Epoch 148, test loss: 0.166203\n",
      "Epoch 149, test loss: 0.164742\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60CD871C0>\n",
      "Epoch 0, test loss: 0.162541\n",
      "Epoch 1, test loss: 0.162558\n",
      "Epoch 2, test loss: 0.164028\n",
      "Epoch 3, test loss: 0.162332\n",
      "Epoch 4, test loss: 0.162619\n",
      "Epoch 5, test loss: 0.162794\n",
      "Epoch 6, test loss: 0.162621\n",
      "Epoch 7, test loss: 0.162868\n",
      "Epoch 8, test loss: 0.164704\n",
      "Epoch 9, test loss: 0.162707\n",
      "Epoch 10, test loss: 0.162532\n",
      "Epoch 11, test loss: 0.162825\n",
      "Epoch 12, test loss: 0.163120\n",
      "Epoch 13, test loss: 0.162617\n",
      "Epoch 14, test loss: 0.162908\n",
      "Epoch 15, test loss: 0.164027\n",
      "Epoch 16, test loss: 0.162854\n",
      "Epoch 17, test loss: 0.162728\n",
      "Epoch 18, test loss: 0.168386\n",
      "Epoch 19, test loss: 0.163146\n",
      "Epoch 20, test loss: 0.167059\n",
      "Epoch 21, test loss: 0.162997\n",
      "Epoch 22, test loss: 0.162874\n",
      "Epoch 23, test loss: 0.163133\n",
      "Epoch 24, test loss: 0.162927\n",
      "Epoch 25, test loss: 0.162836\n",
      "Epoch 26, test loss: 0.162830\n",
      "Epoch 27, test loss: 0.163196\n",
      "Epoch 28, test loss: 0.162709\n",
      "Epoch 29, test loss: 0.163352\n",
      "Epoch 30, test loss: 0.164051\n",
      "Epoch 31, test loss: 0.163136\n",
      "Epoch 32, test loss: 0.163609\n",
      "Epoch 33, test loss: 0.163793\n",
      "Epoch 34, test loss: 0.163397\n",
      "Epoch 35, test loss: 0.163870\n",
      "Epoch 36, test loss: 0.164408\n",
      "Epoch 37, test loss: 0.165421\n",
      "Epoch 38, test loss: 0.164193\n",
      "Epoch 39, test loss: 0.165996\n",
      "Epoch 40, test loss: 0.163680\n",
      "Epoch 41, test loss: 0.163527\n",
      "Epoch 42, test loss: 0.164121\n",
      "Epoch 43, test loss: 0.164382\n",
      "Epoch 44, test loss: 0.164854\n",
      "Epoch 45, test loss: 0.163775\n",
      "Epoch 46, test loss: 0.164363\n",
      "Epoch 47, test loss: 0.163973\n",
      "Epoch 48, test loss: 0.163673\n",
      "Epoch 49, test loss: 0.163991\n",
      "Epoch 50, test loss: 0.164736\n",
      "Epoch 51, test loss: 0.166710\n",
      "Epoch 52, test loss: 0.167137\n",
      "Epoch 53, test loss: 0.164521\n",
      "Epoch 54, test loss: 0.164318\n",
      "Epoch 55, test loss: 0.164083\n",
      "Epoch 56, test loss: 0.164281\n",
      "Epoch 57, test loss: 0.165615\n",
      "Epoch 58, test loss: 0.171279\n",
      "Epoch 59, test loss: 0.164285\n",
      "Epoch 60, test loss: 0.165150\n",
      "Epoch 61, test loss: 0.164526\n",
      "Epoch 62, test loss: 0.165236\n",
      "Epoch 63, test loss: 0.164498\n",
      "Epoch 64, test loss: 0.165834\n",
      "Epoch 65, test loss: 0.164631\n",
      "Epoch 66, test loss: 0.164481\n",
      "Epoch 67, test loss: 0.164918\n",
      "Epoch 68, test loss: 0.165561\n",
      "Epoch 69, test loss: 0.165510\n",
      "Epoch 70, test loss: 0.165054\n",
      "Epoch 71, test loss: 0.164837\n",
      "Epoch 72, test loss: 0.164688\n",
      "Epoch 73, test loss: 0.164728\n",
      "Epoch 74, test loss: 0.164490\n",
      "Epoch 75, test loss: 0.164641\n",
      "Epoch 76, test loss: 0.167038\n",
      "Epoch 77, test loss: 0.165039\n",
      "Epoch 78, test loss: 0.169212\n",
      "Epoch 79, test loss: 0.164491\n",
      "Epoch 80, test loss: 0.165630\n",
      "Epoch 81, test loss: 0.164826\n",
      "Epoch 82, test loss: 0.165124\n",
      "Epoch 83, test loss: 0.167638\n",
      "Epoch 84, test loss: 0.164780\n",
      "Epoch 85, test loss: 0.165342\n",
      "Epoch 86, test loss: 0.165008\n",
      "Epoch 87, test loss: 0.164985\n",
      "Epoch 88, test loss: 0.165140\n",
      "Epoch 89, test loss: 0.165045\n",
      "Epoch 90, test loss: 0.164768\n",
      "Epoch 91, test loss: 0.165977\n",
      "Epoch 92, test loss: 0.164543\n",
      "Epoch 93, test loss: 0.165493\n",
      "Epoch 94, test loss: 0.164936\n",
      "Epoch 95, test loss: 0.164382\n",
      "Epoch 96, test loss: 0.165462\n",
      "Epoch 97, test loss: 0.169231\n",
      "Epoch 98, test loss: 0.164442\n",
      "Epoch 99, test loss: 0.164742\n",
      "Epoch 100, test loss: 0.164890\n",
      "Epoch 101, test loss: 0.165015\n",
      "Epoch 102, test loss: 0.164736\n",
      "Epoch 103, test loss: 0.164623\n",
      "Epoch 104, test loss: 0.165010\n",
      "Epoch 105, test loss: 0.165322\n",
      "Epoch 106, test loss: 0.164838\n",
      "Epoch 107, test loss: 0.165349\n",
      "Epoch 108, test loss: 0.164454\n",
      "Epoch 109, test loss: 0.164842\n",
      "Epoch 110, test loss: 0.166244\n",
      "Epoch 111, test loss: 0.165361\n",
      "Epoch 112, test loss: 0.165420\n",
      "Epoch 113, test loss: 0.164952\n",
      "Epoch 114, test loss: 0.165071\n",
      "Epoch 115, test loss: 0.166041\n",
      "Epoch 116, test loss: 0.165046\n",
      "Epoch 117, test loss: 0.165515\n",
      "Epoch 118, test loss: 0.165812\n",
      "Epoch 119, test loss: 0.165442\n",
      "Epoch 120, test loss: 0.164894\n",
      "Epoch 121, test loss: 0.165494\n",
      "Epoch 122, test loss: 0.165209\n",
      "Epoch 123, test loss: 0.165418\n",
      "Epoch 124, test loss: 0.167979\n",
      "Epoch 125, test loss: 0.165595\n",
      "Epoch 126, test loss: 0.165262\n",
      "Epoch 127, test loss: 0.164836\n",
      "Epoch 128, test loss: 0.165178\n",
      "Epoch 129, test loss: 0.164980\n",
      "Epoch 130, test loss: 0.165469\n",
      "Epoch 131, test loss: 0.164749\n",
      "Epoch 132, test loss: 0.165092\n",
      "Epoch 133, test loss: 0.165044\n",
      "Epoch 134, test loss: 0.166560\n",
      "Epoch 135, test loss: 0.165420\n",
      "Epoch 136, test loss: 0.165254\n",
      "Epoch 137, test loss: 0.165873\n",
      "Epoch 138, test loss: 0.166022\n",
      "Epoch 139, test loss: 0.165230\n",
      "Epoch 140, test loss: 0.166090\n",
      "Epoch 141, test loss: 0.167186\n",
      "Epoch 142, test loss: 0.166615\n",
      "Epoch 143, test loss: 0.165956\n",
      "Epoch 144, test loss: 0.165609\n",
      "Epoch 145, test loss: 0.164855\n",
      "Epoch 146, test loss: 0.165695\n",
      "Epoch 147, test loss: 0.164431\n",
      "Epoch 148, test loss: 0.165097\n",
      "Epoch 149, test loss: 0.165263\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60CD871C0>\n",
      "Epoch 0, test loss: 0.702072\n",
      "Epoch 1, test loss: 0.198383\n",
      "Epoch 2, test loss: 0.179246\n",
      "Epoch 3, test loss: 0.178326\n",
      "Epoch 4, test loss: 0.174638\n",
      "Epoch 5, test loss: 0.173035\n",
      "Epoch 6, test loss: 0.171581\n",
      "Epoch 7, test loss: 0.170764\n",
      "Epoch 8, test loss: 0.170449\n",
      "Epoch 9, test loss: 0.170112\n",
      "Epoch 10, test loss: 0.168869\n",
      "Epoch 11, test loss: 0.169600\n",
      "Epoch 12, test loss: 0.170993\n",
      "Epoch 13, test loss: 0.168419\n",
      "Epoch 14, test loss: 0.170293\n",
      "Epoch 15, test loss: 0.168327\n",
      "Epoch 16, test loss: 0.170913\n",
      "Epoch 17, test loss: 0.168909\n",
      "Epoch 18, test loss: 0.168223\n",
      "Epoch 19, test loss: 0.169664\n",
      "Epoch 20, test loss: 0.167893\n",
      "Epoch 21, test loss: 0.168146\n",
      "Epoch 22, test loss: 0.168090\n",
      "Epoch 23, test loss: 0.168055\n",
      "Epoch 24, test loss: 0.168723\n",
      "Epoch 25, test loss: 0.168464\n",
      "Epoch 26, test loss: 0.168411\n",
      "Epoch 27, test loss: 0.170193\n",
      "Epoch 28, test loss: 0.168341\n",
      "Epoch 29, test loss: 0.171964\n",
      "Epoch 30, test loss: 0.167962\n",
      "Epoch 31, test loss: 0.168336\n",
      "Epoch 32, test loss: 0.168395\n",
      "Epoch 33, test loss: 0.168553\n",
      "Epoch 34, test loss: 0.169095\n",
      "Epoch 35, test loss: 0.168663\n",
      "Epoch 36, test loss: 0.169416\n",
      "Epoch 37, test loss: 0.170723\n",
      "Epoch 38, test loss: 0.168661\n",
      "Epoch 39, test loss: 0.169139\n",
      "Epoch 40, test loss: 0.168894\n",
      "Epoch 41, test loss: 0.168901\n",
      "Epoch 42, test loss: 0.168579\n",
      "Epoch 43, test loss: 0.169040\n",
      "Epoch 44, test loss: 0.169650\n",
      "Epoch 45, test loss: 0.169535\n",
      "Epoch 46, test loss: 0.168999\n",
      "Epoch 47, test loss: 0.169305\n",
      "Epoch 48, test loss: 0.169365\n",
      "Epoch 49, test loss: 0.168599\n",
      "Epoch 50, test loss: 0.168756\n",
      "Epoch 51, test loss: 0.170706\n",
      "Epoch 52, test loss: 0.169149\n",
      "Epoch 53, test loss: 0.169929\n",
      "Epoch 54, test loss: 0.169126\n",
      "Epoch 55, test loss: 0.170830\n",
      "Epoch 56, test loss: 0.169216\n",
      "Epoch 57, test loss: 0.168187\n",
      "Epoch 58, test loss: 0.169472\n",
      "Epoch 59, test loss: 0.168624\n",
      "Epoch 60, test loss: 0.168689\n",
      "Epoch 61, test loss: 0.168872\n",
      "Epoch 62, test loss: 0.170503\n",
      "Epoch 63, test loss: 0.168593\n",
      "Epoch 64, test loss: 0.170048\n",
      "Epoch 65, test loss: 0.170089\n",
      "Epoch 66, test loss: 0.169624\n",
      "Epoch 67, test loss: 0.169469\n",
      "Epoch 68, test loss: 0.168574\n",
      "Epoch 69, test loss: 0.169262\n",
      "Epoch 70, test loss: 0.169501\n",
      "Epoch 71, test loss: 0.168539\n",
      "Epoch 72, test loss: 0.169891\n",
      "Epoch 73, test loss: 0.169176\n",
      "Epoch 74, test loss: 0.176083\n",
      "Epoch 75, test loss: 0.169328\n",
      "Epoch 76, test loss: 0.168486\n",
      "Epoch 77, test loss: 0.168576\n",
      "Epoch 78, test loss: 0.168222\n",
      "Epoch 79, test loss: 0.170629\n",
      "Epoch 80, test loss: 0.168338\n",
      "Epoch 81, test loss: 0.168151\n",
      "Epoch 82, test loss: 0.168441\n",
      "Epoch 83, test loss: 0.169984\n",
      "Epoch 84, test loss: 0.168559\n",
      "Epoch 85, test loss: 0.169002\n",
      "Epoch 86, test loss: 0.169412\n",
      "Epoch 87, test loss: 0.167840\n",
      "Epoch 88, test loss: 0.167856\n",
      "Epoch 89, test loss: 0.168054\n",
      "Epoch 90, test loss: 0.169661\n",
      "Epoch 91, test loss: 0.167876\n",
      "Epoch 92, test loss: 0.167854\n",
      "Epoch 93, test loss: 0.167742\n",
      "Epoch 94, test loss: 0.167701\n",
      "Epoch 95, test loss: 0.169802\n",
      "Epoch 96, test loss: 0.167745\n",
      "Epoch 97, test loss: 0.170509\n",
      "Epoch 98, test loss: 0.168255\n",
      "Epoch 99, test loss: 0.167412\n",
      "Epoch 100, test loss: 0.167947\n",
      "Epoch 101, test loss: 0.169980\n",
      "Epoch 102, test loss: 0.167367\n",
      "Epoch 103, test loss: 0.168014\n",
      "Epoch 104, test loss: 0.168787\n",
      "Epoch 105, test loss: 0.167223\n",
      "Epoch 106, test loss: 0.167286\n",
      "Epoch 107, test loss: 0.167434\n",
      "Epoch 108, test loss: 0.168300\n",
      "Epoch 109, test loss: 0.167636\n",
      "Epoch 110, test loss: 0.167984\n",
      "Epoch 111, test loss: 0.169510\n",
      "Epoch 112, test loss: 0.168705\n",
      "Epoch 113, test loss: 0.168195\n",
      "Epoch 114, test loss: 0.168232\n",
      "Epoch 115, test loss: 0.168377\n",
      "Epoch 116, test loss: 0.169858\n",
      "Epoch 117, test loss: 0.167182\n",
      "Epoch 118, test loss: 0.167218\n",
      "Epoch 119, test loss: 0.169655\n",
      "Epoch 120, test loss: 0.167590\n",
      "Epoch 121, test loss: 0.166935\n",
      "Epoch 122, test loss: 0.168087\n",
      "Epoch 123, test loss: 0.167625\n",
      "Epoch 124, test loss: 0.167873\n",
      "Epoch 125, test loss: 0.167871\n",
      "Epoch 126, test loss: 0.169189\n",
      "Epoch 127, test loss: 0.167359\n",
      "Epoch 128, test loss: 0.167598\n",
      "Epoch 129, test loss: 0.172406\n",
      "Epoch 130, test loss: 0.167920\n",
      "Epoch 131, test loss: 0.166901\n",
      "Epoch 132, test loss: 0.167346\n",
      "Epoch 133, test loss: 0.167893\n",
      "Epoch 134, test loss: 0.166915\n",
      "Epoch 135, test loss: 0.168106\n",
      "Epoch 136, test loss: 0.167265\n",
      "Epoch 137, test loss: 0.168068\n",
      "Epoch 138, test loss: 0.167255\n",
      "Epoch 139, test loss: 0.167277\n",
      "Epoch 140, test loss: 0.167222\n",
      "Epoch 141, test loss: 0.167127\n",
      "Epoch 142, test loss: 0.166990\n",
      "Epoch 143, test loss: 0.167558\n",
      "Epoch 144, test loss: 0.167650\n",
      "Epoch 145, test loss: 0.167179\n",
      "Epoch 146, test loss: 0.171612\n",
      "Epoch 147, test loss: 0.168748\n",
      "Epoch 148, test loss: 0.167285\n",
      "Epoch 149, test loss: 0.167148\n",
      "Epoch 150, test loss: 0.167275\n",
      "Epoch 151, test loss: 0.166675\n",
      "Epoch 152, test loss: 0.166696\n",
      "Epoch 153, test loss: 0.170087\n",
      "Epoch 154, test loss: 0.167836\n",
      "Epoch 155, test loss: 0.167224\n",
      "Epoch 156, test loss: 0.167200\n",
      "Epoch 157, test loss: 0.169653\n",
      "Epoch 158, test loss: 0.167814\n",
      "Epoch 159, test loss: 0.169517\n",
      "Epoch 160, test loss: 0.166860\n",
      "Epoch 161, test loss: 0.167346\n",
      "Epoch 162, test loss: 0.168779\n",
      "Epoch 163, test loss: 0.166684\n",
      "Epoch 164, test loss: 0.167206\n",
      "Epoch 165, test loss: 0.169704\n",
      "Epoch 166, test loss: 0.167629\n",
      "Epoch 167, test loss: 0.166964\n",
      "Epoch 168, test loss: 0.167205\n",
      "Epoch 169, test loss: 0.167006\n",
      "Epoch 170, test loss: 0.168149\n",
      "Epoch 171, test loss: 0.169408\n",
      "Epoch 172, test loss: 0.168179\n",
      "Epoch 173, test loss: 0.167204\n",
      "Epoch 174, test loss: 0.168194\n",
      "Epoch 175, test loss: 0.167215\n",
      "Epoch 176, test loss: 0.167582\n",
      "Epoch 177, test loss: 0.167333\n",
      "Epoch 178, test loss: 0.174056\n",
      "Epoch 179, test loss: 0.166736\n",
      "Epoch 180, test loss: 0.166887\n",
      "Epoch 181, test loss: 0.167224\n",
      "Epoch 182, test loss: 0.166816\n",
      "Epoch 183, test loss: 0.167345\n",
      "Epoch 184, test loss: 0.169468\n",
      "Epoch 185, test loss: 0.167589\n",
      "Epoch 186, test loss: 0.168055\n",
      "Epoch 187, test loss: 0.166569\n",
      "Epoch 188, test loss: 0.168500\n",
      "Epoch 189, test loss: 0.167315\n",
      "Epoch 190, test loss: 0.167213\n",
      "Epoch 191, test loss: 0.166724\n",
      "Epoch 192, test loss: 0.166839\n",
      "Epoch 193, test loss: 0.168849\n",
      "Epoch 194, test loss: 0.169783\n",
      "Epoch 195, test loss: 0.167318\n",
      "Epoch 196, test loss: 0.167090\n",
      "Epoch 197, test loss: 0.168438\n",
      "Epoch 198, test loss: 0.168046\n",
      "Epoch 199, test loss: 0.174375\n",
      "Epoch 200, test loss: 0.167930\n",
      "Epoch 201, test loss: 0.168064\n",
      "Epoch 202, test loss: 0.166924\n",
      "Epoch 203, test loss: 0.167743\n",
      "Epoch 204, test loss: 0.167188\n",
      "Epoch 205, test loss: 0.167547\n",
      "Epoch 206, test loss: 0.166687\n",
      "Epoch 207, test loss: 0.166379\n",
      "Epoch 208, test loss: 0.166824\n",
      "Epoch 209, test loss: 0.167364\n",
      "Epoch 210, test loss: 0.167628\n",
      "Epoch 211, test loss: 0.166541\n",
      "Epoch 212, test loss: 0.168413\n",
      "Epoch 213, test loss: 0.167209\n",
      "Epoch 214, test loss: 0.167201\n",
      "Epoch 215, test loss: 0.166879\n",
      "Epoch 216, test loss: 0.167626\n",
      "Epoch 217, test loss: 0.167323\n",
      "Epoch 218, test loss: 0.167894\n",
      "Epoch 219, test loss: 0.166794\n",
      "Epoch 220, test loss: 0.166637\n",
      "Epoch 221, test loss: 0.168316\n",
      "Epoch 222, test loss: 0.166827\n",
      "Epoch 223, test loss: 0.169455\n",
      "Epoch 224, test loss: 0.169871\n",
      "Epoch 225, test loss: 0.166979\n",
      "Epoch 226, test loss: 0.168121\n",
      "Epoch 227, test loss: 0.166607\n",
      "Epoch 228, test loss: 0.166387\n",
      "Epoch 229, test loss: 0.167615\n",
      "Epoch 230, test loss: 0.167048\n",
      "Epoch 231, test loss: 0.168873\n",
      "Epoch 232, test loss: 0.168304\n",
      "Epoch 233, test loss: 0.167189\n",
      "Epoch 234, test loss: 0.168295\n",
      "Epoch 235, test loss: 0.168354\n",
      "Epoch 236, test loss: 0.166952\n",
      "Epoch 237, test loss: 0.167180\n",
      "Epoch 238, test loss: 0.167428\n",
      "Epoch 239, test loss: 0.166567\n",
      "Epoch 240, test loss: 0.168417\n",
      "Epoch 241, test loss: 0.171329\n",
      "Epoch 242, test loss: 0.167645\n",
      "Epoch 243, test loss: 0.169503\n",
      "Epoch 244, test loss: 0.169214\n",
      "Epoch 245, test loss: 0.167261\n",
      "Epoch 246, test loss: 0.167970\n",
      "Epoch 247, test loss: 0.166701\n",
      "Epoch 248, test loss: 0.169453\n",
      "Epoch 249, test loss: 0.167349\n",
      "Epoch 250, test loss: 0.166493\n",
      "Epoch 251, test loss: 0.172821\n",
      "Epoch 252, test loss: 0.167508\n",
      "Epoch 253, test loss: 0.169784\n",
      "Epoch 254, test loss: 0.168372\n",
      "Epoch 255, test loss: 0.166357\n",
      "Epoch 256, test loss: 0.167714\n",
      "Epoch 257, test loss: 0.166678\n",
      "Epoch 258, test loss: 0.166520\n",
      "Epoch 259, test loss: 0.166953\n",
      "Epoch 260, test loss: 0.166850\n",
      "Epoch 261, test loss: 0.167192\n",
      "Epoch 262, test loss: 0.167894\n",
      "Epoch 263, test loss: 0.166864\n",
      "Epoch 264, test loss: 0.169290\n",
      "Epoch 265, test loss: 0.166549\n",
      "Epoch 266, test loss: 0.167677\n",
      "Epoch 267, test loss: 0.166773\n",
      "Epoch 268, test loss: 0.166869\n",
      "Epoch 269, test loss: 0.166609\n",
      "Epoch 270, test loss: 0.166845\n",
      "Epoch 271, test loss: 0.166565\n",
      "Epoch 272, test loss: 0.166806\n",
      "Epoch 273, test loss: 0.166587\n",
      "Epoch 274, test loss: 0.167321\n",
      "Epoch 275, test loss: 0.168901\n",
      "Epoch 276, test loss: 0.166597\n",
      "Epoch 277, test loss: 0.166586\n",
      "Epoch 278, test loss: 0.167484\n",
      "Epoch 279, test loss: 0.166702\n",
      "Epoch 280, test loss: 0.167048\n",
      "Epoch 281, test loss: 0.166817\n",
      "Epoch 282, test loss: 0.166364\n",
      "Epoch 283, test loss: 0.166351\n",
      "Epoch 284, test loss: 0.167730\n",
      "Epoch 285, test loss: 0.167535\n",
      "Epoch 286, test loss: 0.166574\n",
      "Epoch 287, test loss: 0.166932\n",
      "Epoch 288, test loss: 0.168413\n",
      "Epoch 289, test loss: 0.166801\n",
      "Epoch 290, test loss: 0.167183\n",
      "Epoch 291, test loss: 0.169785\n",
      "Epoch 292, test loss: 0.170300\n",
      "Epoch 293, test loss: 0.171156\n",
      "Epoch 294, test loss: 0.168190\n",
      "Epoch 295, test loss: 0.166780\n",
      "Epoch 296, test loss: 0.166933\n",
      "Epoch 297, test loss: 0.167921\n",
      "Epoch 298, test loss: 0.167796\n",
      "Epoch 299, test loss: 0.168207\n",
      "Epoch 300, test loss: 0.167804\n",
      "Epoch 301, test loss: 0.166924\n",
      "Epoch 302, test loss: 0.171977\n",
      "Epoch 303, test loss: 0.166755\n",
      "Epoch 304, test loss: 0.168291\n",
      "Epoch 305, test loss: 0.169895\n",
      "Epoch 306, test loss: 0.166695\n",
      "Epoch 307, test loss: 0.166712\n",
      "Epoch 308, test loss: 0.166768\n",
      "Epoch 309, test loss: 0.166859\n",
      "Epoch 310, test loss: 0.169113\n",
      "Epoch 311, test loss: 0.167939\n",
      "Epoch 312, test loss: 0.167507\n",
      "Epoch 313, test loss: 0.168495\n",
      "Epoch 314, test loss: 0.166548\n",
      "Epoch 315, test loss: 0.167720\n",
      "Epoch 316, test loss: 0.166460\n",
      "Epoch 317, test loss: 0.169376\n",
      "Epoch 318, test loss: 0.166643\n",
      "Epoch 319, test loss: 0.167224\n",
      "Epoch 320, test loss: 0.169583\n",
      "Epoch 321, test loss: 0.166610\n",
      "Epoch 322, test loss: 0.166613\n",
      "Epoch 323, test loss: 0.167398\n",
      "Epoch 324, test loss: 0.166688\n",
      "Epoch 325, test loss: 0.167247\n",
      "Epoch 326, test loss: 0.167711\n",
      "Epoch 327, test loss: 0.167871\n",
      "Epoch 328, test loss: 0.167499\n",
      "Epoch 329, test loss: 0.166621\n",
      "Epoch 330, test loss: 0.168031\n",
      "Epoch 331, test loss: 0.166779\n",
      "Epoch 332, test loss: 0.167655\n",
      "Epoch 333, test loss: 0.167950\n",
      "Epoch 334, test loss: 0.169102\n",
      "Epoch 335, test loss: 0.167144\n",
      "Epoch 336, test loss: 0.166058\n",
      "Epoch 337, test loss: 0.167484\n",
      "Epoch 338, test loss: 0.168263\n",
      "Epoch 339, test loss: 0.168344\n",
      "Epoch 340, test loss: 0.166956\n",
      "Epoch 341, test loss: 0.168057\n",
      "Epoch 342, test loss: 0.167648\n",
      "Epoch 343, test loss: 0.167325\n",
      "Epoch 344, test loss: 0.168988\n",
      "Epoch 345, test loss: 0.167916\n",
      "Epoch 346, test loss: 0.167013\n",
      "Epoch 347, test loss: 0.166722\n",
      "Epoch 348, test loss: 0.169580\n",
      "Epoch 349, test loss: 0.168351\n",
      "Epoch 350, test loss: 0.166860\n",
      "Epoch 351, test loss: 0.166853\n",
      "Epoch 352, test loss: 0.168397\n",
      "Epoch 353, test loss: 0.166801\n",
      "Epoch 354, test loss: 0.166695\n",
      "Epoch 355, test loss: 0.167512\n",
      "Epoch 356, test loss: 0.166756\n",
      "Epoch 357, test loss: 0.167424\n",
      "Epoch 358, test loss: 0.168538\n",
      "Epoch 359, test loss: 0.167928\n",
      "Epoch 360, test loss: 0.168008\n",
      "Epoch 361, test loss: 0.168104\n",
      "Epoch 362, test loss: 0.166706\n",
      "Epoch 363, test loss: 0.167978\n",
      "Epoch 364, test loss: 0.168421\n",
      "Epoch 365, test loss: 0.166625\n",
      "Epoch 366, test loss: 0.166442\n",
      "Epoch 367, test loss: 0.166906\n",
      "Epoch 368, test loss: 0.168179\n",
      "Epoch 369, test loss: 0.166729\n",
      "Epoch 370, test loss: 0.167005\n",
      "Epoch 371, test loss: 0.168589\n",
      "Epoch 372, test loss: 0.166479\n",
      "Epoch 373, test loss: 0.166551\n",
      "Epoch 374, test loss: 0.168194\n",
      "Epoch 375, test loss: 0.167718\n",
      "Epoch 376, test loss: 0.166963\n",
      "Epoch 377, test loss: 0.166792\n",
      "Epoch 378, test loss: 0.167031\n",
      "Epoch 379, test loss: 0.167463\n",
      "Epoch 380, test loss: 0.166733\n",
      "Epoch 381, test loss: 0.166425\n",
      "Epoch 382, test loss: 0.168933\n",
      "Epoch 383, test loss: 0.166742\n",
      "Epoch 384, test loss: 0.166706\n",
      "Epoch 385, test loss: 0.166742\n",
      "Epoch 386, test loss: 0.167251\n",
      "Epoch 387, test loss: 0.166526\n",
      "Epoch 388, test loss: 0.168545\n",
      "Epoch 389, test loss: 0.166843\n",
      "Epoch 390, test loss: 0.168046\n",
      "Epoch 391, test loss: 0.166968\n",
      "Epoch 392, test loss: 0.170408\n",
      "Epoch 393, test loss: 0.167895\n",
      "Epoch 394, test loss: 0.167470\n",
      "Epoch 395, test loss: 0.167984\n",
      "Epoch 396, test loss: 0.166639\n",
      "Epoch 397, test loss: 0.167893\n",
      "Epoch 398, test loss: 0.171189\n",
      "Epoch 399, test loss: 0.167202\n",
      "Epoch 400, test loss: 0.167704\n",
      "Epoch 401, test loss: 0.168398\n",
      "Epoch 402, test loss: 0.166548\n",
      "Epoch 403, test loss: 0.167606\n",
      "Epoch 404, test loss: 0.166629\n",
      "Epoch 405, test loss: 0.168814\n",
      "Epoch 406, test loss: 0.167731\n",
      "Epoch 407, test loss: 0.167819\n",
      "Epoch 408, test loss: 0.167379\n",
      "Epoch 409, test loss: 0.167299\n",
      "Epoch 410, test loss: 0.166744\n",
      "Epoch 411, test loss: 0.169492\n",
      "Epoch 412, test loss: 0.167096\n",
      "Epoch 413, test loss: 0.166884\n",
      "Epoch 414, test loss: 0.166959\n",
      "Epoch 415, test loss: 0.166723\n",
      "Epoch 416, test loss: 0.167451\n",
      "Epoch 417, test loss: 0.167108\n",
      "Epoch 418, test loss: 0.166978\n",
      "Epoch 419, test loss: 0.169409\n",
      "Epoch 420, test loss: 0.167764\n",
      "Epoch 421, test loss: 0.168218\n",
      "Epoch 422, test loss: 0.167168\n",
      "Epoch 423, test loss: 0.166645\n",
      "Epoch 424, test loss: 0.170188\n",
      "Epoch 425, test loss: 0.167376\n",
      "Epoch 426, test loss: 0.166485\n",
      "Epoch 427, test loss: 0.168275\n",
      "Epoch 428, test loss: 0.169231\n",
      "Epoch 429, test loss: 0.168575\n",
      "Epoch 430, test loss: 0.167084\n",
      "Epoch 431, test loss: 0.167377\n",
      "Epoch 432, test loss: 0.167019\n",
      "Epoch 433, test loss: 0.168187\n",
      "Epoch 434, test loss: 0.170298\n",
      "Epoch 435, test loss: 0.167143\n",
      "Epoch 436, test loss: 0.167483\n",
      "Epoch 437, test loss: 0.166843\n",
      "Epoch 438, test loss: 0.168064\n",
      "Epoch 439, test loss: 0.167175\n",
      "Epoch 440, test loss: 0.169036\n",
      "Epoch 441, test loss: 0.170571\n",
      "Epoch 442, test loss: 0.168025\n",
      "Epoch 443, test loss: 0.167470\n",
      "Epoch 444, test loss: 0.169631\n",
      "Epoch 445, test loss: 0.166726\n",
      "Epoch 446, test loss: 0.167118\n",
      "Epoch 447, test loss: 0.176458\n",
      "Epoch 448, test loss: 0.167555\n",
      "Epoch 449, test loss: 0.169294\n",
      "Pretrain data: 19669610.0\n",
      "Building dataset, requesting data from 0 to 819\n",
      "x here is\n",
      "[[ 95.  86.  81.  81.  82.  82.]\n",
      " [ 86.  81.  81.  82.  82.  84.]\n",
      " [ 81.  81.  82.  82.  84.  88.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[ 97.  97.  97.  97.  97.  97.]\n",
      " [ 97.  97.  97.  97.  97.  97.]\n",
      " [ 95.  95.  95.  95.  95.  95.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7835/107513\n",
      "Found 819 continuous time series\n",
      "Data shape: (115350, 6), Train/test: 115348/2\n",
      "Train test ratio: 57674.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1802\n",
      "Epoch 0, train loss: 0.266040\n",
      "Epoch 1, train loss: 0.177128\n",
      "Epoch 2, train loss: 0.237975\n",
      "Epoch 3, train loss: 0.194583\n",
      "Epoch 4, train loss: 0.256291\n",
      "Epoch 5, train loss: 0.201802\n",
      "Epoch 6, train loss: 0.187376\n",
      "Epoch 7, train loss: 0.220447\n",
      "Epoch 8, train loss: 0.208713\n",
      "Epoch 9, train loss: 0.194573\n",
      "Epoch 10, train loss: 0.144373\n",
      "Epoch 11, train loss: 0.193749\n",
      "Epoch 12, train loss: 0.194125\n",
      "Epoch 13, train loss: 0.285649\n",
      "Epoch 14, train loss: 0.218214\n",
      "Epoch 15, train loss: 0.251533\n",
      "Epoch 16, train loss: 0.233837\n",
      "Epoch 17, train loss: 0.175189\n",
      "Epoch 18, train loss: 0.264264\n",
      "Epoch 19, train loss: 0.179571\n",
      "Epoch 20, train loss: 0.192771\n",
      "Epoch 21, train loss: 0.206216\n",
      "Epoch 22, train loss: 0.195132\n",
      "Epoch 23, train loss: 0.207287\n",
      "Epoch 24, train loss: 0.240202\n",
      "Epoch 25, train loss: 0.219453\n",
      "Epoch 26, train loss: 0.183971\n",
      "Epoch 27, train loss: 0.186080\n",
      "Epoch 28, train loss: 0.217736\n",
      "Epoch 29, train loss: 0.213097\n",
      "Epoch 30, train loss: 0.226083\n",
      "Epoch 31, train loss: 0.255062\n",
      "Epoch 32, train loss: 0.160809\n",
      "Epoch 33, train loss: 0.139140\n",
      "Epoch 34, train loss: 0.182104\n",
      "Epoch 35, train loss: 0.186864\n",
      "Epoch 36, train loss: 0.214790\n",
      "Epoch 37, train loss: 0.137351\n",
      "Epoch 38, train loss: 0.148931\n",
      "Epoch 39, train loss: 0.182289\n",
      "Epoch 40, train loss: 0.201999\n",
      "Epoch 41, train loss: 0.165191\n",
      "Epoch 42, train loss: 0.186995\n",
      "Epoch 43, train loss: 0.172559\n",
      "Epoch 44, train loss: 0.261147\n",
      "Epoch 45, train loss: 0.188351\n",
      "Epoch 46, train loss: 0.167633\n",
      "Epoch 47, train loss: 0.192330\n",
      "Epoch 48, train loss: 0.261019\n",
      "Epoch 49, train loss: 0.190117\n",
      "Epoch 50, train loss: 0.213260\n",
      "Epoch 51, train loss: 0.167798\n",
      "Epoch 52, train loss: 0.191483\n",
      "Epoch 53, train loss: 0.266743\n",
      "Epoch 54, train loss: 0.234398\n",
      "Epoch 55, train loss: 0.185713\n",
      "Epoch 56, train loss: 0.203393\n",
      "Epoch 57, train loss: 0.194678\n",
      "Epoch 58, train loss: 0.198861\n",
      "Epoch 59, train loss: 0.226279\n",
      "Epoch 60, train loss: 0.219727\n",
      "Epoch 61, train loss: 0.262381\n",
      "Epoch 62, train loss: 0.170694\n",
      "Epoch 63, train loss: 0.323740\n",
      "Epoch 64, train loss: 0.191778\n",
      "Epoch 65, train loss: 0.261661\n",
      "Epoch 66, train loss: 0.201136\n",
      "Epoch 67, train loss: 0.218816\n",
      "Epoch 68, train loss: 0.279712\n",
      "Epoch 69, train loss: 0.209544\n",
      "Epoch 70, train loss: 0.213487\n",
      "Epoch 71, train loss: 0.227890\n",
      "Epoch 72, train loss: 0.220369\n",
      "Epoch 73, train loss: 0.203569\n",
      "Epoch 74, train loss: 0.236002\n",
      "Epoch 75, train loss: 0.168232\n",
      "Epoch 76, train loss: 0.252804\n",
      "Epoch 77, train loss: 0.190920\n",
      "Epoch 78, train loss: 0.209229\n",
      "Epoch 79, train loss: 0.183450\n",
      "Epoch 80, train loss: 0.220775\n",
      "Epoch 81, train loss: 0.160232\n",
      "Epoch 82, train loss: 0.174056\n",
      "Epoch 83, train loss: 0.139114\n",
      "Epoch 84, train loss: 0.210346\n",
      "Epoch 85, train loss: 0.223971\n",
      "Epoch 86, train loss: 0.156025\n",
      "Epoch 87, train loss: 0.178646\n",
      "Epoch 88, train loss: 0.214344\n",
      "Epoch 89, train loss: 0.176734\n",
      "Epoch 90, train loss: 0.278152\n",
      "Epoch 91, train loss: 0.187439\n",
      "Epoch 92, train loss: 0.201581\n",
      "Epoch 93, train loss: 0.233560\n",
      "Epoch 94, train loss: 0.196981\n",
      "Epoch 95, train loss: 0.231220\n",
      "Epoch 96, train loss: 0.223588\n",
      "Epoch 97, train loss: 0.254641\n",
      "Epoch 98, train loss: 0.259248\n",
      "Epoch 99, train loss: 0.264192\n",
      "Epoch 100, train loss: 0.193377\n",
      "Epoch 101, train loss: 0.232558\n",
      "Epoch 102, train loss: 0.200950\n",
      "Epoch 103, train loss: 0.249777\n",
      "Epoch 104, train loss: 0.226632\n",
      "Epoch 105, train loss: 0.179530\n",
      "Epoch 106, train loss: 0.220190\n",
      "Epoch 107, train loss: 0.225686\n",
      "Epoch 108, train loss: 0.178783\n",
      "Epoch 109, train loss: 0.187302\n",
      "Epoch 110, train loss: 0.241670\n",
      "Epoch 111, train loss: 0.219728\n",
      "Epoch 112, train loss: 0.187021\n",
      "Epoch 113, train loss: 0.217550\n",
      "Epoch 114, train loss: 0.191584\n",
      "Epoch 115, train loss: 0.234948\n",
      "Epoch 116, train loss: 0.228408\n",
      "Epoch 117, train loss: 0.206787\n",
      "Epoch 118, train loss: 0.186241\n",
      "Epoch 119, train loss: 0.216211\n",
      "Epoch 120, train loss: 0.165922\n",
      "Epoch 121, train loss: 0.198593\n",
      "Epoch 122, train loss: 0.190401\n",
      "Epoch 123, train loss: 0.166391\n",
      "Epoch 124, train loss: 0.193077\n",
      "Epoch 125, train loss: 0.220457\n",
      "Epoch 126, train loss: 0.189152\n",
      "Epoch 127, train loss: 0.170581\n",
      "Epoch 128, train loss: 0.202586\n",
      "Epoch 129, train loss: 0.244901\n",
      "Epoch 130, train loss: 0.199021\n",
      "Epoch 131, train loss: 0.181057\n",
      "Epoch 132, train loss: 0.185389\n",
      "Epoch 133, train loss: 0.196463\n",
      "Epoch 134, train loss: 0.195837\n",
      "Epoch 135, train loss: 0.204160\n",
      "Epoch 136, train loss: 0.188159\n",
      "Epoch 137, train loss: 0.219964\n",
      "Epoch 138, train loss: 0.222541\n",
      "Epoch 139, train loss: 0.280706\n",
      "Epoch 140, train loss: 0.216084\n",
      "Epoch 141, train loss: 0.213741\n",
      "Epoch 142, train loss: 0.239067\n",
      "Epoch 143, train loss: 0.175905\n",
      "Epoch 144, train loss: 0.229175\n",
      "Epoch 145, train loss: 0.221019\n",
      "Epoch 146, train loss: 0.211128\n",
      "Epoch 147, train loss: 0.178549\n",
      "Epoch 148, train loss: 0.214110\n",
      "Epoch 149, train loss: 0.205122\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[161. 155. 150. 147. 144. 141.]\n",
      " [155. 150. 147. 144. 141. 139.]\n",
      " [150. 147. 144. 141. 139. 135.]\n",
      " ...\n",
      " [310. 301. 293. 293. 301. 307.]\n",
      " [301. 293. 293. 301. 307. 306.]\n",
      " [293. 293. 301. 307. 306. 300.]]\n",
      "y here is\n",
      "[[121. 121. 121. 121. 121. 121.]\n",
      " [117. 117. 117. 117. 117. 117.]\n",
      " [113. 113. 113. 113. 113. 113.]\n",
      " ...\n",
      " [284. 284. 284. 284. 284. 284.]\n",
      " [273. 273. 273. 273. 273. 273.]\n",
      " [262. 262. 262. 262. 262. 262.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2639, 6), Train/test: 1/2638\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 23 segments\n",
      "Building dataset, requesting data from 0 to 23\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [180. 181. 178. 176. 173. 172.]\n",
      " [181. 178. 176. 173. 172. 173.]\n",
      " [178. 176. 173. 172. 173. 173.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [164. 164. 164. 164. 164. 164.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 450/9926\n",
      "Found 23 continuous time series\n",
      "Data shape: (10378, 6), Train/test: 10376/2\n",
      "Train test ratio: 5188.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F6AAD70>\n",
      "Epoch 0, test loss: 0.169040\n",
      "Epoch 1, test loss: 0.169098\n",
      "Epoch 2, test loss: 0.169059\n",
      "Epoch 3, test loss: 0.168925\n",
      "Epoch 4, test loss: 0.169284\n",
      "Epoch 5, test loss: 0.169018\n",
      "Epoch 6, test loss: 0.172124\n",
      "Epoch 7, test loss: 0.170447\n",
      "Epoch 8, test loss: 0.172586\n",
      "Epoch 9, test loss: 0.169753\n",
      "Epoch 10, test loss: 0.169873\n",
      "Epoch 11, test loss: 0.169318\n",
      "Epoch 12, test loss: 0.169647\n",
      "Epoch 13, test loss: 0.172920\n",
      "Epoch 14, test loss: 0.170052\n",
      "Epoch 15, test loss: 0.168813\n",
      "Epoch 16, test loss: 0.169471\n",
      "Epoch 17, test loss: 0.172876\n",
      "Epoch 18, test loss: 0.170253\n",
      "Epoch 19, test loss: 0.171390\n",
      "Epoch 20, test loss: 0.169755\n",
      "Epoch 21, test loss: 0.172883\n",
      "Epoch 22, test loss: 0.169868\n",
      "Epoch 23, test loss: 0.169454\n",
      "Epoch 24, test loss: 0.169177\n",
      "Epoch 25, test loss: 0.170152\n",
      "Epoch 26, test loss: 0.169691\n",
      "Epoch 27, test loss: 0.172109\n",
      "Epoch 28, test loss: 0.169939\n",
      "Epoch 29, test loss: 0.169304\n",
      "Epoch 30, test loss: 0.169727\n",
      "Epoch 31, test loss: 0.169564\n",
      "Epoch 32, test loss: 0.172304\n",
      "Epoch 33, test loss: 0.169761\n",
      "Epoch 34, test loss: 0.170395\n",
      "Epoch 35, test loss: 0.169756\n",
      "Epoch 36, test loss: 0.171508\n",
      "Epoch 37, test loss: 0.170275\n",
      "Epoch 38, test loss: 0.169313\n",
      "Epoch 39, test loss: 0.170206\n",
      "Epoch 40, test loss: 0.169839\n",
      "Epoch 41, test loss: 0.169799\n",
      "Epoch 42, test loss: 0.169377\n",
      "Epoch 43, test loss: 0.169928\n",
      "Epoch 44, test loss: 0.170382\n",
      "Epoch 45, test loss: 0.169986\n",
      "Epoch 46, test loss: 0.170135\n",
      "Epoch 47, test loss: 0.170144\n",
      "Epoch 48, test loss: 0.170423\n",
      "Epoch 49, test loss: 0.169398\n",
      "Epoch 50, test loss: 0.171280\n",
      "Epoch 51, test loss: 0.171356\n",
      "Epoch 52, test loss: 0.170976\n",
      "Epoch 53, test loss: 0.169931\n",
      "Epoch 54, test loss: 0.170264\n",
      "Epoch 55, test loss: 0.170654\n",
      "Epoch 56, test loss: 0.169376\n",
      "Epoch 57, test loss: 0.170486\n",
      "Epoch 58, test loss: 0.169928\n",
      "Epoch 59, test loss: 0.170114\n",
      "Epoch 60, test loss: 0.170367\n",
      "Epoch 61, test loss: 0.169197\n",
      "Epoch 62, test loss: 0.171197\n",
      "Epoch 63, test loss: 0.169780\n",
      "Epoch 64, test loss: 0.169215\n",
      "Epoch 65, test loss: 0.171574\n",
      "Epoch 66, test loss: 0.169569\n",
      "Epoch 67, test loss: 0.172511\n",
      "Epoch 68, test loss: 0.170072\n",
      "Epoch 69, test loss: 0.170680\n",
      "Epoch 70, test loss: 0.169210\n",
      "Epoch 71, test loss: 0.169711\n",
      "Epoch 72, test loss: 0.169814\n",
      "Epoch 73, test loss: 0.169703\n",
      "Epoch 74, test loss: 0.170022\n",
      "Epoch 75, test loss: 0.172350\n",
      "Epoch 76, test loss: 0.169500\n",
      "Epoch 77, test loss: 0.169387\n",
      "Epoch 78, test loss: 0.170691\n",
      "Epoch 79, test loss: 0.169665\n",
      "Epoch 80, test loss: 0.169745\n",
      "Epoch 81, test loss: 0.169851\n",
      "Epoch 82, test loss: 0.170529\n",
      "Epoch 83, test loss: 0.169782\n",
      "Epoch 84, test loss: 0.169261\n",
      "Epoch 85, test loss: 0.169255\n",
      "Epoch 86, test loss: 0.170005\n",
      "Epoch 87, test loss: 0.171002\n",
      "Epoch 88, test loss: 0.171103\n",
      "Epoch 89, test loss: 0.170545\n",
      "Epoch 90, test loss: 0.169929\n",
      "Epoch 91, test loss: 0.169687\n",
      "Epoch 92, test loss: 0.171020\n",
      "Epoch 93, test loss: 0.169800\n",
      "Epoch 94, test loss: 0.169801\n",
      "Epoch 95, test loss: 0.169715\n",
      "Epoch 96, test loss: 0.169563\n",
      "Epoch 97, test loss: 0.169778\n",
      "Epoch 98, test loss: 0.169510\n",
      "Epoch 99, test loss: 0.170134\n",
      "Epoch 100, test loss: 0.169227\n",
      "Epoch 101, test loss: 0.170890\n",
      "Epoch 102, test loss: 0.169166\n",
      "Epoch 103, test loss: 0.169278\n",
      "Epoch 104, test loss: 0.170253\n",
      "Epoch 105, test loss: 0.168980\n",
      "Epoch 106, test loss: 0.168962\n",
      "Epoch 107, test loss: 0.169192\n",
      "Epoch 108, test loss: 0.172236\n",
      "Epoch 109, test loss: 0.169808\n",
      "Epoch 110, test loss: 0.169368\n",
      "Epoch 111, test loss: 0.173759\n",
      "Epoch 112, test loss: 0.170131\n",
      "Epoch 113, test loss: 0.169582\n",
      "Epoch 114, test loss: 0.170804\n",
      "Epoch 115, test loss: 0.170305\n",
      "Epoch 116, test loss: 0.169294\n",
      "Epoch 117, test loss: 0.170883\n",
      "Epoch 118, test loss: 0.169502\n",
      "Epoch 119, test loss: 0.175258\n",
      "Epoch 120, test loss: 0.169447\n",
      "Epoch 121, test loss: 0.171866\n",
      "Epoch 122, test loss: 0.169180\n",
      "Epoch 123, test loss: 0.171579\n",
      "Epoch 124, test loss: 0.169317\n",
      "Epoch 125, test loss: 0.169982\n",
      "Epoch 126, test loss: 0.169497\n",
      "Epoch 127, test loss: 0.169401\n",
      "Epoch 128, test loss: 0.176859\n",
      "Epoch 129, test loss: 0.170948\n",
      "Epoch 130, test loss: 0.170244\n",
      "Epoch 131, test loss: 0.169007\n",
      "Epoch 132, test loss: 0.169475\n",
      "Epoch 133, test loss: 0.170414\n",
      "Epoch 134, test loss: 0.169917\n",
      "Epoch 135, test loss: 0.169056\n",
      "Epoch 136, test loss: 0.172021\n",
      "Epoch 137, test loss: 0.169026\n",
      "Epoch 138, test loss: 0.170640\n",
      "Epoch 139, test loss: 0.169010\n",
      "Epoch 140, test loss: 0.170072\n",
      "Epoch 141, test loss: 0.169352\n",
      "Epoch 142, test loss: 0.169502\n",
      "Epoch 143, test loss: 0.170437\n",
      "Epoch 144, test loss: 0.169690\n",
      "Epoch 145, test loss: 0.172394\n",
      "Epoch 146, test loss: 0.171425\n",
      "Epoch 147, test loss: 0.170829\n",
      "Epoch 148, test loss: 0.169656\n",
      "Epoch 149, test loss: 0.169822\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F6AAD70>\n",
      "Epoch 0, test loss: 0.168765\n",
      "Epoch 1, test loss: 0.168409\n",
      "Epoch 2, test loss: 0.169875\n",
      "Epoch 3, test loss: 0.169333\n",
      "Epoch 4, test loss: 0.168809\n",
      "Epoch 5, test loss: 0.168425\n",
      "Epoch 6, test loss: 0.168939\n",
      "Epoch 7, test loss: 0.169480\n",
      "Epoch 8, test loss: 0.169468\n",
      "Epoch 9, test loss: 0.168832\n",
      "Epoch 10, test loss: 0.169304\n",
      "Epoch 11, test loss: 0.169333\n",
      "Epoch 12, test loss: 0.168896\n",
      "Epoch 13, test loss: 0.171196\n",
      "Epoch 14, test loss: 0.168807\n",
      "Epoch 15, test loss: 0.169830\n",
      "Epoch 16, test loss: 0.169843\n",
      "Epoch 17, test loss: 0.170633\n",
      "Epoch 18, test loss: 0.168909\n",
      "Epoch 19, test loss: 0.169602\n",
      "Epoch 20, test loss: 0.169075\n",
      "Epoch 21, test loss: 0.171213\n",
      "Epoch 22, test loss: 0.169752\n",
      "Epoch 23, test loss: 0.168582\n",
      "Epoch 24, test loss: 0.168650\n",
      "Epoch 25, test loss: 0.168811\n",
      "Epoch 26, test loss: 0.168800\n",
      "Epoch 27, test loss: 0.169195\n",
      "Epoch 28, test loss: 0.168857\n",
      "Epoch 29, test loss: 0.168791\n",
      "Epoch 30, test loss: 0.168289\n",
      "Epoch 31, test loss: 0.168417\n",
      "Epoch 32, test loss: 0.169264\n",
      "Epoch 33, test loss: 0.170423\n",
      "Epoch 34, test loss: 0.169045\n",
      "Epoch 35, test loss: 0.168920\n",
      "Epoch 36, test loss: 0.168961\n",
      "Epoch 37, test loss: 0.168657\n",
      "Epoch 38, test loss: 0.169380\n",
      "Epoch 39, test loss: 0.168547\n",
      "Epoch 40, test loss: 0.168596\n",
      "Epoch 41, test loss: 0.169585\n",
      "Epoch 42, test loss: 0.168542\n",
      "Epoch 43, test loss: 0.168625\n",
      "Epoch 44, test loss: 0.168352\n",
      "Epoch 45, test loss: 0.169789\n",
      "Epoch 46, test loss: 0.170034\n",
      "Epoch 47, test loss: 0.169167\n",
      "Epoch 48, test loss: 0.168835\n",
      "Epoch 49, test loss: 0.168854\n",
      "Epoch 50, test loss: 0.169577\n",
      "Epoch 51, test loss: 0.169291\n",
      "Epoch 52, test loss: 0.169415\n",
      "Epoch 53, test loss: 0.168424\n",
      "Epoch 54, test loss: 0.170145\n",
      "Epoch 55, test loss: 0.168652\n",
      "Epoch 56, test loss: 0.168710\n",
      "Epoch 57, test loss: 0.168406\n",
      "Epoch 58, test loss: 0.169890\n",
      "Epoch 59, test loss: 0.169586\n",
      "Epoch 60, test loss: 0.168494\n",
      "Epoch 61, test loss: 0.168714\n",
      "Epoch 62, test loss: 0.168805\n",
      "Epoch 63, test loss: 0.169163\n",
      "Epoch 64, test loss: 0.169493\n",
      "Epoch 65, test loss: 0.168592\n",
      "Epoch 66, test loss: 0.169594\n",
      "Epoch 67, test loss: 0.168859\n",
      "Epoch 68, test loss: 0.168666\n",
      "Epoch 69, test loss: 0.168867\n",
      "Epoch 70, test loss: 0.169391\n",
      "Epoch 71, test loss: 0.170795\n",
      "Epoch 72, test loss: 0.169409\n",
      "Epoch 73, test loss: 0.168913\n",
      "Epoch 74, test loss: 0.168627\n",
      "Epoch 75, test loss: 0.169298\n",
      "Epoch 76, test loss: 0.169489\n",
      "Epoch 77, test loss: 0.170082\n",
      "Epoch 78, test loss: 0.168721\n",
      "Epoch 79, test loss: 0.169384\n",
      "Epoch 80, test loss: 0.170666\n",
      "Epoch 81, test loss: 0.168973\n",
      "Epoch 82, test loss: 0.168419\n",
      "Epoch 83, test loss: 0.168669\n",
      "Epoch 84, test loss: 0.169714\n",
      "Epoch 85, test loss: 0.168719\n",
      "Epoch 86, test loss: 0.168391\n",
      "Epoch 87, test loss: 0.168321\n",
      "Epoch 88, test loss: 0.168572\n",
      "Epoch 89, test loss: 0.168796\n",
      "Epoch 90, test loss: 0.168971\n",
      "Epoch 91, test loss: 0.168982\n",
      "Epoch 92, test loss: 0.169619\n",
      "Epoch 93, test loss: 0.168771\n",
      "Epoch 94, test loss: 0.169247\n",
      "Epoch 95, test loss: 0.172141\n",
      "Epoch 96, test loss: 0.168566\n",
      "Epoch 97, test loss: 0.168910\n",
      "Epoch 98, test loss: 0.169507\n",
      "Epoch 99, test loss: 0.168790\n",
      "Epoch 100, test loss: 0.169197\n",
      "Epoch 101, test loss: 0.168506\n",
      "Epoch 102, test loss: 0.168483\n",
      "Epoch 103, test loss: 0.169725\n",
      "Epoch 104, test loss: 0.171929\n",
      "Epoch 105, test loss: 0.169414\n",
      "Epoch 106, test loss: 0.168724\n",
      "Epoch 107, test loss: 0.168959\n",
      "Epoch 108, test loss: 0.169030\n",
      "Epoch 109, test loss: 0.169535\n",
      "Epoch 110, test loss: 0.168732\n",
      "Epoch 111, test loss: 0.168873\n",
      "Epoch 112, test loss: 0.168769\n",
      "Epoch 113, test loss: 0.169204\n",
      "Epoch 114, test loss: 0.168783\n",
      "Epoch 115, test loss: 0.168947\n",
      "Epoch 116, test loss: 0.168647\n",
      "Epoch 117, test loss: 0.169385\n",
      "Epoch 118, test loss: 0.168940\n",
      "Epoch 119, test loss: 0.169502\n",
      "Epoch 120, test loss: 0.170919\n",
      "Epoch 121, test loss: 0.168772\n",
      "Epoch 122, test loss: 0.168608\n",
      "Epoch 123, test loss: 0.169554\n",
      "Epoch 124, test loss: 0.168971\n",
      "Epoch 125, test loss: 0.168837\n",
      "Epoch 126, test loss: 0.169096\n",
      "Epoch 127, test loss: 0.168848\n",
      "Epoch 128, test loss: 0.168881\n",
      "Epoch 129, test loss: 0.168642\n",
      "Epoch 130, test loss: 0.169680\n",
      "Epoch 131, test loss: 0.169242\n",
      "Epoch 132, test loss: 0.168889\n",
      "Epoch 133, test loss: 0.168952\n",
      "Epoch 134, test loss: 0.170089\n",
      "Epoch 135, test loss: 0.170812\n",
      "Epoch 136, test loss: 0.168565\n",
      "Epoch 137, test loss: 0.168621\n",
      "Epoch 138, test loss: 0.169116\n",
      "Epoch 139, test loss: 0.169057\n",
      "Epoch 140, test loss: 0.168905\n",
      "Epoch 141, test loss: 0.169098\n",
      "Epoch 142, test loss: 0.168944\n",
      "Epoch 143, test loss: 0.168891\n",
      "Epoch 144, test loss: 0.169598\n",
      "Epoch 145, test loss: 0.169460\n",
      "Epoch 146, test loss: 0.168792\n",
      "Epoch 147, test loss: 0.168660\n",
      "Epoch 148, test loss: 0.168941\n",
      "Epoch 149, test loss: 0.168678\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F60F6AAD70>\n",
      "Epoch 0, test loss: 0.203868\n",
      "Epoch 1, test loss: 0.181261\n",
      "Epoch 2, test loss: 0.176223\n",
      "Epoch 3, test loss: 0.175196\n",
      "Epoch 4, test loss: 0.173804\n",
      "Epoch 5, test loss: 0.172421\n",
      "Epoch 6, test loss: 0.171980\n",
      "Epoch 7, test loss: 0.173008\n",
      "Epoch 8, test loss: 0.171303\n",
      "Epoch 9, test loss: 0.171006\n",
      "Epoch 10, test loss: 0.171564\n",
      "Epoch 11, test loss: 0.171370\n",
      "Epoch 12, test loss: 0.170912\n",
      "Epoch 13, test loss: 0.170939\n",
      "Epoch 14, test loss: 0.171526\n",
      "Epoch 15, test loss: 0.171155\n",
      "Epoch 16, test loss: 0.170210\n",
      "Epoch 17, test loss: 0.172571\n",
      "Epoch 18, test loss: 0.170280\n",
      "Epoch 19, test loss: 0.170184\n",
      "Epoch 20, test loss: 0.171210\n",
      "Epoch 21, test loss: 0.171096\n",
      "Epoch 22, test loss: 0.171431\n",
      "Epoch 23, test loss: 0.170897\n",
      "Epoch 24, test loss: 0.170112\n",
      "Epoch 25, test loss: 0.174765\n",
      "Epoch 26, test loss: 0.170173\n",
      "Epoch 27, test loss: 0.170318\n",
      "Epoch 28, test loss: 0.170400\n",
      "Epoch 29, test loss: 0.170204\n",
      "Epoch 30, test loss: 0.170668\n",
      "Epoch 31, test loss: 0.171707\n",
      "Epoch 32, test loss: 0.170452\n",
      "Epoch 33, test loss: 0.170867\n",
      "Epoch 34, test loss: 0.170635\n",
      "Epoch 35, test loss: 0.170605\n",
      "Epoch 36, test loss: 0.170626\n",
      "Epoch 37, test loss: 0.171821\n",
      "Epoch 38, test loss: 0.170692\n",
      "Epoch 39, test loss: 0.170586\n",
      "Epoch 40, test loss: 0.171161\n",
      "Epoch 41, test loss: 0.170172\n",
      "Epoch 42, test loss: 0.170361\n",
      "Epoch 43, test loss: 0.170493\n",
      "Epoch 44, test loss: 0.171217\n",
      "Epoch 45, test loss: 0.170302\n",
      "Epoch 46, test loss: 0.169969\n",
      "Epoch 47, test loss: 0.170576\n",
      "Epoch 48, test loss: 0.170175\n",
      "Epoch 49, test loss: 0.169843\n",
      "Epoch 50, test loss: 0.170392\n",
      "Epoch 51, test loss: 0.171152\n",
      "Epoch 52, test loss: 0.170354\n",
      "Epoch 53, test loss: 0.171091\n",
      "Epoch 54, test loss: 0.171189\n",
      "Epoch 55, test loss: 0.170931\n",
      "Epoch 56, test loss: 0.170939\n",
      "Epoch 57, test loss: 0.171699\n",
      "Epoch 58, test loss: 0.170658\n",
      "Epoch 59, test loss: 0.170611\n",
      "Epoch 60, test loss: 0.170200\n",
      "Epoch 61, test loss: 0.170525\n",
      "Epoch 62, test loss: 0.169816\n",
      "Epoch 63, test loss: 0.170523\n",
      "Epoch 64, test loss: 0.170168\n",
      "Epoch 65, test loss: 0.170158\n",
      "Epoch 66, test loss: 0.169961\n",
      "Epoch 67, test loss: 0.171402\n",
      "Epoch 68, test loss: 0.169913\n",
      "Epoch 69, test loss: 0.169871\n",
      "Epoch 70, test loss: 0.169940\n",
      "Epoch 71, test loss: 0.170285\n",
      "Epoch 72, test loss: 0.170842\n",
      "Epoch 73, test loss: 0.170446\n",
      "Epoch 74, test loss: 0.172743\n",
      "Epoch 75, test loss: 0.170148\n",
      "Epoch 76, test loss: 0.170186\n",
      "Epoch 77, test loss: 0.170271\n",
      "Epoch 78, test loss: 0.170701\n",
      "Epoch 79, test loss: 0.174254\n",
      "Epoch 80, test loss: 0.170822\n",
      "Epoch 81, test loss: 0.172447\n",
      "Epoch 82, test loss: 0.170434\n",
      "Epoch 83, test loss: 0.170500\n",
      "Epoch 84, test loss: 0.170235\n",
      "Epoch 85, test loss: 0.170184\n",
      "Epoch 86, test loss: 0.170873\n",
      "Epoch 87, test loss: 0.170473\n",
      "Epoch 88, test loss: 0.172206\n",
      "Epoch 89, test loss: 0.170597\n",
      "Epoch 90, test loss: 0.170402\n",
      "Epoch 91, test loss: 0.170142\n",
      "Epoch 92, test loss: 0.171918\n",
      "Epoch 93, test loss: 0.170195\n",
      "Epoch 94, test loss: 0.170570\n",
      "Epoch 95, test loss: 0.170901\n",
      "Epoch 96, test loss: 0.171369\n",
      "Epoch 97, test loss: 0.170505\n",
      "Epoch 98, test loss: 0.171497\n",
      "Epoch 99, test loss: 0.171508\n",
      "Epoch 100, test loss: 0.170667\n",
      "Epoch 101, test loss: 0.170822\n",
      "Epoch 102, test loss: 0.170339\n",
      "Epoch 103, test loss: 0.170750\n",
      "Epoch 104, test loss: 0.170766\n",
      "Epoch 105, test loss: 0.170646\n",
      "Epoch 106, test loss: 0.170599\n",
      "Epoch 107, test loss: 0.170500\n",
      "Epoch 108, test loss: 0.170771\n",
      "Epoch 109, test loss: 0.171423\n",
      "Epoch 110, test loss: 0.171098\n",
      "Epoch 111, test loss: 0.170552\n",
      "Epoch 112, test loss: 0.170431\n",
      "Epoch 113, test loss: 0.170803\n",
      "Epoch 114, test loss: 0.170519\n",
      "Epoch 115, test loss: 0.170356\n",
      "Epoch 116, test loss: 0.171886\n",
      "Epoch 117, test loss: 0.170299\n",
      "Epoch 118, test loss: 0.170709\n",
      "Epoch 119, test loss: 0.170400\n",
      "Epoch 120, test loss: 0.172026\n",
      "Epoch 121, test loss: 0.171339\n",
      "Epoch 122, test loss: 0.170250\n",
      "Epoch 123, test loss: 0.170213\n",
      "Epoch 124, test loss: 0.170741\n",
      "Epoch 125, test loss: 0.170675\n",
      "Epoch 126, test loss: 0.170292\n",
      "Epoch 127, test loss: 0.170760\n",
      "Epoch 128, test loss: 0.172564\n",
      "Epoch 129, test loss: 0.170512\n",
      "Epoch 130, test loss: 0.170610\n",
      "Epoch 131, test loss: 0.170549\n",
      "Epoch 132, test loss: 0.170334\n",
      "Epoch 133, test loss: 0.171114\n",
      "Epoch 134, test loss: 0.172295\n",
      "Epoch 135, test loss: 0.170839\n",
      "Epoch 136, test loss: 0.171459\n",
      "Epoch 137, test loss: 0.170246\n",
      "Epoch 138, test loss: 0.173002\n",
      "Epoch 139, test loss: 0.170499\n",
      "Epoch 140, test loss: 0.170091\n",
      "Epoch 141, test loss: 0.170014\n",
      "Epoch 142, test loss: 0.171777\n",
      "Epoch 143, test loss: 0.171706\n",
      "Epoch 144, test loss: 0.171734\n",
      "Epoch 145, test loss: 0.170197\n",
      "Epoch 146, test loss: 0.170907\n",
      "Epoch 147, test loss: 0.170362\n",
      "Epoch 148, test loss: 0.170305\n",
      "Epoch 149, test loss: 0.170272\n",
      "Epoch 150, test loss: 0.170070\n",
      "Epoch 151, test loss: 0.173357\n",
      "Epoch 152, test loss: 0.171919\n",
      "Epoch 153, test loss: 0.170055\n",
      "Epoch 154, test loss: 0.170181\n",
      "Epoch 155, test loss: 0.170058\n",
      "Epoch 156, test loss: 0.170428\n",
      "Epoch 157, test loss: 0.172342\n",
      "Epoch 158, test loss: 0.172504\n",
      "Epoch 159, test loss: 0.169891\n",
      "Epoch 160, test loss: 0.172089\n",
      "Epoch 161, test loss: 0.169996\n",
      "Epoch 162, test loss: 0.170813\n",
      "Epoch 163, test loss: 0.170603\n",
      "Epoch 164, test loss: 0.170489\n",
      "Epoch 165, test loss: 0.170524\n",
      "Epoch 166, test loss: 0.169890\n",
      "Epoch 167, test loss: 0.169833\n",
      "Epoch 168, test loss: 0.169958\n",
      "Epoch 169, test loss: 0.170645\n",
      "Epoch 170, test loss: 0.169860\n",
      "Epoch 171, test loss: 0.170361\n",
      "Epoch 172, test loss: 0.170277\n",
      "Epoch 173, test loss: 0.171562\n",
      "Epoch 174, test loss: 0.170481\n",
      "Epoch 175, test loss: 0.170041\n",
      "Epoch 176, test loss: 0.170243\n",
      "Epoch 177, test loss: 0.169855\n",
      "Epoch 178, test loss: 0.169811\n",
      "Epoch 179, test loss: 0.170475\n",
      "Epoch 180, test loss: 0.169511\n",
      "Epoch 181, test loss: 0.170722\n",
      "Epoch 182, test loss: 0.169512\n",
      "Epoch 183, test loss: 0.170451\n",
      "Epoch 184, test loss: 0.169746\n",
      "Epoch 185, test loss: 0.170640\n",
      "Epoch 186, test loss: 0.171216\n",
      "Epoch 187, test loss: 0.169785\n",
      "Epoch 188, test loss: 0.169973\n",
      "Epoch 189, test loss: 0.172342\n",
      "Epoch 190, test loss: 0.169688\n",
      "Epoch 191, test loss: 0.171981\n",
      "Epoch 192, test loss: 0.169828\n",
      "Epoch 193, test loss: 0.170211\n",
      "Epoch 194, test loss: 0.169953\n",
      "Epoch 195, test loss: 0.169675\n",
      "Epoch 196, test loss: 0.169652\n",
      "Epoch 197, test loss: 0.171742\n",
      "Epoch 198, test loss: 0.171760\n",
      "Epoch 199, test loss: 0.171036\n",
      "Epoch 200, test loss: 0.170558\n",
      "Epoch 201, test loss: 0.169774\n",
      "Epoch 202, test loss: 0.170404\n",
      "Epoch 203, test loss: 0.172374\n",
      "Epoch 204, test loss: 0.169557\n",
      "Epoch 205, test loss: 0.170043\n",
      "Epoch 206, test loss: 0.169617\n",
      "Epoch 207, test loss: 0.170085\n",
      "Epoch 208, test loss: 0.170141\n",
      "Epoch 209, test loss: 0.169759\n",
      "Epoch 210, test loss: 0.169929\n",
      "Epoch 211, test loss: 0.170688\n",
      "Epoch 212, test loss: 0.169722\n",
      "Epoch 213, test loss: 0.170497\n",
      "Epoch 214, test loss: 0.169638\n",
      "Epoch 215, test loss: 0.169769\n",
      "Epoch 216, test loss: 0.169353\n",
      "Epoch 217, test loss: 0.171405\n",
      "Epoch 218, test loss: 0.170244\n",
      "Epoch 219, test loss: 0.169897\n",
      "Epoch 220, test loss: 0.170098\n",
      "Epoch 221, test loss: 0.169432\n",
      "Epoch 222, test loss: 0.170215\n",
      "Epoch 223, test loss: 0.172941\n",
      "Epoch 224, test loss: 0.169828\n",
      "Epoch 225, test loss: 0.169553\n",
      "Epoch 226, test loss: 0.169936\n",
      "Epoch 227, test loss: 0.171778\n",
      "Epoch 228, test loss: 0.169559\n",
      "Epoch 229, test loss: 0.169588\n",
      "Epoch 230, test loss: 0.170895\n",
      "Epoch 231, test loss: 0.170390\n",
      "Epoch 232, test loss: 0.170689\n",
      "Epoch 233, test loss: 0.169789\n",
      "Epoch 234, test loss: 0.171263\n",
      "Epoch 235, test loss: 0.169378\n",
      "Epoch 236, test loss: 0.170049\n",
      "Epoch 237, test loss: 0.169387\n",
      "Epoch 238, test loss: 0.169482\n",
      "Epoch 239, test loss: 0.170436\n",
      "Epoch 240, test loss: 0.170109\n",
      "Epoch 241, test loss: 0.170403\n",
      "Epoch 242, test loss: 0.169894\n",
      "Epoch 243, test loss: 0.170001\n",
      "Epoch 244, test loss: 0.171015\n",
      "Epoch 245, test loss: 0.169807\n",
      "Epoch 246, test loss: 0.169632\n",
      "Epoch 247, test loss: 0.169358\n",
      "Epoch 248, test loss: 0.169678\n",
      "Epoch 249, test loss: 0.170322\n",
      "Epoch 250, test loss: 0.169858\n",
      "Epoch 251, test loss: 0.170116\n",
      "Epoch 252, test loss: 0.169624\n",
      "Epoch 253, test loss: 0.170741\n",
      "Epoch 254, test loss: 0.171065\n",
      "Epoch 255, test loss: 0.171244\n",
      "Epoch 256, test loss: 0.171139\n",
      "Epoch 257, test loss: 0.169820\n",
      "Epoch 258, test loss: 0.171428\n",
      "Epoch 259, test loss: 0.169742\n",
      "Epoch 260, test loss: 0.171498\n",
      "Epoch 261, test loss: 0.169828\n",
      "Epoch 262, test loss: 0.169901\n",
      "Epoch 263, test loss: 0.169740\n",
      "Epoch 264, test loss: 0.170856\n",
      "Epoch 265, test loss: 0.169922\n",
      "Epoch 266, test loss: 0.169599\n",
      "Epoch 267, test loss: 0.169973\n",
      "Epoch 268, test loss: 0.171397\n",
      "Epoch 269, test loss: 0.171050\n",
      "Epoch 270, test loss: 0.171845\n",
      "Epoch 271, test loss: 0.169550\n",
      "Epoch 272, test loss: 0.169720\n",
      "Epoch 273, test loss: 0.169404\n",
      "Epoch 274, test loss: 0.169627\n",
      "Epoch 275, test loss: 0.169395\n",
      "Epoch 276, test loss: 0.170006\n",
      "Epoch 277, test loss: 0.169914\n",
      "Epoch 278, test loss: 0.169468\n",
      "Epoch 279, test loss: 0.170135\n",
      "Epoch 280, test loss: 0.169525\n",
      "Epoch 281, test loss: 0.170863\n",
      "Epoch 282, test loss: 0.169372\n",
      "Epoch 283, test loss: 0.171321\n",
      "Epoch 284, test loss: 0.169513\n",
      "Epoch 285, test loss: 0.169651\n",
      "Epoch 286, test loss: 0.171145\n",
      "Epoch 287, test loss: 0.170853\n",
      "Epoch 288, test loss: 0.169481\n",
      "Epoch 289, test loss: 0.169659\n",
      "Epoch 290, test loss: 0.169396\n",
      "Epoch 291, test loss: 0.169492\n",
      "Epoch 292, test loss: 0.170002\n",
      "Epoch 293, test loss: 0.169905\n",
      "Epoch 294, test loss: 0.169530\n",
      "Epoch 295, test loss: 0.169648\n",
      "Epoch 296, test loss: 0.169485\n",
      "Epoch 297, test loss: 0.170216\n",
      "Epoch 298, test loss: 0.171148\n",
      "Epoch 299, test loss: 0.169809\n",
      "Epoch 300, test loss: 0.169430\n",
      "Epoch 301, test loss: 0.169991\n",
      "Epoch 302, test loss: 0.169509\n",
      "Epoch 303, test loss: 0.173875\n",
      "Epoch 304, test loss: 0.170351\n",
      "Epoch 305, test loss: 0.169301\n",
      "Epoch 306, test loss: 0.169286\n",
      "Epoch 307, test loss: 0.173068\n",
      "Epoch 308, test loss: 0.173837\n",
      "Epoch 309, test loss: 0.170407\n",
      "Epoch 310, test loss: 0.169885\n",
      "Epoch 311, test loss: 0.169430\n",
      "Epoch 312, test loss: 0.170087\n",
      "Epoch 313, test loss: 0.169690\n",
      "Epoch 314, test loss: 0.169568\n",
      "Epoch 315, test loss: 0.171631\n",
      "Epoch 316, test loss: 0.169466\n",
      "Epoch 317, test loss: 0.169701\n",
      "Epoch 318, test loss: 0.169453\n",
      "Epoch 319, test loss: 0.169545\n",
      "Epoch 320, test loss: 0.169951\n",
      "Epoch 321, test loss: 0.170219\n",
      "Epoch 322, test loss: 0.170967\n",
      "Epoch 323, test loss: 0.170096\n",
      "Epoch 324, test loss: 0.169379\n",
      "Epoch 325, test loss: 0.170633\n",
      "Epoch 326, test loss: 0.171197\n",
      "Epoch 327, test loss: 0.171637\n",
      "Epoch 328, test loss: 0.172365\n",
      "Epoch 329, test loss: 0.169980\n",
      "Epoch 330, test loss: 0.169455\n",
      "Epoch 331, test loss: 0.169586\n",
      "Epoch 332, test loss: 0.169716\n",
      "Epoch 333, test loss: 0.169373\n",
      "Epoch 334, test loss: 0.169883\n",
      "Epoch 335, test loss: 0.171196\n",
      "Epoch 336, test loss: 0.169543\n",
      "Epoch 337, test loss: 0.172296\n",
      "Epoch 338, test loss: 0.169910\n",
      "Epoch 339, test loss: 0.169544\n",
      "Epoch 340, test loss: 0.169566\n",
      "Epoch 341, test loss: 0.169704\n",
      "Epoch 342, test loss: 0.169368\n",
      "Epoch 343, test loss: 0.169399\n",
      "Epoch 344, test loss: 0.171962\n",
      "Epoch 345, test loss: 0.169517\n",
      "Epoch 346, test loss: 0.170043\n",
      "Epoch 347, test loss: 0.170601\n",
      "Epoch 348, test loss: 0.171170\n",
      "Epoch 349, test loss: 0.169590\n",
      "Epoch 350, test loss: 0.170406\n",
      "Epoch 351, test loss: 0.170500\n",
      "Epoch 352, test loss: 0.169421\n",
      "Epoch 353, test loss: 0.170529\n",
      "Epoch 354, test loss: 0.170234\n",
      "Epoch 355, test loss: 0.169319\n",
      "Epoch 356, test loss: 0.169715\n",
      "Epoch 357, test loss: 0.170026\n",
      "Epoch 358, test loss: 0.169870\n",
      "Epoch 359, test loss: 0.170132\n",
      "Epoch 360, test loss: 0.169335\n",
      "Epoch 361, test loss: 0.169509\n",
      "Epoch 362, test loss: 0.170413\n",
      "Epoch 363, test loss: 0.170063\n",
      "Epoch 364, test loss: 0.169915\n",
      "Epoch 365, test loss: 0.170298\n",
      "Epoch 366, test loss: 0.169570\n",
      "Epoch 367, test loss: 0.170201\n",
      "Epoch 368, test loss: 0.169485\n",
      "Epoch 369, test loss: 0.169832\n",
      "Epoch 370, test loss: 0.171972\n",
      "Epoch 371, test loss: 0.170076\n",
      "Epoch 372, test loss: 0.170602\n",
      "Epoch 373, test loss: 0.169279\n",
      "Epoch 374, test loss: 0.169570\n",
      "Epoch 375, test loss: 0.170127\n",
      "Epoch 376, test loss: 0.170068\n",
      "Epoch 377, test loss: 0.169290\n",
      "Epoch 378, test loss: 0.170734\n",
      "Epoch 379, test loss: 0.170291\n",
      "Epoch 380, test loss: 0.169528\n",
      "Epoch 381, test loss: 0.170104\n",
      "Epoch 382, test loss: 0.171554\n",
      "Epoch 383, test loss: 0.169787\n",
      "Epoch 384, test loss: 0.169982\n",
      "Epoch 385, test loss: 0.169924\n",
      "Epoch 386, test loss: 0.169566\n",
      "Epoch 387, test loss: 0.169624\n",
      "Epoch 388, test loss: 0.169535\n",
      "Epoch 389, test loss: 0.169597\n",
      "Epoch 390, test loss: 0.169411\n",
      "Epoch 391, test loss: 0.169540\n",
      "Epoch 392, test loss: 0.170331\n",
      "Epoch 393, test loss: 0.170119\n",
      "Epoch 394, test loss: 0.169631\n",
      "Epoch 395, test loss: 0.169799\n",
      "Epoch 396, test loss: 0.169243\n",
      "Epoch 397, test loss: 0.169457\n",
      "Epoch 398, test loss: 0.169981\n",
      "Epoch 399, test loss: 0.170250\n",
      "Epoch 400, test loss: 0.169782\n",
      "Epoch 401, test loss: 0.170111\n",
      "Epoch 402, test loss: 0.172449\n",
      "Epoch 403, test loss: 0.170590\n",
      "Epoch 404, test loss: 0.169746\n",
      "Epoch 405, test loss: 0.169717\n",
      "Epoch 406, test loss: 0.171163\n",
      "Epoch 407, test loss: 0.170284\n",
      "Epoch 408, test loss: 0.169579\n",
      "Epoch 409, test loss: 0.170598\n",
      "Epoch 410, test loss: 0.169704\n",
      "Epoch 411, test loss: 0.170466\n",
      "Epoch 412, test loss: 0.171926\n",
      "Epoch 413, test loss: 0.170112\n",
      "Epoch 414, test loss: 0.169699\n",
      "Epoch 415, test loss: 0.170876\n",
      "Epoch 416, test loss: 0.169596\n",
      "Epoch 417, test loss: 0.170480\n",
      "Epoch 418, test loss: 0.169933\n",
      "Epoch 419, test loss: 0.169544\n",
      "Epoch 420, test loss: 0.169610\n",
      "Epoch 421, test loss: 0.169514\n",
      "Epoch 422, test loss: 0.170158\n",
      "Epoch 423, test loss: 0.169547\n",
      "Epoch 424, test loss: 0.169264\n",
      "Epoch 425, test loss: 0.169679\n",
      "Epoch 426, test loss: 0.169740\n",
      "Epoch 427, test loss: 0.169537\n",
      "Epoch 428, test loss: 0.169909\n",
      "Epoch 429, test loss: 0.170367\n",
      "Epoch 430, test loss: 0.169308\n",
      "Epoch 431, test loss: 0.170753\n",
      "Epoch 432, test loss: 0.169748\n",
      "Epoch 433, test loss: 0.171239\n",
      "Epoch 434, test loss: 0.169376\n",
      "Epoch 435, test loss: 0.170086\n",
      "Epoch 436, test loss: 0.169764\n",
      "Epoch 437, test loss: 0.169514\n",
      "Epoch 438, test loss: 0.169664\n",
      "Epoch 439, test loss: 0.170126\n",
      "Epoch 440, test loss: 0.170893\n",
      "Epoch 441, test loss: 0.169656\n",
      "Epoch 442, test loss: 0.169369\n",
      "Epoch 443, test loss: 0.170582\n",
      "Epoch 444, test loss: 0.169442\n",
      "Epoch 445, test loss: 0.169222\n",
      "Epoch 446, test loss: 0.169588\n",
      "Epoch 447, test loss: 0.169576\n",
      "Epoch 448, test loss: 0.169547\n",
      "Epoch 449, test loss: 0.169985\n",
      "Pretrain data: 19754507.0\n",
      "Building dataset, requesting data from 0 to 655\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7434/109388\n",
      "Found 655 continuous time series\n",
      "Data shape: (116824, 6), Train/test: 116822/2\n",
      "Train test ratio: 58411.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1825\n",
      "Epoch 0, train loss: 0.190810\n",
      "Epoch 1, train loss: 0.222631\n",
      "Epoch 2, train loss: 0.202037\n",
      "Epoch 3, train loss: 0.254405\n",
      "Epoch 4, train loss: 0.196627\n",
      "Epoch 5, train loss: 0.200913\n",
      "Epoch 6, train loss: 0.298744\n",
      "Epoch 7, train loss: 0.159013\n",
      "Epoch 8, train loss: 0.171429\n",
      "Epoch 9, train loss: 0.215297\n",
      "Epoch 10, train loss: 0.188010\n",
      "Epoch 11, train loss: 0.178695\n",
      "Epoch 12, train loss: 0.218411\n",
      "Epoch 13, train loss: 0.219798\n",
      "Epoch 14, train loss: 0.234855\n",
      "Epoch 15, train loss: 0.202687\n",
      "Epoch 16, train loss: 0.150742\n",
      "Epoch 17, train loss: 0.165985\n",
      "Epoch 18, train loss: 0.166539\n",
      "Epoch 19, train loss: 0.188952\n",
      "Epoch 20, train loss: 0.269044\n",
      "Epoch 21, train loss: 0.208285\n",
      "Epoch 22, train loss: 0.171801\n",
      "Epoch 23, train loss: 0.242195\n",
      "Epoch 24, train loss: 0.203119\n",
      "Epoch 25, train loss: 0.165385\n",
      "Epoch 26, train loss: 0.223889\n",
      "Epoch 27, train loss: 0.202365\n",
      "Epoch 28, train loss: 0.219563\n",
      "Epoch 29, train loss: 0.257086\n",
      "Epoch 30, train loss: 0.222284\n",
      "Epoch 31, train loss: 0.232091\n",
      "Epoch 32, train loss: 0.215811\n",
      "Epoch 33, train loss: 0.197467\n",
      "Epoch 34, train loss: 0.202716\n",
      "Epoch 35, train loss: 0.173409\n",
      "Epoch 36, train loss: 0.211001\n",
      "Epoch 37, train loss: 0.191363\n",
      "Epoch 38, train loss: 0.194407\n",
      "Epoch 39, train loss: 0.243380\n",
      "Epoch 40, train loss: 0.204039\n",
      "Epoch 41, train loss: 0.209826\n",
      "Epoch 42, train loss: 0.211078\n",
      "Epoch 43, train loss: 0.188742\n",
      "Epoch 44, train loss: 0.185196\n",
      "Epoch 45, train loss: 0.222553\n",
      "Epoch 46, train loss: 0.198417\n",
      "Epoch 47, train loss: 0.129362\n",
      "Epoch 48, train loss: 0.214378\n",
      "Epoch 49, train loss: 0.191494\n",
      "Epoch 50, train loss: 0.242365\n",
      "Epoch 51, train loss: 0.179364\n",
      "Epoch 52, train loss: 0.222227\n",
      "Epoch 53, train loss: 0.195974\n",
      "Epoch 54, train loss: 0.220397\n",
      "Epoch 55, train loss: 0.195674\n",
      "Epoch 56, train loss: 0.244682\n",
      "Epoch 57, train loss: 0.231145\n",
      "Epoch 58, train loss: 0.243392\n",
      "Epoch 59, train loss: 0.234588\n",
      "Epoch 60, train loss: 0.261525\n",
      "Epoch 61, train loss: 0.171547\n",
      "Epoch 62, train loss: 0.214929\n",
      "Epoch 63, train loss: 0.232268\n",
      "Epoch 64, train loss: 0.184749\n",
      "Epoch 65, train loss: 0.164179\n",
      "Epoch 66, train loss: 0.199174\n",
      "Epoch 67, train loss: 0.179981\n",
      "Epoch 68, train loss: 0.158376\n",
      "Epoch 69, train loss: 0.242315\n",
      "Epoch 70, train loss: 0.226121\n",
      "Epoch 71, train loss: 0.208577\n",
      "Epoch 72, train loss: 0.230286\n",
      "Epoch 73, train loss: 0.208198\n",
      "Epoch 74, train loss: 0.229282\n",
      "Epoch 75, train loss: 0.213019\n",
      "Epoch 76, train loss: 0.209935\n",
      "Epoch 77, train loss: 0.228530\n",
      "Epoch 78, train loss: 0.187264\n",
      "Epoch 79, train loss: 0.228163\n",
      "Epoch 80, train loss: 0.178693\n",
      "Epoch 81, train loss: 0.224255\n",
      "Epoch 82, train loss: 0.207157\n",
      "Epoch 83, train loss: 0.224466\n",
      "Epoch 84, train loss: 0.216848\n",
      "Epoch 85, train loss: 0.218886\n",
      "Epoch 86, train loss: 0.156284\n",
      "Epoch 87, train loss: 0.213941\n",
      "Epoch 88, train loss: 0.170684\n",
      "Epoch 89, train loss: 0.197787\n",
      "Epoch 90, train loss: 0.187471\n",
      "Epoch 91, train loss: 0.166895\n",
      "Epoch 92, train loss: 0.181359\n",
      "Epoch 93, train loss: 0.184046\n",
      "Epoch 94, train loss: 0.198450\n",
      "Epoch 95, train loss: 0.208408\n",
      "Epoch 96, train loss: 0.180975\n",
      "Epoch 97, train loss: 0.290311\n",
      "Epoch 98, train loss: 0.186744\n",
      "Epoch 99, train loss: 0.223794\n",
      "Epoch 100, train loss: 0.226666\n",
      "Epoch 101, train loss: 0.212953\n",
      "Epoch 102, train loss: 0.178590\n",
      "Epoch 103, train loss: 0.209106\n",
      "Epoch 104, train loss: 0.166017\n",
      "Epoch 105, train loss: 0.179232\n",
      "Epoch 106, train loss: 0.193769\n",
      "Epoch 107, train loss: 0.165254\n",
      "Epoch 108, train loss: 0.258194\n",
      "Epoch 109, train loss: 0.199037\n",
      "Epoch 110, train loss: 0.193769\n",
      "Epoch 111, train loss: 0.163060\n",
      "Epoch 112, train loss: 0.234939\n",
      "Epoch 113, train loss: 0.306932\n",
      "Epoch 114, train loss: 0.160636\n",
      "Epoch 115, train loss: 0.202187\n",
      "Epoch 116, train loss: 0.204358\n",
      "Epoch 117, train loss: 0.206404\n",
      "Epoch 118, train loss: 0.210023\n",
      "Epoch 119, train loss: 0.245131\n",
      "Epoch 120, train loss: 0.190438\n",
      "Epoch 121, train loss: 0.185389\n",
      "Epoch 122, train loss: 0.239182\n",
      "Epoch 123, train loss: 0.171676\n",
      "Epoch 124, train loss: 0.189090\n",
      "Epoch 125, train loss: 0.212338\n",
      "Epoch 126, train loss: 0.240991\n",
      "Epoch 127, train loss: 0.254277\n",
      "Epoch 128, train loss: 0.175091\n",
      "Epoch 129, train loss: 0.140621\n",
      "Epoch 130, train loss: 0.218020\n",
      "Epoch 131, train loss: 0.166914\n",
      "Epoch 132, train loss: 0.199908\n",
      "Epoch 133, train loss: 0.289395\n",
      "Epoch 134, train loss: 0.185307\n",
      "Epoch 135, train loss: 0.234524\n",
      "Epoch 136, train loss: 0.244438\n",
      "Epoch 137, train loss: 0.214039\n",
      "Epoch 138, train loss: 0.247973\n",
      "Epoch 139, train loss: 0.194538\n",
      "Epoch 140, train loss: 0.225963\n",
      "Epoch 141, train loss: 0.167612\n",
      "Epoch 142, train loss: 0.207665\n",
      "Epoch 143, train loss: 0.180897\n",
      "Epoch 144, train loss: 0.162061\n",
      "Epoch 145, train loss: 0.241884\n",
      "Epoch 146, train loss: 0.198132\n",
      "Epoch 147, train loss: 0.228225\n",
      "Epoch 148, train loss: 0.181476\n",
      "Epoch 149, train loss: 0.272361\n",
      "Reading 43 segments\n",
      "Building dataset, requesting data from 0 to 43\n",
      "x here is\n",
      "[[296. 290. 284. 279. 272. 266.]\n",
      " [290. 284. 279. 272. 266. 259.]\n",
      " [284. 279. 272. 266. 259. 253.]\n",
      " ...\n",
      " [186. 179. 174. 172. 171. 178.]\n",
      " [179. 174. 172. 171. 178. 180.]\n",
      " [174. 172. 171. 178. 180. 180.]]\n",
      "y here is\n",
      "[[234. 234. 234. 234. 234. 234.]\n",
      " [229. 229. 229. 229. 229. 229.]\n",
      " [222. 222. 222. 222. 222. 222.]\n",
      " ...\n",
      " [169. 169. 169. 169. 169. 169.]\n",
      " [164. 164. 164. 164. 164. 164.]\n",
      " [157. 157. 157. 157. 157. 157.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 43 continuous time series\n",
      "Data shape: (1928, 6), Train/test: 1/1927\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 187 segments\n",
      "Building dataset, requesting data from 0 to 187\n",
      "x here is\n",
      "[[ 71.  71.  71.  71.  72.  71.]\n",
      " [ 71.  71.  71.  72.  71.  71.]\n",
      " [ 71.  71.  72.  71.  71.  71.]\n",
      " ...\n",
      " [280. 281. 279. 277. 277. 277.]\n",
      " [281. 279. 277. 277. 277. 279.]\n",
      " [279. 277. 277. 277. 279. 282.]]\n",
      "y here is\n",
      "[[ 71.  71.  71.  71.  71.  71.]\n",
      " [ 71.  71.  71.  71.  71.  71.]\n",
      " [ 71.  71.  71.  71.  71.  71.]\n",
      " ...\n",
      " [315. 315. 315. 315. 315. 315.]\n",
      " [310. 310. 310. 310. 310. 310.]\n",
      " [303. 303. 303. 303. 303. 303.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 851/8051\n",
      "Found 187 continuous time series\n",
      "Data shape: (8904, 6), Train/test: 8902/2\n",
      "Train test ratio: 4451.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7F586F820>\n",
      "Epoch 0, test loss: 0.211519\n",
      "Epoch 1, test loss: 0.206665\n",
      "Epoch 2, test loss: 0.208002\n",
      "Epoch 3, test loss: 0.212282\n",
      "Epoch 4, test loss: 0.207854\n",
      "Epoch 5, test loss: 0.207633\n",
      "Epoch 6, test loss: 0.208534\n",
      "Epoch 7, test loss: 0.207882\n",
      "Epoch 8, test loss: 0.206568\n",
      "Epoch 9, test loss: 0.207271\n",
      "Epoch 10, test loss: 0.207636\n",
      "Epoch 11, test loss: 0.206648\n",
      "Epoch 12, test loss: 0.213388\n",
      "Epoch 13, test loss: 0.207256\n",
      "Epoch 14, test loss: 0.207527\n",
      "Epoch 15, test loss: 0.210654\n",
      "Epoch 16, test loss: 0.210065\n",
      "Epoch 17, test loss: 0.211778\n",
      "Epoch 18, test loss: 0.207094\n",
      "Epoch 19, test loss: 0.213447\n",
      "Epoch 20, test loss: 0.210333\n",
      "Epoch 21, test loss: 0.211434\n",
      "Epoch 22, test loss: 0.207805\n",
      "Epoch 23, test loss: 0.207396\n",
      "Epoch 24, test loss: 0.207165\n",
      "Epoch 25, test loss: 0.210083\n",
      "Epoch 26, test loss: 0.209314\n",
      "Epoch 27, test loss: 0.206815\n",
      "Epoch 28, test loss: 0.207278\n",
      "Epoch 29, test loss: 0.207198\n",
      "Epoch 30, test loss: 0.208083\n",
      "Epoch 31, test loss: 0.209118\n",
      "Epoch 32, test loss: 0.211464\n",
      "Epoch 33, test loss: 0.209066\n",
      "Epoch 34, test loss: 0.212169\n",
      "Epoch 35, test loss: 0.215763\n",
      "Epoch 36, test loss: 0.207145\n",
      "Epoch 37, test loss: 0.209204\n",
      "Epoch 38, test loss: 0.211091\n",
      "Epoch 39, test loss: 0.207751\n",
      "Epoch 40, test loss: 0.207045\n",
      "Epoch 41, test loss: 0.207357\n",
      "Epoch 42, test loss: 0.208890\n",
      "Epoch 43, test loss: 0.212679\n",
      "Epoch 44, test loss: 0.216182\n",
      "Epoch 45, test loss: 0.208387\n",
      "Epoch 46, test loss: 0.214141\n",
      "Epoch 47, test loss: 0.206644\n",
      "Epoch 48, test loss: 0.208244\n",
      "Epoch 49, test loss: 0.209136\n",
      "Epoch 50, test loss: 0.207440\n",
      "Epoch 51, test loss: 0.208728\n",
      "Epoch 52, test loss: 0.207763\n",
      "Epoch 53, test loss: 0.206855\n",
      "Epoch 54, test loss: 0.210623\n",
      "Epoch 55, test loss: 0.206740\n",
      "Epoch 56, test loss: 0.206808\n",
      "Epoch 57, test loss: 0.218471\n",
      "Epoch 58, test loss: 0.207511\n",
      "Epoch 59, test loss: 0.207390\n",
      "Epoch 60, test loss: 0.213641\n",
      "Epoch 61, test loss: 0.207444\n",
      "Epoch 62, test loss: 0.209049\n",
      "Epoch 63, test loss: 0.210377\n",
      "Epoch 64, test loss: 0.207810\n",
      "Epoch 65, test loss: 0.207652\n",
      "Epoch 66, test loss: 0.208042\n",
      "Epoch 67, test loss: 0.207816\n",
      "Epoch 68, test loss: 0.206964\n",
      "Epoch 69, test loss: 0.210514\n",
      "Epoch 70, test loss: 0.208705\n",
      "Epoch 71, test loss: 0.209032\n",
      "Epoch 72, test loss: 0.209336\n",
      "Epoch 73, test loss: 0.213269\n",
      "Epoch 74, test loss: 0.207604\n",
      "Epoch 75, test loss: 0.213465\n",
      "Epoch 76, test loss: 0.208483\n",
      "Epoch 77, test loss: 0.207782\n",
      "Epoch 78, test loss: 0.209365\n",
      "Epoch 79, test loss: 0.212393\n",
      "Epoch 80, test loss: 0.208841\n",
      "Epoch 81, test loss: 0.219003\n",
      "Epoch 82, test loss: 0.206583\n",
      "Epoch 83, test loss: 0.206275\n",
      "Epoch 84, test loss: 0.206408\n",
      "Epoch 85, test loss: 0.213675\n",
      "Epoch 86, test loss: 0.220335\n",
      "Epoch 87, test loss: 0.206776\n",
      "Epoch 88, test loss: 0.208832\n",
      "Epoch 89, test loss: 0.211223\n",
      "Epoch 90, test loss: 0.210812\n",
      "Epoch 91, test loss: 0.209744\n",
      "Epoch 92, test loss: 0.207017\n",
      "Epoch 93, test loss: 0.210628\n",
      "Epoch 94, test loss: 0.208867\n",
      "Epoch 95, test loss: 0.206763\n",
      "Epoch 96, test loss: 0.207967\n",
      "Epoch 97, test loss: 0.209571\n",
      "Epoch 98, test loss: 0.208558\n",
      "Epoch 99, test loss: 0.207170\n",
      "Epoch 100, test loss: 0.212131\n",
      "Epoch 101, test loss: 0.209926\n",
      "Epoch 102, test loss: 0.209475\n",
      "Epoch 103, test loss: 0.210232\n",
      "Epoch 104, test loss: 0.208568\n",
      "Epoch 105, test loss: 0.211890\n",
      "Epoch 106, test loss: 0.207443\n",
      "Epoch 107, test loss: 0.209848\n",
      "Epoch 108, test loss: 0.209020\n",
      "Epoch 109, test loss: 0.210647\n",
      "Epoch 110, test loss: 0.208990\n",
      "Epoch 111, test loss: 0.207860\n",
      "Epoch 112, test loss: 0.208862\n",
      "Epoch 113, test loss: 0.208643\n",
      "Epoch 114, test loss: 0.210188\n",
      "Epoch 115, test loss: 0.206761\n",
      "Epoch 116, test loss: 0.208228\n",
      "Epoch 117, test loss: 0.209938\n",
      "Epoch 118, test loss: 0.217320\n",
      "Epoch 119, test loss: 0.208199\n",
      "Epoch 120, test loss: 0.208628\n",
      "Epoch 121, test loss: 0.207970\n",
      "Epoch 122, test loss: 0.210933\n",
      "Epoch 123, test loss: 0.206813\n",
      "Epoch 124, test loss: 0.207826\n",
      "Epoch 125, test loss: 0.208783\n",
      "Epoch 126, test loss: 0.210789\n",
      "Epoch 127, test loss: 0.209963\n",
      "Epoch 128, test loss: 0.208092\n",
      "Epoch 129, test loss: 0.208652\n",
      "Epoch 130, test loss: 0.207436\n",
      "Epoch 131, test loss: 0.208273\n",
      "Epoch 132, test loss: 0.208332\n",
      "Epoch 133, test loss: 0.207009\n",
      "Epoch 134, test loss: 0.208457\n",
      "Epoch 135, test loss: 0.207324\n",
      "Epoch 136, test loss: 0.207276\n",
      "Epoch 137, test loss: 0.207900\n",
      "Epoch 138, test loss: 0.211821\n",
      "Epoch 139, test loss: 0.209924\n",
      "Epoch 140, test loss: 0.208088\n",
      "Epoch 141, test loss: 0.211025\n",
      "Epoch 142, test loss: 0.215571\n",
      "Epoch 143, test loss: 0.207691\n",
      "Epoch 144, test loss: 0.206941\n",
      "Epoch 145, test loss: 0.207700\n",
      "Epoch 146, test loss: 0.207759\n",
      "Epoch 147, test loss: 0.208982\n",
      "Epoch 148, test loss: 0.207241\n",
      "Epoch 149, test loss: 0.207825\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7F586F820>\n",
      "Epoch 0, test loss: 0.212001\n",
      "Epoch 1, test loss: 0.208559\n",
      "Epoch 2, test loss: 0.210680\n",
      "Epoch 3, test loss: 0.208856\n",
      "Epoch 4, test loss: 0.217325\n",
      "Epoch 5, test loss: 0.209253\n",
      "Epoch 6, test loss: 0.212407\n",
      "Epoch 7, test loss: 0.210972\n",
      "Epoch 8, test loss: 0.209517\n",
      "Epoch 9, test loss: 0.211799\n",
      "Epoch 10, test loss: 0.210130\n",
      "Epoch 11, test loss: 0.209436\n",
      "Epoch 12, test loss: 0.212948\n",
      "Epoch 13, test loss: 0.210453\n",
      "Epoch 14, test loss: 0.210739\n",
      "Epoch 15, test loss: 0.209950\n",
      "Epoch 16, test loss: 0.211523\n",
      "Epoch 17, test loss: 0.210890\n",
      "Epoch 18, test loss: 0.210518\n",
      "Epoch 19, test loss: 0.210853\n",
      "Epoch 20, test loss: 0.211625\n",
      "Epoch 21, test loss: 0.211109\n",
      "Epoch 22, test loss: 0.211044\n",
      "Epoch 23, test loss: 0.215619\n",
      "Epoch 24, test loss: 0.209970\n",
      "Epoch 25, test loss: 0.210996\n",
      "Epoch 26, test loss: 0.212541\n",
      "Epoch 27, test loss: 0.210207\n",
      "Epoch 28, test loss: 0.211107\n",
      "Epoch 29, test loss: 0.211280\n",
      "Epoch 30, test loss: 0.212598\n",
      "Epoch 31, test loss: 0.212805\n",
      "Epoch 32, test loss: 0.210939\n",
      "Epoch 33, test loss: 0.212010\n",
      "Epoch 34, test loss: 0.210264\n",
      "Epoch 35, test loss: 0.211542\n",
      "Epoch 36, test loss: 0.212031\n",
      "Epoch 37, test loss: 0.211499\n",
      "Epoch 38, test loss: 0.209510\n",
      "Epoch 39, test loss: 0.210323\n",
      "Epoch 40, test loss: 0.210357\n",
      "Epoch 41, test loss: 0.212179\n",
      "Epoch 42, test loss: 0.209655\n",
      "Epoch 43, test loss: 0.210612\n",
      "Epoch 44, test loss: 0.210941\n",
      "Epoch 45, test loss: 0.211796\n",
      "Epoch 46, test loss: 0.212111\n",
      "Epoch 47, test loss: 0.210117\n",
      "Epoch 48, test loss: 0.210980\n",
      "Epoch 49, test loss: 0.211043\n",
      "Epoch 50, test loss: 0.210344\n",
      "Epoch 51, test loss: 0.211940\n",
      "Epoch 52, test loss: 0.211410\n",
      "Epoch 53, test loss: 0.212047\n",
      "Epoch 54, test loss: 0.210013\n",
      "Epoch 55, test loss: 0.212399\n",
      "Epoch 56, test loss: 0.210500\n",
      "Epoch 57, test loss: 0.211619\n",
      "Epoch 58, test loss: 0.214942\n",
      "Epoch 59, test loss: 0.211345\n",
      "Epoch 60, test loss: 0.210833\n",
      "Epoch 61, test loss: 0.211164\n",
      "Epoch 62, test loss: 0.211652\n",
      "Epoch 63, test loss: 0.212942\n",
      "Epoch 64, test loss: 0.211406\n",
      "Epoch 65, test loss: 0.209895\n",
      "Epoch 66, test loss: 0.210606\n",
      "Epoch 67, test loss: 0.210106\n",
      "Epoch 68, test loss: 0.212208\n",
      "Epoch 69, test loss: 0.210424\n",
      "Epoch 70, test loss: 0.211049\n",
      "Epoch 71, test loss: 0.211344\n",
      "Epoch 72, test loss: 0.212180\n",
      "Epoch 73, test loss: 0.210186\n",
      "Epoch 74, test loss: 0.210665\n",
      "Epoch 75, test loss: 0.214429\n",
      "Epoch 76, test loss: 0.210728\n",
      "Epoch 77, test loss: 0.210512\n",
      "Epoch 78, test loss: 0.210576\n",
      "Epoch 79, test loss: 0.210292\n",
      "Epoch 80, test loss: 0.211662\n",
      "Epoch 81, test loss: 0.211587\n",
      "Epoch 82, test loss: 0.211727\n",
      "Epoch 83, test loss: 0.210982\n",
      "Epoch 84, test loss: 0.211811\n",
      "Epoch 85, test loss: 0.210724\n",
      "Epoch 86, test loss: 0.211085\n",
      "Epoch 87, test loss: 0.210708\n",
      "Epoch 88, test loss: 0.211285\n",
      "Epoch 89, test loss: 0.210096\n",
      "Epoch 90, test loss: 0.210630\n",
      "Epoch 91, test loss: 0.211444\n",
      "Epoch 92, test loss: 0.211111\n",
      "Epoch 93, test loss: 0.210893\n",
      "Epoch 94, test loss: 0.210730\n",
      "Epoch 95, test loss: 0.210926\n",
      "Epoch 96, test loss: 0.210288\n",
      "Epoch 97, test loss: 0.210623\n",
      "Epoch 98, test loss: 0.211201\n",
      "Epoch 99, test loss: 0.210688\n",
      "Epoch 100, test loss: 0.210593\n",
      "Epoch 101, test loss: 0.215459\n",
      "Epoch 102, test loss: 0.210474\n",
      "Epoch 103, test loss: 0.212060\n",
      "Epoch 104, test loss: 0.210525\n",
      "Epoch 105, test loss: 0.212345\n",
      "Epoch 106, test loss: 0.210315\n",
      "Epoch 107, test loss: 0.211639\n",
      "Epoch 108, test loss: 0.210877\n",
      "Epoch 109, test loss: 0.212180\n",
      "Epoch 110, test loss: 0.210997\n",
      "Epoch 111, test loss: 0.210429\n",
      "Epoch 112, test loss: 0.210817\n",
      "Epoch 113, test loss: 0.211091\n",
      "Epoch 114, test loss: 0.212632\n",
      "Epoch 115, test loss: 0.212341\n",
      "Epoch 116, test loss: 0.212046\n",
      "Epoch 117, test loss: 0.210296\n",
      "Epoch 118, test loss: 0.210418\n",
      "Epoch 119, test loss: 0.211147\n",
      "Epoch 120, test loss: 0.210869\n",
      "Epoch 121, test loss: 0.210312\n",
      "Epoch 122, test loss: 0.210347\n",
      "Epoch 123, test loss: 0.211666\n",
      "Epoch 124, test loss: 0.210122\n",
      "Epoch 125, test loss: 0.214595\n",
      "Epoch 126, test loss: 0.212959\n",
      "Epoch 127, test loss: 0.210337\n",
      "Epoch 128, test loss: 0.212940\n",
      "Epoch 129, test loss: 0.212876\n",
      "Epoch 130, test loss: 0.210375\n",
      "Epoch 131, test loss: 0.211318\n",
      "Epoch 132, test loss: 0.210418\n",
      "Epoch 133, test loss: 0.212978\n",
      "Epoch 134, test loss: 0.215974\n",
      "Epoch 135, test loss: 0.210489\n",
      "Epoch 136, test loss: 0.210121\n",
      "Epoch 137, test loss: 0.211003\n",
      "Epoch 138, test loss: 0.210338\n",
      "Epoch 139, test loss: 0.214243\n",
      "Epoch 140, test loss: 0.210137\n",
      "Epoch 141, test loss: 0.210167\n",
      "Epoch 142, test loss: 0.210343\n",
      "Epoch 143, test loss: 0.210813\n",
      "Epoch 144, test loss: 0.210389\n",
      "Epoch 145, test loss: 0.210588\n",
      "Epoch 146, test loss: 0.210407\n",
      "Epoch 147, test loss: 0.211909\n",
      "Epoch 148, test loss: 0.215103\n",
      "Epoch 149, test loss: 0.210541\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7F586F820>\n",
      "Epoch 0, test loss: 0.263214\n",
      "Epoch 1, test loss: 0.250432\n",
      "Epoch 2, test loss: 0.235565\n",
      "Epoch 3, test loss: 0.224157\n",
      "Epoch 4, test loss: 0.217448\n",
      "Epoch 5, test loss: 0.213248\n",
      "Epoch 6, test loss: 0.211929\n",
      "Epoch 7, test loss: 0.210013\n",
      "Epoch 8, test loss: 0.209408\n",
      "Epoch 9, test loss: 0.209036\n",
      "Epoch 10, test loss: 0.208052\n",
      "Epoch 11, test loss: 0.209157\n",
      "Epoch 12, test loss: 0.208109\n",
      "Epoch 13, test loss: 0.217424\n",
      "Epoch 14, test loss: 0.207209\n",
      "Epoch 15, test loss: 0.207579\n",
      "Epoch 16, test loss: 0.207300\n",
      "Epoch 17, test loss: 0.208417\n",
      "Epoch 18, test loss: 0.207005\n",
      "Epoch 19, test loss: 0.210173\n",
      "Epoch 20, test loss: 0.207479\n",
      "Epoch 21, test loss: 0.207127\n",
      "Epoch 22, test loss: 0.210580\n",
      "Epoch 23, test loss: 0.207377\n",
      "Epoch 24, test loss: 0.209867\n",
      "Epoch 25, test loss: 0.209692\n",
      "Epoch 26, test loss: 0.215413\n",
      "Epoch 27, test loss: 0.207041\n",
      "Epoch 28, test loss: 0.208225\n",
      "Epoch 29, test loss: 0.211612\n",
      "Epoch 30, test loss: 0.208932\n",
      "Epoch 31, test loss: 0.208313\n",
      "Epoch 32, test loss: 0.209398\n",
      "Epoch 33, test loss: 0.207428\n",
      "Epoch 34, test loss: 0.207152\n",
      "Epoch 35, test loss: 0.207330\n",
      "Epoch 36, test loss: 0.206936\n",
      "Epoch 37, test loss: 0.206785\n",
      "Epoch 38, test loss: 0.206862\n",
      "Epoch 39, test loss: 0.208541\n",
      "Epoch 40, test loss: 0.207436\n",
      "Epoch 41, test loss: 0.208869\n",
      "Epoch 42, test loss: 0.207751\n",
      "Epoch 43, test loss: 0.207308\n",
      "Epoch 44, test loss: 0.208328\n",
      "Epoch 45, test loss: 0.207408\n",
      "Epoch 46, test loss: 0.209783\n",
      "Epoch 47, test loss: 0.211154\n",
      "Epoch 48, test loss: 0.207979\n",
      "Epoch 49, test loss: 0.207296\n",
      "Epoch 50, test loss: 0.207766\n",
      "Epoch 51, test loss: 0.208902\n",
      "Epoch 52, test loss: 0.208631\n",
      "Epoch 53, test loss: 0.210751\n",
      "Epoch 54, test loss: 0.208564\n",
      "Epoch 55, test loss: 0.207524\n",
      "Epoch 56, test loss: 0.214255\n",
      "Epoch 57, test loss: 0.207629\n",
      "Epoch 58, test loss: 0.207467\n",
      "Epoch 59, test loss: 0.207301\n",
      "Epoch 60, test loss: 0.207664\n",
      "Epoch 61, test loss: 0.208031\n",
      "Epoch 62, test loss: 0.207998\n",
      "Epoch 63, test loss: 0.206706\n",
      "Epoch 64, test loss: 0.210554\n",
      "Epoch 65, test loss: 0.207209\n",
      "Epoch 66, test loss: 0.207221\n",
      "Epoch 67, test loss: 0.208969\n",
      "Epoch 68, test loss: 0.208684\n",
      "Epoch 69, test loss: 0.208105\n",
      "Epoch 70, test loss: 0.207853\n",
      "Epoch 71, test loss: 0.207496\n",
      "Epoch 72, test loss: 0.210074\n",
      "Epoch 73, test loss: 0.207413\n",
      "Epoch 74, test loss: 0.207365\n",
      "Epoch 75, test loss: 0.208660\n",
      "Epoch 76, test loss: 0.206765\n",
      "Epoch 77, test loss: 0.209379\n",
      "Epoch 78, test loss: 0.210140\n",
      "Epoch 79, test loss: 0.212202\n",
      "Epoch 80, test loss: 0.206873\n",
      "Epoch 81, test loss: 0.216149\n",
      "Epoch 82, test loss: 0.208954\n",
      "Epoch 83, test loss: 0.208008\n",
      "Epoch 84, test loss: 0.207362\n",
      "Epoch 85, test loss: 0.208316\n",
      "Epoch 86, test loss: 0.207096\n",
      "Epoch 87, test loss: 0.209308\n",
      "Epoch 88, test loss: 0.212215\n",
      "Epoch 89, test loss: 0.207205\n",
      "Epoch 90, test loss: 0.207131\n",
      "Epoch 91, test loss: 0.212002\n",
      "Epoch 92, test loss: 0.209132\n",
      "Epoch 93, test loss: 0.207752\n",
      "Epoch 94, test loss: 0.206334\n",
      "Epoch 95, test loss: 0.206661\n",
      "Epoch 96, test loss: 0.206779\n",
      "Epoch 97, test loss: 0.206800\n",
      "Epoch 98, test loss: 0.207652\n",
      "Epoch 99, test loss: 0.207058\n",
      "Epoch 100, test loss: 0.209883\n",
      "Epoch 101, test loss: 0.206554\n",
      "Epoch 102, test loss: 0.206729\n",
      "Epoch 103, test loss: 0.213111\n",
      "Epoch 104, test loss: 0.207381\n",
      "Epoch 105, test loss: 0.207561\n",
      "Epoch 106, test loss: 0.206577\n",
      "Epoch 107, test loss: 0.206832\n",
      "Epoch 108, test loss: 0.208274\n",
      "Epoch 109, test loss: 0.205988\n",
      "Epoch 110, test loss: 0.208673\n",
      "Epoch 111, test loss: 0.207040\n",
      "Epoch 112, test loss: 0.208134\n",
      "Epoch 113, test loss: 0.208211\n",
      "Epoch 114, test loss: 0.206830\n",
      "Epoch 115, test loss: 0.206907\n",
      "Epoch 116, test loss: 0.206232\n",
      "Epoch 117, test loss: 0.207146\n",
      "Epoch 118, test loss: 0.206481\n",
      "Epoch 119, test loss: 0.209406\n",
      "Epoch 120, test loss: 0.206418\n",
      "Epoch 121, test loss: 0.206176\n",
      "Epoch 122, test loss: 0.211924\n",
      "Epoch 123, test loss: 0.206467\n",
      "Epoch 124, test loss: 0.205511\n",
      "Epoch 125, test loss: 0.205767\n",
      "Epoch 126, test loss: 0.205722\n",
      "Epoch 127, test loss: 0.213963\n",
      "Epoch 128, test loss: 0.207354\n",
      "Epoch 129, test loss: 0.209227\n",
      "Epoch 130, test loss: 0.205198\n",
      "Epoch 131, test loss: 0.206460\n",
      "Epoch 132, test loss: 0.207478\n",
      "Epoch 133, test loss: 0.206342\n",
      "Epoch 134, test loss: 0.206221\n",
      "Epoch 135, test loss: 0.208239\n",
      "Epoch 136, test loss: 0.207340\n",
      "Epoch 137, test loss: 0.210180\n",
      "Epoch 138, test loss: 0.208122\n",
      "Epoch 139, test loss: 0.205030\n",
      "Epoch 140, test loss: 0.205898\n",
      "Epoch 141, test loss: 0.207665\n",
      "Epoch 142, test loss: 0.207292\n",
      "Epoch 143, test loss: 0.205617\n",
      "Epoch 144, test loss: 0.205817\n",
      "Epoch 145, test loss: 0.205433\n",
      "Epoch 146, test loss: 0.207427\n",
      "Epoch 147, test loss: 0.206147\n",
      "Epoch 148, test loss: 0.205967\n",
      "Epoch 149, test loss: 0.205634\n",
      "Epoch 150, test loss: 0.205981\n",
      "Epoch 151, test loss: 0.205499\n",
      "Epoch 152, test loss: 0.207975\n",
      "Epoch 153, test loss: 0.205528\n",
      "Epoch 154, test loss: 0.205267\n",
      "Epoch 155, test loss: 0.206247\n",
      "Epoch 156, test loss: 0.205439\n",
      "Epoch 157, test loss: 0.205717\n",
      "Epoch 158, test loss: 0.206051\n",
      "Epoch 159, test loss: 0.205260\n",
      "Epoch 160, test loss: 0.205490\n",
      "Epoch 161, test loss: 0.205362\n",
      "Epoch 162, test loss: 0.206871\n",
      "Epoch 163, test loss: 0.205196\n",
      "Epoch 164, test loss: 0.208179\n",
      "Epoch 165, test loss: 0.207145\n",
      "Epoch 166, test loss: 0.205168\n",
      "Epoch 167, test loss: 0.206991\n",
      "Epoch 168, test loss: 0.205500\n",
      "Epoch 169, test loss: 0.205184\n",
      "Epoch 170, test loss: 0.206378\n",
      "Epoch 171, test loss: 0.207200\n",
      "Epoch 172, test loss: 0.204726\n",
      "Epoch 173, test loss: 0.205600\n",
      "Epoch 174, test loss: 0.206305\n",
      "Epoch 175, test loss: 0.205742\n",
      "Epoch 176, test loss: 0.205227\n",
      "Epoch 177, test loss: 0.204808\n",
      "Epoch 178, test loss: 0.212655\n",
      "Epoch 179, test loss: 0.205157\n",
      "Epoch 180, test loss: 0.206642\n",
      "Epoch 181, test loss: 0.209155\n",
      "Epoch 182, test loss: 0.205331\n",
      "Epoch 183, test loss: 0.206702\n",
      "Epoch 184, test loss: 0.213174\n",
      "Epoch 185, test loss: 0.205030\n",
      "Epoch 186, test loss: 0.206509\n",
      "Epoch 187, test loss: 0.204912\n",
      "Epoch 188, test loss: 0.205122\n",
      "Epoch 189, test loss: 0.207208\n",
      "Epoch 190, test loss: 0.205716\n",
      "Epoch 191, test loss: 0.207814\n",
      "Epoch 192, test loss: 0.205804\n",
      "Epoch 193, test loss: 0.205705\n",
      "Epoch 194, test loss: 0.206191\n",
      "Epoch 195, test loss: 0.206640\n",
      "Epoch 196, test loss: 0.206317\n",
      "Epoch 197, test loss: 0.205058\n",
      "Epoch 198, test loss: 0.204551\n",
      "Epoch 199, test loss: 0.206680\n",
      "Epoch 200, test loss: 0.206415\n",
      "Epoch 201, test loss: 0.205006\n",
      "Epoch 202, test loss: 0.206182\n",
      "Epoch 203, test loss: 0.205829\n",
      "Epoch 204, test loss: 0.209660\n",
      "Epoch 205, test loss: 0.205885\n",
      "Epoch 206, test loss: 0.208583\n",
      "Epoch 207, test loss: 0.206192\n",
      "Epoch 208, test loss: 0.204881\n",
      "Epoch 209, test loss: 0.206146\n",
      "Epoch 210, test loss: 0.207073\n",
      "Epoch 211, test loss: 0.208773\n",
      "Epoch 212, test loss: 0.211188\n",
      "Epoch 213, test loss: 0.208409\n",
      "Epoch 214, test loss: 0.207920\n",
      "Epoch 215, test loss: 0.205679\n",
      "Epoch 216, test loss: 0.210660\n",
      "Epoch 217, test loss: 0.205182\n",
      "Epoch 218, test loss: 0.205318\n",
      "Epoch 219, test loss: 0.205645\n",
      "Epoch 220, test loss: 0.205364\n",
      "Epoch 221, test loss: 0.211243\n",
      "Epoch 222, test loss: 0.206945\n",
      "Epoch 223, test loss: 0.205022\n",
      "Epoch 224, test loss: 0.207458\n",
      "Epoch 225, test loss: 0.206591\n",
      "Epoch 226, test loss: 0.206904\n",
      "Epoch 227, test loss: 0.207154\n",
      "Epoch 228, test loss: 0.205992\n",
      "Epoch 229, test loss: 0.204936\n",
      "Epoch 230, test loss: 0.204885\n",
      "Epoch 231, test loss: 0.204936\n",
      "Epoch 232, test loss: 0.205927\n",
      "Epoch 233, test loss: 0.204607\n",
      "Epoch 234, test loss: 0.207289\n",
      "Epoch 235, test loss: 0.205249\n",
      "Epoch 236, test loss: 0.215631\n",
      "Epoch 237, test loss: 0.205564\n",
      "Epoch 238, test loss: 0.205857\n",
      "Epoch 239, test loss: 0.205451\n",
      "Epoch 240, test loss: 0.210942\n",
      "Epoch 241, test loss: 0.206516\n",
      "Epoch 242, test loss: 0.205209\n",
      "Epoch 243, test loss: 0.207181\n",
      "Epoch 244, test loss: 0.208523\n",
      "Epoch 245, test loss: 0.206412\n",
      "Epoch 246, test loss: 0.205756\n",
      "Epoch 247, test loss: 0.205368\n",
      "Epoch 248, test loss: 0.205606\n",
      "Epoch 249, test loss: 0.206652\n",
      "Epoch 250, test loss: 0.205669\n",
      "Epoch 251, test loss: 0.205533\n",
      "Epoch 252, test loss: 0.207555\n",
      "Epoch 253, test loss: 0.206577\n",
      "Epoch 254, test loss: 0.210781\n",
      "Epoch 255, test loss: 0.205317\n",
      "Epoch 256, test loss: 0.205458\n",
      "Epoch 257, test loss: 0.205445\n",
      "Epoch 258, test loss: 0.207501\n",
      "Epoch 259, test loss: 0.208250\n",
      "Epoch 260, test loss: 0.206410\n",
      "Epoch 261, test loss: 0.204776\n",
      "Epoch 262, test loss: 0.204896\n",
      "Epoch 263, test loss: 0.204873\n",
      "Epoch 264, test loss: 0.205209\n",
      "Epoch 265, test loss: 0.205196\n",
      "Epoch 266, test loss: 0.206801\n",
      "Epoch 267, test loss: 0.206966\n",
      "Epoch 268, test loss: 0.205874\n",
      "Epoch 269, test loss: 0.210279\n",
      "Epoch 270, test loss: 0.204840\n",
      "Epoch 271, test loss: 0.205454\n",
      "Epoch 272, test loss: 0.205864\n",
      "Epoch 273, test loss: 0.206666\n",
      "Epoch 274, test loss: 0.205019\n",
      "Epoch 275, test loss: 0.205115\n",
      "Epoch 276, test loss: 0.205599\n",
      "Epoch 277, test loss: 0.206370\n",
      "Epoch 278, test loss: 0.205910\n",
      "Epoch 279, test loss: 0.205353\n",
      "Epoch 280, test loss: 0.204886\n",
      "Epoch 281, test loss: 0.204814\n",
      "Epoch 282, test loss: 0.205176\n",
      "Epoch 283, test loss: 0.207288\n",
      "Epoch 284, test loss: 0.207480\n",
      "Epoch 285, test loss: 0.207989\n",
      "Epoch 286, test loss: 0.205442\n",
      "Epoch 287, test loss: 0.204861\n",
      "Epoch 288, test loss: 0.204969\n",
      "Epoch 289, test loss: 0.204957\n",
      "Epoch 290, test loss: 0.205344\n",
      "Epoch 291, test loss: 0.204771\n",
      "Epoch 292, test loss: 0.204321\n",
      "Epoch 293, test loss: 0.212156\n",
      "Epoch 294, test loss: 0.208401\n",
      "Epoch 295, test loss: 0.205071\n",
      "Epoch 296, test loss: 0.205362\n",
      "Epoch 297, test loss: 0.205966\n",
      "Epoch 298, test loss: 0.204444\n",
      "Epoch 299, test loss: 0.205378\n",
      "Epoch 300, test loss: 0.206148\n",
      "Epoch 301, test loss: 0.205244\n",
      "Epoch 302, test loss: 0.205650\n",
      "Epoch 303, test loss: 0.205335\n",
      "Epoch 304, test loss: 0.206993\n",
      "Epoch 305, test loss: 0.205486\n",
      "Epoch 306, test loss: 0.204342\n",
      "Epoch 307, test loss: 0.205090\n",
      "Epoch 308, test loss: 0.205298\n",
      "Epoch 309, test loss: 0.206374\n",
      "Epoch 310, test loss: 0.204603\n",
      "Epoch 311, test loss: 0.209290\n",
      "Epoch 312, test loss: 0.205978\n",
      "Epoch 313, test loss: 0.204601\n",
      "Epoch 314, test loss: 0.204990\n",
      "Epoch 315, test loss: 0.206181\n",
      "Epoch 316, test loss: 0.204813\n",
      "Epoch 317, test loss: 0.204253\n",
      "Epoch 318, test loss: 0.204365\n",
      "Epoch 319, test loss: 0.205322\n",
      "Epoch 320, test loss: 0.204493\n",
      "Epoch 321, test loss: 0.204747\n",
      "Epoch 322, test loss: 0.205273\n",
      "Epoch 323, test loss: 0.204034\n",
      "Epoch 324, test loss: 0.205498\n",
      "Epoch 325, test loss: 0.206366\n",
      "Epoch 326, test loss: 0.205472\n",
      "Epoch 327, test loss: 0.207731\n",
      "Epoch 328, test loss: 0.206867\n",
      "Epoch 329, test loss: 0.205002\n",
      "Epoch 330, test loss: 0.204122\n",
      "Epoch 331, test loss: 0.204052\n",
      "Epoch 332, test loss: 0.207215\n",
      "Epoch 333, test loss: 0.206155\n",
      "Epoch 334, test loss: 0.208644\n",
      "Epoch 335, test loss: 0.205192\n",
      "Epoch 336, test loss: 0.205475\n",
      "Epoch 337, test loss: 0.208072\n",
      "Epoch 338, test loss: 0.204719\n",
      "Epoch 339, test loss: 0.205045\n",
      "Epoch 340, test loss: 0.205334\n",
      "Epoch 341, test loss: 0.205495\n",
      "Epoch 342, test loss: 0.205832\n",
      "Epoch 343, test loss: 0.206149\n",
      "Epoch 344, test loss: 0.203829\n",
      "Epoch 345, test loss: 0.206180\n",
      "Epoch 346, test loss: 0.204716\n",
      "Epoch 347, test loss: 0.204004\n",
      "Epoch 348, test loss: 0.205463\n",
      "Epoch 349, test loss: 0.204471\n",
      "Epoch 350, test loss: 0.204632\n",
      "Epoch 351, test loss: 0.204208\n",
      "Epoch 352, test loss: 0.206372\n",
      "Epoch 353, test loss: 0.204304\n",
      "Epoch 354, test loss: 0.205439\n",
      "Epoch 355, test loss: 0.204816\n",
      "Epoch 356, test loss: 0.204271\n",
      "Epoch 357, test loss: 0.204134\n",
      "Epoch 358, test loss: 0.205668\n",
      "Epoch 359, test loss: 0.204709\n",
      "Epoch 360, test loss: 0.205714\n",
      "Epoch 361, test loss: 0.205723\n",
      "Epoch 362, test loss: 0.204847\n",
      "Epoch 363, test loss: 0.209598\n",
      "Epoch 364, test loss: 0.204733\n",
      "Epoch 365, test loss: 0.204539\n",
      "Epoch 366, test loss: 0.205006\n",
      "Epoch 367, test loss: 0.206629\n",
      "Epoch 368, test loss: 0.207590\n",
      "Epoch 369, test loss: 0.204976\n",
      "Epoch 370, test loss: 0.205604\n",
      "Epoch 371, test loss: 0.205699\n",
      "Epoch 372, test loss: 0.206750\n",
      "Epoch 373, test loss: 0.206234\n",
      "Epoch 374, test loss: 0.204360\n",
      "Epoch 375, test loss: 0.205387\n",
      "Epoch 376, test loss: 0.208847\n",
      "Epoch 377, test loss: 0.206460\n",
      "Epoch 378, test loss: 0.208002\n",
      "Epoch 379, test loss: 0.204469\n",
      "Epoch 380, test loss: 0.205109\n",
      "Epoch 381, test loss: 0.206893\n",
      "Epoch 382, test loss: 0.204857\n",
      "Epoch 383, test loss: 0.205238\n",
      "Epoch 384, test loss: 0.205890\n",
      "Epoch 385, test loss: 0.205989\n",
      "Epoch 386, test loss: 0.205110\n",
      "Epoch 387, test loss: 0.204344\n",
      "Epoch 388, test loss: 0.208766\n",
      "Epoch 389, test loss: 0.213930\n",
      "Epoch 390, test loss: 0.205323\n",
      "Epoch 391, test loss: 0.205527\n",
      "Epoch 392, test loss: 0.209442\n",
      "Epoch 393, test loss: 0.205006\n",
      "Epoch 394, test loss: 0.205611\n",
      "Epoch 395, test loss: 0.205750\n",
      "Epoch 396, test loss: 0.205767\n",
      "Epoch 397, test loss: 0.204728\n",
      "Epoch 398, test loss: 0.204993\n",
      "Epoch 399, test loss: 0.204786\n",
      "Epoch 400, test loss: 0.205634\n",
      "Epoch 401, test loss: 0.204511\n",
      "Epoch 402, test loss: 0.205400\n",
      "Epoch 403, test loss: 0.205246\n",
      "Epoch 404, test loss: 0.204706\n",
      "Epoch 405, test loss: 0.205538\n",
      "Epoch 406, test loss: 0.206514\n",
      "Epoch 407, test loss: 0.205659\n",
      "Epoch 408, test loss: 0.212542\n",
      "Epoch 409, test loss: 0.205494\n",
      "Epoch 410, test loss: 0.205214\n",
      "Epoch 411, test loss: 0.205574\n",
      "Epoch 412, test loss: 0.205134\n",
      "Epoch 413, test loss: 0.207856\n",
      "Epoch 414, test loss: 0.204830\n",
      "Epoch 415, test loss: 0.207163\n",
      "Epoch 416, test loss: 0.210681\n",
      "Epoch 417, test loss: 0.209190\n",
      "Epoch 418, test loss: 0.205428\n",
      "Epoch 419, test loss: 0.205024\n",
      "Epoch 420, test loss: 0.204914\n",
      "Epoch 421, test loss: 0.209239\n",
      "Epoch 422, test loss: 0.210365\n",
      "Epoch 423, test loss: 0.205044\n",
      "Epoch 424, test loss: 0.205150\n",
      "Epoch 425, test loss: 0.205532\n",
      "Epoch 426, test loss: 0.206169\n",
      "Epoch 427, test loss: 0.207448\n",
      "Epoch 428, test loss: 0.207890\n",
      "Epoch 429, test loss: 0.206582\n",
      "Epoch 430, test loss: 0.209164\n",
      "Epoch 431, test loss: 0.206402\n",
      "Epoch 432, test loss: 0.205635\n",
      "Epoch 433, test loss: 0.206995\n",
      "Epoch 434, test loss: 0.206828\n",
      "Epoch 435, test loss: 0.207155\n",
      "Epoch 436, test loss: 0.206101\n",
      "Epoch 437, test loss: 0.205504\n",
      "Epoch 438, test loss: 0.205810\n",
      "Epoch 439, test loss: 0.209063\n",
      "Epoch 440, test loss: 0.205754\n",
      "Epoch 441, test loss: 0.207362\n",
      "Epoch 442, test loss: 0.205640\n",
      "Epoch 443, test loss: 0.204477\n",
      "Epoch 444, test loss: 0.206155\n",
      "Epoch 445, test loss: 0.205704\n",
      "Epoch 446, test loss: 0.204606\n",
      "Epoch 447, test loss: 0.204967\n",
      "Epoch 448, test loss: 0.205263\n",
      "Epoch 449, test loss: 0.207786\n",
      "Pretrain data: 19086006.0\n",
      "Building dataset, requesting data from 0 to 782\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 8121/106107\n",
      "Found 782 continuous time series\n",
      "Data shape: (114230, 6), Train/test: 114228/2\n",
      "Train test ratio: 57114.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1784\n",
      "Epoch 0, train loss: 0.201003\n",
      "Epoch 1, train loss: 0.192269\n",
      "Epoch 2, train loss: 0.186149\n",
      "Epoch 3, train loss: 0.187402\n",
      "Epoch 4, train loss: 0.169032\n",
      "Epoch 5, train loss: 0.228791\n",
      "Epoch 6, train loss: 0.163573\n",
      "Epoch 7, train loss: 0.283668\n",
      "Epoch 8, train loss: 0.232940\n",
      "Epoch 9, train loss: 0.205577\n",
      "Epoch 10, train loss: 0.189467\n",
      "Epoch 11, train loss: 0.216275\n",
      "Epoch 12, train loss: 0.209673\n",
      "Epoch 13, train loss: 0.229804\n",
      "Epoch 14, train loss: 0.190258\n",
      "Epoch 15, train loss: 0.247573\n",
      "Epoch 16, train loss: 0.214747\n",
      "Epoch 17, train loss: 0.222459\n",
      "Epoch 18, train loss: 0.246114\n",
      "Epoch 19, train loss: 0.182903\n",
      "Epoch 20, train loss: 0.231385\n",
      "Epoch 21, train loss: 0.168577\n",
      "Epoch 22, train loss: 0.187222\n",
      "Epoch 23, train loss: 0.217074\n",
      "Epoch 24, train loss: 0.217797\n",
      "Epoch 25, train loss: 0.205000\n",
      "Epoch 26, train loss: 0.216802\n",
      "Epoch 27, train loss: 0.215080\n",
      "Epoch 28, train loss: 0.146767\n",
      "Epoch 29, train loss: 0.173999\n",
      "Epoch 30, train loss: 0.186712\n",
      "Epoch 31, train loss: 0.178521\n",
      "Epoch 32, train loss: 0.188040\n",
      "Epoch 33, train loss: 0.200025\n",
      "Epoch 34, train loss: 0.197559\n",
      "Epoch 35, train loss: 0.190948\n",
      "Epoch 36, train loss: 0.222644\n",
      "Epoch 37, train loss: 0.193629\n",
      "Epoch 38, train loss: 0.176114\n",
      "Epoch 39, train loss: 0.178885\n",
      "Epoch 40, train loss: 0.187275\n",
      "Epoch 41, train loss: 0.251238\n",
      "Epoch 42, train loss: 0.212815\n",
      "Epoch 43, train loss: 0.167986\n",
      "Epoch 44, train loss: 0.259170\n",
      "Epoch 45, train loss: 0.189133\n",
      "Epoch 46, train loss: 0.226989\n",
      "Epoch 47, train loss: 0.229345\n",
      "Epoch 48, train loss: 0.183428\n",
      "Epoch 49, train loss: 0.215658\n",
      "Epoch 50, train loss: 0.194987\n",
      "Epoch 51, train loss: 0.184507\n",
      "Epoch 52, train loss: 0.185314\n",
      "Epoch 53, train loss: 0.216919\n",
      "Epoch 54, train loss: 0.218726\n",
      "Epoch 55, train loss: 0.231649\n",
      "Epoch 56, train loss: 0.254052\n",
      "Epoch 57, train loss: 0.189381\n",
      "Epoch 58, train loss: 0.210794\n",
      "Epoch 59, train loss: 0.171331\n",
      "Epoch 60, train loss: 0.259041\n",
      "Epoch 61, train loss: 0.185697\n",
      "Epoch 62, train loss: 0.212026\n",
      "Epoch 63, train loss: 0.192954\n",
      "Epoch 64, train loss: 0.207413\n",
      "Epoch 65, train loss: 0.179127\n",
      "Epoch 66, train loss: 0.171925\n",
      "Epoch 67, train loss: 0.157626\n",
      "Epoch 68, train loss: 0.173057\n",
      "Epoch 69, train loss: 0.218899\n",
      "Epoch 70, train loss: 0.158349\n",
      "Epoch 71, train loss: 0.242369\n",
      "Epoch 72, train loss: 0.249370\n",
      "Epoch 73, train loss: 0.203295\n",
      "Epoch 74, train loss: 0.189268\n",
      "Epoch 75, train loss: 0.206127\n",
      "Epoch 76, train loss: 0.201883\n",
      "Epoch 77, train loss: 0.164401\n",
      "Epoch 78, train loss: 0.289774\n",
      "Epoch 79, train loss: 0.183049\n",
      "Epoch 80, train loss: 0.147357\n",
      "Epoch 81, train loss: 0.160973\n",
      "Epoch 82, train loss: 0.158531\n",
      "Epoch 83, train loss: 0.209379\n",
      "Epoch 84, train loss: 0.236623\n",
      "Epoch 85, train loss: 0.192470\n",
      "Epoch 86, train loss: 0.155968\n",
      "Epoch 87, train loss: 0.183534\n",
      "Epoch 88, train loss: 0.177814\n",
      "Epoch 89, train loss: 0.217692\n",
      "Epoch 90, train loss: 0.177656\n",
      "Epoch 91, train loss: 0.213358\n",
      "Epoch 92, train loss: 0.225672\n",
      "Epoch 93, train loss: 0.255459\n",
      "Epoch 94, train loss: 0.197754\n",
      "Epoch 95, train loss: 0.189013\n",
      "Epoch 96, train loss: 0.209716\n",
      "Epoch 97, train loss: 0.205314\n",
      "Epoch 98, train loss: 0.190164\n",
      "Epoch 99, train loss: 0.212272\n",
      "Epoch 100, train loss: 0.209665\n",
      "Epoch 101, train loss: 0.212299\n",
      "Epoch 102, train loss: 0.208503\n",
      "Epoch 103, train loss: 0.214193\n",
      "Epoch 104, train loss: 0.258548\n",
      "Epoch 105, train loss: 0.216194\n",
      "Epoch 106, train loss: 0.174935\n",
      "Epoch 107, train loss: 0.184071\n",
      "Epoch 108, train loss: 0.207126\n",
      "Epoch 109, train loss: 0.205554\n",
      "Epoch 110, train loss: 0.231565\n",
      "Epoch 111, train loss: 0.198441\n",
      "Epoch 112, train loss: 0.209780\n",
      "Epoch 113, train loss: 0.178691\n",
      "Epoch 114, train loss: 0.178370\n",
      "Epoch 115, train loss: 0.174672\n",
      "Epoch 116, train loss: 0.171872\n",
      "Epoch 117, train loss: 0.193518\n",
      "Epoch 118, train loss: 0.203457\n",
      "Epoch 119, train loss: 0.195351\n",
      "Epoch 120, train loss: 0.188114\n",
      "Epoch 121, train loss: 0.160685\n",
      "Epoch 122, train loss: 0.158951\n",
      "Epoch 123, train loss: 0.199176\n",
      "Epoch 124, train loss: 0.134803\n",
      "Epoch 125, train loss: 0.222875\n",
      "Epoch 126, train loss: 0.183257\n",
      "Epoch 127, train loss: 0.166212\n",
      "Epoch 128, train loss: 0.259123\n",
      "Epoch 129, train loss: 0.238868\n",
      "Epoch 130, train loss: 0.181889\n",
      "Epoch 131, train loss: 0.214214\n",
      "Epoch 132, train loss: 0.217922\n",
      "Epoch 133, train loss: 0.181265\n",
      "Epoch 134, train loss: 0.259346\n",
      "Epoch 135, train loss: 0.234529\n",
      "Epoch 136, train loss: 0.160864\n",
      "Epoch 137, train loss: 0.194569\n",
      "Epoch 138, train loss: 0.198625\n",
      "Epoch 139, train loss: 0.228111\n",
      "Epoch 140, train loss: 0.207354\n",
      "Epoch 141, train loss: 0.144976\n",
      "Epoch 142, train loss: 0.174906\n",
      "Epoch 143, train loss: 0.155857\n",
      "Epoch 144, train loss: 0.197969\n",
      "Epoch 145, train loss: 0.221236\n",
      "Epoch 146, train loss: 0.206140\n",
      "Epoch 147, train loss: 0.170200\n",
      "Epoch 148, train loss: 0.172588\n",
      "Epoch 149, train loss: 0.237645\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "x here is\n",
      "[[243. 253. 262. 269. 269. 257.]\n",
      " [253. 262. 269. 269. 257. 258.]\n",
      " [262. 269. 269. 257. 258. 267.]\n",
      " ...\n",
      " [ 41.  49.  55.  53.  66.  90.]\n",
      " [ 49.  55.  53.  66.  90.  87.]\n",
      " [ 55.  53.  66.  90.  87.  86.]]\n",
      "y here is\n",
      "[[299. 299. 299. 299. 299. 299.]\n",
      " [300. 300. 300. 300. 300. 300.]\n",
      " [309. 309. 309. 309. 309. 309.]\n",
      " ...\n",
      " [ 72.  72.  72.  72.  72.  72.]\n",
      " [ 78.  78.  78.  78.  78.  78.]\n",
      " [ 79.  79.  79.  79.  79.  79.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (2489, 6), Train/test: 1/2488\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 60 segments\n",
      "Building dataset, requesting data from 0 to 60\n",
      "x here is\n",
      "[[ 48.  48.  53.  63.  69.  74.]\n",
      " [ 48.  53.  63.  69.  74.  77.]\n",
      " [ 53.  63.  69.  74.  77.  70.]\n",
      " ...\n",
      " [173. 185. 199. 212. 223. 227.]\n",
      " [185. 199. 212. 223. 227. 223.]\n",
      " [199. 212. 223. 227. 223. 215.]]\n",
      "y here is\n",
      "[[ 44.  44.  44.  44.  44.  44.]\n",
      " [ 44.  44.  44.  44.  44.  44.]\n",
      " [ 51.  51.  51.  51.  51.  51.]\n",
      " ...\n",
      " [213. 213. 213. 213. 213. 213.]\n",
      " [224. 224. 224. 224. 224. 224.]\n",
      " [235. 235. 235. 235. 235. 235.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 164/11332\n",
      "Found 60 continuous time series\n",
      "Data shape: (11498, 6), Train/test: 11496/2\n",
      "Train test ratio: 5748.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7EFDCFF40>\n",
      "Epoch 0, test loss: 0.215879\n",
      "Epoch 1, test loss: 0.217172\n",
      "Epoch 2, test loss: 0.216197\n",
      "Epoch 3, test loss: 0.217252\n",
      "Epoch 4, test loss: 0.215776\n",
      "Epoch 5, test loss: 0.220026\n",
      "Epoch 6, test loss: 0.216356\n",
      "Epoch 7, test loss: 0.216765\n",
      "Epoch 8, test loss: 0.217231\n",
      "Epoch 9, test loss: 0.217040\n",
      "Epoch 10, test loss: 0.218780\n",
      "Epoch 11, test loss: 0.217703\n",
      "Epoch 12, test loss: 0.217067\n",
      "Epoch 13, test loss: 0.217630\n",
      "Epoch 14, test loss: 0.217572\n",
      "Epoch 15, test loss: 0.219605\n",
      "Epoch 16, test loss: 0.218330\n",
      "Epoch 17, test loss: 0.217556\n",
      "Epoch 18, test loss: 0.218825\n",
      "Epoch 19, test loss: 0.217280\n",
      "Epoch 20, test loss: 0.217383\n",
      "Epoch 21, test loss: 0.216948\n",
      "Epoch 22, test loss: 0.218347\n",
      "Epoch 23, test loss: 0.217566\n",
      "Epoch 24, test loss: 0.219228\n",
      "Epoch 25, test loss: 0.219281\n",
      "Epoch 26, test loss: 0.221006\n",
      "Epoch 27, test loss: 0.219137\n",
      "Epoch 28, test loss: 0.222191\n",
      "Epoch 29, test loss: 0.217473\n",
      "Epoch 30, test loss: 0.221130\n",
      "Epoch 31, test loss: 0.217244\n",
      "Epoch 32, test loss: 0.216852\n",
      "Epoch 33, test loss: 0.217771\n",
      "Epoch 34, test loss: 0.217290\n",
      "Epoch 35, test loss: 0.218637\n",
      "Epoch 36, test loss: 0.217260\n",
      "Epoch 37, test loss: 0.217545\n",
      "Epoch 38, test loss: 0.223386\n",
      "Epoch 39, test loss: 0.216906\n",
      "Epoch 40, test loss: 0.217643\n",
      "Epoch 41, test loss: 0.217884\n",
      "Epoch 42, test loss: 0.217597\n",
      "Epoch 43, test loss: 0.217488\n",
      "Epoch 44, test loss: 0.217209\n",
      "Epoch 45, test loss: 0.216700\n",
      "Epoch 46, test loss: 0.218450\n",
      "Epoch 47, test loss: 0.216410\n",
      "Epoch 48, test loss: 0.221469\n",
      "Epoch 49, test loss: 0.217376\n",
      "Epoch 50, test loss: 0.219174\n",
      "Epoch 51, test loss: 0.225654\n",
      "Epoch 52, test loss: 0.219557\n",
      "Epoch 53, test loss: 0.220392\n",
      "Epoch 54, test loss: 0.216649\n",
      "Epoch 55, test loss: 0.217624\n",
      "Epoch 56, test loss: 0.217476\n",
      "Epoch 57, test loss: 0.218033\n",
      "Epoch 58, test loss: 0.217483\n",
      "Epoch 59, test loss: 0.218437\n",
      "Epoch 60, test loss: 0.222721\n",
      "Epoch 61, test loss: 0.217783\n",
      "Epoch 62, test loss: 0.217757\n",
      "Epoch 63, test loss: 0.219445\n",
      "Epoch 64, test loss: 0.217807\n",
      "Epoch 65, test loss: 0.217266\n",
      "Epoch 66, test loss: 0.217349\n",
      "Epoch 67, test loss: 0.223278\n",
      "Epoch 68, test loss: 0.224270\n",
      "Epoch 69, test loss: 0.219023\n",
      "Epoch 70, test loss: 0.216691\n",
      "Epoch 71, test loss: 0.218562\n",
      "Epoch 72, test loss: 0.218692\n",
      "Epoch 73, test loss: 0.217644\n",
      "Epoch 74, test loss: 0.216243\n",
      "Epoch 75, test loss: 0.216333\n",
      "Epoch 76, test loss: 0.218645\n",
      "Epoch 77, test loss: 0.219512\n",
      "Epoch 78, test loss: 0.216966\n",
      "Epoch 79, test loss: 0.216975\n",
      "Epoch 80, test loss: 0.219274\n",
      "Epoch 81, test loss: 0.219638\n",
      "Epoch 82, test loss: 0.223684\n",
      "Epoch 83, test loss: 0.217257\n",
      "Epoch 84, test loss: 0.218509\n",
      "Epoch 85, test loss: 0.217300\n",
      "Epoch 86, test loss: 0.218463\n",
      "Epoch 87, test loss: 0.219503\n",
      "Epoch 88, test loss: 0.216658\n",
      "Epoch 89, test loss: 0.218865\n",
      "Epoch 90, test loss: 0.217328\n",
      "Epoch 91, test loss: 0.216580\n",
      "Epoch 92, test loss: 0.218063\n",
      "Epoch 93, test loss: 0.218055\n",
      "Epoch 94, test loss: 0.217436\n",
      "Epoch 95, test loss: 0.219468\n",
      "Epoch 96, test loss: 0.217868\n",
      "Epoch 97, test loss: 0.217639\n",
      "Epoch 98, test loss: 0.217975\n",
      "Epoch 99, test loss: 0.216544\n",
      "Epoch 100, test loss: 0.218040\n",
      "Epoch 101, test loss: 0.217401\n",
      "Epoch 102, test loss: 0.216960\n",
      "Epoch 103, test loss: 0.217078\n",
      "Epoch 104, test loss: 0.218339\n",
      "Epoch 105, test loss: 0.216771\n",
      "Epoch 106, test loss: 0.216956\n",
      "Epoch 107, test loss: 0.217559\n",
      "Epoch 108, test loss: 0.222547\n",
      "Epoch 109, test loss: 0.216762\n",
      "Epoch 110, test loss: 0.218247\n",
      "Epoch 111, test loss: 0.216852\n",
      "Epoch 112, test loss: 0.222027\n",
      "Epoch 113, test loss: 0.217242\n",
      "Epoch 114, test loss: 0.217899\n",
      "Epoch 115, test loss: 0.217054\n",
      "Epoch 116, test loss: 0.217323\n",
      "Epoch 117, test loss: 0.217712\n",
      "Epoch 118, test loss: 0.216930\n",
      "Epoch 119, test loss: 0.218378\n",
      "Epoch 120, test loss: 0.217330\n",
      "Epoch 121, test loss: 0.222168\n",
      "Epoch 122, test loss: 0.220184\n",
      "Epoch 123, test loss: 0.217073\n",
      "Epoch 124, test loss: 0.216981\n",
      "Epoch 125, test loss: 0.216992\n",
      "Epoch 126, test loss: 0.216330\n",
      "Epoch 127, test loss: 0.216817\n",
      "Epoch 128, test loss: 0.218175\n",
      "Epoch 129, test loss: 0.219149\n",
      "Epoch 130, test loss: 0.217844\n",
      "Epoch 131, test loss: 0.221451\n",
      "Epoch 132, test loss: 0.218617\n",
      "Epoch 133, test loss: 0.219489\n",
      "Epoch 134, test loss: 0.217499\n",
      "Epoch 135, test loss: 0.216902\n",
      "Epoch 136, test loss: 0.219407\n",
      "Epoch 137, test loss: 0.216872\n",
      "Epoch 138, test loss: 0.217190\n",
      "Epoch 139, test loss: 0.217810\n",
      "Epoch 140, test loss: 0.217644\n",
      "Epoch 141, test loss: 0.220021\n",
      "Epoch 142, test loss: 0.217952\n",
      "Epoch 143, test loss: 0.217078\n",
      "Epoch 144, test loss: 0.217029\n",
      "Epoch 145, test loss: 0.217355\n",
      "Epoch 146, test loss: 0.218146\n",
      "Epoch 147, test loss: 0.218060\n",
      "Epoch 148, test loss: 0.216815\n",
      "Epoch 149, test loss: 0.217829\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7EFDCFF40>\n",
      "Epoch 0, test loss: 0.215523\n",
      "Epoch 1, test loss: 0.215309\n",
      "Epoch 2, test loss: 0.215251\n",
      "Epoch 3, test loss: 0.216171\n",
      "Epoch 4, test loss: 0.215946\n",
      "Epoch 5, test loss: 0.215261\n",
      "Epoch 6, test loss: 0.214974\n",
      "Epoch 7, test loss: 0.215547\n",
      "Epoch 8, test loss: 0.215275\n",
      "Epoch 9, test loss: 0.216352\n",
      "Epoch 10, test loss: 0.216104\n",
      "Epoch 11, test loss: 0.220299\n",
      "Epoch 12, test loss: 0.215507\n",
      "Epoch 13, test loss: 0.214807\n",
      "Epoch 14, test loss: 0.215918\n",
      "Epoch 15, test loss: 0.214650\n",
      "Epoch 16, test loss: 0.215911\n",
      "Epoch 17, test loss: 0.219013\n",
      "Epoch 18, test loss: 0.217899\n",
      "Epoch 19, test loss: 0.215581\n",
      "Epoch 20, test loss: 0.215106\n",
      "Epoch 21, test loss: 0.215554\n",
      "Epoch 22, test loss: 0.217163\n",
      "Epoch 23, test loss: 0.215714\n",
      "Epoch 24, test loss: 0.219417\n",
      "Epoch 25, test loss: 0.215868\n",
      "Epoch 26, test loss: 0.215677\n",
      "Epoch 27, test loss: 0.216537\n",
      "Epoch 28, test loss: 0.215401\n",
      "Epoch 29, test loss: 0.216983\n",
      "Epoch 30, test loss: 0.219398\n",
      "Epoch 31, test loss: 0.215157\n",
      "Epoch 32, test loss: 0.217507\n",
      "Epoch 33, test loss: 0.216461\n",
      "Epoch 34, test loss: 0.215479\n",
      "Epoch 35, test loss: 0.215363\n",
      "Epoch 36, test loss: 0.220183\n",
      "Epoch 37, test loss: 0.214672\n",
      "Epoch 38, test loss: 0.215647\n",
      "Epoch 39, test loss: 0.217516\n",
      "Epoch 40, test loss: 0.217083\n",
      "Epoch 41, test loss: 0.219408\n",
      "Epoch 42, test loss: 0.217030\n",
      "Epoch 43, test loss: 0.221952\n",
      "Epoch 44, test loss: 0.217576\n",
      "Epoch 45, test loss: 0.215101\n",
      "Epoch 46, test loss: 0.216143\n",
      "Epoch 47, test loss: 0.215662\n",
      "Epoch 48, test loss: 0.215467\n",
      "Epoch 49, test loss: 0.215568\n",
      "Epoch 50, test loss: 0.216233\n",
      "Epoch 51, test loss: 0.220066\n",
      "Epoch 52, test loss: 0.215277\n",
      "Epoch 53, test loss: 0.216609\n",
      "Epoch 54, test loss: 0.215862\n",
      "Epoch 55, test loss: 0.218200\n",
      "Epoch 56, test loss: 0.218424\n",
      "Epoch 57, test loss: 0.215703\n",
      "Epoch 58, test loss: 0.216134\n",
      "Epoch 59, test loss: 0.217310\n",
      "Epoch 60, test loss: 0.216885\n",
      "Epoch 61, test loss: 0.217281\n",
      "Epoch 62, test loss: 0.216956\n",
      "Epoch 63, test loss: 0.216263\n",
      "Epoch 64, test loss: 0.216659\n",
      "Epoch 65, test loss: 0.225681\n",
      "Epoch 66, test loss: 0.215726\n",
      "Epoch 67, test loss: 0.216238\n",
      "Epoch 68, test loss: 0.219091\n",
      "Epoch 69, test loss: 0.215730\n",
      "Epoch 70, test loss: 0.215447\n",
      "Epoch 71, test loss: 0.216095\n",
      "Epoch 72, test loss: 0.216310\n",
      "Epoch 73, test loss: 0.215894\n",
      "Epoch 74, test loss: 0.217511\n",
      "Epoch 75, test loss: 0.215924\n",
      "Epoch 76, test loss: 0.215397\n",
      "Epoch 77, test loss: 0.217973\n",
      "Epoch 78, test loss: 0.217506\n",
      "Epoch 79, test loss: 0.215350\n",
      "Epoch 80, test loss: 0.217735\n",
      "Epoch 81, test loss: 0.215786\n",
      "Epoch 82, test loss: 0.216969\n",
      "Epoch 83, test loss: 0.217287\n",
      "Epoch 84, test loss: 0.216860\n",
      "Epoch 85, test loss: 0.221651\n",
      "Epoch 86, test loss: 0.216218\n",
      "Epoch 87, test loss: 0.217682\n",
      "Epoch 88, test loss: 0.217452\n",
      "Epoch 89, test loss: 0.216382\n",
      "Epoch 90, test loss: 0.215677\n",
      "Epoch 91, test loss: 0.218012\n",
      "Epoch 92, test loss: 0.219035\n",
      "Epoch 93, test loss: 0.218713\n",
      "Epoch 94, test loss: 0.216182\n",
      "Epoch 95, test loss: 0.219388\n",
      "Epoch 96, test loss: 0.216934\n",
      "Epoch 97, test loss: 0.217372\n",
      "Epoch 98, test loss: 0.219650\n",
      "Epoch 99, test loss: 0.215807\n",
      "Epoch 100, test loss: 0.216874\n",
      "Epoch 101, test loss: 0.216424\n",
      "Epoch 102, test loss: 0.216673\n",
      "Epoch 103, test loss: 0.220820\n",
      "Epoch 104, test loss: 0.219033\n",
      "Epoch 105, test loss: 0.217523\n",
      "Epoch 106, test loss: 0.217235\n",
      "Epoch 107, test loss: 0.217121\n",
      "Epoch 108, test loss: 0.216848\n",
      "Epoch 109, test loss: 0.218177\n",
      "Epoch 110, test loss: 0.216524\n",
      "Epoch 111, test loss: 0.219010\n",
      "Epoch 112, test loss: 0.220855\n",
      "Epoch 113, test loss: 0.215671\n",
      "Epoch 114, test loss: 0.216894\n",
      "Epoch 115, test loss: 0.218986\n",
      "Epoch 116, test loss: 0.216770\n",
      "Epoch 117, test loss: 0.216329\n",
      "Epoch 118, test loss: 0.219752\n",
      "Epoch 119, test loss: 0.218879\n",
      "Epoch 120, test loss: 0.216412\n",
      "Epoch 121, test loss: 0.219412\n",
      "Epoch 122, test loss: 0.219124\n",
      "Epoch 123, test loss: 0.218843\n",
      "Epoch 124, test loss: 0.216560\n",
      "Epoch 125, test loss: 0.217560\n",
      "Epoch 126, test loss: 0.216814\n",
      "Epoch 127, test loss: 0.216525\n",
      "Epoch 128, test loss: 0.216802\n",
      "Epoch 129, test loss: 0.216671\n",
      "Epoch 130, test loss: 0.217237\n",
      "Epoch 131, test loss: 0.215841\n",
      "Epoch 132, test loss: 0.217141\n",
      "Epoch 133, test loss: 0.216341\n",
      "Epoch 134, test loss: 0.218101\n",
      "Epoch 135, test loss: 0.220341\n",
      "Epoch 136, test loss: 0.217215\n",
      "Epoch 137, test loss: 0.216240\n",
      "Epoch 138, test loss: 0.218529\n",
      "Epoch 139, test loss: 0.220957\n",
      "Epoch 140, test loss: 0.216310\n",
      "Epoch 141, test loss: 0.218926\n",
      "Epoch 142, test loss: 0.216156\n",
      "Epoch 143, test loss: 0.216222\n",
      "Epoch 144, test loss: 0.215837\n",
      "Epoch 145, test loss: 0.216431\n",
      "Epoch 146, test loss: 0.219228\n",
      "Epoch 147, test loss: 0.215991\n",
      "Epoch 148, test loss: 0.216012\n",
      "Epoch 149, test loss: 0.217522\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F7EFDCFF40>\n",
      "Epoch 0, test loss: 0.238637\n",
      "Epoch 1, test loss: 0.230622\n",
      "Epoch 2, test loss: 0.224930\n",
      "Epoch 3, test loss: 0.222502\n",
      "Epoch 4, test loss: 0.223017\n",
      "Epoch 5, test loss: 0.222336\n",
      "Epoch 6, test loss: 0.221308\n",
      "Epoch 7, test loss: 0.220711\n",
      "Epoch 8, test loss: 0.220825\n",
      "Epoch 9, test loss: 0.218527\n",
      "Epoch 10, test loss: 0.220188\n",
      "Epoch 11, test loss: 0.220164\n",
      "Epoch 12, test loss: 0.221109\n",
      "Epoch 13, test loss: 0.218127\n",
      "Epoch 14, test loss: 0.219480\n",
      "Epoch 15, test loss: 0.223075\n",
      "Epoch 16, test loss: 0.218281\n",
      "Epoch 17, test loss: 0.219492\n",
      "Epoch 18, test loss: 0.223442\n",
      "Epoch 19, test loss: 0.217516\n",
      "Epoch 20, test loss: 0.217801\n",
      "Epoch 21, test loss: 0.221873\n",
      "Epoch 22, test loss: 0.218279\n",
      "Epoch 23, test loss: 0.219395\n",
      "Epoch 24, test loss: 0.222267\n",
      "Epoch 25, test loss: 0.226233\n",
      "Epoch 26, test loss: 0.218843\n",
      "Epoch 27, test loss: 0.218692\n",
      "Epoch 28, test loss: 0.217969\n",
      "Epoch 29, test loss: 0.217580\n",
      "Epoch 30, test loss: 0.217236\n",
      "Epoch 31, test loss: 0.218758\n",
      "Epoch 32, test loss: 0.217855\n",
      "Epoch 33, test loss: 0.216731\n",
      "Epoch 34, test loss: 0.218696\n",
      "Epoch 35, test loss: 0.220558\n",
      "Epoch 36, test loss: 0.219841\n",
      "Epoch 37, test loss: 0.218300\n",
      "Epoch 38, test loss: 0.219501\n",
      "Epoch 39, test loss: 0.218336\n",
      "Epoch 40, test loss: 0.218364\n",
      "Epoch 41, test loss: 0.217157\n",
      "Epoch 42, test loss: 0.218741\n",
      "Epoch 43, test loss: 0.218877\n",
      "Epoch 44, test loss: 0.219579\n",
      "Epoch 45, test loss: 0.218513\n",
      "Epoch 46, test loss: 0.218195\n",
      "Epoch 47, test loss: 0.218855\n",
      "Epoch 48, test loss: 0.219188\n",
      "Epoch 49, test loss: 0.218264\n",
      "Epoch 50, test loss: 0.218901\n",
      "Epoch 51, test loss: 0.217415\n",
      "Epoch 52, test loss: 0.221133\n",
      "Epoch 53, test loss: 0.219137\n",
      "Epoch 54, test loss: 0.220132\n",
      "Epoch 55, test loss: 0.221691\n",
      "Epoch 56, test loss: 0.223424\n",
      "Epoch 57, test loss: 0.219477\n",
      "Epoch 58, test loss: 0.218101\n",
      "Epoch 59, test loss: 0.220137\n",
      "Epoch 60, test loss: 0.220224\n",
      "Epoch 61, test loss: 0.218705\n",
      "Epoch 62, test loss: 0.222004\n",
      "Epoch 63, test loss: 0.217921\n",
      "Epoch 64, test loss: 0.221621\n",
      "Epoch 65, test loss: 0.218495\n",
      "Epoch 66, test loss: 0.220500\n",
      "Epoch 67, test loss: 0.217673\n",
      "Epoch 68, test loss: 0.218806\n",
      "Epoch 69, test loss: 0.220501\n",
      "Epoch 70, test loss: 0.218357\n",
      "Epoch 71, test loss: 0.225656\n",
      "Epoch 72, test loss: 0.221360\n",
      "Epoch 73, test loss: 0.219250\n",
      "Epoch 74, test loss: 0.218757\n",
      "Epoch 75, test loss: 0.227134\n",
      "Epoch 76, test loss: 0.218706\n",
      "Epoch 77, test loss: 0.219044\n",
      "Epoch 78, test loss: 0.220169\n",
      "Epoch 79, test loss: 0.227105\n",
      "Epoch 80, test loss: 0.222957\n",
      "Epoch 81, test loss: 0.219065\n",
      "Epoch 82, test loss: 0.218338\n",
      "Epoch 83, test loss: 0.221824\n",
      "Epoch 84, test loss: 0.219269\n",
      "Epoch 85, test loss: 0.220337\n",
      "Epoch 86, test loss: 0.218661\n",
      "Epoch 87, test loss: 0.218054\n",
      "Epoch 88, test loss: 0.218907\n",
      "Epoch 89, test loss: 0.218742\n",
      "Epoch 90, test loss: 0.220967\n",
      "Epoch 91, test loss: 0.219057\n",
      "Epoch 92, test loss: 0.222101\n",
      "Epoch 93, test loss: 0.218823\n",
      "Epoch 94, test loss: 0.217898\n",
      "Epoch 95, test loss: 0.222250\n",
      "Epoch 96, test loss: 0.219525\n",
      "Epoch 97, test loss: 0.219897\n",
      "Epoch 98, test loss: 0.219331\n",
      "Epoch 99, test loss: 0.217776\n",
      "Epoch 100, test loss: 0.218688\n",
      "Epoch 101, test loss: 0.220140\n",
      "Epoch 102, test loss: 0.218942\n",
      "Epoch 103, test loss: 0.219025\n",
      "Epoch 104, test loss: 0.219175\n",
      "Epoch 105, test loss: 0.218448\n",
      "Epoch 106, test loss: 0.217901\n",
      "Epoch 107, test loss: 0.219014\n",
      "Epoch 108, test loss: 0.219925\n",
      "Epoch 109, test loss: 0.217762\n",
      "Epoch 110, test loss: 0.224908\n",
      "Epoch 111, test loss: 0.219347\n",
      "Epoch 112, test loss: 0.219396\n",
      "Epoch 113, test loss: 0.223375\n",
      "Epoch 114, test loss: 0.218875\n",
      "Epoch 115, test loss: 0.219271\n",
      "Epoch 116, test loss: 0.218959\n",
      "Epoch 117, test loss: 0.218486\n",
      "Epoch 118, test loss: 0.218570\n",
      "Epoch 119, test loss: 0.218707\n",
      "Epoch 120, test loss: 0.220324\n",
      "Epoch 121, test loss: 0.218555\n",
      "Epoch 122, test loss: 0.218839\n",
      "Epoch 123, test loss: 0.219168\n",
      "Epoch 124, test loss: 0.219093\n",
      "Epoch 125, test loss: 0.222152\n",
      "Epoch 126, test loss: 0.221120\n",
      "Epoch 127, test loss: 0.221733\n",
      "Epoch 128, test loss: 0.218661\n",
      "Epoch 129, test loss: 0.218814\n",
      "Epoch 130, test loss: 0.223084\n",
      "Epoch 131, test loss: 0.222559\n",
      "Epoch 132, test loss: 0.224383\n",
      "Epoch 133, test loss: 0.219323\n",
      "Epoch 134, test loss: 0.219370\n",
      "Epoch 135, test loss: 0.219254\n",
      "Epoch 136, test loss: 0.219597\n",
      "Epoch 137, test loss: 0.220843\n",
      "Epoch 138, test loss: 0.219100\n",
      "Epoch 139, test loss: 0.218712\n",
      "Epoch 140, test loss: 0.222027\n",
      "Epoch 141, test loss: 0.218609\n",
      "Epoch 142, test loss: 0.219734\n",
      "Epoch 143, test loss: 0.219674\n",
      "Epoch 144, test loss: 0.219643\n",
      "Epoch 145, test loss: 0.219818\n",
      "Epoch 146, test loss: 0.220319\n",
      "Epoch 147, test loss: 0.219635\n",
      "Epoch 148, test loss: 0.218674\n",
      "Epoch 149, test loss: 0.218902\n",
      "Epoch 150, test loss: 0.219439\n",
      "Epoch 151, test loss: 0.219116\n",
      "Epoch 152, test loss: 0.218971\n",
      "Epoch 153, test loss: 0.218887\n",
      "Epoch 154, test loss: 0.218407\n",
      "Epoch 155, test loss: 0.221552\n",
      "Epoch 156, test loss: 0.228369\n",
      "Epoch 157, test loss: 0.221073\n",
      "Epoch 158, test loss: 0.219726\n",
      "Epoch 159, test loss: 0.219445\n",
      "Epoch 160, test loss: 0.220086\n",
      "Epoch 161, test loss: 0.221355\n",
      "Epoch 162, test loss: 0.218792\n",
      "Epoch 163, test loss: 0.223720\n",
      "Epoch 164, test loss: 0.219448\n",
      "Epoch 165, test loss: 0.220185\n",
      "Epoch 166, test loss: 0.219480\n",
      "Epoch 167, test loss: 0.220608\n",
      "Epoch 168, test loss: 0.218353\n",
      "Epoch 169, test loss: 0.218149\n",
      "Epoch 170, test loss: 0.219348\n",
      "Epoch 171, test loss: 0.219834\n",
      "Epoch 172, test loss: 0.222337\n",
      "Epoch 173, test loss: 0.218258\n",
      "Epoch 174, test loss: 0.219907\n",
      "Epoch 175, test loss: 0.218526\n",
      "Epoch 176, test loss: 0.222415\n",
      "Epoch 177, test loss: 0.219237\n",
      "Epoch 178, test loss: 0.221558\n",
      "Epoch 179, test loss: 0.217925\n",
      "Epoch 180, test loss: 0.218657\n",
      "Epoch 181, test loss: 0.219250\n",
      "Epoch 182, test loss: 0.221839\n",
      "Epoch 183, test loss: 0.226390\n",
      "Epoch 184, test loss: 0.221222\n",
      "Epoch 185, test loss: 0.219787\n",
      "Epoch 186, test loss: 0.218701\n",
      "Epoch 187, test loss: 0.221485\n",
      "Epoch 188, test loss: 0.220734\n",
      "Epoch 189, test loss: 0.221533\n",
      "Epoch 190, test loss: 0.218015\n",
      "Epoch 191, test loss: 0.218338\n",
      "Epoch 192, test loss: 0.218994\n",
      "Epoch 193, test loss: 0.220609\n",
      "Epoch 194, test loss: 0.219157\n",
      "Epoch 195, test loss: 0.220956\n",
      "Epoch 196, test loss: 0.220075\n",
      "Epoch 197, test loss: 0.220102\n",
      "Epoch 198, test loss: 0.217935\n",
      "Epoch 199, test loss: 0.217862\n",
      "Epoch 200, test loss: 0.219679\n",
      "Epoch 201, test loss: 0.220143\n",
      "Epoch 202, test loss: 0.217881\n",
      "Epoch 203, test loss: 0.220416\n",
      "Epoch 204, test loss: 0.218757\n",
      "Epoch 205, test loss: 0.221029\n",
      "Epoch 206, test loss: 0.219957\n",
      "Epoch 207, test loss: 0.220167\n",
      "Epoch 208, test loss: 0.217973\n",
      "Epoch 209, test loss: 0.218186\n",
      "Epoch 210, test loss: 0.219592\n",
      "Epoch 211, test loss: 0.218512\n",
      "Epoch 212, test loss: 0.218222\n",
      "Epoch 213, test loss: 0.218607\n",
      "Epoch 214, test loss: 0.219255\n",
      "Epoch 215, test loss: 0.218330\n",
      "Epoch 216, test loss: 0.218303\n",
      "Epoch 217, test loss: 0.218969\n",
      "Epoch 218, test loss: 0.218991\n",
      "Epoch 219, test loss: 0.219106\n",
      "Epoch 220, test loss: 0.219052\n",
      "Epoch 221, test loss: 0.222111\n",
      "Epoch 222, test loss: 0.219316\n",
      "Epoch 223, test loss: 0.221924\n",
      "Epoch 224, test loss: 0.221166\n",
      "Epoch 225, test loss: 0.220525\n",
      "Epoch 226, test loss: 0.220616\n",
      "Epoch 227, test loss: 0.223254\n",
      "Epoch 228, test loss: 0.219652\n",
      "Epoch 229, test loss: 0.218506\n",
      "Epoch 230, test loss: 0.220065\n",
      "Epoch 231, test loss: 0.218595\n",
      "Epoch 232, test loss: 0.219675\n",
      "Epoch 233, test loss: 0.218618\n",
      "Epoch 234, test loss: 0.220987\n",
      "Epoch 235, test loss: 0.221142\n",
      "Epoch 236, test loss: 0.217083\n",
      "Epoch 237, test loss: 0.225353\n",
      "Epoch 238, test loss: 0.218778\n",
      "Epoch 239, test loss: 0.220329\n",
      "Epoch 240, test loss: 0.217697\n",
      "Epoch 241, test loss: 0.220247\n",
      "Epoch 242, test loss: 0.217575\n",
      "Epoch 243, test loss: 0.218797\n",
      "Epoch 244, test loss: 0.219300\n",
      "Epoch 245, test loss: 0.220442\n",
      "Epoch 246, test loss: 0.218618\n",
      "Epoch 247, test loss: 0.218555\n",
      "Epoch 248, test loss: 0.218753\n",
      "Epoch 249, test loss: 0.219853\n",
      "Epoch 250, test loss: 0.219401\n",
      "Epoch 251, test loss: 0.219384\n",
      "Epoch 252, test loss: 0.220540\n",
      "Epoch 253, test loss: 0.232121\n",
      "Epoch 254, test loss: 0.225025\n",
      "Epoch 255, test loss: 0.221045\n",
      "Epoch 256, test loss: 0.219883\n",
      "Epoch 257, test loss: 0.218299\n",
      "Epoch 258, test loss: 0.225776\n",
      "Epoch 259, test loss: 0.219895\n",
      "Epoch 260, test loss: 0.219071\n",
      "Epoch 261, test loss: 0.218552\n",
      "Epoch 262, test loss: 0.225832\n",
      "Epoch 263, test loss: 0.222056\n",
      "Epoch 264, test loss: 0.219326\n",
      "Epoch 265, test loss: 0.224816\n",
      "Epoch 266, test loss: 0.225958\n",
      "Epoch 267, test loss: 0.218772\n",
      "Epoch 268, test loss: 0.218442\n",
      "Epoch 269, test loss: 0.219266\n",
      "Epoch 270, test loss: 0.218301\n",
      "Epoch 271, test loss: 0.218922\n",
      "Epoch 272, test loss: 0.219276\n",
      "Epoch 273, test loss: 0.219689\n",
      "Epoch 274, test loss: 0.219145\n",
      "Epoch 275, test loss: 0.218962\n",
      "Epoch 276, test loss: 0.220428\n",
      "Epoch 277, test loss: 0.219599\n",
      "Epoch 278, test loss: 0.219525\n",
      "Epoch 279, test loss: 0.218997\n",
      "Epoch 280, test loss: 0.218421\n",
      "Epoch 281, test loss: 0.219980\n",
      "Epoch 282, test loss: 0.224511\n",
      "Epoch 283, test loss: 0.222203\n",
      "Epoch 284, test loss: 0.218922\n",
      "Epoch 285, test loss: 0.223549\n",
      "Epoch 286, test loss: 0.219224\n",
      "Epoch 287, test loss: 0.220945\n",
      "Epoch 288, test loss: 0.219554\n",
      "Epoch 289, test loss: 0.218034\n",
      "Epoch 290, test loss: 0.218150\n",
      "Epoch 291, test loss: 0.219705\n",
      "Epoch 292, test loss: 0.219602\n",
      "Epoch 293, test loss: 0.218469\n",
      "Epoch 294, test loss: 0.220468\n",
      "Epoch 295, test loss: 0.219860\n",
      "Epoch 296, test loss: 0.221210\n",
      "Epoch 297, test loss: 0.218676\n",
      "Epoch 298, test loss: 0.220032\n",
      "Epoch 299, test loss: 0.220472\n",
      "Epoch 300, test loss: 0.222994\n",
      "Epoch 301, test loss: 0.218339\n",
      "Epoch 302, test loss: 0.219525\n",
      "Epoch 303, test loss: 0.219660\n",
      "Epoch 304, test loss: 0.219323\n",
      "Epoch 305, test loss: 0.219886\n",
      "Epoch 306, test loss: 0.219161\n",
      "Epoch 307, test loss: 0.219083\n",
      "Epoch 308, test loss: 0.218228\n",
      "Epoch 309, test loss: 0.227067\n",
      "Epoch 310, test loss: 0.221987\n",
      "Epoch 311, test loss: 0.220528\n",
      "Epoch 312, test loss: 0.218802\n",
      "Epoch 313, test loss: 0.219930\n",
      "Epoch 314, test loss: 0.222043\n",
      "Epoch 315, test loss: 0.219242\n",
      "Epoch 316, test loss: 0.218978\n",
      "Epoch 317, test loss: 0.219461\n",
      "Epoch 318, test loss: 0.220417\n",
      "Epoch 319, test loss: 0.220207\n",
      "Epoch 320, test loss: 0.222572\n",
      "Epoch 321, test loss: 0.218104\n",
      "Epoch 322, test loss: 0.218053\n",
      "Epoch 323, test loss: 0.218741\n",
      "Epoch 324, test loss: 0.220721\n",
      "Epoch 325, test loss: 0.218475\n",
      "Epoch 326, test loss: 0.222137\n",
      "Epoch 327, test loss: 0.218520\n",
      "Epoch 328, test loss: 0.219322\n",
      "Epoch 329, test loss: 0.218626\n",
      "Epoch 330, test loss: 0.218495\n",
      "Epoch 331, test loss: 0.217317\n",
      "Epoch 332, test loss: 0.222066\n",
      "Epoch 333, test loss: 0.220282\n",
      "Epoch 334, test loss: 0.228509\n",
      "Epoch 335, test loss: 0.221013\n",
      "Epoch 336, test loss: 0.217430\n",
      "Epoch 337, test loss: 0.218885\n",
      "Epoch 338, test loss: 0.220355\n",
      "Epoch 339, test loss: 0.217987\n",
      "Epoch 340, test loss: 0.218450\n",
      "Epoch 341, test loss: 0.217954\n",
      "Epoch 342, test loss: 0.219044\n",
      "Epoch 343, test loss: 0.218738\n",
      "Epoch 344, test loss: 0.218716\n",
      "Epoch 345, test loss: 0.221641\n",
      "Epoch 346, test loss: 0.219526\n",
      "Epoch 347, test loss: 0.218601\n",
      "Epoch 348, test loss: 0.220284\n",
      "Epoch 349, test loss: 0.218916\n",
      "Epoch 350, test loss: 0.220754\n",
      "Epoch 351, test loss: 0.220641\n",
      "Epoch 352, test loss: 0.221232\n",
      "Epoch 353, test loss: 0.220692\n",
      "Epoch 354, test loss: 0.218953\n",
      "Epoch 355, test loss: 0.220154\n",
      "Epoch 356, test loss: 0.220198\n",
      "Epoch 357, test loss: 0.219810\n",
      "Epoch 358, test loss: 0.218038\n",
      "Epoch 359, test loss: 0.217694\n",
      "Epoch 360, test loss: 0.226838\n",
      "Epoch 361, test loss: 0.219604\n",
      "Epoch 362, test loss: 0.219299\n",
      "Epoch 363, test loss: 0.219629\n",
      "Epoch 364, test loss: 0.223081\n",
      "Epoch 365, test loss: 0.220414\n",
      "Epoch 366, test loss: 0.219141\n",
      "Epoch 367, test loss: 0.222284\n",
      "Epoch 368, test loss: 0.219442\n",
      "Epoch 369, test loss: 0.223060\n",
      "Epoch 370, test loss: 0.220814\n",
      "Epoch 371, test loss: 0.218622\n",
      "Epoch 372, test loss: 0.219140\n",
      "Epoch 373, test loss: 0.219078\n",
      "Epoch 374, test loss: 0.217684\n",
      "Epoch 375, test loss: 0.221312\n",
      "Epoch 376, test loss: 0.219949\n",
      "Epoch 377, test loss: 0.219264\n",
      "Epoch 378, test loss: 0.219722\n",
      "Epoch 379, test loss: 0.221271\n",
      "Epoch 380, test loss: 0.221037\n",
      "Epoch 381, test loss: 0.222261\n",
      "Epoch 382, test loss: 0.220402\n",
      "Epoch 383, test loss: 0.219210\n",
      "Epoch 384, test loss: 0.219357\n",
      "Epoch 385, test loss: 0.223987\n",
      "Epoch 386, test loss: 0.217871\n",
      "Epoch 387, test loss: 0.219052\n",
      "Epoch 388, test loss: 0.219440\n",
      "Epoch 389, test loss: 0.220766\n",
      "Epoch 390, test loss: 0.218579\n",
      "Epoch 391, test loss: 0.218788\n",
      "Epoch 392, test loss: 0.217418\n",
      "Epoch 393, test loss: 0.219635\n",
      "Epoch 394, test loss: 0.218448\n",
      "Epoch 395, test loss: 0.221372\n",
      "Epoch 396, test loss: 0.223890\n",
      "Epoch 397, test loss: 0.219281\n",
      "Epoch 398, test loss: 0.218335\n",
      "Epoch 399, test loss: 0.220827\n",
      "Epoch 400, test loss: 0.220045\n",
      "Epoch 401, test loss: 0.219876\n",
      "Epoch 402, test loss: 0.219931\n",
      "Epoch 403, test loss: 0.218205\n",
      "Epoch 404, test loss: 0.226954\n",
      "Epoch 405, test loss: 0.218530\n",
      "Epoch 406, test loss: 0.218428\n",
      "Epoch 407, test loss: 0.220840\n",
      "Epoch 408, test loss: 0.218976\n",
      "Epoch 409, test loss: 0.222072\n",
      "Epoch 410, test loss: 0.218854\n",
      "Epoch 411, test loss: 0.225319\n",
      "Epoch 412, test loss: 0.222290\n",
      "Epoch 413, test loss: 0.217961\n",
      "Epoch 414, test loss: 0.219877\n",
      "Epoch 415, test loss: 0.220148\n",
      "Epoch 416, test loss: 0.219906\n",
      "Epoch 417, test loss: 0.219395\n",
      "Epoch 418, test loss: 0.220185\n",
      "Epoch 419, test loss: 0.218947\n",
      "Epoch 420, test loss: 0.221124\n",
      "Epoch 421, test loss: 0.219297\n",
      "Epoch 422, test loss: 0.219455\n",
      "Epoch 423, test loss: 0.219983\n",
      "Epoch 424, test loss: 0.218493\n",
      "Epoch 425, test loss: 0.219578\n",
      "Epoch 426, test loss: 0.221409\n",
      "Epoch 427, test loss: 0.218443\n",
      "Epoch 428, test loss: 0.218011\n",
      "Epoch 429, test loss: 0.217827\n",
      "Epoch 430, test loss: 0.219570\n",
      "Epoch 431, test loss: 0.220323\n",
      "Epoch 432, test loss: 0.219061\n",
      "Epoch 433, test loss: 0.218713\n",
      "Epoch 434, test loss: 0.219355\n",
      "Epoch 435, test loss: 0.219268\n",
      "Epoch 436, test loss: 0.219387\n",
      "Epoch 437, test loss: 0.221128\n",
      "Epoch 438, test loss: 0.218501\n",
      "Epoch 439, test loss: 0.219890\n",
      "Epoch 440, test loss: 0.221376\n",
      "Epoch 441, test loss: 0.220030\n",
      "Epoch 442, test loss: 0.220843\n",
      "Epoch 443, test loss: 0.220177\n",
      "Epoch 444, test loss: 0.222304\n",
      "Epoch 445, test loss: 0.218805\n",
      "Epoch 446, test loss: 0.219805\n",
      "Epoch 447, test loss: 0.223009\n",
      "Epoch 448, test loss: 0.217757\n",
      "Epoch 449, test loss: 0.217920\n",
      "Pretrain data: 19823659.0\n",
      "Building dataset, requesting data from 0 to 815\n",
      "x here is\n",
      "[[129. 128. 129. 131. 133. 132.]\n",
      " [128. 129. 131. 133. 132. 131.]\n",
      " [129. 131. 133. 132. 131. 134.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[142. 142. 142. 142. 142. 142.]\n",
      " [141. 141. 141. 141. 141. 141.]\n",
      " [143. 143. 143. 143. 143. 143.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7717/107424\n",
      "Found 815 continuous time series\n",
      "Data shape: (115143, 6), Train/test: 115141/2\n",
      "Train test ratio: 57570.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 6), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 6), dtype=float32)\n",
      "line73: Shape of y: (None, 6)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1799\n",
      "Epoch 0, train loss: 0.245706\n",
      "Epoch 1, train loss: 0.264107\n",
      "Epoch 2, train loss: 0.216426\n",
      "Epoch 3, train loss: 0.179189\n",
      "Epoch 4, train loss: 0.218731\n",
      "Epoch 5, train loss: 0.239077\n",
      "Epoch 6, train loss: 0.220250\n",
      "Epoch 7, train loss: 0.189152\n",
      "Epoch 8, train loss: 0.219630\n",
      "Epoch 9, train loss: 0.292885\n",
      "Epoch 10, train loss: 0.212810\n",
      "Epoch 11, train loss: 0.211956\n",
      "Epoch 12, train loss: 0.223599\n",
      "Epoch 13, train loss: 0.213281\n",
      "Epoch 14, train loss: 0.187000\n",
      "Epoch 15, train loss: 0.268424\n",
      "Epoch 16, train loss: 0.306040\n",
      "Epoch 17, train loss: 0.258687\n",
      "Epoch 18, train loss: 0.163567\n",
      "Epoch 19, train loss: 0.215951\n",
      "Epoch 20, train loss: 0.224252\n",
      "Epoch 21, train loss: 0.252007\n",
      "Epoch 22, train loss: 0.186298\n",
      "Epoch 23, train loss: 0.272642\n",
      "Epoch 24, train loss: 0.216603\n",
      "Epoch 25, train loss: 0.235090\n",
      "Epoch 26, train loss: 0.200598\n",
      "Epoch 27, train loss: 0.225426\n",
      "Epoch 28, train loss: 0.208612\n",
      "Epoch 29, train loss: 0.225984\n",
      "Epoch 30, train loss: 0.207472\n",
      "Epoch 31, train loss: 0.154480\n",
      "Epoch 32, train loss: 0.188719\n",
      "Epoch 33, train loss: 0.195321\n",
      "Epoch 34, train loss: 0.162054\n",
      "Epoch 35, train loss: 0.155630\n",
      "Epoch 36, train loss: 0.200719\n",
      "Epoch 37, train loss: 0.148309\n",
      "Epoch 38, train loss: 0.288716\n",
      "Epoch 39, train loss: 0.184361\n",
      "Epoch 40, train loss: 0.177582\n",
      "Epoch 41, train loss: 0.151384\n",
      "Epoch 42, train loss: 0.211648\n",
      "Epoch 43, train loss: 0.308961\n",
      "Epoch 44, train loss: 0.186446\n",
      "Epoch 45, train loss: 0.161221\n",
      "Epoch 46, train loss: 0.265682\n",
      "Epoch 47, train loss: 0.267751\n",
      "Epoch 48, train loss: 0.241730\n",
      "Epoch 49, train loss: 0.216351\n",
      "Epoch 50, train loss: 0.194054\n",
      "Epoch 51, train loss: 0.194434\n",
      "Epoch 52, train loss: 0.183352\n",
      "Epoch 53, train loss: 0.215963\n",
      "Epoch 54, train loss: 0.205367\n",
      "Epoch 55, train loss: 0.220591\n",
      "Epoch 56, train loss: 0.172207\n",
      "Epoch 57, train loss: 0.254158\n",
      "Epoch 58, train loss: 0.168166\n",
      "Epoch 59, train loss: 0.183779\n",
      "Epoch 60, train loss: 0.291953\n",
      "Epoch 61, train loss: 0.193853\n",
      "Epoch 62, train loss: 0.224415\n",
      "Epoch 63, train loss: 0.281422\n",
      "Epoch 64, train loss: 0.198285\n",
      "Epoch 65, train loss: 0.204022\n",
      "Epoch 66, train loss: 0.187769\n",
      "Epoch 67, train loss: 0.173246\n",
      "Epoch 68, train loss: 0.197674\n",
      "Epoch 69, train loss: 0.221235\n",
      "Epoch 70, train loss: 0.230620\n",
      "Epoch 71, train loss: 0.233957\n",
      "Epoch 72, train loss: 0.132483\n",
      "Epoch 73, train loss: 0.188704\n",
      "Epoch 74, train loss: 0.241464\n",
      "Epoch 75, train loss: 0.159529\n",
      "Epoch 76, train loss: 0.261546\n",
      "Epoch 77, train loss: 0.171772\n",
      "Epoch 78, train loss: 0.206211\n",
      "Epoch 79, train loss: 0.172120\n",
      "Epoch 80, train loss: 0.176933\n",
      "Epoch 81, train loss: 0.175688\n",
      "Epoch 82, train loss: 0.194026\n",
      "Epoch 83, train loss: 0.220581\n",
      "Epoch 84, train loss: 0.161894\n",
      "Epoch 85, train loss: 0.210985\n",
      "Epoch 86, train loss: 0.221786\n",
      "Epoch 87, train loss: 0.222443\n",
      "Epoch 88, train loss: 0.197289\n",
      "Epoch 89, train loss: 0.199364\n",
      "Epoch 90, train loss: 0.192734\n",
      "Epoch 91, train loss: 0.226616\n",
      "Epoch 92, train loss: 0.258847\n",
      "Epoch 93, train loss: 0.273290\n",
      "Epoch 94, train loss: 0.218826\n",
      "Epoch 95, train loss: 0.272029\n",
      "Epoch 96, train loss: 0.232482\n",
      "Epoch 97, train loss: 0.212089\n",
      "Epoch 98, train loss: 0.164808\n",
      "Epoch 99, train loss: 0.195262\n",
      "Epoch 100, train loss: 0.193896\n",
      "Epoch 101, train loss: 0.217692\n",
      "Epoch 102, train loss: 0.207851\n",
      "Epoch 103, train loss: 0.171003\n",
      "Epoch 104, train loss: 0.179592\n",
      "Epoch 105, train loss: 0.195947\n",
      "Epoch 106, train loss: 0.221325\n",
      "Epoch 107, train loss: 0.196348\n",
      "Epoch 108, train loss: 0.188820\n",
      "Epoch 109, train loss: 0.196117\n",
      "Epoch 110, train loss: 0.259650\n",
      "Epoch 111, train loss: 0.227146\n",
      "Epoch 112, train loss: 0.189252\n",
      "Epoch 113, train loss: 0.210614\n",
      "Epoch 114, train loss: 0.231395\n",
      "Epoch 115, train loss: 0.226729\n",
      "Epoch 116, train loss: 0.215582\n",
      "Epoch 117, train loss: 0.234910\n",
      "Epoch 118, train loss: 0.250408\n",
      "Epoch 119, train loss: 0.210044\n",
      "Epoch 120, train loss: 0.219474\n",
      "Epoch 121, train loss: 0.181706\n",
      "Epoch 122, train loss: 0.220565\n",
      "Epoch 123, train loss: 0.155091\n",
      "Epoch 124, train loss: 0.206817\n",
      "Epoch 125, train loss: 0.209266\n",
      "Epoch 126, train loss: 0.171119\n",
      "Epoch 127, train loss: 0.227805\n",
      "Epoch 128, train loss: 0.175400\n",
      "Epoch 129, train loss: 0.268146\n",
      "Epoch 130, train loss: 0.253122\n",
      "Epoch 131, train loss: 0.189538\n",
      "Epoch 132, train loss: 0.200105\n",
      "Epoch 133, train loss: 0.201438\n",
      "Epoch 134, train loss: 0.220399\n",
      "Epoch 135, train loss: 0.237820\n",
      "Epoch 136, train loss: 0.247408\n",
      "Epoch 137, train loss: 0.232515\n",
      "Epoch 138, train loss: 0.183366\n",
      "Epoch 139, train loss: 0.210153\n",
      "Epoch 140, train loss: 0.225295\n",
      "Epoch 141, train loss: 0.193394\n",
      "Epoch 142, train loss: 0.210172\n",
      "Epoch 143, train loss: 0.256814\n",
      "Epoch 144, train loss: 0.202823\n",
      "Epoch 145, train loss: 0.184300\n",
      "Epoch 146, train loss: 0.230311\n",
      "Epoch 147, train loss: 0.172013\n",
      "Epoch 148, train loss: 0.218550\n",
      "Epoch 149, train loss: 0.235175\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "x here is\n",
      "[[174. 173. 173. 172. 170. 169.]\n",
      " [173. 173. 172. 170. 169. 169.]\n",
      " [173. 172. 170. 169. 169. 168.]\n",
      " ...\n",
      " [103.  99.  95.  92.  88.  85.]\n",
      " [ 99.  95.  92.  88.  85.  84.]\n",
      " [ 95.  92.  88.  85.  84.  86.]]\n",
      "y here is\n",
      "[[165. 165. 165. 165. 165. 165.]\n",
      " [165. 165. 165. 165. 165. 165.]\n",
      " [165. 165. 165. 165. 165. 165.]\n",
      " ...\n",
      " [ 90.  90.  90.  90.  90.  90.]\n",
      " [ 86.  86.  86.  86.  86.  86.]\n",
      " [ 87.  87.  87.  87.  87.  87.]]\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (2666, 6), Train/test: 1/2665\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "x here is\n",
      "[[142. 142. 142. 141. 139. 137.]\n",
      " [142. 142. 141. 139. 137. 136.]\n",
      " [142. 141. 139. 137. 136. 137.]\n",
      " ...\n",
      " [158. 159. 161. 163. 166. 168.]\n",
      " [159. 161. 163. 166. 168. 167.]\n",
      " [161. 163. 166. 168. 167. 168.]]\n",
      "y here is\n",
      "[[127. 127. 127. 127. 127. 127.]\n",
      " [127. 127. 127. 127. 127. 127.]\n",
      " [126. 126. 126. 126. 126. 126.]\n",
      " ...\n",
      " [173. 173. 173. 173. 173. 173.]\n",
      " [174. 174. 174. 174. 174. 174.]\n",
      " [174. 174. 174. 174. 174. 174.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 568/10015\n",
      "Found 27 continuous time series\n",
      "Data shape: (10585, 6), Train/test: 10583/2\n",
      "Train test ratio: 5291.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610867160>\n",
      "Epoch 0, test loss: 0.171211\n",
      "Epoch 1, test loss: 0.173187\n",
      "Epoch 2, test loss: 0.170946\n",
      "Epoch 3, test loss: 0.171229\n",
      "Epoch 4, test loss: 0.171481\n",
      "Epoch 5, test loss: 0.170962\n",
      "Epoch 6, test loss: 0.172192\n",
      "Epoch 7, test loss: 0.171058\n",
      "Epoch 8, test loss: 0.171190\n",
      "Epoch 9, test loss: 0.180233\n",
      "Epoch 10, test loss: 0.171137\n",
      "Epoch 11, test loss: 0.171184\n",
      "Epoch 12, test loss: 0.171736\n",
      "Epoch 13, test loss: 0.171129\n",
      "Epoch 14, test loss: 0.171638\n",
      "Epoch 15, test loss: 0.173635\n",
      "Epoch 16, test loss: 0.171148\n",
      "Epoch 17, test loss: 0.171182\n",
      "Epoch 18, test loss: 0.171879\n",
      "Epoch 19, test loss: 0.172746\n",
      "Epoch 20, test loss: 0.171488\n",
      "Epoch 21, test loss: 0.171329\n",
      "Epoch 22, test loss: 0.171996\n",
      "Epoch 23, test loss: 0.172029\n",
      "Epoch 24, test loss: 0.172082\n",
      "Epoch 25, test loss: 0.172168\n",
      "Epoch 26, test loss: 0.171274\n",
      "Epoch 27, test loss: 0.171596\n",
      "Epoch 28, test loss: 0.170970\n",
      "Epoch 29, test loss: 0.171323\n",
      "Epoch 30, test loss: 0.172993\n",
      "Epoch 31, test loss: 0.171316\n",
      "Epoch 32, test loss: 0.172931\n",
      "Epoch 33, test loss: 0.172475\n",
      "Epoch 34, test loss: 0.172584\n",
      "Epoch 35, test loss: 0.171670\n",
      "Epoch 36, test loss: 0.171927\n",
      "Epoch 37, test loss: 0.171200\n",
      "Epoch 38, test loss: 0.171914\n",
      "Epoch 39, test loss: 0.172035\n",
      "Epoch 40, test loss: 0.171189\n",
      "Epoch 41, test loss: 0.171886\n",
      "Epoch 42, test loss: 0.172334\n",
      "Epoch 43, test loss: 0.173287\n",
      "Epoch 44, test loss: 0.174422\n",
      "Epoch 45, test loss: 0.171045\n",
      "Epoch 46, test loss: 0.171334\n",
      "Epoch 47, test loss: 0.171240\n",
      "Epoch 48, test loss: 0.171035\n",
      "Epoch 49, test loss: 0.171577\n",
      "Epoch 50, test loss: 0.171643\n",
      "Epoch 51, test loss: 0.171375\n",
      "Epoch 52, test loss: 0.173341\n",
      "Epoch 53, test loss: 0.172176\n",
      "Epoch 54, test loss: 0.173025\n",
      "Epoch 55, test loss: 0.172196\n",
      "Epoch 56, test loss: 0.170847\n",
      "Epoch 57, test loss: 0.171116\n",
      "Epoch 58, test loss: 0.171620\n",
      "Epoch 59, test loss: 0.171416\n",
      "Epoch 60, test loss: 0.172427\n",
      "Epoch 61, test loss: 0.171668\n",
      "Epoch 62, test loss: 0.170841\n",
      "Epoch 63, test loss: 0.172994\n",
      "Epoch 64, test loss: 0.174171\n",
      "Epoch 65, test loss: 0.171232\n",
      "Epoch 66, test loss: 0.173767\n",
      "Epoch 67, test loss: 0.171371\n",
      "Epoch 68, test loss: 0.171528\n",
      "Epoch 69, test loss: 0.171353\n",
      "Epoch 70, test loss: 0.171149\n",
      "Epoch 71, test loss: 0.172992\n",
      "Epoch 72, test loss: 0.172886\n",
      "Epoch 73, test loss: 0.171191\n",
      "Epoch 74, test loss: 0.171674\n",
      "Epoch 75, test loss: 0.171545\n",
      "Epoch 76, test loss: 0.170932\n",
      "Epoch 77, test loss: 0.173544\n",
      "Epoch 78, test loss: 0.171414\n",
      "Epoch 79, test loss: 0.173744\n",
      "Epoch 80, test loss: 0.175764\n",
      "Epoch 81, test loss: 0.173696\n",
      "Epoch 82, test loss: 0.170940\n",
      "Epoch 83, test loss: 0.170770\n",
      "Epoch 84, test loss: 0.172078\n",
      "Epoch 85, test loss: 0.170845\n",
      "Epoch 86, test loss: 0.171719\n",
      "Epoch 87, test loss: 0.171390\n",
      "Epoch 88, test loss: 0.170944\n",
      "Epoch 89, test loss: 0.172399\n",
      "Epoch 90, test loss: 0.171779\n",
      "Epoch 91, test loss: 0.171856\n",
      "Epoch 92, test loss: 0.171741\n",
      "Epoch 93, test loss: 0.173867\n",
      "Epoch 94, test loss: 0.171674\n",
      "Epoch 95, test loss: 0.175808\n",
      "Epoch 96, test loss: 0.171137\n",
      "Epoch 97, test loss: 0.171747\n",
      "Epoch 98, test loss: 0.170867\n",
      "Epoch 99, test loss: 0.170953\n",
      "Epoch 100, test loss: 0.173260\n",
      "Epoch 101, test loss: 0.171471\n",
      "Epoch 102, test loss: 0.172085\n",
      "Epoch 103, test loss: 0.174481\n",
      "Epoch 104, test loss: 0.171569\n",
      "Epoch 105, test loss: 0.171233\n",
      "Epoch 106, test loss: 0.171582\n",
      "Epoch 107, test loss: 0.172307\n",
      "Epoch 108, test loss: 0.176889\n",
      "Epoch 109, test loss: 0.171212\n",
      "Epoch 110, test loss: 0.170900\n",
      "Epoch 111, test loss: 0.171810\n",
      "Epoch 112, test loss: 0.171592\n",
      "Epoch 113, test loss: 0.172943\n",
      "Epoch 114, test loss: 0.173094\n",
      "Epoch 115, test loss: 0.170994\n",
      "Epoch 116, test loss: 0.171279\n",
      "Epoch 117, test loss: 0.173242\n",
      "Epoch 118, test loss: 0.173239\n",
      "Epoch 119, test loss: 0.171266\n",
      "Epoch 120, test loss: 0.172099\n",
      "Epoch 121, test loss: 0.171148\n",
      "Epoch 122, test loss: 0.170958\n",
      "Epoch 123, test loss: 0.171340\n",
      "Epoch 124, test loss: 0.172109\n",
      "Epoch 125, test loss: 0.172317\n",
      "Epoch 126, test loss: 0.171542\n",
      "Epoch 127, test loss: 0.171152\n",
      "Epoch 128, test loss: 0.173698\n",
      "Epoch 129, test loss: 0.172840\n",
      "Epoch 130, test loss: 0.171664\n",
      "Epoch 131, test loss: 0.177052\n",
      "Epoch 132, test loss: 0.171007\n",
      "Epoch 133, test loss: 0.170943\n",
      "Epoch 134, test loss: 0.172783\n",
      "Epoch 135, test loss: 0.173528\n",
      "Epoch 136, test loss: 0.173941\n",
      "Epoch 137, test loss: 0.171686\n",
      "Epoch 138, test loss: 0.171529\n",
      "Epoch 139, test loss: 0.171253\n",
      "Epoch 140, test loss: 0.171645\n",
      "Epoch 141, test loss: 0.172936\n",
      "Epoch 142, test loss: 0.172539\n",
      "Epoch 143, test loss: 0.172836\n",
      "Epoch 144, test loss: 0.171495\n",
      "Epoch 145, test loss: 0.171123\n",
      "Epoch 146, test loss: 0.174786\n",
      "Epoch 147, test loss: 0.171652\n",
      "Epoch 148, test loss: 0.171177\n",
      "Epoch 149, test loss: 0.171561\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610867160>\n",
      "Epoch 0, test loss: 0.172370\n",
      "Epoch 1, test loss: 0.171617\n",
      "Epoch 2, test loss: 0.171060\n",
      "Epoch 3, test loss: 0.171016\n",
      "Epoch 4, test loss: 0.170783\n",
      "Epoch 5, test loss: 0.171147\n",
      "Epoch 6, test loss: 0.171488\n",
      "Epoch 7, test loss: 0.171636\n",
      "Epoch 8, test loss: 0.171454\n",
      "Epoch 9, test loss: 0.174077\n",
      "Epoch 10, test loss: 0.170536\n",
      "Epoch 11, test loss: 0.171314\n",
      "Epoch 12, test loss: 0.171248\n",
      "Epoch 13, test loss: 0.171566\n",
      "Epoch 14, test loss: 0.172407\n",
      "Epoch 15, test loss: 0.171062\n",
      "Epoch 16, test loss: 0.171523\n",
      "Epoch 17, test loss: 0.171708\n",
      "Epoch 18, test loss: 0.170976\n",
      "Epoch 19, test loss: 0.171687\n",
      "Epoch 20, test loss: 0.171171\n",
      "Epoch 21, test loss: 0.171042\n",
      "Epoch 22, test loss: 0.172013\n",
      "Epoch 23, test loss: 0.170999\n",
      "Epoch 24, test loss: 0.170891\n",
      "Epoch 25, test loss: 0.172233\n",
      "Epoch 26, test loss: 0.172324\n",
      "Epoch 27, test loss: 0.172044\n",
      "Epoch 28, test loss: 0.171810\n",
      "Epoch 29, test loss: 0.170437\n",
      "Epoch 30, test loss: 0.170774\n",
      "Epoch 31, test loss: 0.170683\n",
      "Epoch 32, test loss: 0.170616\n",
      "Epoch 33, test loss: 0.171846\n",
      "Epoch 34, test loss: 0.170947\n",
      "Epoch 35, test loss: 0.170617\n",
      "Epoch 36, test loss: 0.171045\n",
      "Epoch 37, test loss: 0.171781\n",
      "Epoch 38, test loss: 0.171510\n",
      "Epoch 39, test loss: 0.170828\n",
      "Epoch 40, test loss: 0.170738\n",
      "Epoch 41, test loss: 0.171507\n",
      "Epoch 42, test loss: 0.170711\n",
      "Epoch 43, test loss: 0.171748\n",
      "Epoch 44, test loss: 0.170737\n",
      "Epoch 45, test loss: 0.171502\n",
      "Epoch 46, test loss: 0.172062\n",
      "Epoch 47, test loss: 0.171671\n",
      "Epoch 48, test loss: 0.170909\n",
      "Epoch 49, test loss: 0.172346\n",
      "Epoch 50, test loss: 0.171138\n",
      "Epoch 51, test loss: 0.171817\n",
      "Epoch 52, test loss: 0.171160\n",
      "Epoch 53, test loss: 0.173243\n",
      "Epoch 54, test loss: 0.172624\n",
      "Epoch 55, test loss: 0.170657\n",
      "Epoch 56, test loss: 0.172935\n",
      "Epoch 57, test loss: 0.173789\n",
      "Epoch 58, test loss: 0.173660\n",
      "Epoch 59, test loss: 0.172748\n",
      "Epoch 60, test loss: 0.170908\n",
      "Epoch 61, test loss: 0.171102\n",
      "Epoch 62, test loss: 0.170915\n",
      "Epoch 63, test loss: 0.172336\n",
      "Epoch 64, test loss: 0.170744\n",
      "Epoch 65, test loss: 0.171919\n",
      "Epoch 66, test loss: 0.171105\n",
      "Epoch 67, test loss: 0.171442\n",
      "Epoch 68, test loss: 0.170983\n",
      "Epoch 69, test loss: 0.171188\n",
      "Epoch 70, test loss: 0.171206\n",
      "Epoch 71, test loss: 0.171024\n",
      "Epoch 72, test loss: 0.171234\n",
      "Epoch 73, test loss: 0.171092\n",
      "Epoch 74, test loss: 0.172937\n",
      "Epoch 75, test loss: 0.173249\n",
      "Epoch 76, test loss: 0.170872\n",
      "Epoch 77, test loss: 0.172417\n",
      "Epoch 78, test loss: 0.172100\n",
      "Epoch 79, test loss: 0.171203\n",
      "Epoch 80, test loss: 0.171435\n",
      "Epoch 81, test loss: 0.170971\n",
      "Epoch 82, test loss: 0.172512\n",
      "Epoch 83, test loss: 0.171072\n",
      "Epoch 84, test loss: 0.173785\n",
      "Epoch 85, test loss: 0.171410\n",
      "Epoch 86, test loss: 0.172158\n",
      "Epoch 87, test loss: 0.171637\n",
      "Epoch 88, test loss: 0.171189\n",
      "Epoch 89, test loss: 0.171118\n",
      "Epoch 90, test loss: 0.171172\n",
      "Epoch 91, test loss: 0.171139\n",
      "Epoch 92, test loss: 0.170949\n",
      "Epoch 93, test loss: 0.171633\n",
      "Epoch 94, test loss: 0.171394\n",
      "Epoch 95, test loss: 0.170657\n",
      "Epoch 96, test loss: 0.172375\n",
      "Epoch 97, test loss: 0.171689\n",
      "Epoch 98, test loss: 0.171328\n",
      "Epoch 99, test loss: 0.171120\n",
      "Epoch 100, test loss: 0.172201\n",
      "Epoch 101, test loss: 0.171014\n",
      "Epoch 102, test loss: 0.171633\n",
      "Epoch 103, test loss: 0.171279\n",
      "Epoch 104, test loss: 0.172997\n",
      "Epoch 105, test loss: 0.171507\n",
      "Epoch 106, test loss: 0.171432\n",
      "Epoch 107, test loss: 0.172184\n",
      "Epoch 108, test loss: 0.171390\n",
      "Epoch 109, test loss: 0.171541\n",
      "Epoch 110, test loss: 0.172690\n",
      "Epoch 111, test loss: 0.174686\n",
      "Epoch 112, test loss: 0.171276\n",
      "Epoch 113, test loss: 0.171436\n",
      "Epoch 114, test loss: 0.171284\n",
      "Epoch 115, test loss: 0.171375\n",
      "Epoch 116, test loss: 0.171540\n",
      "Epoch 117, test loss: 0.172188\n",
      "Epoch 118, test loss: 0.173448\n",
      "Epoch 119, test loss: 0.171295\n",
      "Epoch 120, test loss: 0.173335\n",
      "Epoch 121, test loss: 0.172787\n",
      "Epoch 122, test loss: 0.172379\n",
      "Epoch 123, test loss: 0.171395\n",
      "Epoch 124, test loss: 0.171446\n",
      "Epoch 125, test loss: 0.171154\n",
      "Epoch 126, test loss: 0.171478\n",
      "Epoch 127, test loss: 0.171594\n",
      "Epoch 128, test loss: 0.171844\n",
      "Epoch 129, test loss: 0.171085\n",
      "Epoch 130, test loss: 0.171223\n",
      "Epoch 131, test loss: 0.171784\n",
      "Epoch 132, test loss: 0.171333\n",
      "Epoch 133, test loss: 0.171328\n",
      "Epoch 134, test loss: 0.173513\n",
      "Epoch 135, test loss: 0.171683\n",
      "Epoch 136, test loss: 0.172354\n",
      "Epoch 137, test loss: 0.173428\n",
      "Epoch 138, test loss: 0.173191\n",
      "Epoch 139, test loss: 0.171417\n",
      "Epoch 140, test loss: 0.172050\n",
      "Epoch 141, test loss: 0.171702\n",
      "Epoch 142, test loss: 0.171503\n",
      "Epoch 143, test loss: 0.171245\n",
      "Epoch 144, test loss: 0.171807\n",
      "Epoch 145, test loss: 0.171972\n",
      "Epoch 146, test loss: 0.171372\n",
      "Epoch 147, test loss: 0.171425\n",
      "Epoch 148, test loss: 0.171562\n",
      "Epoch 149, test loss: 0.171696\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../ohio_results\\ph_6_sh6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg.CGMSDataSeg object at 0x000001F610867160>\n",
      "Epoch 0, test loss: 0.214495\n",
      "Epoch 1, test loss: 0.183645\n",
      "Epoch 2, test loss: 0.181025\n",
      "Epoch 3, test loss: 0.179640\n",
      "Epoch 4, test loss: 0.178816\n",
      "Epoch 5, test loss: 0.179856\n",
      "Epoch 6, test loss: 0.176758\n",
      "Epoch 7, test loss: 0.176304\n",
      "Epoch 8, test loss: 0.175811\n",
      "Epoch 9, test loss: 0.176013\n",
      "Epoch 10, test loss: 0.177201\n",
      "Epoch 11, test loss: 0.175428\n",
      "Epoch 12, test loss: 0.176519\n",
      "Epoch 13, test loss: 0.177039\n",
      "Epoch 14, test loss: 0.175553\n",
      "Epoch 15, test loss: 0.175454\n",
      "Epoch 16, test loss: 0.175207\n",
      "Epoch 17, test loss: 0.175818\n",
      "Epoch 18, test loss: 0.174627\n",
      "Epoch 19, test loss: 0.174915\n",
      "Epoch 20, test loss: 0.176745\n",
      "Epoch 21, test loss: 0.174629\n",
      "Epoch 22, test loss: 0.175979\n",
      "Epoch 23, test loss: 0.174495\n",
      "Epoch 24, test loss: 0.174993\n",
      "Epoch 25, test loss: 0.177147\n",
      "Epoch 26, test loss: 0.174365\n",
      "Epoch 27, test loss: 0.174676\n",
      "Epoch 28, test loss: 0.176052\n",
      "Epoch 29, test loss: 0.174228\n",
      "Epoch 30, test loss: 0.174282\n",
      "Epoch 31, test loss: 0.174029\n",
      "Epoch 32, test loss: 0.174059\n",
      "Epoch 33, test loss: 0.175127\n",
      "Epoch 34, test loss: 0.176880\n",
      "Epoch 35, test loss: 0.174570\n",
      "Epoch 36, test loss: 0.174293\n",
      "Epoch 37, test loss: 0.176725\n",
      "Epoch 38, test loss: 0.174132\n",
      "Epoch 39, test loss: 0.174441\n",
      "Epoch 40, test loss: 0.173793\n",
      "Epoch 41, test loss: 0.174552\n",
      "Epoch 42, test loss: 0.174762\n",
      "Epoch 43, test loss: 0.175605\n",
      "Epoch 44, test loss: 0.174134\n",
      "Epoch 45, test loss: 0.173677\n",
      "Epoch 46, test loss: 0.174785\n",
      "Epoch 47, test loss: 0.173842\n",
      "Epoch 48, test loss: 0.174191\n",
      "Epoch 49, test loss: 0.174255\n",
      "Epoch 50, test loss: 0.173601\n",
      "Epoch 51, test loss: 0.174139\n",
      "Epoch 52, test loss: 0.173615\n",
      "Epoch 53, test loss: 0.173843\n",
      "Epoch 54, test loss: 0.175652\n",
      "Epoch 55, test loss: 0.173878\n",
      "Epoch 56, test loss: 0.173230\n",
      "Epoch 57, test loss: 0.173272\n",
      "Epoch 58, test loss: 0.173588\n",
      "Epoch 59, test loss: 0.173623\n",
      "Epoch 60, test loss: 0.175601\n",
      "Epoch 61, test loss: 0.174928\n",
      "Epoch 62, test loss: 0.175324\n",
      "Epoch 63, test loss: 0.174074\n",
      "Epoch 64, test loss: 0.173659\n",
      "Epoch 65, test loss: 0.175119\n",
      "Epoch 66, test loss: 0.173006\n",
      "Epoch 67, test loss: 0.174110\n",
      "Epoch 68, test loss: 0.173089\n",
      "Epoch 69, test loss: 0.174750\n",
      "Epoch 70, test loss: 0.172855\n",
      "Epoch 71, test loss: 0.174760\n",
      "Epoch 72, test loss: 0.173915\n",
      "Epoch 73, test loss: 0.173278\n",
      "Epoch 74, test loss: 0.173001\n",
      "Epoch 75, test loss: 0.172957\n",
      "Epoch 76, test loss: 0.173391\n",
      "Epoch 77, test loss: 0.173200\n",
      "Epoch 78, test loss: 0.172695\n",
      "Epoch 79, test loss: 0.173907\n",
      "Epoch 80, test loss: 0.172855\n",
      "Epoch 81, test loss: 0.174323\n",
      "Epoch 82, test loss: 0.174250\n",
      "Epoch 83, test loss: 0.173134\n",
      "Epoch 84, test loss: 0.173005\n",
      "Epoch 85, test loss: 0.173077\n",
      "Epoch 86, test loss: 0.175151\n",
      "Epoch 87, test loss: 0.172612\n",
      "Epoch 88, test loss: 0.174048\n",
      "Epoch 89, test loss: 0.172648\n",
      "Epoch 90, test loss: 0.174468\n",
      "Epoch 91, test loss: 0.174038\n",
      "Epoch 92, test loss: 0.173683\n",
      "Epoch 93, test loss: 0.173737\n",
      "Epoch 94, test loss: 0.173364\n",
      "Epoch 95, test loss: 0.174512\n",
      "Epoch 96, test loss: 0.172264\n",
      "Epoch 97, test loss: 0.172488\n",
      "Epoch 98, test loss: 0.173906\n",
      "Epoch 99, test loss: 0.174969\n",
      "Epoch 100, test loss: 0.173720\n",
      "Epoch 101, test loss: 0.172642\n",
      "Epoch 102, test loss: 0.172685\n",
      "Epoch 103, test loss: 0.173062\n",
      "Epoch 104, test loss: 0.173017\n",
      "Epoch 105, test loss: 0.173243\n",
      "Epoch 106, test loss: 0.172324\n",
      "Epoch 107, test loss: 0.172642\n",
      "Epoch 108, test loss: 0.172557\n",
      "Epoch 109, test loss: 0.173011\n",
      "Epoch 110, test loss: 0.172772\n",
      "Epoch 111, test loss: 0.172662\n",
      "Epoch 112, test loss: 0.172184\n",
      "Epoch 113, test loss: 0.172203\n",
      "Epoch 114, test loss: 0.172953\n",
      "Epoch 115, test loss: 0.172780\n",
      "Epoch 116, test loss: 0.173412\n",
      "Epoch 117, test loss: 0.172917\n",
      "Epoch 118, test loss: 0.176573\n",
      "Epoch 119, test loss: 0.174286\n",
      "Epoch 120, test loss: 0.175226\n",
      "Epoch 121, test loss: 0.173023\n",
      "Epoch 122, test loss: 0.172990\n",
      "Epoch 123, test loss: 0.173560\n",
      "Epoch 124, test loss: 0.172273\n",
      "Epoch 125, test loss: 0.173514\n",
      "Epoch 126, test loss: 0.173180\n",
      "Epoch 127, test loss: 0.173464\n",
      "Epoch 128, test loss: 0.173094\n",
      "Epoch 129, test loss: 0.172317\n",
      "Epoch 130, test loss: 0.172285\n",
      "Epoch 131, test loss: 0.172686\n",
      "Epoch 132, test loss: 0.172537\n",
      "Epoch 133, test loss: 0.172996\n",
      "Epoch 134, test loss: 0.173167\n",
      "Epoch 135, test loss: 0.172893\n",
      "Epoch 136, test loss: 0.173704\n",
      "Epoch 137, test loss: 0.172721\n",
      "Epoch 138, test loss: 0.172199\n",
      "Epoch 139, test loss: 0.173907\n",
      "Epoch 140, test loss: 0.172298\n",
      "Epoch 141, test loss: 0.173344\n",
      "Epoch 142, test loss: 0.175321\n",
      "Epoch 143, test loss: 0.173040\n",
      "Epoch 144, test loss: 0.172692\n",
      "Epoch 145, test loss: 0.172205\n",
      "Epoch 146, test loss: 0.172643\n",
      "Epoch 147, test loss: 0.172468\n",
      "Epoch 148, test loss: 0.172593\n",
      "Epoch 149, test loss: 0.172579\n",
      "Epoch 150, test loss: 0.172199\n",
      "Epoch 151, test loss: 0.172626\n",
      "Epoch 152, test loss: 0.172672\n",
      "Epoch 153, test loss: 0.172469\n",
      "Epoch 154, test loss: 0.171997\n",
      "Epoch 155, test loss: 0.172179\n",
      "Epoch 156, test loss: 0.172361\n",
      "Epoch 157, test loss: 0.172502\n",
      "Epoch 158, test loss: 0.172258\n",
      "Epoch 159, test loss: 0.174705\n",
      "Epoch 160, test loss: 0.172402\n",
      "Epoch 161, test loss: 0.172380\n",
      "Epoch 162, test loss: 0.172613\n",
      "Epoch 163, test loss: 0.172441\n",
      "Epoch 164, test loss: 0.174121\n",
      "Epoch 165, test loss: 0.172479\n",
      "Epoch 166, test loss: 0.172383\n",
      "Epoch 167, test loss: 0.172045\n",
      "Epoch 168, test loss: 0.172752\n",
      "Epoch 169, test loss: 0.172126\n",
      "Epoch 170, test loss: 0.172148\n",
      "Epoch 171, test loss: 0.172756\n",
      "Epoch 172, test loss: 0.171943\n",
      "Epoch 173, test loss: 0.172492\n",
      "Epoch 174, test loss: 0.172330\n",
      "Epoch 175, test loss: 0.174330\n",
      "Epoch 176, test loss: 0.172011\n",
      "Epoch 177, test loss: 0.171800\n",
      "Epoch 178, test loss: 0.177451\n",
      "Epoch 179, test loss: 0.173152\n",
      "Epoch 180, test loss: 0.172653\n",
      "Epoch 181, test loss: 0.172359\n",
      "Epoch 182, test loss: 0.173150\n",
      "Epoch 183, test loss: 0.173114\n",
      "Epoch 184, test loss: 0.172468\n",
      "Epoch 185, test loss: 0.172094\n",
      "Epoch 186, test loss: 0.173059\n",
      "Epoch 187, test loss: 0.173163\n",
      "Epoch 188, test loss: 0.172611\n",
      "Epoch 189, test loss: 0.175542\n",
      "Epoch 190, test loss: 0.176155\n",
      "Epoch 191, test loss: 0.172367\n",
      "Epoch 192, test loss: 0.173784\n",
      "Epoch 193, test loss: 0.172256\n",
      "Epoch 194, test loss: 0.171897\n",
      "Epoch 195, test loss: 0.172901\n",
      "Epoch 196, test loss: 0.175717\n",
      "Epoch 197, test loss: 0.172017\n",
      "Epoch 198, test loss: 0.172430\n",
      "Epoch 199, test loss: 0.173811\n",
      "Epoch 200, test loss: 0.172334\n",
      "Epoch 201, test loss: 0.174597\n",
      "Epoch 202, test loss: 0.171937\n",
      "Epoch 203, test loss: 0.172050\n",
      "Epoch 204, test loss: 0.172289\n",
      "Epoch 205, test loss: 0.172742\n",
      "Epoch 206, test loss: 0.175364\n",
      "Epoch 207, test loss: 0.171956\n",
      "Epoch 208, test loss: 0.172867\n",
      "Epoch 209, test loss: 0.174085\n",
      "Epoch 210, test loss: 0.172898\n",
      "Epoch 211, test loss: 0.172193\n",
      "Epoch 212, test loss: 0.171934\n",
      "Epoch 213, test loss: 0.173397\n",
      "Epoch 214, test loss: 0.173024\n",
      "Epoch 215, test loss: 0.171874\n",
      "Epoch 216, test loss: 0.172051\n",
      "Epoch 217, test loss: 0.171836\n",
      "Epoch 218, test loss: 0.173137\n",
      "Epoch 219, test loss: 0.172021\n",
      "Epoch 220, test loss: 0.172232\n",
      "Epoch 221, test loss: 0.172739\n",
      "Epoch 222, test loss: 0.172081\n",
      "Epoch 223, test loss: 0.171738\n",
      "Epoch 224, test loss: 0.172133\n",
      "Epoch 225, test loss: 0.172434\n",
      "Epoch 226, test loss: 0.171991\n",
      "Epoch 227, test loss: 0.172858\n",
      "Epoch 228, test loss: 0.171705\n",
      "Epoch 229, test loss: 0.171658\n",
      "Epoch 230, test loss: 0.172905\n",
      "Epoch 231, test loss: 0.172571\n",
      "Epoch 232, test loss: 0.173187\n",
      "Epoch 233, test loss: 0.172958\n",
      "Epoch 234, test loss: 0.172415\n",
      "Epoch 235, test loss: 0.172979\n",
      "Epoch 236, test loss: 0.171614\n",
      "Epoch 237, test loss: 0.172861\n",
      "Epoch 238, test loss: 0.171987\n",
      "Epoch 239, test loss: 0.172605\n",
      "Epoch 240, test loss: 0.172337\n",
      "Epoch 241, test loss: 0.171965\n",
      "Epoch 242, test loss: 0.176700\n",
      "Epoch 243, test loss: 0.171945\n",
      "Epoch 244, test loss: 0.171905\n",
      "Epoch 245, test loss: 0.172544\n",
      "Epoch 246, test loss: 0.172233\n",
      "Epoch 247, test loss: 0.172797\n",
      "Epoch 248, test loss: 0.173764\n",
      "Epoch 249, test loss: 0.173477\n",
      "Epoch 250, test loss: 0.171812\n",
      "Epoch 251, test loss: 0.172055\n",
      "Epoch 252, test loss: 0.172383\n",
      "Epoch 253, test loss: 0.173153\n",
      "Epoch 254, test loss: 0.172662\n",
      "Epoch 255, test loss: 0.171950\n",
      "Epoch 256, test loss: 0.171972\n",
      "Epoch 257, test loss: 0.175458\n",
      "Epoch 258, test loss: 0.172000\n",
      "Epoch 259, test loss: 0.176544\n",
      "Epoch 260, test loss: 0.171766\n",
      "Epoch 261, test loss: 0.173343\n",
      "Epoch 262, test loss: 0.171497\n",
      "Epoch 263, test loss: 0.172634\n",
      "Epoch 264, test loss: 0.173497\n",
      "Epoch 265, test loss: 0.171787\n",
      "Epoch 266, test loss: 0.172124\n",
      "Epoch 267, test loss: 0.172001\n",
      "Epoch 268, test loss: 0.172200\n",
      "Epoch 269, test loss: 0.171827\n",
      "Epoch 270, test loss: 0.172466\n",
      "Epoch 271, test loss: 0.172777\n",
      "Epoch 272, test loss: 0.171641\n",
      "Epoch 273, test loss: 0.172108\n",
      "Epoch 274, test loss: 0.172578\n",
      "Epoch 275, test loss: 0.171970\n",
      "Epoch 276, test loss: 0.172556\n",
      "Epoch 277, test loss: 0.172114\n",
      "Epoch 278, test loss: 0.172574\n",
      "Epoch 279, test loss: 0.172226\n",
      "Epoch 280, test loss: 0.173575\n",
      "Epoch 281, test loss: 0.171792\n",
      "Epoch 282, test loss: 0.172026\n",
      "Epoch 283, test loss: 0.173204\n",
      "Epoch 284, test loss: 0.172354\n",
      "Epoch 285, test loss: 0.172120\n",
      "Epoch 286, test loss: 0.174388\n",
      "Epoch 287, test loss: 0.172337\n",
      "Epoch 288, test loss: 0.171904\n",
      "Epoch 289, test loss: 0.171614\n",
      "Epoch 290, test loss: 0.174894\n",
      "Epoch 291, test loss: 0.174539\n",
      "Epoch 292, test loss: 0.172768\n",
      "Epoch 293, test loss: 0.171617\n",
      "Epoch 294, test loss: 0.172019\n",
      "Epoch 295, test loss: 0.171960\n",
      "Epoch 296, test loss: 0.174850\n",
      "Epoch 297, test loss: 0.176157\n",
      "Epoch 298, test loss: 0.172372\n",
      "Epoch 299, test loss: 0.174689\n",
      "Epoch 300, test loss: 0.173380\n",
      "Epoch 301, test loss: 0.171943\n",
      "Epoch 302, test loss: 0.171912\n",
      "Epoch 303, test loss: 0.171533\n",
      "Epoch 304, test loss: 0.172461\n",
      "Epoch 305, test loss: 0.172031\n",
      "Epoch 306, test loss: 0.172332\n",
      "Epoch 307, test loss: 0.172753\n",
      "Epoch 308, test loss: 0.171645\n",
      "Epoch 309, test loss: 0.171827\n",
      "Epoch 310, test loss: 0.172657\n",
      "Epoch 311, test loss: 0.171806\n",
      "Epoch 312, test loss: 0.171728\n",
      "Epoch 313, test loss: 0.175408\n",
      "Epoch 314, test loss: 0.172977\n",
      "Epoch 315, test loss: 0.172422\n",
      "Epoch 316, test loss: 0.171932\n",
      "Epoch 317, test loss: 0.173301\n",
      "Epoch 318, test loss: 0.172009\n",
      "Epoch 319, test loss: 0.172289\n",
      "Epoch 320, test loss: 0.172601\n",
      "Epoch 321, test loss: 0.173898\n",
      "Epoch 322, test loss: 0.175255\n",
      "Epoch 323, test loss: 0.171468\n",
      "Epoch 324, test loss: 0.172349\n",
      "Epoch 325, test loss: 0.172340\n",
      "Epoch 326, test loss: 0.171563\n",
      "Epoch 327, test loss: 0.173096\n",
      "Epoch 328, test loss: 0.176695\n",
      "Epoch 329, test loss: 0.172163\n",
      "Epoch 330, test loss: 0.173051\n",
      "Epoch 331, test loss: 0.172502\n",
      "Epoch 332, test loss: 0.174083\n",
      "Epoch 333, test loss: 0.172858\n",
      "Epoch 334, test loss: 0.172453\n",
      "Epoch 335, test loss: 0.171991\n",
      "Epoch 336, test loss: 0.172129\n",
      "Epoch 337, test loss: 0.172086\n",
      "Epoch 338, test loss: 0.171617\n",
      "Epoch 339, test loss: 0.172048\n",
      "Epoch 340, test loss: 0.171521\n",
      "Epoch 341, test loss: 0.172504\n",
      "Epoch 342, test loss: 0.172447\n",
      "Epoch 343, test loss: 0.171698\n",
      "Epoch 344, test loss: 0.173488\n",
      "Epoch 345, test loss: 0.171726\n",
      "Epoch 346, test loss: 0.171821\n",
      "Epoch 347, test loss: 0.171946\n",
      "Epoch 348, test loss: 0.171404\n",
      "Epoch 349, test loss: 0.172438\n",
      "Epoch 350, test loss: 0.172579\n",
      "Epoch 351, test loss: 0.173528\n",
      "Epoch 352, test loss: 0.172395\n",
      "Epoch 353, test loss: 0.172645\n",
      "Epoch 354, test loss: 0.171733\n",
      "Epoch 355, test loss: 0.171532\n",
      "Epoch 356, test loss: 0.173686\n",
      "Epoch 357, test loss: 0.171634\n",
      "Epoch 358, test loss: 0.172358\n",
      "Epoch 359, test loss: 0.171740\n",
      "Epoch 360, test loss: 0.173202\n",
      "Epoch 361, test loss: 0.173683\n",
      "Epoch 362, test loss: 0.171905\n",
      "Epoch 363, test loss: 0.172223\n",
      "Epoch 364, test loss: 0.171896\n",
      "Epoch 365, test loss: 0.172804\n",
      "Epoch 366, test loss: 0.173369\n",
      "Epoch 367, test loss: 0.171477\n",
      "Epoch 368, test loss: 0.172871\n",
      "Epoch 369, test loss: 0.171802\n",
      "Epoch 370, test loss: 0.177163\n",
      "Epoch 371, test loss: 0.171979\n",
      "Epoch 372, test loss: 0.174494\n",
      "Epoch 373, test loss: 0.172605\n",
      "Epoch 374, test loss: 0.171548\n",
      "Epoch 375, test loss: 0.171829\n",
      "Epoch 376, test loss: 0.173203\n",
      "Epoch 377, test loss: 0.172035\n",
      "Epoch 378, test loss: 0.172406\n",
      "Epoch 379, test loss: 0.171800\n",
      "Epoch 380, test loss: 0.172158\n",
      "Epoch 381, test loss: 0.172690\n",
      "Epoch 382, test loss: 0.171951\n",
      "Epoch 383, test loss: 0.172991\n",
      "Epoch 384, test loss: 0.173401\n",
      "Epoch 385, test loss: 0.171789\n",
      "Epoch 386, test loss: 0.172828\n",
      "Epoch 387, test loss: 0.171935\n",
      "Epoch 388, test loss: 0.171866\n",
      "Epoch 389, test loss: 0.172135\n",
      "Epoch 390, test loss: 0.171440\n",
      "Epoch 391, test loss: 0.171967\n",
      "Epoch 392, test loss: 0.172466\n",
      "Epoch 393, test loss: 0.171645\n",
      "Epoch 394, test loss: 0.171401\n",
      "Epoch 395, test loss: 0.173300\n",
      "Epoch 396, test loss: 0.171781\n",
      "Epoch 397, test loss: 0.174383\n",
      "Epoch 398, test loss: 0.171480\n",
      "Epoch 399, test loss: 0.171979\n",
      "Epoch 400, test loss: 0.171277\n",
      "Epoch 401, test loss: 0.171794\n",
      "Epoch 402, test loss: 0.171584\n",
      "Epoch 403, test loss: 0.171863\n",
      "Epoch 404, test loss: 0.171304\n",
      "Epoch 405, test loss: 0.171884\n",
      "Epoch 406, test loss: 0.172196\n",
      "Epoch 407, test loss: 0.172190\n",
      "Epoch 408, test loss: 0.173009\n",
      "Epoch 409, test loss: 0.172151\n",
      "Epoch 410, test loss: 0.172436\n",
      "Epoch 411, test loss: 0.171381\n",
      "Epoch 412, test loss: 0.174386\n",
      "Epoch 413, test loss: 0.171413\n",
      "Epoch 414, test loss: 0.171826\n",
      "Epoch 415, test loss: 0.171912\n",
      "Epoch 416, test loss: 0.171979\n",
      "Epoch 417, test loss: 0.172075\n",
      "Epoch 418, test loss: 0.171184\n",
      "Epoch 419, test loss: 0.172013\n",
      "Epoch 420, test loss: 0.171607\n",
      "Epoch 421, test loss: 0.171199\n",
      "Epoch 422, test loss: 0.171904\n",
      "Epoch 423, test loss: 0.171871\n",
      "Epoch 424, test loss: 0.171849\n",
      "Epoch 425, test loss: 0.172480\n",
      "Epoch 426, test loss: 0.171842\n",
      "Epoch 427, test loss: 0.171440\n",
      "Epoch 428, test loss: 0.171810\n",
      "Epoch 429, test loss: 0.171762\n",
      "Epoch 430, test loss: 0.172392\n",
      "Epoch 431, test loss: 0.172183\n",
      "Epoch 432, test loss: 0.171033\n",
      "Epoch 433, test loss: 0.172751\n",
      "Epoch 434, test loss: 0.171841\n",
      "Epoch 435, test loss: 0.171609\n",
      "Epoch 436, test loss: 0.172967\n",
      "Epoch 437, test loss: 0.173069\n",
      "Epoch 438, test loss: 0.171479\n",
      "Epoch 439, test loss: 0.175487\n",
      "Epoch 440, test loss: 0.171447\n",
      "Epoch 441, test loss: 0.171490\n",
      "Epoch 442, test loss: 0.171681\n",
      "Epoch 443, test loss: 0.171057\n",
      "Epoch 444, test loss: 0.171263\n",
      "Epoch 445, test loss: 0.173207\n",
      "Epoch 446, test loss: 0.172493\n",
      "Epoch 447, test loss: 0.171590\n",
      "Epoch 448, test loss: 0.171202\n",
      "Epoch 449, test loss: 0.172698\n"
     ]
    }
   ],
   "source": [
    "all_errs = []\n",
    "for year in list(pid_year.keys()):\n",
    "    pids = pid_year[year]\n",
    "    for pid in pids:\n",
    "        # only check results of 2020 patients\n",
    "        # if pid not in pid_2020:\n",
    "        #     continue\n",
    "        # 100 is dumb if set_cutpoint is used\n",
    "        train_pids = set(pid_2018 + pid_2020) - set([pid])\n",
    "        local_train_data = []\n",
    "        if use_2018_test:\n",
    "            local_train_data += test_data_2018\n",
    "        for k in train_pids:\n",
    "            local_train_data += train_data[k]\n",
    "        print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "        train_dataset.data = local_train_data\n",
    "        train_dataset.set_cutpoint = -1\n",
    "        train_dataset.reset(\n",
    "            sampling_horizon,\n",
    "            prediction_horizon,\n",
    "            scale,\n",
    "            100,\n",
    "            False,\n",
    "            outtype,\n",
    "            1,\n",
    "            standard,\n",
    "        )\n",
    "        \n",
    "        regressor(train_dataset, *argv, l_type, outdir)\n",
    "        # fine-tune on personal data\n",
    "        target_test_dataset = CGMSDataSeg(\n",
    "            \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/test/{pid}-ws-testing.xml\", 5\n",
    "            \n",
    "        )\n",
    "        target_test_dataset.set_cutpoint = 1\n",
    "        target_test_dataset.reset(\n",
    "            sampling_horizon,\n",
    "            prediction_horizon,\n",
    "            scale,\n",
    "            0.01,\n",
    "            False,\n",
    "            outtype,\n",
    "            1,\n",
    "            standard,\n",
    "        )\n",
    "        target_train_dataset = CGMSDataSeg(\n",
    "            \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/Modify_GenBG/OhioT1DM 2020/{year}/train/{pid}-ws-training.xml\", 5\n",
    "        )\n",
    "\n",
    "        target_train_dataset.set_cutpoint = -1\n",
    "        target_train_dataset.reset(\n",
    "            sampling_horizon,\n",
    "            prediction_horizon,\n",
    "            scale,\n",
    "            100,\n",
    "            False,\n",
    "            outtype,\n",
    "            1,\n",
    "            standard,\n",
    "        )\n",
    "        err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "        errs = [err]\n",
    "        transfer_res = [labels]\n",
    "        for i in range(1, 4):\n",
    "            err, labels = regressor_transfer(\n",
    "                target_train_dataset,\n",
    "                target_test_dataset,\n",
    "                config[\"batch_size\"],\n",
    "                epoch,\n",
    "                outdir,\n",
    "                i,\n",
    "            )\n",
    "            errs.append(err)\n",
    "            transfer_res.append(labels)\n",
    "        transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "        np.savetxt(\n",
    "            f\"{outdir}/{pid}.txt\",\n",
    "            transfer_res,\n",
    "            fmt=\"%.4f %.4f %.4f %.4f %.4f %.4f %.4f %.4f\",\n",
    "        )\n",
    "        all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f %.4f %.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain data: 9290194.0\n",
      "Building dataset, requesting data from 0 to 154\n",
      "x here is\n",
      "[[116. 117. 119. 116. 111. 110.]\n",
      " [117. 119. 116. 111. 110. 111.]\n",
      " [119. 116. 111. 110. 111. 113.]\n",
      " ...\n",
      " [258. 252. 251. 248. 244. 243.]\n",
      " [252. 251. 248. 244. 243. 244.]\n",
      " [251. 248. 244. 243. 244. 239.]]\n",
      "y here is\n",
      "[[126. 126. 126. 126. 126. 126.]\n",
      " [131. 131. 131. 131. 131. 131.]\n",
      " [136. 136. 136. 136. 136. 136.]\n",
      " ...\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [216. 216. 216. 216. 216. 216.]\n",
      " [213. 213. 213. 213. 213. 213.]]\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 3578/53194\n",
      "Found 154 continuous time series\n",
      "Data shape: (56774, 6), Train/test: 56772/2\n",
      "Train test ratio: 28386.00\n",
      "################################################################################\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "../time-gan/results/gen_hypo.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m local_train_data\n\u001b[0;32m     15\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mset_cutpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mouttype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstandard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m regressor(train_dataset, \u001b[38;5;241m*\u001b[39margv, l_type, outdir)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fine-tune and test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\AccurateBG\\accurate_bg\\cgms_data_seg.py:153\u001b[0m, in \u001b[0;36mCGMSDataSeg.reset\u001b[1;34m(self, sampling_horizon, prediction_horizon, scale, train_test_ratio, smooth, padding, target_weight, standardize)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_n)\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgan_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../time-gan/results/gen_hypo.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ../time-gan/results/gen_hypo.txt not found."
     ]
    }
   ],
   "source": [
    "# Loop - Previous\n",
    "use_2018_test = False\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\",5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 1.96096092e-01, 1.95441991e-01],\n",
       "       [5.63000000e+02, 1.86664909e-01, 1.89901307e-01],\n",
       "       [5.70000000e+02, 1.64349169e-01, 1.58961207e-01],\n",
       "       [5.88000000e+02, 1.89570844e-01, 1.85944960e-01],\n",
       "       [5.75000000e+02, 2.38005698e-01, 2.33785048e-01],\n",
       "       [5.91000000e+02, 2.12123454e-01, 2.10404932e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.19780169427394867\n",
      "Average of the third column: 0.19573990752299628\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.20091557254393896\n",
      "Average of the third column: 0.19819297641515732\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cgms_data_seg.CGMSDataSeg at 0x26982710f10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at it before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_2018_test = False\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "\n",
    "    # Test before fine-tuning\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    # Record error before fine-tuning\n",
    "    pre_fine_tune_err, pre_fine_tune_labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [pre_fine_tune_err]  # Initialize the error list with the error before fine-tuning\n",
    "    transfer_res = [pre_fine_tune_labels]\n",
    "\n",
    "    # Fine-tune and test\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/{pid}-ws-training.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/no_fine_tune_errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 2.07258314e-01, 2.07258314e-01, 1.96519971e-01],\n",
       "       [5.63000000e+02, 1.88313693e-01, 1.88313693e-01, 1.96036845e-01],\n",
       "       [5.70000000e+02, 2.06858590e-01, 2.06858590e-01, 1.70068905e-01],\n",
       "       [5.88000000e+02, 1.91546440e-01, 1.91546440e-01, 1.88959986e-01],\n",
       "       [5.75000000e+02, 2.41410896e-01, 2.41410896e-01, 2.40480691e-01],\n",
       "       [5.91000000e+02, 2.20461741e-01, 2.20461741e-01, 2.15062574e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.20499620338280997\n",
      "Average of the third column: 0.19652444124221802\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifically looking at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 43 segments\n"
     ]
    }
   ],
   "source": [
    "# Before the loop\n",
    "# ATTENTION: verify the \\ or / in different system window or unix\n",
    "# read in all patients data\n",
    "pid_2018 = [559, 563] # , 570, 588, 575, 591\n",
    "# pid_2020 = [540, 552, 544, 567, 584, 596]\n",
    "pid_year = {2018: pid_2018}\n",
    "# pid_year = {2018: pid_2018, 2020: pid_2020}\n",
    "\n",
    "train_data = dict()\n",
    "for year in list(pid_year.keys()):\n",
    "    pids = pid_year[year]\n",
    "    for pid in pids:\n",
    "        reader = DataReader(\n",
    "            \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/{year}/train/{pid}-ws-training.xml\", 6\n",
    "        )\n",
    "        train_data[pid] = reader.read()\n",
    "# add test data of 2018 patient\n",
    "use_2018_test = False\n",
    "standard = False  # do not use standard\n",
    "test_data_2018 = []\n",
    "for pid in pid_2018:\n",
    "    reader = DataReader(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 6\n",
    "    )\n",
    "    test_data_2018 += reader.read()\n",
    "\n",
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/559-ws-training.xml\", 6\n",
    ")\n",
    "sampling_horizon = 6\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(os.path.join(path, \"config.json\")) as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cgms_data_seg.CGMSDataSeg at 0x243ff637a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    print(pid)\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\",6\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/{pid}-ws-training.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2): \n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    # np.savetxt(\n",
    "    #     f\"{outdir}/{pid}.txt\",\n",
    "    #     transfer_res,\n",
    "    #     fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    # )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "\n",
    "# The first error represents the output after fine-tuned, the second error represents the output from transfer learning\n",
    "\n",
    "# np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 2.08077967e-01, 2.08703712e-01],\n",
       "       [5.63000000e+02, 1.90867111e-01, 1.93375528e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 3.16456079e-01, 2.21673578e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
