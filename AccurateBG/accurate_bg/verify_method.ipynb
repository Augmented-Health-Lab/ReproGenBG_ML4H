{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from cgms_data_seg import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option --epoch 150 --prediction_horizon 12\n",
    "epoch = 10\n",
    "ph = 6\n",
    "path = \"../ohio_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = ET.parse(r'C:\\Users\\username\\OneDrive\\Desktop\\BGprediction\\OhioT1DM\\2018\\train\\559-ws-training.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Initialize a list to hold all event data\n",
    "data = []\n",
    "\n",
    "\n",
    "# Loop through each glucose_level element in the patient element\n",
    "for glucose_level in root.findall('glucose_level'):\n",
    "    for event in glucose_level.findall('event'):\n",
    "        # Create a dictionary for each event\n",
    "        event_data = {\n",
    "            'ts': event.get('ts'),       # Get timestamp\n",
    "            'value': float(event.get('value')),  # Get glucose value\n",
    "            'patient_id': root.get('id'),        # Get patient ID from root\n",
    "            'weight': float(root.get('weight')),  # Get weight from root\n",
    "            'insulin_type': root.get('insulin_type')  # Get insulin type from root\n",
    "        }\n",
    "        # Add the dictionary to our data list\n",
    "        data.append(event_data)\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 44 segments\n"
     ]
    }
   ],
   "source": [
    "# Before the loop\n",
    "# ATTENTION: verify the \\ or / in different system window or unix\n",
    "# read in all patients data\n",
    "pid_2018 = [559, 563, 570, 588, 575, 591]\n",
    "# pid_2020 = [540, 552, 544, 567, 584, 596]\n",
    "pid_year = {2018: pid_2018}\n",
    "# pid_year = {2018: pid_2018, 2020: pid_2020}\n",
    "\n",
    "train_data = dict()\n",
    "for year in list(pid_year.keys()):\n",
    "    pids = pid_year[year]\n",
    "    for pid in pids:\n",
    "        reader = DataReader(\n",
    "            \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/{year}/train/{pid}-ws-training.xml\", 5\n",
    "        )\n",
    "        train_data[pid] = reader.read()\n",
    "# add test data of 2018 patient\n",
    "use_2018_test = False\n",
    "standard = False  # do not use standard\n",
    "test_data_2018 = []\n",
    "for pid in pid_2018:\n",
    "    reader = DataReader(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    test_data_2018 += reader.read()\n",
    "\n",
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/559-ws-training.xml\", 5\n",
    ")\n",
    "sampling_horizon = 12\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(os.path.join(path, \"config.json\")) as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "use_2018_test = False\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\",5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 1.96096092e-01, 1.95441991e-01],\n",
       "       [5.63000000e+02, 1.86664909e-01, 1.89901307e-01],\n",
       "       [5.70000000e+02, 1.64349169e-01, 1.58961207e-01],\n",
       "       [5.88000000e+02, 1.89570844e-01, 1.85944960e-01],\n",
       "       [5.75000000e+02, 2.38005698e-01, 2.33785048e-01],\n",
       "       [5.91000000e+02, 2.12123454e-01, 2.10404932e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.19780169427394867\n",
      "Average of the third column: 0.19573990752299628\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.20091557254393896\n",
      "Average of the third column: 0.19819297641515732\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cgms_data_seg.CGMSDataSeg at 0x26982710f10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at it before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_2018_test = False\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "\n",
    "    # Test before fine-tuning\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    # Record error before fine-tuning\n",
    "    pre_fine_tune_err, pre_fine_tune_labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [pre_fine_tune_err]  # Initialize the error list with the error before fine-tuning\n",
    "    transfer_res = [pre_fine_tune_labels]\n",
    "\n",
    "    # Fine-tune and test\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/{pid}-ws-training.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/no_fine_tune_errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 2.07258314e-01, 2.07258314e-01, 1.96519971e-01],\n",
       "       [5.63000000e+02, 1.88313693e-01, 1.88313693e-01, 1.96036845e-01],\n",
       "       [5.70000000e+02, 2.06858590e-01, 2.06858590e-01, 1.70068905e-01],\n",
       "       [5.88000000e+02, 1.91546440e-01, 1.91546440e-01, 1.88959986e-01],\n",
       "       [5.75000000e+02, 2.41410896e-01, 2.41410896e-01, 2.40480691e-01],\n",
       "       [5.91000000e+02, 2.20461741e-01, 2.20461741e-01, 2.15062574e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.20499620338280997\n",
      "Average of the third column: 0.19652444124221802\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifically looking at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 43 segments\n"
     ]
    }
   ],
   "source": [
    "# Before the loop\n",
    "# ATTENTION: verify the \\ or / in different system window or unix\n",
    "# read in all patients data\n",
    "pid_2018 = [559, 563] # , 570, 588, 575, 591\n",
    "# pid_2020 = [540, 552, 544, 567, 584, 596]\n",
    "pid_year = {2018: pid_2018}\n",
    "# pid_year = {2018: pid_2018, 2020: pid_2020}\n",
    "\n",
    "train_data = dict()\n",
    "for year in list(pid_year.keys()):\n",
    "    pids = pid_year[year]\n",
    "    for pid in pids:\n",
    "        reader = DataReader(\n",
    "            \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/{year}/train/{pid}-ws-training.xml\", 6\n",
    "        )\n",
    "        train_data[pid] = reader.read()\n",
    "# add test data of 2018 patient\n",
    "use_2018_test = False\n",
    "standard = False  # do not use standard\n",
    "test_data_2018 = []\n",
    "for pid in pid_2018:\n",
    "    reader = DataReader(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 6\n",
    "    )\n",
    "    test_data_2018 += reader.read()\n",
    "\n",
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/559-ws-training.xml\", 6\n",
    ")\n",
    "sampling_horizon = 6\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(os.path.join(path, \"config.json\")) as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cgms_data_seg.CGMSDataSeg at 0x243ff637a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "\n",
    "all_errs = []\n",
    "for pid in pid_2018:\n",
    "    print(pid)\n",
    "    train_pids = set(pid_2018) - set([pid])\n",
    "    local_train_data = []\n",
    "    if use_2018_test:\n",
    "        local_train_data += test_data_2018\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\",6\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "        \"ohio\", f\"C:/Users/username/OneDrive/Desktop/BGprediction/OhioT1DM/2018/train/{pid}-ws-training.xml\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2): \n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    # np.savetxt(\n",
    "    #     f\"{outdir}/{pid}.txt\",\n",
    "    #     transfer_res,\n",
    "    #     fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    # )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "\n",
    "# The first error represents the output after fine-tuned, the second error represents the output from transfer learning\n",
    "\n",
    "# np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%d %.4f %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 2.08077967e-01, 2.08703712e-01],\n",
       "       [5.63000000e+02, 1.90867111e-01, 1.93375528e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59000000e+02, 3.16456079e-01, 2.21673578e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
