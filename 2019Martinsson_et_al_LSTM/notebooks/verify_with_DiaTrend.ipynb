{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 17:17:28,379 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2025-01-29 17:17:28,379 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-29 17:17:28,379 DEBUG interactive is False\n",
      "2025-01-29 17:17:28,379 DEBUG platform is win32\n",
      "2025-01-29 17:17:28,413 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-29 17:17:28,413 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import utils\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to work on the DiaTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('_')[-1].split('.')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "    print(f\"Evaluating for patient_id: {patient_id}\")\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # Calculate MSE\n",
    "    # mse = np.mean((y_test - y_pred) ** 2)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # print(\"RMSE: \", rmse)\n",
    "    # print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_number in range(1, 6):\n",
    "    yaml_filepath = f\"./original_diatrend_experiments_120min/all_final_experiment_fold{fold_number}.yaml\"\n",
    "    mode = \"train\"\n",
    "\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        \n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)\n",
    "\n",
    "\n",
    "    # Get all yaml files in the directory\n",
    "    yaml_files = glob.glob(f\"./original_diatrend_experiments_120min/fold{fold_number}_eval/*.yaml\")\n",
    "    mode = \"evaluate\"\n",
    "    for yaml_filepath in yaml_files:\n",
    "        cfgs = load_cfgs(yaml_filepath)\n",
    "        print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "        for cfg in cfgs:\n",
    "            seed = int(cfg['train']['seed'])\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Print the configuration - just to make sure that you loaded what you\n",
    "            # wanted to load\n",
    "\n",
    "            module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "            module_model         = load_module(cfg['model']['script_path'])\n",
    "            module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "            module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "            module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "            pp = pprint.PrettyPrinter(indent=4)\n",
    "            pp.pprint(cfg)\n",
    "\n",
    "            #print(\"loading dataset ...\")\n",
    "            #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "            #nb_past_steps_tmp = 36\n",
    "            #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "            x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "            #x_train = x_train[:,-nb_past_steps:,:]\n",
    "            #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "            #x_test = x_test[:,-nb_past_steps:,:]\n",
    "            print(\"x_train.shape: \", x_train.shape)\n",
    "            print(\"y_train.shape: \", y_train.shape)\n",
    "            print(\"x_valid.shape: \", x_valid.shape)\n",
    "            print(\"y_valid.shape: \", y_valid.shape)\n",
    "            print(\"x_test.shape: \", x_test.shape)\n",
    "            print(\"y_test.shape: \", y_test.shape)\n",
    "            #print(\"loading optimizer ...\")\n",
    "            optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "            #print(\"loading loss function ...\")\n",
    "            loss_function = module_loss_function.load()\n",
    "            #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "            #print(\"loading model ...\")\n",
    "            if 'tf_nll' in loss_function.__name__:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1]*2,\n",
    "                    cfg['model']\n",
    "                )\n",
    "            else:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1],\n",
    "                    cfg['model']\n",
    "                )\n",
    "\n",
    "            if 'initial_weights_path' in cfg['train']:\n",
    "                #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function\n",
    "            )\n",
    "\n",
    "            #print(model.summary())\n",
    "\n",
    "            # training mode\n",
    "            if mode == 'train':\n",
    "                #print(\"training model ...\")\n",
    "                train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "            if mode == 'plot_nll':\n",
    "                plot_nll(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_noise_experiment':\n",
    "                plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_seg':\n",
    "                plot_seg(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_dist':\n",
    "                plot_target_distribution(y_test, cfg)\n",
    "\n",
    "            # evaluation mode\n",
    "            if mode == 'evaluate':\n",
    "                evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject23_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject24_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject25_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject26_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject27_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject28_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject29_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject30_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject31_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject32_evaluation.yaml',\n",
       " './original_diatrend_experiments_30min/fold3_eval\\\\processed_cgm_data_Subject33_evaluation.yaml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all yaml files in the directory\n",
    "yaml_files = glob.glob(\"./original_diatrend_experiments_30min/fold3_eval/*.yaml\")\n",
    "\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject23.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject23.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11617, 6, 1)\n",
      "y_test.shape:  (11617, 1)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:47:57,217 WARNING Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject23\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-17 09:47:57,321 DEBUG Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject23\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject24.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9961, 6, 1)\n",
      "y_test.shape:  (9961, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:47:59,875 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject24\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject24\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject25.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11796, 6, 1)\n",
      "y_test.shape:  (11796, 1)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:02,750 WARNING Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject25\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject25\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject26.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject26.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11691, 6, 1)\n",
      "y_test.shape:  (11691, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:05,654 WARNING Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject26\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject26\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject27.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject27.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11879, 6, 1)\n",
      "y_test.shape:  (11879, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:08,546 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject27\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject27\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject28.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject28.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11810, 6, 1)\n",
      "y_test.shape:  (11810, 1)\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:11,341 WARNING Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject28\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject28\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject29.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject29.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6579, 6, 1)\n",
      "y_test.shape:  (6579, 1)\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:14,167 WARNING Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject29\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject29\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject30.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject30.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11790, 6, 1)\n",
      "y_test.shape:  (11790, 1)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:16,479 WARNING Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject30\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject30\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject31.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject31.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11941, 6, 1)\n",
      "y_test.shape:  (11941, 1)\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:19,355 WARNING Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject31\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject31\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject32.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject32.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11706, 6, 1)\n",
      "y_test.shape:  (11706, 1)\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:22,340 WARNING Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject32\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject32\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject33.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject33.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11705, 6, 1)\n",
      "y_test.shape:  (11705, 1)\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-17 09:48:25,393 WARNING Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Evaluating for patient_id: Subject33\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  Subject33\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yaml_filepath in yaml_files:\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
       " 'nb_past_steps': 6,\n",
       " 'param_nb_future_steps': [6],\n",
       " 'scale': 0.01,\n",
       " 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
       " 'test_fraction': 1.0,\n",
       " 'train_fraction': 0.0,\n",
       " 'valid_fraction': 0.0,\n",
       " 'nb_future_steps': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a similar function to construct the Diatrend dataset\n",
    "def load_diatrend_series(path):\n",
    "    subject = pd.read_csv(path)\n",
    "\n",
    "    # Lists to store the results\n",
    "    parsed_dates = []\n",
    "    values = []\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in subject.iterrows():\n",
    "        # Parse the date using the custom function\n",
    "        parsed_date = parse_date(row['date'])\n",
    "        \n",
    "        # Append the parsed date and corresponding value to the lists\n",
    "        parsed_dates.append(parsed_date)\n",
    "        values.append(float(row['mg/dl']))\n",
    "\n",
    "    # Now 'parsed_dates' and 'values' contain your data\n",
    "    # print(parsed_dates)\n",
    "    # print(values)\n",
    "    index = pd.DatetimeIndex(parsed_dates)\n",
    "    series = pd.Series(values, index=index)\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg):\n",
    "    if os.path.basename(cfg['csv_path']) == 'all':\n",
    "        print(\"loading training data for all patients ...\")\n",
    "        csvs = os.path.join(os.path.dirname(cfg['csv_path']), \"*.csv\")\n",
    "        csv_paths = glob.glob(csvs)\n",
    "        tups = []\n",
    "        for csv_path in csv_paths:\n",
    "            cfg['csv_path'] = csv_path\n",
    "            tups.append(load_data(cfg))\n",
    "        x_train = np.concatenate([t[0] for t in tups], axis=0)\n",
    "        y_train = np.concatenate([t[1] for t in tups], axis=0)\n",
    "        \n",
    "        x_valid = np.concatenate([t[2] for t in tups], axis=0)\n",
    "        y_valid = np.concatenate([t[3] for t in tups], axis=0)\n",
    "        x_test = np.concatenate([t[4] for t in tups], axis=0)\n",
    "        y_test = np.concatenate([t[5] for t in tups], axis=0)\n",
    "\n",
    "        cfg['csv_path'] = 'all'\n",
    "        return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
    "    else:\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(cfg)\n",
    "        return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
    "\n",
    "def load_data(cfg):\n",
    "    csv_path        = cfg['csv_path']\n",
    "    nb_past_steps   = int(cfg['nb_past_steps'])\n",
    "    nb_future_steps = int(cfg['nb_future_steps'])\n",
    "    train_fraction  = float(cfg['train_fraction'])\n",
    "    valid_fraction  = float(cfg['valid_fraction'])\n",
    "    test_fraction   = float(cfg['test_fraction'])\n",
    "    print(\"nb_future_steps \", nb_future_steps)\n",
    "\n",
    "    xs, ys = load_glucose_data(csv_path, nb_past_steps, nb_future_steps)\n",
    "    ys = np.expand_dims(ys, axis=1)\n",
    "\n",
    "    x_train, x_valid, x_test = utils.split_data(xs, train_fraction,\n",
    "            valid_fraction, test_fraction)\n",
    "    y_train, y_valid, y_test = utils.split_data(ys, train_fraction,\n",
    "            valid_fraction, test_fraction)\n",
    "\n",
    "    # scale data\n",
    "    scale = float(cfg['scale'])\n",
    "    x_train *= scale\n",
    "    y_train *= scale\n",
    "    x_valid *= scale\n",
    "    y_valid *= scale\n",
    "    x_test  *= scale\n",
    "    y_test  *= scale\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
    "\n",
    "def load_glucose_data(csv_path, nb_past_steps, nb_future_steps):\n",
    "    df_glucose_level = load_diatrend_series(csv_path)\n",
    "    dt = df_glucose_level.index.to_series().diff().dropna()\n",
    "    # print(dt.size)\n",
    "    idx_breaks = np.argwhere(dt!=pd.Timedelta(6, 'm'))\n",
    "    # print(dt.size)\n",
    "\n",
    "    # It would be possible to load more features here\n",
    "    nd_glucose_level = df_glucose_level.values\n",
    "    consecutive_segments = np.split(nd_glucose_level, idx_breaks.flatten())\n",
    "\n",
    "    consecutive_segments = [c for c in consecutive_segments if len(c) >=\n",
    "            nb_past_steps+nb_future_steps]\n",
    "    print(len(consecutive_segments))\n",
    "\n",
    "    sups = [utils.sequence_to_supervised(c, nb_past_steps, nb_future_steps) for\n",
    "            c in consecutive_segments]\n",
    "\n",
    "    xss = [sup[0] for sup in sups]\n",
    "    yss = [sup[1] for sup in sups]\n",
    "\n",
    "    xs = np.concatenate(xss)\n",
    "    ys = np.concatenate(yss)\n",
    "\n",
    "    return np.expand_dims(xs, axis=2), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
