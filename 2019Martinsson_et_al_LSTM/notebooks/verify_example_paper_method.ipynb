{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 09:51:51,998 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2025-01-15 09:51:51,998 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-15 09:51:52,012 DEBUG interactive is False\n",
      "2025-01-15 09:51:52,012 DEBUG platform is win32\n",
      "2025-01-15 09:51:52,058 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-15 09:51:52,061 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments_120min/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # Calculate SEG\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # Calculate baseline (t0) metrics\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    t0_mse = np.mean((y_test - t0) ** 2)\n",
    "    t0_mape = np.mean(np.abs((y_test - t0) / y_test)) * 100\n",
    "    t0_mae = np.mean(np.abs(y_test - t0))\n",
    "    \n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mse))\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mape))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # # Calculate MAE\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mae))\n",
    "\n",
    "    # Print all metrics\n",
    "    # print(\"Model Performance Metrics:\")\n",
    "    # print(\"-\" * 25)\n",
    "    # print(f\"RMSE: {rmse:.2f}\")\n",
    "    # print(f\"MSE:  {mse:.2f}\")\n",
    "    # print(f\"MAPE: {mape:.2f}%\")\n",
    "    # # print(f\"SEG:  {seg:.2f}\")\n",
    "    # print(\"\\nBaseline (t0) Performance:\")\n",
    "    # print(\"-\" * 25)\n",
    "    # print(f\"t0 RMSE: {t0_rmse:.2f}\")\n",
    "    # print(f\"t0 MSE:  {t0_mse:.2f}\")\n",
    "    # print(f\"t0 MAPE: {t0_mape:.2f}%\")\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 17:17:37,002 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\OhioT1DM 2020\\both\\all does not exist.\n",
      "2025-01-14 17:17:37,002 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh does not exist.\n",
      "Running 1 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2025-01-14 17:17:37,011 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\both\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (97653, 24, 1)\n",
      "y_train.shape:  (97653, 1)\n",
      "x_valid.shape:  (24407, 24, 1)\n",
      "y_valid.shape:  (24407, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:17:46,795 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-14 17:17:46,969 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-14 17:17:46,969 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-14 17:17:46,984 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 97653 samples, validate on 24407 samples\n",
      "Epoch 1/10000\n",
      "97653/97653 [==============================] - ETA: 0s - loss: 0.4538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97653/97653 [==============================] - 10s 107us/sample - loss: 0.4538 - val_loss: 0.0643\n",
      "Epoch 2/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: 0.0809 - val_loss: -0.0113\n",
      "Epoch 3/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: 0.0331 - val_loss: -0.0853\n",
      "Epoch 4/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.0079 - val_loss: -0.1087\n",
      "Epoch 5/10000\n",
      "97280/97653 [============================>.] - ETA: 0s - loss: -0.03502025-01-14 17:18:05,634 DEBUG Creating converter from 5 to 3\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.0350 - val_loss: -0.0041\n",
      "Epoch 6/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.0593 - val_loss: -0.0786\n",
      "Epoch 7/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.0712 - val_loss: -0.1472\n",
      "Epoch 8/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.0845 - val_loss: -0.1889\n",
      "Epoch 9/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.0945 - val_loss: -0.1809\n",
      "Epoch 10/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1116 - val_loss: -0.1811\n",
      "Epoch 11/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1123 - val_loss: -0.1710\n",
      "Epoch 12/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1288 - val_loss: -0.1855\n",
      "Epoch 13/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1340 - val_loss: -0.1765\n",
      "Epoch 14/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1306 - val_loss: -0.2038\n",
      "Epoch 15/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1389 - val_loss: -0.1867\n",
      "Epoch 16/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1511 - val_loss: -0.1986\n",
      "Epoch 17/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1558 - val_loss: -0.2124\n",
      "Epoch 18/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1592 - val_loss: -0.1991\n",
      "Epoch 19/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1665 - val_loss: -0.2036\n",
      "Epoch 20/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1714 - val_loss: -0.2241\n",
      "Epoch 21/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1743 - val_loss: -0.2212\n",
      "Epoch 22/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1826 - val_loss: -0.2232\n",
      "Epoch 23/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.1835 - val_loss: -0.2316\n",
      "Epoch 24/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1891 - val_loss: -0.2283\n",
      "Epoch 25/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1929 - val_loss: -0.2261\n",
      "Epoch 26/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1984 - val_loss: -0.2318\n",
      "Epoch 27/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.1987 - val_loss: -0.2279\n",
      "Epoch 28/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.2027 - val_loss: -0.2243\n",
      "Epoch 29/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.2068 - val_loss: -0.2185\n",
      "Epoch 30/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2064 - val_loss: -0.2187\n",
      "Epoch 31/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2075 - val_loss: -0.2403\n",
      "Epoch 32/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.2181 - val_loss: -0.2409\n",
      "Epoch 33/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2148 - val_loss: -0.2253\n",
      "Epoch 34/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2224 - val_loss: -0.2355\n",
      "Epoch 35/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2181 - val_loss: -0.2376\n",
      "Epoch 36/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2251 - val_loss: -0.2381\n",
      "Epoch 37/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2281 - val_loss: -0.2319\n",
      "Epoch 38/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2282 - val_loss: -0.2328\n",
      "Epoch 39/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2247 - val_loss: -0.2328\n",
      "Epoch 40/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2349 - val_loss: -0.2108\n",
      "Epoch 41/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2363 - val_loss: -0.2344\n",
      "Epoch 42/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2383 - val_loss: -0.2380\n",
      "Epoch 43/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2414 - val_loss: -0.2273\n",
      "Epoch 44/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.2433 - val_loss: -0.2482\n",
      "Epoch 45/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2490 - val_loss: -0.2540\n",
      "Epoch 46/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2488 - val_loss: -0.2443\n",
      "Epoch 47/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2466 - val_loss: -0.2592\n",
      "Epoch 48/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2539 - val_loss: -0.2430\n",
      "Epoch 49/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2530 - val_loss: -0.2448\n",
      "Epoch 50/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.2500 - val_loss: -0.2484\n",
      "Epoch 51/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2583 - val_loss: -0.2382\n",
      "Epoch 52/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2567 - val_loss: -0.2262\n",
      "Epoch 53/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2592 - val_loss: -0.2604\n",
      "Epoch 54/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2643 - val_loss: -0.2564\n",
      "Epoch 55/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.2659 - val_loss: -0.2500\n",
      "Epoch 56/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2679 - val_loss: -0.2411\n",
      "Epoch 57/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2660 - val_loss: -0.2439\n",
      "Epoch 58/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2711 - val_loss: -0.2549\n",
      "Epoch 59/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2699 - val_loss: -0.2526\n",
      "Epoch 60/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2702 - val_loss: -0.2573\n",
      "Epoch 61/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2706 - val_loss: -0.2536\n",
      "Epoch 62/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2738 - val_loss: -0.2471\n",
      "Epoch 63/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2751 - val_loss: -0.2464\n",
      "Epoch 64/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2752 - val_loss: -0.2468\n",
      "Epoch 65/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2790 - val_loss: -0.2454\n",
      "Epoch 66/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2806 - val_loss: -0.2590\n",
      "Epoch 67/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2776 - val_loss: -0.2550\n",
      "Epoch 68/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2824 - val_loss: -0.2449\n",
      "Epoch 69/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2826 - val_loss: -0.2530\n",
      "Epoch 70/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2870 - val_loss: -0.2377\n",
      "Epoch 71/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2871 - val_loss: -0.2641\n",
      "Epoch 72/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2882 - val_loss: -0.2527\n",
      "Epoch 73/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2867 - val_loss: -0.2461\n",
      "Epoch 74/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2891 - val_loss: -0.2445\n",
      "Epoch 75/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2927 - val_loss: -0.2421\n",
      "Epoch 76/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2914 - val_loss: -0.2549\n",
      "Epoch 77/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2971 - val_loss: -0.2400\n",
      "Epoch 78/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2952 - val_loss: -0.2620\n",
      "Epoch 79/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2962 - val_loss: -0.2523\n",
      "Epoch 80/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.2943 - val_loss: -0.2483\n",
      "Epoch 81/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3017 - val_loss: -0.2427\n",
      "Epoch 82/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3038 - val_loss: -0.2370\n",
      "Epoch 83/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3039 - val_loss: -0.2332\n",
      "Epoch 84/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3021 - val_loss: -0.2494\n",
      "Epoch 85/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3056 - val_loss: -0.2487\n",
      "Epoch 86/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.3066 - val_loss: -0.2482\n",
      "Epoch 87/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3063 - val_loss: -0.2508\n",
      "Epoch 88/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3046 - val_loss: -0.2386\n",
      "Epoch 89/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3096 - val_loss: -0.2397\n",
      "Epoch 90/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3087 - val_loss: -0.2363\n",
      "Epoch 91/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3083 - val_loss: -0.2402\n",
      "Epoch 92/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.3126 - val_loss: -0.2322\n",
      "Epoch 93/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3101 - val_loss: -0.2455\n",
      "Epoch 94/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3150 - val_loss: -0.2412\n",
      "Epoch 95/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3183 - val_loss: -0.2374\n",
      "Epoch 96/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3145 - val_loss: -0.2300\n",
      "Epoch 97/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3169 - val_loss: -0.2374\n",
      "Epoch 98/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3155 - val_loss: -0.2380\n",
      "Epoch 99/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3198 - val_loss: -0.2415\n",
      "Epoch 100/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3209 - val_loss: -0.2264\n",
      "Epoch 101/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3218 - val_loss: -0.2347\n",
      "Epoch 102/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3228 - val_loss: -0.2366\n",
      "Epoch 103/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.3251 - val_loss: -0.2333\n",
      "Epoch 104/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3240 - val_loss: -0.2306\n",
      "Epoch 105/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3272 - val_loss: -0.2230\n",
      "Epoch 106/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3305 - val_loss: -0.2214\n",
      "Epoch 107/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3301 - val_loss: -0.2335\n",
      "Epoch 108/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3324 - val_loss: -0.2352\n",
      "Epoch 109/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3328 - val_loss: -0.2279\n",
      "Epoch 110/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3360 - val_loss: -0.2273\n",
      "Epoch 111/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3326 - val_loss: -0.2126\n",
      "Epoch 112/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3381 - val_loss: -0.2181\n",
      "Epoch 113/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3364 - val_loss: -0.2191\n",
      "Epoch 114/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.3367 - val_loss: -0.2172\n",
      "Epoch 115/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3401 - val_loss: -0.2197\n",
      "Epoch 116/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.3443 - val_loss: -0.2202\n",
      "Epoch 117/10000\n",
      "97653/97653 [==============================] - 2s 24us/sample - loss: -0.3442 - val_loss: -0.2208\n",
      "Epoch 118/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3417 - val_loss: -0.2097\n",
      "Epoch 119/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3460 - val_loss: -0.2120\n",
      "Epoch 120/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3485 - val_loss: -0.2180\n",
      "Epoch 121/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.3452 - val_loss: -0.2087\n",
      "Epoch 122/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3439 - val_loss: -0.2093\n",
      "Epoch 123/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3540 - val_loss: -0.2032\n",
      "Epoch 124/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3535 - val_loss: -0.2006\n",
      "Epoch 125/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3529 - val_loss: -0.1980\n",
      "Epoch 126/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3547 - val_loss: -0.2030\n",
      "Epoch 127/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3586 - val_loss: -0.2013\n",
      "Epoch 128/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3579 - val_loss: -0.2006\n",
      "Epoch 129/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3602 - val_loss: -0.2054\n",
      "Epoch 130/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3623 - val_loss: -0.1927\n",
      "Epoch 131/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3649 - val_loss: -0.1906\n",
      "Epoch 132/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3661 - val_loss: -0.2004\n",
      "Epoch 133/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3670 - val_loss: -0.1680\n",
      "Epoch 134/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.3676 - val_loss: -0.1991\n",
      "Epoch 135/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3714 - val_loss: -0.1690\n",
      "Epoch 136/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3725 - val_loss: -0.1702\n",
      "Epoch 137/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3699 - val_loss: -0.1940\n",
      "Epoch 138/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3724 - val_loss: -0.1818\n",
      "Epoch 139/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3792 - val_loss: -0.1822\n",
      "Epoch 140/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.3788 - val_loss: -0.1647\n",
      "Epoch 141/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3787 - val_loss: -0.1565\n",
      "Epoch 142/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3816 - val_loss: -0.1483\n",
      "Epoch 143/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3853 - val_loss: -0.1793\n",
      "Epoch 144/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3837 - val_loss: -0.1661\n",
      "Epoch 145/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3870 - val_loss: -0.1588\n",
      "Epoch 146/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3887 - val_loss: -0.1743\n",
      "Epoch 147/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3888 - val_loss: -0.1564\n",
      "Epoch 148/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3918 - val_loss: -0.1315\n",
      "Epoch 149/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.3920 - val_loss: -0.1601\n",
      "Epoch 150/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.3982 - val_loss: -0.1311\n",
      "Epoch 151/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4000 - val_loss: -0.1220\n",
      "Epoch 152/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.4044 - val_loss: -0.1298\n",
      "Epoch 153/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4079 - val_loss: -0.1301\n",
      "Epoch 154/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.4097 - val_loss: -0.1290\n",
      "Epoch 155/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.4058 - val_loss: -0.1245\n",
      "Epoch 156/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4075 - val_loss: -0.1401\n",
      "Epoch 157/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4168 - val_loss: -0.1318\n",
      "Epoch 158/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4202 - val_loss: -0.1292\n",
      "Epoch 159/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4125 - val_loss: -0.1197\n",
      "Epoch 160/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.4202 - val_loss: -0.1314\n",
      "Epoch 161/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4195 - val_loss: -0.1214\n",
      "Epoch 162/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4271 - val_loss: -0.0798\n",
      "Epoch 163/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4275 - val_loss: -0.1107\n",
      "Epoch 164/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4302 - val_loss: -0.1137\n",
      "Epoch 165/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.4314 - val_loss: -0.1020\n",
      "Epoch 166/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.4362 - val_loss: -0.0882\n",
      "Epoch 167/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4359 - val_loss: -0.0705\n",
      "Epoch 168/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4407 - val_loss: -0.0780\n",
      "Epoch 169/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4471 - val_loss: -0.0713\n",
      "Epoch 170/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4455 - val_loss: -0.0566\n",
      "Epoch 171/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4522 - val_loss: -0.0548\n",
      "Epoch 172/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.4452 - val_loss: -0.0728\n",
      "Epoch 173/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4574 - val_loss: -0.0631\n",
      "Epoch 174/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4580 - val_loss: -0.0326\n",
      "Epoch 175/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4618 - val_loss: -0.0242\n",
      "Epoch 176/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4615 - val_loss: -0.0259\n",
      "Epoch 177/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4652 - val_loss: -0.0042\n",
      "Epoch 178/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4683 - val_loss: -0.0248\n",
      "Epoch 179/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4739 - val_loss: -0.0096\n",
      "Epoch 180/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4787 - val_loss: -0.0236\n",
      "Epoch 181/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4786 - val_loss: 0.0164\n",
      "Epoch 182/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4816 - val_loss: -0.0262\n",
      "Epoch 183/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4830 - val_loss: 0.0110\n",
      "Epoch 184/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.4867 - val_loss: 0.0550\n",
      "Epoch 185/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4894 - val_loss: 0.0084\n",
      "Epoch 186/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4997 - val_loss: 0.0025\n",
      "Epoch 187/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.4997 - val_loss: -0.0012\n",
      "Epoch 188/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5014 - val_loss: 0.0513\n",
      "Epoch 189/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5049 - val_loss: 0.0452\n",
      "Epoch 190/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5073 - val_loss: 0.0537\n",
      "Epoch 191/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5125 - val_loss: 0.0367\n",
      "Epoch 192/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5131 - val_loss: 0.0459\n",
      "Epoch 193/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5192 - val_loss: 0.0987\n",
      "Epoch 194/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5227 - val_loss: 0.0920\n",
      "Epoch 195/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5265 - val_loss: 0.0919\n",
      "Epoch 196/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5271 - val_loss: 0.1051\n",
      "Epoch 197/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5313 - val_loss: 0.0774\n",
      "Epoch 198/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.5368 - val_loss: 0.0948\n",
      "Epoch 199/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5411 - val_loss: 0.1417\n",
      "Epoch 200/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5417 - val_loss: 0.1391\n",
      "Epoch 201/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5512 - val_loss: 0.1176\n",
      "Epoch 202/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5547 - val_loss: 0.1506\n",
      "Epoch 203/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5618 - val_loss: 0.1273\n",
      "Epoch 204/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5620 - val_loss: 0.1941\n",
      "Epoch 205/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5676 - val_loss: 0.1974\n",
      "Epoch 206/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5680 - val_loss: 0.1276\n",
      "Epoch 207/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5760 - val_loss: 0.2192\n",
      "Epoch 208/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5787 - val_loss: 0.1987\n",
      "Epoch 209/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5842 - val_loss: 0.2258\n",
      "Epoch 210/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5875 - val_loss: 0.2487\n",
      "Epoch 211/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5875 - val_loss: 0.1819\n",
      "Epoch 212/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5927 - val_loss: 0.1848\n",
      "Epoch 213/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.5991 - val_loss: 0.2563\n",
      "Epoch 214/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.5967 - val_loss: 0.2295\n",
      "Epoch 215/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6022 - val_loss: 0.2997\n",
      "Epoch 216/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6074 - val_loss: 0.2782\n",
      "Epoch 217/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6118 - val_loss: 0.2264\n",
      "Epoch 218/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6166 - val_loss: 0.3358\n",
      "Epoch 219/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6217 - val_loss: 0.3237\n",
      "Epoch 220/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6292 - val_loss: 0.2468\n",
      "Epoch 221/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6289 - val_loss: 0.3021\n",
      "Epoch 222/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6330 - val_loss: 0.3221\n",
      "Epoch 223/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6392 - val_loss: 0.3488\n",
      "Epoch 224/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6430 - val_loss: 0.3380\n",
      "Epoch 225/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6448 - val_loss: 0.4004\n",
      "Epoch 226/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6504 - val_loss: 0.3934\n",
      "Epoch 227/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6537 - val_loss: 0.3685\n",
      "Epoch 228/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6586 - val_loss: 0.3578\n",
      "Epoch 229/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6597 - val_loss: 0.4498\n",
      "Epoch 230/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6645 - val_loss: 0.3751\n",
      "Epoch 231/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6682 - val_loss: 0.4347\n",
      "Epoch 232/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6676 - val_loss: 0.4056\n",
      "Epoch 233/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6779 - val_loss: 0.4320\n",
      "Epoch 234/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6798 - val_loss: 0.5123\n",
      "Epoch 235/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6861 - val_loss: 0.4834\n",
      "Epoch 236/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6869 - val_loss: 0.4419\n",
      "Epoch 237/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6920 - val_loss: 0.5899\n",
      "Epoch 238/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6966 - val_loss: 0.4772\n",
      "Epoch 239/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.6980 - val_loss: 0.4604\n",
      "Epoch 240/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7048 - val_loss: 0.5380\n",
      "Epoch 241/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7071 - val_loss: 0.5557\n",
      "Epoch 242/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7132 - val_loss: 0.5924\n",
      "Epoch 243/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7164 - val_loss: 0.6030\n",
      "Epoch 244/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7186 - val_loss: 0.5919\n",
      "Epoch 245/10000\n",
      "97653/97653 [==============================] - 2s 19us/sample - loss: -0.7268 - val_loss: 0.6070\n",
      "Epoch 246/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7279 - val_loss: 0.6508\n",
      "Epoch 247/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7335 - val_loss: 0.5752\n",
      "Epoch 248/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7367 - val_loss: 0.6535\n",
      "Epoch 249/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.7392 - val_loss: 0.7078\n",
      "Epoch 250/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7409 - val_loss: 0.6316\n",
      "Epoch 251/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.7457 - val_loss: 0.6323\n",
      "Epoch 252/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7509 - val_loss: 0.6927\n",
      "Epoch 253/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7514 - val_loss: 0.7059\n",
      "Epoch 254/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7550 - val_loss: 0.7868\n",
      "Epoch 255/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7619 - val_loss: 0.7279\n",
      "Epoch 256/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7643 - val_loss: 0.6813\n",
      "Epoch 257/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7675 - val_loss: 0.6959\n",
      "Epoch 258/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7711 - val_loss: 0.6892\n",
      "Epoch 259/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7749 - val_loss: 0.7479\n",
      "Epoch 260/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7813 - val_loss: 0.7875\n",
      "Epoch 261/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7820 - val_loss: 0.8023\n",
      "Epoch 262/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7854 - val_loss: 0.7383\n",
      "Epoch 263/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.7940 - val_loss: 0.8365\n",
      "Epoch 264/10000\n",
      "97653/97653 [==============================] - 2s 21us/sample - loss: -0.7928 - val_loss: 0.8021\n",
      "Epoch 265/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8007 - val_loss: 0.8802\n",
      "Epoch 266/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8012 - val_loss: 0.8210\n",
      "Epoch 267/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8052 - val_loss: 0.8303\n",
      "Epoch 268/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8124 - val_loss: 0.8983\n",
      "Epoch 269/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8132 - val_loss: 0.8548\n",
      "Epoch 270/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8158 - val_loss: 0.8488\n",
      "Epoch 271/10000\n",
      "97653/97653 [==============================] - 2s 20us/sample - loss: -0.8197 - val_loss: 0.8788\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    loss_function = module_loss_function.load()\n",
    "\n",
    "    print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./original_ohio_experiments_120min\\\\540_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\544_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\552_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\559_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\563_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\567_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\570_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\575_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\584_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\588_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\591_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\596_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_120min\\\\all_final_experiment.yaml']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get all yaml files in the directory\n",
    "yaml_files = glob.glob(\"./original_ohio_experiments_120min/*.yaml\")\n",
    "\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\540-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2606, 24, 1)\n",
      "y_test.shape:  (2606, 1)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:15,019 WARNING Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-14 17:37:15,123 DEBUG Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  540\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\544-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2484, 24, 1)\n",
      "y_test.shape:  (2484, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:16,646 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  544\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\552-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2045, 24, 1)\n",
      "y_test.shape:  (2045, 1)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:18,013 WARNING Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  552\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\559-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2166, 24, 1)\n",
      "y_test.shape:  (2166, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:19,140 WARNING Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  559\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\563-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2454, 24, 1)\n",
      "y_test.shape:  (2454, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:20,306 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  563\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\567-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2015, 24, 1)\n",
      "y_test.shape:  (2015, 1)\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:21,695 WARNING Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  567\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\570-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2455, 24, 1)\n",
      "y_test.shape:  (2455, 1)\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:22,945 WARNING Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  570\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\575-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2286, 24, 1)\n",
      "y_test.shape:  (2286, 1)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:24,324 WARNING Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  575\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\584-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2201, 24, 1)\n",
      "y_test.shape:  (2201, 1)\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:25,741 WARNING Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  584\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\588-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2704, 24, 1)\n",
      "y_test.shape:  (2704, 1)\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:27,031 WARNING Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  588\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2616, 24, 1)\n",
      "y_test.shape:  (2616, 1)\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:28,490 WARNING Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  591\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\596-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2540, 24, 1)\n",
      "y_test.shape:  (2540, 1)\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-14 17:37:29,961 WARNING Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  596\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yaml_filepath in yaml_files[:-1]:\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments/559_all_final_evaluation.yaml\" # Replace the yaml\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\559-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_60_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [60],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 60,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2310, 12, 1)\n",
      "y_test.shape:  (2310, 1)\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-09 23:42:15,299 WARNING Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\all_final_experiment\\nb_future_steps_6_seed_60_\\model.hdf5\n",
      "patient id:  559\n",
      "Model Performance Metrics:\n",
      "-------------------------\n",
      "RMSE: 18.62\n",
      "MSE:  346.63\n",
      "MAPE: 8.23%\n",
      "\n",
      "Baseline (t0) Performance:\n",
      "-------------------------\n",
      "t0 RMSE: 23.40\n",
      "t0 MSE:  547.60\n",
      "t0 MAPE: 10.57%\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
