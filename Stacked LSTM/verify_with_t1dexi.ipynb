{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 7\n",
    "ph = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 0\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t1dexi_cgm(path, round):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject.groupby('LBCAT')\n",
    "    # Create a dictionary to store the split DataFrames\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "    new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "\n",
    "    new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    if round == True:\n",
    "        rounded_timestamp = []\n",
    "        for ts in new_df_cgm[\"LBDTC\"]:\n",
    "            rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "        new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "        formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "    else:\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_15min(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=0.25)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carb_effect'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carb_effect'] == closest_meal['carb_effect']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carb_effect'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_meal_entry(meal_row):\n",
    "    meal_time = meal_row['ts']\n",
    "    end_effect_time = meal_time + timedelta(hours=3)\n",
    "    carb = float(meal_row['carbs'])\n",
    "\n",
    "    c_eff_list = [0, 0, 0, ]\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        c_eff = (i * 0.111) * carb\n",
    "        if c_eff > carb:\n",
    "            print(\"C_eff > carb\")\n",
    "            c_eff = carb\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    for j in range(1, 25):\n",
    "        c_eff = (1 - (j * 0.028)) * carb\n",
    "        if c_eff < 0:\n",
    "            print(\"C_eff < 0\")\n",
    "            c_eff = 0\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    timestamp_list = pd.date_range(start=meal_time, end=end_effect_time, freq='5min')\n",
    "    d = {\"ts\": timestamp_list[:-1], \"carb_effect\": c_eff_list}\n",
    "    meal_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return meal_effect_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def preprocess_t1dexi_bolus_tempbasal(filepath, round):\n",
    "    subject_facm = pd.read_csv(filepath)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_bolus = split_dfs[\"BOLUS\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_bolus.reset_index(drop=True, inplace=True)\n",
    "    new_df_bolus = new_df_bolus.rename(columns={'FAORRES': 'dose', 'FADTC': 'ts_begin'})\n",
    "    new_df_bolus['assigned'] = False\n",
    "    # new_df_bolus['end_ts'] = new_df_bolus['ts_begin'].shift(-1)\n",
    "    return new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['bolus_effect']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts'] < closest_bolus['ts'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bolus_entry(bolus_row):\n",
    "    bolus_time = bolus_row['ts_begin']\n",
    "    timestamp_list = [bolus_time, ]\n",
    "    # end_effect_time = bolus_time + timedelta(hours=3)\n",
    "    dose = float(bolus_row['dose'])\n",
    "\n",
    "    b_eff_list = [dose, ]\n",
    "    b_eff = dose\n",
    "\n",
    "    i = 1\n",
    "    while b_eff > 0:\n",
    "        b_eff = dose - (i * 0.07)\n",
    "        b_eff_list.append(b_eff)\n",
    "        timestamp_list.append(bolus_time + timedelta(minutes=5 * i))\n",
    "        i += 1\n",
    "    # print(len(timestamp_list[:-1]))\n",
    "    # print(len(b_eff_list[:-1]))\n",
    "\n",
    "\n",
    "    d = {\"ts\": timestamp_list[:-1], \"bolus_effect\": b_eff_list[:-1]}\n",
    "    bolus_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return bolus_effect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_step(window_list, step_df):\n",
    "    start_time = window_list[0]\n",
    "    end_time = window_list[-1]\n",
    "\n",
    "    step_list = []\n",
    "    counter = 1\n",
    "    for idx, step_row in step_df.iterrows():\n",
    "        \n",
    "        if step_row['ts'] >= start_time and step_row['ts'] < end_time:\n",
    "            step_list.append(counter * float(step_row['value']))\n",
    "            counter += 1\n",
    "\n",
    "        if step_row['ts'] >= end_time:\n",
    "            break\n",
    "    # print(\"length of step_list \", len(step_list))\n",
    "    if len(step_list) == 0:\n",
    "        return None\n",
    "    accumulate_step = sum(step_list)/len(step_list)\n",
    "    return accumulate_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "def prepare_dataset(segments, ph):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carb_effect'] = pd.to_numeric(segment_df['carb_effect'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "        segment_df['steps'] = pd.to_numeric(segment_df['steps'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value', 'carb_effect', 'bolus_dose', 'steps']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)\n",
    "        self.fc2 = nn.Linear(512, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_size).to(device)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from the input tensor\n",
    "\n",
    "        # Initialize hidden and cell state for the first LSTM layer\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, (hn, cn) = self.lstm1(x, (h0, c0))\n",
    "        \n",
    "        # Dropout layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Initialize hidden and cell state for the second LSTM layer\n",
    "        h1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        out, (hn, cn) = self.lstm2(out, (h1, c1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = out[:, -1, :]  # Get the last time step output\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4# Number of input features\n",
    "hidden_size = 128  # Hidden vector size\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single output\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "\n",
    "\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_channels = 4  # Number of features\n",
    "# output_channels = 1  # Predicting a single value (glucose level)\n",
    "# num_blocks = 4  # Number of WaveNet blocks\n",
    "# dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "# model = StackedLSTM(input_channels, output_channels, num_blocks, dilations)\n",
    "# print(model)\n",
    "\n",
    "# # Example of how to define the loss and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv'] \n",
    "# '85.csv',\n",
    "# '911.csv',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subject \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv'"
     ]
    }
   ],
   "source": [
    "subject = pd.read_csv(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose = preprocess_t1dexi_cgm(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2% lower threshold: 69.0\n",
      "98% upper threshold: 226.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>glucose_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-29 00:04:38</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-29 00:09:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-29 00:14:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-29 00:19:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-29 00:24:38</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>2020-10-26 23:38:59</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>2020-10-26 23:43:59</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>2020-10-26 23:48:59</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>2020-10-26 23:53:59</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>2020-10-26 23:58:59</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  glucose_value\n",
       "0    2020-09-29 00:04:38           84.0\n",
       "1    2020-09-29 00:09:38           83.0\n",
       "2    2020-09-29 00:14:38           83.0\n",
       "3    2020-09-29 00:19:38           83.0\n",
       "4    2020-09-29 00:24:38           83.0\n",
       "...                  ...            ...\n",
       "7864 2020-10-26 23:38:59          211.0\n",
       "7865 2020-10-26 23:43:59          206.0\n",
       "7866 2020-10-26 23:48:59          201.0\n",
       "7867 2020-10-26 23:53:59          197.0\n",
       "7868 2020-10-26 23:58:59          194.0\n",
       "\n",
       "[7869 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: print each segment\n",
    "segments = segement_data_as_15min(glucose_df)\n",
    "# interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/555642131.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  whole_meal_effect_df['assigned'] = False\n",
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/2210063495.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '4.995' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']\n",
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/2210063495.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2.775' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']\n"
     ]
    }
   ],
   "source": [
    "meal = pd.read_csv(\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/ML_split/854.csv\")\n",
    "selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "# Fix: Use format='mixed' to handle different date formats\n",
    "meal_df['ts'] = pd.to_datetime(meal_df['ts'], format='mixed')\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "# Extract unique dates\n",
    "unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "# Convert to list\n",
    "meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "cleaned_segments = {}\n",
    "\n",
    "# Iterate through each segment and filter by unique dates\n",
    "for segment_name, df in segments.items():\n",
    "    # Convert timestamp column to datetime and then extract the date part\n",
    "    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    \n",
    "    # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "    filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "    \n",
    "    # Drop the 'date' column as it's no longer needed\n",
    "    filtered_df = filtered_df.drop(columns=['date'])\n",
    "    \n",
    "    # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "    cleaned_segments[segment_name] = filtered_df\n",
    "empty_d = {\"ts\": [], \"carb_effect\": []}\n",
    "whole_meal_effect_df = pd.DataFrame(data = empty_d)\n",
    "# Expand meal entries\n",
    "for index, meal_row in meal_df.iterrows():\n",
    "    meal_effect_df = expand_meal_entry(meal_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_meal_effect_df, meal_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['carb_effect_df1'] = merged_df['carb_effect_df1'].fillna(0)\n",
    "    merged_df['carb_effect_df2'] = merged_df['carb_effect_df2'].fillna(0)\n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['carb_effect'] = merged_df['carb_effect_df1'] + merged_df['carb_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_meal_effect_df = merged_df[['ts', 'carb_effect']]\n",
    "\n",
    "whole_meal_effect_df['assigned'] = False\n",
    "\n",
    "    # Update the segments with meal data\n",
    "meal_updated_segments = update_segments_with_meals(cleaned_segments, whole_meal_effect_df)\n",
    "# Update the segments with meal data\n",
    "# meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-06-20 00:00:00\n",
       "1   2020-06-20 00:00:00\n",
       "2   2020-09-30 16:21:08\n",
       "3   2020-10-11 15:33:05\n",
       "Name: ts, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meal_df['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Local\\Temp\\ipykernel_17332\\726347256.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>ts</th>\n",
       "      <th>assigned</th>\n",
       "      <th>end_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2020-09-29 00:01:12</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 00:46:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101</td>\n",
       "      <td>2020-09-29 00:46:03</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 00:51:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2020-09-29 00:51:02</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:10:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121</td>\n",
       "      <td>2020-09-29 01:10:58</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:15:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178</td>\n",
       "      <td>2020-09-29 01:15:58</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:20:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.300</td>\n",
       "      <td>2020-09-29 01:20:57</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:25:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.367</td>\n",
       "      <td>2020-09-29 01:25:56</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:30:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.351</td>\n",
       "      <td>2020-09-29 01:30:56</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:35:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.375</td>\n",
       "      <td>2020-09-29 01:35:55</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:40:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.409</td>\n",
       "      <td>2020-09-29 01:40:54</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-29 01:45:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value                  ts  assigned              end_ts\n",
       "0  0.000 2020-09-29 00:01:12     False 2020-09-29 00:46:03\n",
       "1  0.101 2020-09-29 00:46:03     False 2020-09-29 00:51:02\n",
       "2  0.000 2020-09-29 00:51:02     False 2020-09-29 01:10:58\n",
       "3  0.121 2020-09-29 01:10:58     False 2020-09-29 01:15:58\n",
       "4  0.178 2020-09-29 01:15:58     False 2020-09-29 01:20:57\n",
       "5  0.300 2020-09-29 01:20:57     False 2020-09-29 01:25:56\n",
       "6  0.367 2020-09-29 01:25:56     False 2020-09-29 01:30:56\n",
       "7  0.351 2020-09-29 01:30:56     False 2020-09-29 01:35:55\n",
       "8  0.375 2020-09-29 01:35:55     False 2020-09-29 01:40:54\n",
       "9  0.409 2020-09-29 01:40:54     False 2020-09-29 01:45:54"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subject_facm = pd.read_csv(f\"../FACM_split/854.csv\")\n",
    "# # Group by 'Category' column\n",
    "# grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "# split_dfs = {category: group for category, group in grouped}\n",
    "# # Step 1: Extract the desired columns\n",
    "# new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "# new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# new_df_basal.reset_index(drop=True, inplace=True)\n",
    "# new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "# new_df_basal['assigned'] = False\n",
    "# new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "# new_df_basal[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/777620760.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/1706296686.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  whole_bolus_effect_df[\"assigned\"] = False\n",
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/3951887240.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.517' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']\n",
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/3951887240.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0.787' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']\n"
     ]
    }
   ],
   "source": [
    "new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/FACM_split/854.csv\", False)\n",
    "\n",
    "empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "whole_bolus_effect_df = pd.DataFrame(data = empty_b)\n",
    "\n",
    "for index, bolus_row in new_df_bolus.iterrows():\n",
    "    bolus_effect_df = expand_bolus_entry(bolus_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_bolus_effect_df, bolus_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['bolus_effect_df1'] = merged_df['bolus_effect_df1'].fillna(0)\n",
    "    merged_df['bolus_effect_df2'] = merged_df['bolus_effect_df2'].fillna(0)\n",
    "    \n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['bolus_effect'] = merged_df['bolus_effect_df1'] + merged_df['bolus_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_bolus_effect_df = merged_df[['ts', 'bolus_effect']]\n",
    "\n",
    "whole_bolus_effect_df[\"assigned\"] = False\n",
    "\n",
    "bolus_updated_segments = update_segments_with_bolus(meal_updated_segments, whole_bolus_effect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_1': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carb_effect, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_2':               timestamp  glucose_value  carb_effect basal_rate  bolus_dose\n",
       " 142 2020-09-30 00:04:40          186.0          0.0      0.425       0.000\n",
       " 143 2020-09-30 00:09:39          184.0          0.0      0.425       0.000\n",
       " 144 2020-09-30 00:14:39          187.0          0.0      0.425       0.000\n",
       " 145 2020-09-30 00:19:39          189.0          0.0      0.573       0.517\n",
       " 146 2020-09-30 00:24:39          190.0          0.0      0.425       0.447\n",
       " ..                  ...            ...          ...        ...         ...\n",
       " 425 2020-09-30 23:39:41          172.0          0.0      0.727       0.000\n",
       " 426 2020-09-30 23:44:42          168.0          0.0      0.661       0.000\n",
       " 427 2020-09-30 23:49:42          161.0          0.0      0.625       0.000\n",
       " 428 2020-09-30 23:54:41          164.0          0.0      0.625       0.000\n",
       " 429 2020-09-30 23:59:41          171.0          0.0      0.625       0.000\n",
       " \n",
       " [288 rows x 5 columns],\n",
       " 'segment_3':               timestamp  glucose_value  carb_effect basal_rate  bolus_dose\n",
       " 383 2020-10-11 00:04:59           63.0          0.0        0.0         0.0\n",
       " 384 2020-10-11 00:09:59           63.0          0.0        0.0         0.0\n",
       " 385 2020-10-11 00:14:59           64.0          0.0        0.0         0.0\n",
       " 386 2020-10-11 00:19:59           65.0          0.0        0.0         0.0\n",
       " 387 2020-10-11 00:24:59           65.0          0.0        0.0         0.0\n",
       " ..                  ...            ...          ...        ...         ...\n",
       " 665 2020-10-11 23:35:00          103.0          0.0       0.32         0.0\n",
       " 666 2020-10-11 23:40:01           95.0          0.0      0.117         0.0\n",
       " 667 2020-10-11 23:45:01           92.0          0.0        0.0         0.0\n",
       " 668 2020-10-11 23:50:01           92.0          0.0        0.0         0.0\n",
       " 669 2020-10-11 23:55:01           92.0          0.0        0.0         0.0\n",
       " \n",
       " [287 rows x 5 columns],\n",
       " 'segment_4': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carb_effect, basal_rate, bolus_dose]\n",
       " Index: [],\n",
       " 'segment_5': Empty DataFrame\n",
       " Columns: [timestamp, glucose_value, carb_effect, basal_rate, bolus_dose]\n",
       " Index: []}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bolus_updated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_step_processing(bolus_updated_segments, step_df):\n",
    "    # Convert step_df timestamps to datetime if they aren't already\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    step_df['stepvalue'] = pd.to_numeric(step_df['stepvalue'])\n",
    "    \n",
    "    # Pre-calculate weights for step accumulation (1 to 10 for 50 minutes window)\n",
    "    weights = np.arange(1, 11)\n",
    "    \n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        print(segment_name)  # Keep the progress print if needed\n",
    "        # Convert timestamps if needed\n",
    "        segment_df['timestamp'] = pd.to_datetime(segment_df['timestamp'])\n",
    "        \n",
    "        # Create array to store accumulated steps\n",
    "        accumulate_step_list = []\n",
    "        \n",
    "        # Get all unique window starts for this segment\n",
    "        window_starts = segment_df['timestamp'].apply(lambda x: x - timedelta(minutes=50))\n",
    "        window_ends = segment_df['timestamp']\n",
    "        \n",
    "        # Process each window\n",
    "        for start, end in zip(window_starts, window_ends):\n",
    "            # Filter steps within the window\n",
    "            mask = (step_df['ts'] >= start) & (step_df['ts'] < end)\n",
    "            window_steps = step_df.loc[mask, 'stepvalue']\n",
    "            \n",
    "            if len(window_steps) == 0:\n",
    "                accumulate_step_list.append(None)\n",
    "            else:\n",
    "                # Take last 10 steps (or pad with zeros if less than 10)\n",
    "                last_steps = window_steps.iloc[-10:] if len(window_steps) > 10 else window_steps\n",
    "                weighted_sum = (last_steps.values * weights[:len(last_steps)]).sum()\n",
    "                accumulate_step_list.append(weighted_sum / len(last_steps))\n",
    "        \n",
    "        # Assign accumulated steps to segment\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "    return bolus_updated_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/tbqtlr256g5b8t969rk8d5kr0000gn/T/ipykernel_41405/453606402.py:2: DtypeWarning: Columns (10,11,12,14,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  subject_fa = pd.read_csv(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/FA_split/854.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_1\n",
      "segment_2\n",
      "segment_3\n",
      "segment_4\n",
      "segment_5\n"
     ]
    }
   ],
   "source": [
    "# Read in the Step count data\n",
    "subject_fa = pd.read_csv(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/FA_split/854.csv\")\n",
    "grouped = subject_fa.groupby('FAOBJ')\n",
    "split_dfs = {category: group for category, group in grouped}\n",
    "new_df_step = split_dfs[\"10-SECOND INTERVAL STEP COUNT\"][[\"FAORRES\", \"FADTC\"]]\n",
    "step_df = new_df_step.rename(columns={'FAORRES': 'stepvalue', 'FADTC': 'ts'})\n",
    "step_df.reset_index(inplace=True)\n",
    "step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "\n",
    "# Process steps with optimized function\n",
    "step_updated_segments = optimize_step_processing(bolus_updated_segments, step_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_1\n",
      "segment_2\n",
      "segment_3\n",
      "segment_4\n",
      "segment_5\n"
     ]
    }
   ],
   "source": [
    "# accumulate_step_list = []\n",
    "# # test_segment = segments[\"segment_1\"]\n",
    "# for segment_name, segment_df in bolus_updated_segments.items():\n",
    "#     print(segment_name)\n",
    "#     accumulate_step_list = []\n",
    "#     for index, cgm_row in segment_df.iterrows():\n",
    "#         current = cgm_row['timestamp']\n",
    "#         first_timestamp = current - timedelta(minutes=50)\n",
    "#         window_list = pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "#         accumulated_step = compute_accumulated_step(window_list, step_df)\n",
    "#         accumulate_step_list.append(accumulated_step)\n",
    "#     segment_df['steps'] = accumulate_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  0\n",
      "len of segment_df is  288\n",
      "len of segment_df is  287\n",
      "len of segment_df is  0\n",
      "len of segment_df is  0\n",
      "len of features_list 549\n"
     ]
    }
   ],
   "source": [
    "features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "# Assuming features_list and raw_glu_list are already defined\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation set: 25.823055267333984\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement on the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_rmse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ffile in overlap[:-1]:\n",
    "    print(ffile)\n",
    "    subject = pd.read_csv(f\"../LB_split/{ffile}\")\n",
    "    glucose = preprocess_t1dexi_cgm(f\"../LB_split/{ffile}\", False)\n",
    "    glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "    # Create the multi-channel database\n",
    "    g_data = []\n",
    "    for timestamp in glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': glucose_dict[timestamp],\n",
    "            # 'meal_type': None,\n",
    "            # 'meal_carbs': 0\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "    segments = segement_data_as_15min(glucose_df)\n",
    "\n",
    "    meal = pd.read_csv(f\"../ML_split/{ffile}\")\n",
    "    selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "    meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "    meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    meal_df['assigned'] = False\n",
    "\n",
    "    # Extract unique dates\n",
    "    unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "    # Convert to list\n",
    "    meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "    cleaned_segments = {}\n",
    "\n",
    "    # Iterate through each segment and filter by unique dates\n",
    "    for segment_name, df in segments.items():\n",
    "        # Convert timestamp column to datetime and then extract the date part\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        \n",
    "        # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "        filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "        \n",
    "        # Drop the 'date' column as it's no longer needed\n",
    "        filtered_df = filtered_df.drop(columns=['date'])\n",
    "        \n",
    "        # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "        cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "    # Expand meal entries\n",
    "    for index, meal_row in meal_df.iterrows():\n",
    "        meal_effect_df = expand_meal_entry(meal_row)\n",
    "\n",
    "        # Merge the DataFrames on the 'ts' column with an outer join\n",
    "        merged_df = pd.merge(whole_meal_effect_df, meal_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "        # Fill NaN values with 0 for the carb_effect columns\n",
    "        merged_df['carb_effect_df1'] = merged_df['carb_effect_df1'].fillna(0)\n",
    "        merged_df['carb_effect_df2'] = merged_df['carb_effect_df2'].fillna(0)\n",
    "\n",
    "        # Sum the carb_effect values\n",
    "        merged_df['carb_effect'] = merged_df['carb_effect_df1'] + merged_df['carb_effect_df2']\n",
    "\n",
    "        # Keep only the required columns\n",
    "        whole_meal_effect_df = merged_df[['ts', 'carb_effect']]\n",
    "\n",
    "    whole_meal_effect_df['assigned'] = False\n",
    "\n",
    "    # Update the segments with meal data\n",
    "    meal_updated_segments = update_segments_with_meals(cleaned_segments, whole_meal_effect_df)\n",
    "\n",
    "    subject_facm = pd.read_csv(f\"../FACM_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_basal.reset_index(drop=True, inplace=True)\n",
    "    new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "    new_df_basal['assigned'] = False\n",
    "    new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "    \n",
    "    basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n",
    "\n",
    "    new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/{ffile}\", False)\n",
    "\n",
    "\n",
    "    empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "    whole_bolus_effect_df = pd.DataFrame(data = empty_b)\n",
    "\n",
    "    for index, bolus_row in new_df_bolus.iterrows():\n",
    "        bolus_effect_df = expand_bolus_entry(bolus_row)\n",
    "\n",
    "        # Merge the DataFrames on the 'ts' column with an outer join\n",
    "        merged_df = pd.merge(whole_bolus_effect_df, bolus_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "        # Fill NaN values with 0 for the carb_effect columns\n",
    "        merged_df['bolus_effect_df1'] = merged_df['bolus_effect_df1'].fillna(0)\n",
    "        merged_df['bolus_effect_df2'] = merged_df['bolus_effect_df2'].fillna(0)\n",
    "        \n",
    "\n",
    "        # Sum the carb_effect values\n",
    "        merged_df['bolus_effect'] = merged_df['bolus_effect_df1'] + merged_df['bolus_effect_df2']\n",
    "\n",
    "        # Keep only the required columns\n",
    "        whole_bolus_effect_df = merged_df[['ts', 'bolus_effect']]\n",
    "\n",
    "    whole_bolus_effect_df[\"assigned\"] = False\n",
    "\n",
    "    bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, whole_bolus_effect_df)\n",
    "\n",
    "    # bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)\n",
    "    # Steps\n",
    "    # Read in the Step count data\n",
    "    subject_fa = pd.read_csv(f\"../FA_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_fa.groupby('FAOBJ')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_step = split_dfs[\"10-SECOND INTERVAL STEP COUNT\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    step_df = new_df_step.rename(columns={'FAORRES': 'stepvalue', 'FADTC': 'ts'})\n",
    "    step_df.reset_index(inplace=True)\n",
    "    # for i in range(len(step_df)):\n",
    "    #     step_ts = datetime.strptime(step_df['ts'][i], \"%Y-%m-%d %H:%M:%S\")\n",
    "    #     step_df['ts'][i] = step_ts\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    accumulate_step_list = []\n",
    "    # test_segment = segments[\"segment_1\"]\n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        print(segment_name)\n",
    "        accumulate_step_list = []\n",
    "        for index, cgm_row in segment_df.iterrows():\n",
    "            current = cgm_row['timestamp']\n",
    "            first_timestamp = current - timedelta(minutes=50)\n",
    "            window_list = pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "            accumulated_step = compute_accumulated_step(window_list, step_df)\n",
    "            accumulate_step_list.append(accumulated_step)\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "\n",
    "    features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "    # Assuming features_list and raw_glu_list are already defined\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(raw_glu_list)\n",
    "\n",
    "    # Step 1: Split into 80% train+val and 20% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Convert the splits to torch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "    num_epochs =500\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(val_loader)\n",
    "            print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on validation set: {rmse}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on validation set: {rmse}')\n",
    "    new_test_rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
