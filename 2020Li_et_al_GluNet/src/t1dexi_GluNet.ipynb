{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate GluNet on T1DEXI\n",
    "\n",
    "GluNet was mainly reported as a personalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = 6\n",
    "history_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 5\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "def preprocess_t1dexi_cgm(path, round):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    # Group by 'Category' column\n",
    "    # grouped = subject.groupby('LBCAT')\n",
    "    # Create a dictionary to store the split DataFrames\n",
    "    # split_dfs = {category: group for category, group in grouped}\n",
    "    # selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "    # new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "    new_df_cgm = subject[[\"LBORRES\", \"LBDTC\"]]\n",
    "\n",
    "    new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    if round == True:\n",
    "        rounded_timestamp = []\n",
    "        for ts in new_df_cgm[\"LBDTC\"]:\n",
    "            rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "        new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "        formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "    else:\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "    \n",
    "    return formatted_data\n",
    "    \n",
    "    # # Assuming self.interval_timedelta is set, for example:\n",
    "    # interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # # Create a list to store the results\n",
    "    # res = []\n",
    "\n",
    "    # # Initialize the first group\n",
    "    # if not subject.empty:\n",
    "    #     current_group = [subject.iloc[0]['LBORRES']]\n",
    "    #     last_time = subject.iloc[0]['LBDTC']\n",
    "\n",
    "    # # Iterate over rows in DataFrame starting from the second row\n",
    "    # for index, row in subject.iloc[1:].iterrows():\n",
    "    #     current_time = row['LBDTC']\n",
    "    #     if (current_time - last_time) <= interval_timedelta:\n",
    "    #         # If the time difference is within the limit, add to the current group\n",
    "    #         current_group.append(row['LBORRES'])\n",
    "    #     else:\n",
    "    #         # Otherwise, start a new group\n",
    "    #         res.append(current_group)\n",
    "    #         current_group = [row['LBORRES']]\n",
    "    #     last_time = current_time\n",
    "\n",
    "    # # Add the last group if it's not empty\n",
    "    # if current_group:\n",
    "    #     res.append(current_group)\n",
    "    \n",
    "    # # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "\n",
    "\n",
    "    # return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_1hour(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=1)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_and_spline_interpolate(segments):\n",
    "    for sequence in segments:\n",
    "        # sequence = \"segment_3\"\n",
    "        detected_missing = 0\n",
    "        for ts in range(len( segments[sequence]['timestamp'])-1):\n",
    "            if segments[sequence]['timestamp'][ts+1] - segments[sequence]['timestamp'][ts] > timedelta(minutes = 6):\n",
    "                print(sequence)\n",
    "                print(\"before: \", segments[sequence]['timestamp'][ts])\n",
    "                print(\"after: \", segments[sequence]['timestamp'][ts+1])\n",
    "                detected_missing = 1\n",
    "            \n",
    "        if detected_missing == 1:\n",
    "            datetime_list = np.array(pd.date_range(start=min(segments[sequence]['timestamp']), end=max(segments[sequence]['timestamp']), freq='5T').tolist())\n",
    "            reference_time = min(segments[sequence]['timestamp'])\n",
    "\n",
    "            # Convert datetime objects to the number of seconds since the reference time\n",
    "            datetime_seconds_since_start = [((dt - reference_time).total_seconds())/60 for dt in datetime_list] # Make it into minute\n",
    "            original_timestamp_in_segement = [((dt - reference_time).total_seconds())/60 for dt in segments[sequence]['timestamp']]\n",
    "\n",
    "            x = original_timestamp_in_segement\n",
    "            y = np.array(segments[sequence]['glucose_value'])\n",
    "            cs = CubicSpline(x, y)\n",
    "            xs = datetime_seconds_since_start\n",
    "\n",
    "            interpolated_xs = cs(xs)\n",
    "            time_index_interpolated = pd.date_range(start=reference_time, periods=len(interpolated_xs), freq='5T')\n",
    "\n",
    "            # Create DataFrame from the time index and glucose values\n",
    "            df_interpolated = pd.DataFrame({'timestamp': time_index_interpolated, 'glucose_value': interpolated_xs})\n",
    "            segments[sequence] = df_interpolated\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carbs'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carbs'] = closest_meal['carbs']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carbs'] == closest_meal['carbs']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carbs'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carbs'] = closest_meal['carbs']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def preprocess_t1dexi_bolus_tempbasal(filepath, round):\n",
    "    subject_facm = pd.read_csv(filepath)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_bolus = split_dfs[\"BOLUS\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_bolus.reset_index(drop=True, inplace=True)\n",
    "    new_df_bolus = new_df_bolus.rename(columns={'FAORRES': 'dose', 'FADTC': 'ts_begin'})\n",
    "    new_df_bolus['assigned'] = False\n",
    "    # new_df_bolus['end_ts'] = new_df_bolus['ts_begin'].shift(-1)\n",
    "    return new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts_begin'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['dose']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts_begin'] < closest_bolus['ts_begin'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['dose']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "# def prepare_dataset(segments, ph):\n",
    "#     '''\n",
    "#     ph = 6, 30 minutes ahead\n",
    "#     ph = 12, 60 minutes ahead\n",
    "#     '''\n",
    "#     features_list = []\n",
    "#     labels_list = []\n",
    "#     raw_glu_list = []\n",
    "    \n",
    "#     # Iterate over each segment\n",
    "#     for segment_name, segment_df in segments.items():\n",
    "#         # Ensure all columns are of numeric type\n",
    "#         segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "#         segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "#         segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "#         # Fill NaNs that might have been introduced by conversion errors\n",
    "#         segment_df.fillna(0, inplace=True)\n",
    "\n",
    "#         # Maximum index for creating a complete feature set\n",
    "#         max_index = len(segment_df) - (15+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + 15 to exist\n",
    "        \n",
    "#         # Iterate through the data to create feature-label pairs\n",
    "#         for i in range(max_index + 1):\n",
    "#             # Extracting features from index i to i+15\n",
    "#             features = segment_df.loc[i:i+15, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values#.flatten()\n",
    "#             # Extracting label for index i+21\n",
    "#             # Do the label transform\n",
    "#             label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "#             raw_glu_list.append(segment_df.loc[i+15+ph, 'glucose_value'])\n",
    "#             features_list.append(features)\n",
    "#             labels_list.append(label)\n",
    "            \n",
    "#     print(\"len of features_list \" + str(len(features_list)))\n",
    "#     print(\"len of labels_list \" + str(len(labels_list)))\n",
    "#     new_labels_list = label_delta_transform(labels_list)    \n",
    "#     print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "#     return features_list, labels_list, new_labels_list, raw_glu_list\n",
    "\n",
    "def prepare_dataset(segments, history_len):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    ph = 6\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        # segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "        # segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        # segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WaveNet(\n",
      "  (initial_conv): Conv1d(1, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (2): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(4,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(4,), dilation=(4,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (3): WaveNetBlock(\n",
      "      (conv1): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(8,))\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(2,), stride=(1,), padding=(8,), dilation=(8,))\n",
      "      (res_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (final_conv1): Conv1d(32, 128, kernel_size=(2,), stride=(1,))\n",
      "  (final_conv2): Conv1d(128, 256, kernel_size=(2,), stride=(1,))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the dilate CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation):\n",
    "        super(WaveNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=1+dilation - 2^(dilation-1))\n",
    "        self.conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=dilation)\n",
    "        self.res_conv = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape of x: \", x.shape)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        # print(\"shape of first out: \", out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        # print(\"shape of second out: \", out.shape)\n",
    "        res = self.res_conv(x)\n",
    "        # print(\"shape of res: \", res.shape)\n",
    "        return out + res\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, dilations):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv1d(in_channels, 32, kernel_size=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([WaveNetBlock(32, dilation) for dilation in dilations])\n",
    "        self.final_conv1 = nn.Conv1d(32, 128, kernel_size=2, padding=0)\n",
    "        self.final_conv2 = nn.Conv1d(128, 256, kernel_size=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        for block in self.blocks:\n",
    "            # print(\"enter the block loop\")\n",
    "            x = block(x)\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = F.relu(self.final_conv2(x))\n",
    "        x = x[:, :, -1]  # Get the last time step\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_channels = 1  # Number of features\n",
    "output_channels = 1  # Predicting a single value (glucose level)\n",
    "num_blocks = 4  # Number of WaveNet blocks\n",
    "dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "print(model)\n",
    "\n",
    "# Example of how to define the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap = ['854.csv',\n",
    "#  '979.csv',\n",
    "#  '816.csv',\n",
    "#  '953.csv',\n",
    "#  '981.csv',\n",
    "#  '1617.csv',\n",
    "#  '1343.csv',\n",
    "#  '987.csv',\n",
    "#  '255.csv',\n",
    "#  '907.csv',\n",
    "#  '856.csv',\n",
    "#  '354.csv',\n",
    "#  '894.csv',\n",
    "#  '862.csv',\n",
    "#  '900.csv',\n",
    "#  '695.csv']\n",
    "\n",
    "# #  '1287.csv','1112.csv' no basal  '85.csv', '911.csv',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = pd.read_csv(f\"../T1DEXI_processed/1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>LBORRES</th>\n",
       "      <th>LBDTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2020-05-11 00:01:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2020-05-11 00:06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2020-05-11 00:11:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2020-05-11 00:16:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2020-05-11 00:21:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7882</th>\n",
       "      <td>1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2020-06-07 23:37:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>1</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2020-06-07 23:42:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>1</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2020-06-07 23:47:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2020-06-07 23:52:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7886</th>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2020-06-07 23:57:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7887 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      USUBJID  LBORRES                LBDTC\n",
       "0           1    115.0  2020-05-11 00:01:17\n",
       "1           1    109.0  2020-05-11 00:06:17\n",
       "2           1    105.0  2020-05-11 00:11:17\n",
       "3           1    106.0  2020-05-11 00:16:18\n",
       "4           1    110.0  2020-05-11 00:21:18\n",
       "...       ...      ...                  ...\n",
       "7882        1    101.0  2020-06-07 23:37:18\n",
       "7883        1    104.0  2020-06-07 23:42:18\n",
       "7884        1    111.0  2020-06-07 23:47:18\n",
       "7885        1    122.0  2020-06-07 23:52:18\n",
       "7886        1    132.0  2020-06-07 23:57:18\n",
       "\n",
       "[7887 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biratal\\AppData\\Local\\Temp\\ipykernel_239316\\1250620286.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
      "C:\\Users\\Biratal\\AppData\\Local\\Temp\\ipykernel_239316\\1250620286.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'ts': datetime.datetime(2020, 5, 11, 0, 1, 17), 'value': 115.0}],\n",
       " [{'ts': datetime.datetime(2020, 5, 11, 0, 6, 17), 'value': 109.0}],\n",
       " [{'ts': datetime.datetime(2020, 5, 11, 0, 11, 17), 'value': 105.0}],\n",
       " [{'ts': datetime.datetime(2020, 5, 11, 0, 16, 18), 'value': 106.0}],\n",
       " [{'ts': datetime.datetime(2020, 5, 11, 0, 21, 18), 'value': 110.0}]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose = preprocess_t1dexi_cgm(f\"../T1DEXI_processed/1.csv\", False)\n",
    "glucose[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glucose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m glucose_dict \u001b[38;5;241m=\u001b[39m {entry[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m]: entry[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglucose\u001b[49m}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create the multi-channel database\u001b[39;00m\n\u001b[0;32m      4\u001b[0m g_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glucose' is not defined"
     ]
    }
   ],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline interpolation and extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_2\n",
      "before:  2020-05-19 20:56:37\n",
      "after:  2020-05-19 21:11:37\n",
      "segment_2\n",
      "before:  2020-05-20 21:11:38\n",
      "after:  2020-05-20 21:36:38\n",
      "segment_2\n",
      "before:  2020-05-20 22:46:39\n",
      "after:  2020-05-20 23:06:40\n"
     ]
    }
   ],
   "source": [
    "# Example: print each segment\n",
    "segments = segement_data_as_1hour(glucose_df)\n",
    "interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align other factors with the glucose information\n",
    "\n",
    "## Include meal info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meal = pd.read_csv(\"../ML_split/854.csv\")\n",
    "# selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "# meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "# meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# meal_df['assigned'] = False\n",
    "\n",
    "# # Extract unique dates\n",
    "# unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "# # Convert to list\n",
    "# meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "# cleaned_segments = {}\n",
    "\n",
    "# # Iterate through each segment and filter by unique dates\n",
    "# for segment_name, df in interpolated_segements.items():\n",
    "#     # Convert timestamp column to datetime and then extract the date part\n",
    "#     df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    \n",
    "#     # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "#     filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "    \n",
    "#     # Drop the 'date' column as it's no longer needed\n",
    "#     filtered_df = filtered_df.drop(columns=['date'])\n",
    "    \n",
    "#     # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "#     cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "# # Update the segments with meal data\n",
    "# meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include basal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_facm = pd.read_csv(f\"../FACM_split/854.csv\")\n",
    "# # Group by 'Category' column\n",
    "# grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "# split_dfs = {category: group for category, group in grouped}\n",
    "# # Step 1: Extract the desired columns\n",
    "# new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "# new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# new_df_basal.reset_index(drop=True, inplace=True)\n",
    "# new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "# new_df_basal['assigned'] = False\n",
    "# new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "# new_df_basal[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update the segments with meal data\n",
    "# basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/854.csv\", False)\n",
    "# bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to deal with large meal missingness\n",
    "\n",
    "1. Use 0 to impute \n",
    "2. Only use the days with meal record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the method 2\n",
    "# Therefore we have the meal_avaiable_dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct X and y, training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Build training and validation loader\n",
    "# features_array = np.array(features_list)\n",
    "# labels_array = np.array(raw_glu_list) # Maybe need to replace this\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(features_array, labels_array, test_size=0.2, shuffle= False)\n",
    "\n",
    "# # Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# # Create DataLoader\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolus_updated_segments = interpolated_segements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of segment_df is  143\n",
      "len of segment_df is  2745\n",
      "len of segment_df is  99\n",
      "len of segment_df is  2856\n",
      "len of segment_df is  2053\n",
      "len of features_list 7791\n"
     ]
    }
   ],
   "source": [
    "features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "# Assuming features_list and raw_glu_list are already defined\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 898.0566175079346\n",
      "Epoch 2, Validation Loss: 850.8951765441894\n",
      "Epoch 3, Validation Loss: 847.6031005859375\n",
      "Epoch 4, Validation Loss: 822.0280195617676\n",
      "Epoch 5, Validation Loss: 807.0320712280273\n",
      "Epoch 6, Validation Loss: 803.772841796875\n",
      "Epoch 7, Validation Loss: 807.7599073791504\n",
      "Epoch 8, Validation Loss: 802.8045564270019\n",
      "Epoch 9, Validation Loss: 816.3535382080078\n",
      "Epoch 10, Validation Loss: 812.6193087768555\n",
      "Epoch 11, Validation Loss: 819.4468509674073\n",
      "Epoch 12, Validation Loss: 823.5911292266845\n",
      "Epoch 13, Validation Loss: 838.3426860809326\n",
      "Epoch 14, Validation Loss: 845.2055068206787\n",
      "Epoch 15, Validation Loss: 863.0965649414062\n",
      "Epoch 16, Validation Loss: 883.3451287841797\n",
      "Epoch 17, Validation Loss: 907.8996710205079\n",
      "Epoch 18, Validation Loss: 905.556478805542\n",
      "Epoch 19, Validation Loss: 927.5228723144531\n",
      "Epoch 20, Validation Loss: 913.1901150512696\n",
      "Epoch 21, Validation Loss: 899.7616032409668\n",
      "Epoch 22, Validation Loss: 895.270046081543\n",
      "Epoch 23, Validation Loss: 892.0279689788819\n",
      "Epoch 24, Validation Loss: 875.6808317565918\n",
      "Epoch 25, Validation Loss: 876.461075668335\n",
      "Epoch 26, Validation Loss: 870.5802074432373\n",
      "Epoch 27, Validation Loss: 862.9243197631836\n",
      "Epoch 28, Validation Loss: 858.4420855712891\n",
      "Epoch 29, Validation Loss: 927.8399237823486\n",
      "Epoch 30, Validation Loss: 872.118978729248\n",
      "Epoch 31, Validation Loss: 851.747336730957\n",
      "Epoch 32, Validation Loss: 851.201245803833\n",
      "Epoch 33, Validation Loss: 861.1799718475341\n",
      "Epoch 34, Validation Loss: 866.6520810699462\n",
      "Epoch 35, Validation Loss: 806.6518167877198\n",
      "Epoch 36, Validation Loss: 813.5854441833496\n",
      "Epoch 37, Validation Loss: 853.0671588134766\n",
      "Epoch 38, Validation Loss: 825.572046546936\n",
      "Epoch 39, Validation Loss: 843.5993029022217\n",
      "Epoch 40, Validation Loss: 848.8419877243042\n",
      "Epoch 41, Validation Loss: 839.8745019912719\n",
      "Epoch 42, Validation Loss: 816.8862532043457\n",
      "Epoch 43, Validation Loss: 773.7254538726806\n",
      "Epoch 44, Validation Loss: 779.962301902771\n",
      "Epoch 45, Validation Loss: 795.0924812316895\n",
      "Epoch 46, Validation Loss: 852.23370677948\n",
      "Epoch 47, Validation Loss: 804.7095586013794\n",
      "Epoch 48, Validation Loss: 726.5098052215576\n",
      "Epoch 49, Validation Loss: 766.0885832977295\n",
      "Epoch 50, Validation Loss: 719.9447888183594\n",
      "Epoch 51, Validation Loss: 714.3041641235352\n",
      "Epoch 52, Validation Loss: 787.5803308868408\n",
      "Epoch 53, Validation Loss: 701.6240427398682\n",
      "Epoch 54, Validation Loss: 704.9616641998291\n",
      "Epoch 55, Validation Loss: 673.6136463928223\n",
      "Epoch 56, Validation Loss: 671.9992926788331\n",
      "Epoch 57, Validation Loss: 670.2482907104492\n",
      "Epoch 58, Validation Loss: 700.1356713104248\n",
      "Epoch 59, Validation Loss: 674.9323268127441\n",
      "Epoch 60, Validation Loss: 681.1520600891113\n",
      "Epoch 61, Validation Loss: 666.3225462341309\n",
      "Epoch 62, Validation Loss: 662.8178424835205\n",
      "Epoch 63, Validation Loss: 663.9778163146973\n",
      "Epoch 64, Validation Loss: 769.0283660125732\n",
      "Epoch 65, Validation Loss: 637.8615197753907\n",
      "Epoch 66, Validation Loss: 625.6365878295899\n",
      "Epoch 67, Validation Loss: 633.7997177124023\n",
      "Epoch 68, Validation Loss: 611.8559622192383\n",
      "Epoch 69, Validation Loss: 616.0535556030273\n",
      "Epoch 70, Validation Loss: 606.7532441711426\n",
      "Epoch 71, Validation Loss: 608.1610775756836\n",
      "Epoch 72, Validation Loss: 620.6041561126709\n",
      "Epoch 73, Validation Loss: 622.3060137939453\n",
      "Epoch 74, Validation Loss: 642.7341064453125\n",
      "Epoch 75, Validation Loss: 635.4576312255859\n",
      "Epoch 76, Validation Loss: 629.1675466918946\n",
      "Epoch 77, Validation Loss: 656.6978179931641\n",
      "Epoch 78, Validation Loss: 641.5899179077148\n",
      "Epoch 79, Validation Loss: 634.0772038269043\n",
      "Epoch 80, Validation Loss: 607.400520248413\n",
      "Epoch 81, Validation Loss: 637.4315469741821\n",
      "Epoch 82, Validation Loss: 642.424387664795\n",
      "Epoch 83, Validation Loss: 632.7217301940918\n",
      "Epoch 84, Validation Loss: 638.7551342010498\n",
      "Epoch 85, Validation Loss: 643.0159024047852\n",
      "Epoch 86, Validation Loss: 653.0706454467773\n",
      "Epoch 87, Validation Loss: 633.829051361084\n",
      "Epoch 88, Validation Loss: 648.8344555664063\n",
      "Epoch 89, Validation Loss: 681.9437437438964\n",
      "Epoch 90, Validation Loss: 672.708050994873\n",
      "Epoch 91, Validation Loss: 681.0316577148437\n",
      "Epoch 92, Validation Loss: 680.905111541748\n",
      "Epoch 93, Validation Loss: 674.9997254943847\n",
      "Epoch 94, Validation Loss: 670.4610052490234\n",
      "Epoch 95, Validation Loss: 656.4942425537109\n",
      "Epoch 96, Validation Loss: 640.638978652954\n",
      "Epoch 97, Validation Loss: 653.7001606750488\n",
      "Epoch 98, Validation Loss: 649.7079551696777\n",
      "Epoch 99, Validation Loss: 666.5749848937988\n",
      "Epoch 100, Validation Loss: 675.7868260955811\n",
      "Epoch 101, Validation Loss: 648.5478051757813\n",
      "Epoch 102, Validation Loss: 665.3153396606446\n",
      "Epoch 103, Validation Loss: 747.2020050048828\n",
      "Epoch 104, Validation Loss: 683.4558938598633\n",
      "Epoch 105, Validation Loss: 736.7453829956055\n",
      "Epoch 106, Validation Loss: 772.6901989746094\n",
      "Epoch 107, Validation Loss: 738.0810134887695\n",
      "Epoch 108, Validation Loss: 758.9904626464844\n",
      "Epoch 109, Validation Loss: 747.0337657165527\n",
      "Epoch 110, Validation Loss: 789.0257847595215\n",
      "Epoch 111, Validation Loss: 827.2317454528809\n",
      "Epoch 112, Validation Loss: 823.0988148498535\n",
      "Epoch 113, Validation Loss: 849.358173828125\n",
      "Epoch 114, Validation Loss: 839.4723220825196\n",
      "Epoch 115, Validation Loss: 795.4015791320801\n",
      "Epoch 116, Validation Loss: 763.5515089416504\n",
      "Epoch 117, Validation Loss: 820.5270845031738\n",
      "Epoch 118, Validation Loss: 783.0898400878906\n",
      "Epoch 119, Validation Loss: 770.8003779602051\n",
      "Epoch 120, Validation Loss: 775.4148011779785\n",
      "Epoch 121, Validation Loss: 799.3616116333008\n",
      "Epoch 122, Validation Loss: 810.5447372436523\n",
      "Epoch 123, Validation Loss: 778.006531829834\n",
      "Epoch 124, Validation Loss: 785.8502284240723\n",
      "Epoch 125, Validation Loss: 791.7682374572754\n",
      "Epoch 126, Validation Loss: 768.8445375061035\n",
      "Epoch 127, Validation Loss: 746.4700398254395\n",
      "Epoch 128, Validation Loss: 765.5468627929688\n",
      "Epoch 129, Validation Loss: 773.4750444030761\n",
      "Epoch 130, Validation Loss: 733.7019557189941\n",
      "Epoch 131, Validation Loss: 773.1401052856445\n",
      "Epoch 132, Validation Loss: 793.3731461334229\n",
      "Epoch 133, Validation Loss: 741.3628218078613\n",
      "Epoch 134, Validation Loss: 824.3247149658204\n",
      "Epoch 135, Validation Loss: 819.3221706390381\n",
      "Epoch 136, Validation Loss: 841.7249253845215\n",
      "Epoch 137, Validation Loss: 745.1945539093017\n",
      "Epoch 138, Validation Loss: 725.240435333252\n",
      "Epoch 139, Validation Loss: 741.8796440124512\n",
      "Epoch 140, Validation Loss: 765.2554385375977\n",
      "Epoch 141, Validation Loss: 776.4634159851074\n",
      "Epoch 142, Validation Loss: 816.5202980804444\n",
      "Epoch 143, Validation Loss: 693.9141432189941\n",
      "Epoch 144, Validation Loss: 725.3708781433105\n",
      "Epoch 145, Validation Loss: 657.4212232971191\n",
      "Epoch 146, Validation Loss: 712.7924488067626\n",
      "Epoch 147, Validation Loss: 699.8304548645019\n",
      "Epoch 148, Validation Loss: 711.0958838653564\n",
      "Epoch 149, Validation Loss: 690.7025463867187\n",
      "Epoch 150, Validation Loss: 709.8315563964844\n",
      "Epoch 151, Validation Loss: 756.6069873046876\n",
      "Epoch 152, Validation Loss: 718.3443972778321\n",
      "Epoch 153, Validation Loss: 714.2651370239258\n",
      "Epoch 154, Validation Loss: 776.2348446655274\n",
      "Epoch 155, Validation Loss: 761.8561929321289\n",
      "Epoch 156, Validation Loss: 789.662946472168\n",
      "Epoch 157, Validation Loss: 856.6356155395508\n",
      "Epoch 158, Validation Loss: 782.5787219238281\n",
      "Epoch 159, Validation Loss: 768.9017391967774\n",
      "Epoch 160, Validation Loss: 770.9768377685547\n",
      "Epoch 161, Validation Loss: 762.1593820190429\n",
      "Epoch 162, Validation Loss: 807.6486651611328\n",
      "Epoch 163, Validation Loss: 881.5007843017578\n",
      "Epoch 164, Validation Loss: 952.7241552734375\n",
      "Epoch 165, Validation Loss: 1358.3187158203125\n",
      "Epoch 166, Validation Loss: 1527.0507958984374\n",
      "Epoch 167, Validation Loss: 1803.0189672851564\n",
      "Epoch 168, Validation Loss: 1461.8529174804687\n",
      "Epoch 169, Validation Loss: 1363.395255126953\n",
      "Epoch 170, Validation Loss: 1234.7051318359374\n",
      "Epoch 171, Validation Loss: 1028.6916857910155\n",
      "Epoch 172, Validation Loss: 1150.2491271972656\n",
      "Epoch 173, Validation Loss: 1041.475897216797\n",
      "Epoch 174, Validation Loss: 1096.1561584472656\n",
      "Epoch 175, Validation Loss: 1100.3287591552735\n",
      "Epoch 176, Validation Loss: 1108.5937860107422\n",
      "Epoch 177, Validation Loss: 918.1351196289063\n",
      "Epoch 178, Validation Loss: 843.5477328491211\n",
      "Epoch 179, Validation Loss: 846.6818612670899\n",
      "Epoch 180, Validation Loss: 852.3401330566406\n",
      "Epoch 181, Validation Loss: 968.5550836181641\n",
      "Epoch 182, Validation Loss: 900.0584967041016\n",
      "Epoch 183, Validation Loss: 785.326411743164\n",
      "Epoch 184, Validation Loss: 792.23984375\n",
      "Epoch 185, Validation Loss: 826.5293041992187\n",
      "Epoch 186, Validation Loss: 786.3748538208008\n",
      "Epoch 187, Validation Loss: 818.7335327148437\n",
      "Epoch 188, Validation Loss: 826.7044546508789\n",
      "Epoch 189, Validation Loss: 794.7921603393555\n",
      "Epoch 190, Validation Loss: 743.2675714874267\n",
      "Epoch 191, Validation Loss: 766.593733215332\n",
      "Epoch 192, Validation Loss: 760.6479898071289\n",
      "Epoch 193, Validation Loss: 798.9183192443847\n",
      "Epoch 194, Validation Loss: 861.6077349853516\n",
      "Epoch 195, Validation Loss: 881.3859684753418\n",
      "Epoch 196, Validation Loss: 957.7586126708984\n",
      "Epoch 197, Validation Loss: 1205.0545727539063\n",
      "Epoch 198, Validation Loss: 1418.2092529296874\n",
      "Epoch 199, Validation Loss: 1258.135704345703\n",
      "Epoch 200, Validation Loss: 1328.3631311035156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "        # use squeeze\n",
    "        outputs = outputs.squeeze()\n",
    "        targets = targets.squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            outputs = outputs.squeeze()\n",
    "            targets = targets.squeeze()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 35.89223861694336\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs.permute(0, 2, 1))\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actuals,predictions))\n",
    "print(f'RMSE on test set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
