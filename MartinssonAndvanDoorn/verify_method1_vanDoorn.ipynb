{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-19 10:35:00,314 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2025-01-19 10:35:00,314 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-19 10:35:00,314 DEBUG interactive is False\n",
      "2025-01-19 10:35:00,314 DEBUG platform is win32\n",
      "2025-01-19 10:35:00,368 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-19 10:35:00,373 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_vandoorn_experiments_120min/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "        # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 16:32:06,513 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\OhioT1DM 2020\\both\\all does not exist.\n",
      "2025-01-14 16:32:06,513 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\both\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (97653, 24, 1)\n",
      "y_train.shape:  (97653, 1)\n",
      "x_valid.shape:  (24407, 24, 1)\n",
      "y_valid.shape:  (24407, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-14 16:32:17,211 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "96/96 [==============================] - 3s 12ms/step - loss: 2.5836 - val_loss: 2.4097\n",
      "Epoch 2/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 1.6785 - val_loss: 1.3309\n",
      "Epoch 3/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 1.0779 - val_loss: 0.8698\n",
      "Epoch 4/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.7545 - val_loss: 0.5928\n",
      "Epoch 5/10000\n",
      "93/96 [============================>.] - ETA: 0s - loss: 0.61902025-01-14 16:32:22,951 DEBUG Creating converter from 5 to 3\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.6181 - val_loss: 0.4784\n",
      "Epoch 6/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.5689 - val_loss: 0.4201\n",
      "Epoch 7/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.5365 - val_loss: 0.3840\n",
      "Epoch 8/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.4953 - val_loss: 0.3403\n",
      "Epoch 9/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.4382 - val_loss: 0.2899\n",
      "Epoch 10/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3988 - val_loss: 0.2539\n",
      "Epoch 11/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3663 - val_loss: 0.2220\n",
      "Epoch 12/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.3396 - val_loss: 0.1972\n",
      "Epoch 13/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.3204 - val_loss: 0.1837\n",
      "Epoch 14/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.3055 - val_loss: 0.1672\n",
      "Epoch 15/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2844 - val_loss: 0.1506\n",
      "Epoch 16/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2626 - val_loss: 0.1339\n",
      "Epoch 17/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.1226\n",
      "Epoch 18/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.2257 - val_loss: 0.1141\n",
      "Epoch 19/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.2160 - val_loss: 0.1027\n",
      "Epoch 20/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.2058 - val_loss: 0.1001\n",
      "Epoch 21/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1973 - val_loss: 0.0996\n",
      "Epoch 22/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1914 - val_loss: 0.0908\n",
      "Epoch 23/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1850 - val_loss: 0.0889\n",
      "Epoch 24/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1790 - val_loss: 0.0846\n",
      "Epoch 25/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1752 - val_loss: 0.0834\n",
      "Epoch 26/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1719 - val_loss: 0.0812\n",
      "Epoch 27/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1707 - val_loss: 0.0818\n",
      "Epoch 28/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1666 - val_loss: 0.0779\n",
      "Epoch 29/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1641 - val_loss: 0.0773\n",
      "Epoch 30/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1624 - val_loss: 0.0768\n",
      "Epoch 31/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1606 - val_loss: 0.0769\n",
      "Epoch 32/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1589 - val_loss: 0.0745\n",
      "Epoch 33/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1558 - val_loss: 0.0707\n",
      "Epoch 34/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1540 - val_loss: 0.0704\n",
      "Epoch 35/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1515 - val_loss: 0.0720\n",
      "Epoch 36/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1501 - val_loss: 0.0691\n",
      "Epoch 37/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1478 - val_loss: 0.0696\n",
      "Epoch 38/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1459 - val_loss: 0.0661\n",
      "Epoch 39/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1448 - val_loss: 0.0709\n",
      "Epoch 40/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1422 - val_loss: 0.0669\n",
      "Epoch 41/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1398 - val_loss: 0.0683\n",
      "Epoch 42/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1390 - val_loss: 0.0643\n",
      "Epoch 43/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1381 - val_loss: 0.0649\n",
      "Epoch 44/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1359 - val_loss: 0.0660\n",
      "Epoch 45/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1344 - val_loss: 0.0652\n",
      "Epoch 46/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1336 - val_loss: 0.0672\n",
      "Epoch 47/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1321 - val_loss: 0.0620\n",
      "Epoch 48/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1307 - val_loss: 0.0642\n",
      "Epoch 49/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1289 - val_loss: 0.0635\n",
      "Epoch 50/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.0645\n",
      "Epoch 51/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1265 - val_loss: 0.0634\n",
      "Epoch 52/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1254 - val_loss: 0.0607\n",
      "Epoch 53/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1225 - val_loss: 0.0593\n",
      "Epoch 54/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1220 - val_loss: 0.0618\n",
      "Epoch 55/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1210 - val_loss: 0.0604\n",
      "Epoch 56/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1200 - val_loss: 0.0612\n",
      "Epoch 57/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.0631\n",
      "Epoch 58/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1169 - val_loss: 0.0590\n",
      "Epoch 59/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.0602\n",
      "Epoch 60/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.0615\n",
      "Epoch 61/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1143 - val_loss: 0.0577\n",
      "Epoch 62/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.0597\n",
      "Epoch 63/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.0602\n",
      "Epoch 64/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1112 - val_loss: 0.0597\n",
      "Epoch 65/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1088 - val_loss: 0.0594\n",
      "Epoch 66/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1083 - val_loss: 0.0580\n",
      "Epoch 67/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1070 - val_loss: 0.0581\n",
      "Epoch 68/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1066 - val_loss: 0.0557\n",
      "Epoch 69/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1063 - val_loss: 0.0578\n",
      "Epoch 70/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1044 - val_loss: 0.0559\n",
      "Epoch 71/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.0596\n",
      "Epoch 72/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1028 - val_loss: 0.0568\n",
      "Epoch 73/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.0570\n",
      "Epoch 74/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1010 - val_loss: 0.0562\n",
      "Epoch 75/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0993 - val_loss: 0.0549\n",
      "Epoch 76/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0988 - val_loss: 0.0579\n",
      "Epoch 77/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0982 - val_loss: 0.0568\n",
      "Epoch 78/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0967 - val_loss: 0.0562\n",
      "Epoch 79/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.0547\n",
      "Epoch 80/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.0555\n",
      "Epoch 81/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.0540\n",
      "Epoch 82/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0940 - val_loss: 0.0548\n",
      "Epoch 83/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.0552\n",
      "Epoch 84/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0915 - val_loss: 0.0568\n",
      "Epoch 85/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0914 - val_loss: 0.0543\n",
      "Epoch 86/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0900 - val_loss: 0.0532\n",
      "Epoch 87/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0906 - val_loss: 0.0533\n",
      "Epoch 88/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0880 - val_loss: 0.0529\n",
      "Epoch 89/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0883 - val_loss: 0.0526\n",
      "Epoch 90/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0879 - val_loss: 0.0552\n",
      "Epoch 91/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0872 - val_loss: 0.0540\n",
      "Epoch 92/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0865 - val_loss: 0.0520\n",
      "Epoch 93/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0865 - val_loss: 0.0530\n",
      "Epoch 94/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0848 - val_loss: 0.0520\n",
      "Epoch 95/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0848 - val_loss: 0.0517\n",
      "Epoch 96/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0829 - val_loss: 0.0516\n",
      "Epoch 97/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0835 - val_loss: 0.0513\n",
      "Epoch 98/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0825 - val_loss: 0.0535\n",
      "Epoch 99/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0812 - val_loss: 0.0513\n",
      "Epoch 100/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.0528\n",
      "Epoch 101/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.0554\n",
      "Epoch 102/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0799 - val_loss: 0.0511\n",
      "Epoch 103/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0787 - val_loss: 0.0525\n",
      "Epoch 104/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.0508\n",
      "Epoch 105/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0783 - val_loss: 0.0512\n",
      "Epoch 106/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0772 - val_loss: 0.0525\n",
      "Epoch 107/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0780 - val_loss: 0.0503\n",
      "Epoch 108/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0767 - val_loss: 0.0503\n",
      "Epoch 109/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0763 - val_loss: 0.0508\n",
      "Epoch 110/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0753 - val_loss: 0.0510\n",
      "Epoch 111/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0749 - val_loss: 0.0508\n",
      "Epoch 112/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0750 - val_loss: 0.0505\n",
      "Epoch 113/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0739 - val_loss: 0.0511\n",
      "Epoch 114/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0740 - val_loss: 0.0505\n",
      "Epoch 115/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0732 - val_loss: 0.0500\n",
      "Epoch 116/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0729 - val_loss: 0.0500\n",
      "Epoch 117/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0723 - val_loss: 0.0501\n",
      "Epoch 118/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0713 - val_loss: 0.0500\n",
      "Epoch 119/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0713 - val_loss: 0.0506\n",
      "Epoch 120/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0705 - val_loss: 0.0503\n",
      "Epoch 121/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0698 - val_loss: 0.0492\n",
      "Epoch 122/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0701 - val_loss: 0.0516\n",
      "Epoch 123/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0694 - val_loss: 0.0491\n",
      "Epoch 124/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0689 - val_loss: 0.0493\n",
      "Epoch 125/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0686 - val_loss: 0.0492\n",
      "Epoch 126/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0684 - val_loss: 0.0487\n",
      "Epoch 127/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0676 - val_loss: 0.0493\n",
      "Epoch 128/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0677 - val_loss: 0.0492\n",
      "Epoch 129/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0675 - val_loss: 0.0506\n",
      "Epoch 130/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0670 - val_loss: 0.0488\n",
      "Epoch 131/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0662 - val_loss: 0.0502\n",
      "Epoch 132/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.0489\n",
      "Epoch 133/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0660 - val_loss: 0.0495\n",
      "Epoch 134/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0657 - val_loss: 0.0491\n",
      "Epoch 135/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0487\n",
      "Epoch 136/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0645 - val_loss: 0.0505\n",
      "Epoch 137/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0649 - val_loss: 0.0490\n",
      "Epoch 138/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0643 - val_loss: 0.0483\n",
      "Epoch 139/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0641 - val_loss: 0.0489\n",
      "Epoch 140/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0640 - val_loss: 0.0481\n",
      "Epoch 141/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0639 - val_loss: 0.0489\n",
      "Epoch 142/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0628 - val_loss: 0.0488\n",
      "Epoch 143/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0628 - val_loss: 0.0482\n",
      "Epoch 144/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0626 - val_loss: 0.0486\n",
      "Epoch 145/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0625 - val_loss: 0.0483\n",
      "Epoch 146/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0624 - val_loss: 0.0484\n",
      "Epoch 147/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0615 - val_loss: 0.0484\n",
      "Epoch 148/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0615 - val_loss: 0.0485\n",
      "Epoch 149/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0617 - val_loss: 0.0488\n",
      "Epoch 150/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0615 - val_loss: 0.0481\n",
      "Epoch 151/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0609 - val_loss: 0.0481\n",
      "Epoch 152/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0607 - val_loss: 0.0490\n",
      "Epoch 153/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0606 - val_loss: 0.0491\n",
      "Epoch 154/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0608 - val_loss: 0.0487\n",
      "Epoch 155/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0601 - val_loss: 0.0492\n",
      "Epoch 156/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0603 - val_loss: 0.0487\n",
      "Epoch 157/10000\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.0599 - val_loss: 0.0483\n",
      "Epoch 158/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0597 - val_loss: 0.0488\n",
      "Epoch 159/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0596 - val_loss: 0.0488\n",
      "Epoch 160/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0595 - val_loss: 0.0499\n",
      "Epoch 161/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0592 - val_loss: 0.0481\n",
      "Epoch 162/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0585 - val_loss: 0.0481\n",
      "Epoch 163/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0585 - val_loss: 0.0485\n",
      "Epoch 164/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0588 - val_loss: 0.0481\n",
      "Epoch 165/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0584 - val_loss: 0.0484\n",
      "Epoch 166/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0585 - val_loss: 0.0480\n",
      "Epoch 167/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0481\n",
      "Epoch 168/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0580 - val_loss: 0.0480\n",
      "Epoch 169/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0582 - val_loss: 0.0494\n",
      "Epoch 170/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0580 - val_loss: 0.0480\n",
      "Epoch 171/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.0478\n",
      "Epoch 172/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0481\n",
      "Epoch 173/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.0481\n",
      "Epoch 174/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0571 - val_loss: 0.0483\n",
      "Epoch 175/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0568 - val_loss: 0.0482\n",
      "Epoch 176/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.0495\n",
      "Epoch 177/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0565 - val_loss: 0.0477\n",
      "Epoch 178/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0568 - val_loss: 0.0477\n",
      "Epoch 179/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0570 - val_loss: 0.0487\n",
      "Epoch 180/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.0481\n",
      "Epoch 181/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0558 - val_loss: 0.0481\n",
      "Epoch 182/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0561 - val_loss: 0.0478\n",
      "Epoch 183/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0479\n",
      "Epoch 184/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0494\n",
      "Epoch 185/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0556 - val_loss: 0.0478\n",
      "Epoch 186/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.0479\n",
      "Epoch 187/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0477\n",
      "Epoch 188/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.0478\n",
      "Epoch 189/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0558 - val_loss: 0.0483\n",
      "Epoch 190/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0479\n",
      "Epoch 191/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.0479\n",
      "Epoch 192/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0477\n",
      "Epoch 193/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.0479\n",
      "Epoch 194/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0553 - val_loss: 0.0477\n",
      "Epoch 195/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0550 - val_loss: 0.0478\n",
      "Epoch 196/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0551 - val_loss: 0.0482\n",
      "Epoch 197/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0546 - val_loss: 0.0476\n",
      "Epoch 198/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.0475\n",
      "Epoch 199/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.0481\n",
      "Epoch 200/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0546 - val_loss: 0.0485\n",
      "Epoch 201/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0544 - val_loss: 0.0481\n",
      "Epoch 202/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.0479\n",
      "Epoch 203/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.0481\n",
      "Epoch 204/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0547 - val_loss: 0.0481\n",
      "Epoch 205/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0546 - val_loss: 0.0474\n",
      "Epoch 206/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0547 - val_loss: 0.0476\n",
      "Epoch 207/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0546 - val_loss: 0.0474\n",
      "Epoch 208/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0476\n",
      "Epoch 209/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0543 - val_loss: 0.0474\n",
      "Epoch 210/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0546 - val_loss: 0.0478\n",
      "Epoch 211/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0544 - val_loss: 0.0474\n",
      "Epoch 212/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0543 - val_loss: 0.0473\n",
      "Epoch 213/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0479\n",
      "Epoch 214/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0476\n",
      "Epoch 215/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0537 - val_loss: 0.0475\n",
      "Epoch 216/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0543 - val_loss: 0.0476\n",
      "Epoch 217/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0538 - val_loss: 0.0474\n",
      "Epoch 218/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0539 - val_loss: 0.0475\n",
      "Epoch 219/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0473\n",
      "Epoch 220/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0538 - val_loss: 0.0473\n",
      "Epoch 221/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0539 - val_loss: 0.0474\n",
      "Epoch 222/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0536 - val_loss: 0.0484\n",
      "Epoch 223/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0538 - val_loss: 0.0479\n",
      "Epoch 224/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0538 - val_loss: 0.0474\n",
      "Epoch 225/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0539 - val_loss: 0.0476\n",
      "Epoch 226/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.0477\n",
      "Epoch 227/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0474\n",
      "Epoch 228/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0537 - val_loss: 0.0475\n",
      "Epoch 229/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0539 - val_loss: 0.0473\n",
      "Epoch 230/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0473\n",
      "Epoch 231/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0479\n",
      "Epoch 232/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0472\n",
      "Epoch 233/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0481\n",
      "Epoch 234/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0539 - val_loss: 0.0473\n",
      "Epoch 235/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0535 - val_loss: 0.0474\n",
      "Epoch 236/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0537 - val_loss: 0.0474\n",
      "Epoch 237/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0535 - val_loss: 0.0475\n",
      "Epoch 238/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0472\n",
      "Epoch 239/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0536 - val_loss: 0.0472\n",
      "Epoch 240/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0474\n",
      "Epoch 241/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.0478\n",
      "Epoch 242/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0472\n",
      "Epoch 243/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0537 - val_loss: 0.0477\n",
      "Epoch 244/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.0473\n",
      "Epoch 245/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0536 - val_loss: 0.0487\n",
      "Epoch 246/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0477\n",
      "Epoch 247/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.0471\n",
      "Epoch 248/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0471\n",
      "Epoch 249/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0536 - val_loss: 0.0472\n",
      "Epoch 250/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0475\n",
      "Epoch 251/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.0472\n",
      "Epoch 252/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0532 - val_loss: 0.0470\n",
      "Epoch 253/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0472\n",
      "Epoch 254/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0533 - val_loss: 0.0471\n",
      "Epoch 255/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 256/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0470\n",
      "Epoch 257/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0532 - val_loss: 0.0472\n",
      "Epoch 258/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0474\n",
      "Epoch 259/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0470\n",
      "Epoch 260/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0533 - val_loss: 0.0478\n",
      "Epoch 261/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0473\n",
      "Epoch 262/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0470\n",
      "Epoch 263/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0471\n",
      "Epoch 264/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0471\n",
      "Epoch 265/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0472\n",
      "Epoch 266/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0473\n",
      "Epoch 267/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0533 - val_loss: 0.0470\n",
      "Epoch 268/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0472\n",
      "Epoch 269/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.0472\n",
      "Epoch 270/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0476\n",
      "Epoch 271/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.0472\n",
      "Epoch 272/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0484\n",
      "Epoch 273/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0470\n",
      "Epoch 274/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0470\n",
      "Epoch 275/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0470\n",
      "Epoch 276/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0471\n",
      "Epoch 277/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.0472\n",
      "Epoch 278/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0532 - val_loss: 0.0476\n",
      "Epoch 279/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0475\n",
      "Epoch 280/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0475\n",
      "Epoch 281/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0473\n",
      "Epoch 282/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0471\n",
      "Epoch 283/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0475\n",
      "Epoch 284/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0471\n",
      "Epoch 285/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0473\n",
      "Epoch 286/10000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.0529 - val_loss: 0.0470\n",
      "Epoch 287/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 288/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Epoch 289/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0528 - val_loss: 0.0470\n",
      "Epoch 290/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0529 - val_loss: 0.0473\n",
      "Epoch 291/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0477\n",
      "Epoch 292/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0474\n",
      "Epoch 293/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0469\n",
      "Epoch 294/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0471\n",
      "Epoch 295/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0474\n",
      "Epoch 296/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0528 - val_loss: 0.0471\n",
      "Epoch 297/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0472\n",
      "Epoch 298/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0472\n",
      "Epoch 299/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0471\n",
      "Epoch 300/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0471\n",
      "Epoch 301/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0474\n",
      "Epoch 302/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0473\n",
      "Epoch 303/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0471\n",
      "Epoch 304/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0470\n",
      "Epoch 305/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0476\n",
      "Epoch 306/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0473\n",
      "Epoch 307/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0468\n",
      "Epoch 308/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0476\n",
      "Epoch 309/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0471\n",
      "Epoch 310/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0479\n",
      "Epoch 311/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0468\n",
      "Epoch 312/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 313/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0469\n",
      "Epoch 314/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0475\n",
      "Epoch 315/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0468\n",
      "Epoch 316/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0469\n",
      "Epoch 317/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 318/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 319/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0473\n",
      "Epoch 320/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0470\n",
      "Epoch 321/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0468\n",
      "Epoch 322/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0468\n",
      "Epoch 323/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Epoch 324/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Epoch 325/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0468\n",
      "Epoch 326/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0469\n",
      "Epoch 327/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0476\n",
      "Epoch 328/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 329/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0469\n",
      "Epoch 330/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0470\n",
      "Epoch 331/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 332/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0471\n",
      "Epoch 333/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0475\n",
      "Epoch 334/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0533 - val_loss: 0.0471\n",
      "Epoch 335/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0471\n",
      "Epoch 336/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0469\n",
      "Epoch 337/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0473\n",
      "Epoch 338/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0473\n",
      "Epoch 339/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0468\n",
      "Epoch 340/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0469\n",
      "Epoch 341/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0474\n",
      "Epoch 342/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0469\n",
      "Epoch 343/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 344/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0474\n",
      "Epoch 345/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0472\n",
      "Epoch 346/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0469\n",
      "Epoch 347/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0472\n",
      "Epoch 348/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0527 - val_loss: 0.0470\n",
      "Epoch 349/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 350/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0469\n",
      "Epoch 351/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0474\n",
      "Epoch 352/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 353/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0469\n",
      "Epoch 354/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0469\n",
      "Epoch 355/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 356/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0471\n",
      "Epoch 357/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0477\n",
      "Epoch 358/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0469\n",
      "Epoch 359/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0468\n",
      "Epoch 360/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0473\n",
      "Epoch 361/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0469\n",
      "Epoch 362/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0472\n",
      "Epoch 363/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 364/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 365/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 366/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Epoch 367/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0475\n",
      "Epoch 368/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 369/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0471\n",
      "Epoch 370/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0469\n",
      "Epoch 371/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0470\n",
      "Epoch 372/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0475\n",
      "Epoch 373/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0473\n",
      "Epoch 374/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0472\n",
      "Epoch 375/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0469\n",
      "Epoch 376/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.0471\n",
      "Epoch 377/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 378/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0470\n",
      "Epoch 379/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0469\n",
      "Epoch 380/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 381/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0467\n",
      "Epoch 382/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 383/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0468\n",
      "Epoch 384/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 385/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 386/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0470\n",
      "Epoch 387/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0471\n",
      "Epoch 388/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0473\n",
      "Epoch 389/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0473\n",
      "Epoch 390/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 391/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0484\n",
      "Epoch 392/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0469\n",
      "Epoch 393/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0480\n",
      "Epoch 394/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 395/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0471\n",
      "Epoch 396/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0468\n",
      "Epoch 397/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0468\n",
      "Epoch 398/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 399/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0469\n",
      "Epoch 400/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 401/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 402/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0470\n",
      "Epoch 403/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0470\n",
      "Epoch 404/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0468\n",
      "Epoch 405/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 406/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0480\n",
      "Epoch 407/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0525 - val_loss: 0.0470\n",
      "Epoch 408/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0468\n",
      "Epoch 409/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0468\n",
      "Epoch 410/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 411/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0468\n",
      "Epoch 412/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0482\n",
      "Epoch 413/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0466\n",
      "Epoch 414/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 415/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0473\n",
      "Epoch 416/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 417/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0473\n",
      "Epoch 418/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 419/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0472\n",
      "Epoch 420/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 421/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0472\n",
      "Epoch 422/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 423/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0469\n",
      "Epoch 424/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0470\n",
      "Epoch 425/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 426/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0471\n",
      "Epoch 427/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0471\n",
      "Epoch 428/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 429/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 430/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 431/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0526 - val_loss: 0.0467\n",
      "Epoch 432/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0467\n",
      "Epoch 433/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 434/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0466\n",
      "Epoch 435/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 436/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 437/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0472\n",
      "Epoch 438/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0467\n",
      "Epoch 439/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0467\n",
      "Epoch 440/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 441/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0487\n",
      "Epoch 442/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 443/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 444/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 445/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 446/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0467\n",
      "Epoch 447/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 448/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0473\n",
      "Epoch 449/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 450/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 451/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 452/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 453/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 454/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0470\n",
      "Epoch 455/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0472\n",
      "Epoch 456/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0470\n",
      "Epoch 457/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 458/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 459/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 460/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 461/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0469\n",
      "Epoch 462/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 463/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0470\n",
      "Epoch 464/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0470\n",
      "Epoch 465/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 466/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 467/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0468\n",
      "Epoch 468/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 469/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 470/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0475\n",
      "Epoch 471/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0477\n",
      "Epoch 472/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0473\n",
      "Epoch 473/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 474/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0468\n",
      "Epoch 475/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0471\n",
      "Epoch 476/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 477/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 478/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 479/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 480/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0474\n",
      "Epoch 481/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0468\n",
      "Epoch 482/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 483/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 484/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0467\n",
      "Epoch 485/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0471\n",
      "Epoch 486/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 487/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 488/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 489/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0469\n",
      "Epoch 490/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0481\n",
      "Epoch 491/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 492/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 493/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 494/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 495/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 496/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0467\n",
      "Epoch 497/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0524 - val_loss: 0.0468\n",
      "Epoch 498/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 499/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 500/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0466\n",
      "Epoch 501/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0472\n",
      "Epoch 502/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0469\n",
      "Epoch 503/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0469\n",
      "Epoch 504/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 505/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 506/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0474\n",
      "Epoch 507/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 508/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 509/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 510/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 511/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 512/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 513/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 514/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 515/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0474\n",
      "Epoch 516/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 517/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 518/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0468\n",
      "Epoch 519/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 520/10000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.0524 - val_loss: 0.0472\n",
      "Epoch 521/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0471\n",
      "Epoch 522/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 523/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0466\n",
      "Epoch 524/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 525/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 526/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0473\n",
      "Epoch 527/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0471\n",
      "Epoch 528/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 529/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 530/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 531/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0523 - val_loss: 0.0469\n",
      "Epoch 532/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 533/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 534/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 535/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 536/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 537/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 538/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 539/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 540/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 541/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 542/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 543/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 544/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 545/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 546/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 547/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0471\n",
      "Epoch 548/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0469\n",
      "Epoch 549/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 550/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 551/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0474\n",
      "Epoch 552/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0466\n",
      "Epoch 553/10000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.0521 - val_loss: 0.0483\n",
      "Epoch 554/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 555/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 556/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 557/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 558/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 559/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0469\n",
      "Epoch 560/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 561/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 562/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 563/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 564/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0470\n",
      "Epoch 565/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 566/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 567/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 568/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 569/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0472\n",
      "Epoch 570/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 571/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0467\n",
      "Epoch 572/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 573/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 574/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0470\n",
      "Epoch 575/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0472\n",
      "Epoch 576/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 577/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 578/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0471\n",
      "Epoch 579/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 580/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 581/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 582/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 583/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 584/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0468\n",
      "Epoch 585/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0470\n",
      "Epoch 586/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 587/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0472\n",
      "Epoch 588/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0472\n",
      "Epoch 589/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 590/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 591/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0470\n",
      "Epoch 592/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0471\n",
      "Epoch 593/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 594/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 595/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0469\n",
      "Epoch 596/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 597/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0471\n",
      "Epoch 598/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 599/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0473\n",
      "Epoch 600/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 601/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 602/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0469\n",
      "Epoch 603/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 604/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 605/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 606/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0474\n",
      "Epoch 607/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0471\n",
      "Epoch 608/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 609/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0470\n",
      "Epoch 610/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 611/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 612/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0474\n",
      "Epoch 613/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 614/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 615/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 616/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 617/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 618/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 619/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 620/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 621/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 622/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 623/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 624/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0475\n",
      "Epoch 625/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 626/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0473\n",
      "Epoch 627/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 628/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0471\n",
      "Epoch 629/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0470\n",
      "Epoch 630/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 631/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 632/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0474\n",
      "Epoch 633/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0468\n",
      "Epoch 634/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 635/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 636/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0471\n",
      "Epoch 637/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 638/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 639/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0470\n",
      "Epoch 640/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 641/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 642/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0465\n",
      "Epoch 643/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0476\n",
      "Epoch 644/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 645/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0480\n",
      "Epoch 646/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 647/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 648/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0473\n",
      "Epoch 649/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0470\n",
      "Epoch 650/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0479\n",
      "Epoch 651/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.0467\n",
      "Epoch 652/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 653/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 654/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0471\n",
      "Epoch 655/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 656/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0470\n",
      "Epoch 657/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 658/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0467\n",
      "Epoch 659/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 660/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 661/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 662/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 663/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 664/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 665/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 666/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0472\n",
      "Epoch 667/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0473\n",
      "Epoch 668/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 669/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0477\n",
      "Epoch 670/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 671/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 672/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0471\n",
      "Epoch 673/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0465\n",
      "Epoch 674/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 675/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 676/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0470\n",
      "Epoch 677/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 678/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 679/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 680/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 681/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0469\n",
      "Epoch 682/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 683/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0465\n",
      "Epoch 684/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 685/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0477\n",
      "Epoch 686/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 687/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 688/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 689/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 690/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 691/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 692/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0469\n",
      "Epoch 693/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 694/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0465\n",
      "Epoch 695/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 696/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 697/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 698/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 699/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0470\n",
      "Epoch 700/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 701/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 702/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 703/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0470\n",
      "Epoch 704/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 705/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 706/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0474\n",
      "Epoch 707/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 708/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0480\n",
      "Epoch 709/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0469\n",
      "Epoch 710/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0471\n",
      "Epoch 711/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0470\n",
      "Epoch 712/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0472\n",
      "Epoch 713/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 714/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 715/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 716/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 717/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0465\n",
      "Epoch 718/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0476\n",
      "Epoch 719/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0465\n",
      "Epoch 720/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 721/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0469\n",
      "Epoch 722/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0465\n",
      "Epoch 723/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0478\n",
      "Epoch 724/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 725/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 726/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0465\n",
      "Epoch 727/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 728/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 729/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 730/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0468\n",
      "Epoch 731/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 732/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 733/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0471\n",
      "Epoch 734/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 735/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 736/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 737/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0471\n",
      "Epoch 738/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 739/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 740/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0474\n",
      "Epoch 741/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 742/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0470\n",
      "Epoch 743/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 744/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 745/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0469\n",
      "Epoch 746/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 747/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 748/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 749/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 750/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0468\n",
      "Epoch 751/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0470\n",
      "Epoch 752/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 753/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0470\n",
      "Epoch 754/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0466\n",
      "Epoch 755/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0465\n",
      "Epoch 756/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 757/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 758/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 759/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0466\n",
      "Epoch 760/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0469\n",
      "Epoch 761/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 762/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 763/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 764/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0473\n",
      "Epoch 765/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0465\n",
      "Epoch 766/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0471\n",
      "Epoch 767/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 768/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0469\n",
      "Epoch 769/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0464\n",
      "Epoch 770/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 771/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 772/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 773/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0470\n",
      "Epoch 774/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0467\n",
      "Epoch 775/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 776/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0469\n",
      "Epoch 777/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 778/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 779/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 780/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 781/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0470\n",
      "Epoch 782/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 783/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 784/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 785/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 786/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0465\n",
      "Epoch 787/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0472\n",
      "Epoch 788/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 789/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 790/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0475\n",
      "Epoch 791/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0465\n",
      "Epoch 792/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 793/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0472\n",
      "Epoch 794/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 795/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 796/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 797/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 798/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0466\n",
      "Epoch 799/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0473\n",
      "Epoch 800/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 801/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 802/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 803/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 804/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 805/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0467\n",
      "Epoch 806/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0470\n",
      "Epoch 807/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0470\n",
      "Epoch 808/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0465\n",
      "Epoch 809/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0474\n",
      "Epoch 810/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0469\n",
      "Epoch 811/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 812/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 813/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0519 - val_loss: 0.0465\n",
      "Epoch 814/10000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.0518 - val_loss: 0.0464\n",
      "Epoch 815/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 816/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 817/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 818/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 819/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0468\n",
      "Epoch 820/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0464\n",
      "Epoch 821/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0467\n",
      "Epoch 822/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 823/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0468\n",
      "Epoch 824/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0465\n",
      "Epoch 825/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0470\n",
      "Epoch 826/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 827/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0466\n",
      "Epoch 828/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 829/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0468\n",
      "Epoch 830/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 831/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 832/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0465\n",
      "Epoch 833/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 834/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 835/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 836/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0470\n",
      "Epoch 837/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 838/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 839/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 840/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 841/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0465\n",
      "Epoch 842/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 843/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0464\n",
      "Epoch 844/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0472\n",
      "Epoch 845/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0465\n",
      "Epoch 846/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 847/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 848/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 849/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0464\n",
      "Epoch 850/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 851/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 852/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 853/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0465\n",
      "Epoch 854/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0469\n",
      "Epoch 855/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 856/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0465\n",
      "Epoch 857/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 858/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 859/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0483\n",
      "Epoch 860/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0467\n",
      "Epoch 861/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0465\n",
      "Epoch 862/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0466\n",
      "Epoch 863/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0474\n",
      "Epoch 864/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0476\n",
      "Epoch 865/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0469\n",
      "Epoch 866/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 867/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0472\n",
      "Epoch 868/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0476\n",
      "Epoch 869/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0511 - val_loss: 0.0476\n",
      "Epoch 870/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 871/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 872/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0466\n",
      "Epoch 873/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 874/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0469\n",
      "Epoch 875/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0517 - val_loss: 0.0468\n",
      "Epoch 876/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 877/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0471\n",
      "Epoch 878/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 879/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0470\n",
      "Epoch 880/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0466\n",
      "Epoch 881/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0474\n",
      "Epoch 882/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0465\n",
      "Epoch 883/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 884/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0468\n",
      "Epoch 885/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0509 - val_loss: 0.0466\n",
      "Epoch 886/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0474\n",
      "Epoch 887/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0469\n",
      "Epoch 888/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0469\n",
      "Epoch 889/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 890/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0468\n",
      "Epoch 891/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 892/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0465\n",
      "Epoch 893/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 894/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0470\n",
      "Epoch 895/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0511 - val_loss: 0.0467\n",
      "Epoch 896/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0509 - val_loss: 0.0468\n",
      "Epoch 897/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0509 - val_loss: 0.0466\n",
      "Epoch 898/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0465\n",
      "Epoch 899/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 900/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0511 - val_loss: 0.0469\n",
      "Epoch 901/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 902/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0464\n",
      "Epoch 903/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 904/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0510 - val_loss: 0.0466\n",
      "Epoch 905/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0467\n",
      "Epoch 906/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 907/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0509 - val_loss: 0.0466\n",
      "Epoch 908/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0509 - val_loss: 0.0466\n",
      "Epoch 909/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0465\n",
      "Epoch 910/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0468\n",
      "Epoch 911/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0466\n",
      "Epoch 912/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0474\n",
      "Epoch 913/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 914/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0468\n",
      "Epoch 915/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0469\n",
      "Epoch 916/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0467\n",
      "Epoch 917/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 918/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 919/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0470\n",
      "Epoch 920/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0471\n",
      "Epoch 921/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0467\n",
      "Epoch 922/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0466\n",
      "Epoch 923/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 924/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0466\n",
      "Epoch 925/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0469\n",
      "Epoch 926/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.0471\n",
      "Epoch 927/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0471\n",
      "Epoch 928/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0465\n",
      "Epoch 929/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0471\n",
      "Epoch 930/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0468\n",
      "Epoch 931/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0507 - val_loss: 0.0466\n",
      "Epoch 932/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0466\n",
      "Epoch 933/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0467\n",
      "Epoch 934/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0471\n",
      "Epoch 935/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0465\n",
      "Epoch 936/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 937/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 938/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0467\n",
      "Epoch 939/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0465\n",
      "Epoch 940/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 941/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.0468\n",
      "Epoch 942/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0472\n",
      "Epoch 943/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0511 - val_loss: 0.0466\n",
      "Epoch 944/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0467\n",
      "Epoch 945/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0467\n",
      "Epoch 946/10000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.0466\n",
      "Epoch 947/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0514 - val_loss: 0.0468\n",
      "Epoch 948/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 949/10000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0466\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ohio Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./original_vandoorn_experiments_120min\\\\540_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\544_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\552_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\559_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\563_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\567_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\570_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\575_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\584_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\588_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\591_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\596_all_final_evaluation.yaml',\n",
       " './original_vandoorn_experiments_120min\\\\all_final_experiment.yaml']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get all yaml files in the directory\n",
    "yaml_files = glob.glob(\"./original_vandoorn_experiments_120min/*.yaml\")\n",
    "\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\540-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2606, 24, 1)\n",
      "y_test.shape:  (2606, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "82/82 [==============================] - 1s 5ms/step\n",
      "82/82 [==============================] - 0s 3ms/step\n",
      "patient id:  540\n",
      "RMSE:  21.59059040846157\n",
      "t0 RMSE:  28.585587983604267\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\544-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2484, 24, 1)\n",
      "y_test.shape:  (2484, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "78/78 [==============================] - 1s 3ms/step\n",
      "78/78 [==============================] - 0s 3ms/step\n",
      "patient id:  544\n",
      "RMSE:  17.013195295374175\n",
      "t0 RMSE:  21.91874909744576\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\552-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2045, 24, 1)\n",
      "y_test.shape:  (2045, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "64/64 [==============================] - 1s 3ms/step\n",
      "64/64 [==============================] - 0s 3ms/step\n",
      "patient id:  552\n",
      "RMSE:  16.33974239407702\n",
      "t0 RMSE:  21.009789312217418\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\559-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2166, 24, 1)\n",
      "y_test.shape:  (2166, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "68/68 [==============================] - 1s 3ms/step\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "patient id:  559\n",
      "RMSE:  18.47185587412299\n",
      "t0 RMSE:  22.805887286433613\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\563-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2454, 24, 1)\n",
      "y_test.shape:  (2454, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "77/77 [==============================] - 1s 3ms/step\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "patient id:  563\n",
      "RMSE:  18.1529551167396\n",
      "t0 RMSE:  20.88773500197263\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\567-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2015, 24, 1)\n",
      "y_test.shape:  (2015, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "63/63 [==============================] - 1s 3ms/step\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "patient id:  567\n",
      "RMSE:  20.673559411667032\n",
      "t0 RMSE:  27.32775972546786\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\570-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2455, 24, 1)\n",
      "y_test.shape:  (2455, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "77/77 [==============================] - 1s 3ms/step\n",
      "77/77 [==============================] - 0s 3ms/step\n",
      "patient id:  570\n",
      "RMSE:  15.878500902892755\n",
      "t0 RMSE:  18.585071643222328\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\575-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2286, 24, 1)\n",
      "y_test.shape:  (2286, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "72/72 [==============================] - 1s 3ms/step\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "patient id:  575\n",
      "RMSE:  20.626442834626552\n",
      "t0 RMSE:  24.561357453630375\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\584-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2201, 24, 1)\n",
      "y_test.shape:  (2201, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "69/69 [==============================] - 1s 3ms/step\n",
      "69/69 [==============================] - 0s 3ms/step\n",
      "patient id:  584\n",
      "RMSE:  21.791995138576734\n",
      "t0 RMSE:  24.25703830468297\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\588-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2704, 24, 1)\n",
      "y_test.shape:  (2704, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "85/85 [==============================] - 1s 3ms/step\n",
      "85/85 [==============================] - 0s 3ms/step\n",
      "patient id:  588\n",
      "RMSE:  18.578979805158628\n",
      "t0 RMSE:  21.881497344457554\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2616, 24, 1)\n",
      "y_test.shape:  (2616, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "82/82 [==============================] - 1s 3ms/step\n",
      "82/82 [==============================] - 0s 3ms/step\n",
      "patient id:  591\n",
      "RMSE:  20.205512084097904\n",
      "t0 RMSE:  24.09562213590278\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\596-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2540, 24, 1)\n",
      "y_test.shape:  (2540, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "80/80 [==============================] - 1s 3ms/step\n",
      "80/80 [==============================] - 0s 3ms/step\n",
      "patient id:  596\n",
      "RMSE:  16.927286971845252\n",
      "t0 RMSE:  21.24602788258082\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yaml_filepath in yaml_files[:-1]:\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_filepath = f\"./experiments/559_all_final_evaluation.yaml\"\n",
    "yaml_filepath = f\"./original_vandoorn_experiments_90min/596_all_final_evaluation.yaml\"\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\596-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment_18sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2582, 18, 1)\n",
      "y_test.shape:  (2582, 1)\n",
      "x.shape =  (None, 18, 32)\n",
      "x.shape =  (None, 18, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment_18sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "81/81 [==============================] - 1s 4ms/step\n",
      "81/81 [==============================] - 0s 2ms/step\n",
      "patient id:  596\n",
      "RMSE:  16.967764560507874\n",
      "t0 RMSE:  21.19739581801337\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify DiaTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # # Calculate MSE\n",
    "    # mse = np.mean((y_test - y_pred) ** 2)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "    # print(\"patient id: \", patient_id)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # print(\"RMSE: \", rmse)\n",
    "    # print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-eval loop for multi folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-19 15:58:03,863 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\all does not exist.\n",
      "2025-01-19 15:58:03,865 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 40\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 23\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 154\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (372032, 6, 1)\n",
      "y_train.shape:  (372032, 1)\n",
      "x_valid.shape:  (92988, 6, 1)\n",
      "y_valid.shape:  (92988, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 15:58:52,380 WARNING Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 15:58:52,441 WARNING Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 15:58:52,530 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 372032 samples, validate on 92988 samples\n",
      "Epoch 1/10000\n",
      "370688/372032 [============================>.] - ETA: 0s - loss: 2.4357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372032/372032 [==============================] - 6s 17us/sample - loss: 2.4346 - val_loss: 2.2047\n",
      "Epoch 2/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.8349 - val_loss: 0.0876\n",
      "Epoch 3/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.1657 - val_loss: 0.0871\n",
      "Epoch 4/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.1403 - val_loss: 0.0820\n",
      "Epoch 5/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.1286 - val_loss: 0.0728\n",
      "Epoch 6/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.1195 - val_loss: 0.0674\n",
      "Epoch 7/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.1099 - val_loss: 0.0657\n",
      "Epoch 8/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.1034 - val_loss: 0.0676\n",
      "Epoch 9/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0975 - val_loss: 0.0653\n",
      "Epoch 10/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0938 - val_loss: 0.0662\n",
      "Epoch 11/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0913 - val_loss: 0.0661\n",
      "Epoch 12/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0894 - val_loss: 0.0658\n",
      "Epoch 13/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0892 - val_loss: 0.0661\n",
      "Epoch 14/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0887 - val_loss: 0.0664\n",
      "Epoch 15/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0882 - val_loss: 0.0777\n",
      "Epoch 16/10000\n",
      "372032/372032 [==============================] - 4s 10us/sample - loss: 0.0876 - val_loss: 0.0668\n",
      "Epoch 17/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0872 - val_loss: 0.0697\n",
      "Epoch 18/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0868 - val_loss: 0.0667\n",
      "Epoch 19/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0869 - val_loss: 0.0686\n",
      "Epoch 20/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0865 - val_loss: 0.0678\n",
      "Epoch 21/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0863 - val_loss: 0.0653\n",
      "Epoch 22/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0866 - val_loss: 0.0664\n",
      "Epoch 23/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0864 - val_loss: 0.0665\n",
      "Epoch 24/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0860 - val_loss: 0.0674\n",
      "Epoch 25/10000\n",
      "372032/372032 [==============================] - 4s 10us/sample - loss: 0.0862 - val_loss: 0.0672\n",
      "Epoch 26/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0862 - val_loss: 0.0651\n",
      "Epoch 27/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0863 - val_loss: 0.0653\n",
      "Epoch 28/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0666\n",
      "Epoch 29/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0860 - val_loss: 0.0664\n",
      "Epoch 30/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0657\n",
      "Epoch 31/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0682\n",
      "Epoch 32/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0858 - val_loss: 0.0653\n",
      "Epoch 33/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0862 - val_loss: 0.0649\n",
      "Epoch 34/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0660\n",
      "Epoch 35/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0857 - val_loss: 0.0652\n",
      "Epoch 36/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0666\n",
      "Epoch 37/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0857 - val_loss: 0.0659\n",
      "Epoch 38/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0859 - val_loss: 0.0672\n",
      "Epoch 39/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0825 - val_loss: 0.0624\n",
      "Epoch 40/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0791 - val_loss: 0.0639\n",
      "Epoch 41/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0780 - val_loss: 0.0623\n",
      "Epoch 42/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0775 - val_loss: 0.0635\n",
      "Epoch 43/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0772 - val_loss: 0.0646\n",
      "Epoch 44/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0770 - val_loss: 0.0626\n",
      "Epoch 45/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0769 - val_loss: 0.0623\n",
      "Epoch 46/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0768 - val_loss: 0.0622\n",
      "Epoch 47/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0770 - val_loss: 0.0648\n",
      "Epoch 48/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0768 - val_loss: 0.0638\n",
      "Epoch 49/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0763 - val_loss: 0.0628\n",
      "Epoch 50/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0767 - val_loss: 0.0623\n",
      "Epoch 51/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0766 - val_loss: 0.0654\n",
      "Epoch 52/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0765 - val_loss: 0.0618\n",
      "Epoch 53/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0766 - val_loss: 0.0636\n",
      "Epoch 54/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0764 - val_loss: 0.0623\n",
      "Epoch 55/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0765 - val_loss: 0.0621\n",
      "Epoch 56/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0763 - val_loss: 0.0628\n",
      "Epoch 57/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0764 - val_loss: 0.0620\n",
      "Epoch 58/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0763 - val_loss: 0.0622\n",
      "Epoch 59/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0764 - val_loss: 0.0625\n",
      "Epoch 60/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0764 - val_loss: 0.0620\n",
      "Epoch 61/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0764 - val_loss: 0.0619\n",
      "Epoch 62/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0763 - val_loss: 0.0626\n",
      "Epoch 63/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0761 - val_loss: 0.0619\n",
      "Epoch 64/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0761 - val_loss: 0.0633\n",
      "Epoch 65/10000\n",
      "372032/372032 [==============================] - 4s 10us/sample - loss: 0.0762 - val_loss: 0.0620\n",
      "Epoch 66/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0758 - val_loss: 0.0625\n",
      "Epoch 67/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0761 - val_loss: 0.0625\n",
      "Epoch 68/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0759 - val_loss: 0.0619\n",
      "Epoch 69/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0760 - val_loss: 0.0616\n",
      "Epoch 70/10000\n",
      "372032/372032 [==============================] - 4s 11us/sample - loss: 0.0760 - val_loss: 0.0614\n",
      "Epoch 71/10000\n",
      "372032/372032 [==============================] - 4s 12us/sample - loss: 0.0758 - val_loss: 0.0620\n",
      "Epoch 72/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0625\n",
      "Epoch 73/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0619\n",
      "Epoch 74/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0761 - val_loss: 0.0619\n",
      "Epoch 75/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0620\n",
      "Epoch 76/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0620\n",
      "Epoch 77/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0763 - val_loss: 0.0622\n",
      "Epoch 78/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0615\n",
      "Epoch 79/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0623\n",
      "Epoch 80/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0619\n",
      "Epoch 81/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0639\n",
      "Epoch 82/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0617\n",
      "Epoch 83/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0614\n",
      "Epoch 84/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0613\n",
      "Epoch 85/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0625\n",
      "Epoch 86/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0651\n",
      "Epoch 87/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0623\n",
      "Epoch 88/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0611\n",
      "Epoch 89/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0761 - val_loss: 0.0621\n",
      "Epoch 90/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0617\n",
      "Epoch 91/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0630\n",
      "Epoch 92/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0610\n",
      "Epoch 93/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0760 - val_loss: 0.0641\n",
      "Epoch 94/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0645\n",
      "Epoch 95/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0625\n",
      "Epoch 96/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0618\n",
      "Epoch 97/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 98/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0619\n",
      "Epoch 99/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 100/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0623\n",
      "Epoch 101/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0619\n",
      "Epoch 102/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0628\n",
      "Epoch 103/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0633\n",
      "Epoch 104/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0761 - val_loss: 0.0618\n",
      "Epoch 105/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0622\n",
      "Epoch 106/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0622\n",
      "Epoch 107/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0617\n",
      "Epoch 108/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0620\n",
      "Epoch 109/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0617\n",
      "Epoch 110/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0629\n",
      "Epoch 111/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0618\n",
      "Epoch 112/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0620\n",
      "Epoch 113/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 114/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0617\n",
      "Epoch 115/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0625\n",
      "Epoch 116/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0619\n",
      "Epoch 117/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0614\n",
      "Epoch 118/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0613\n",
      "Epoch 119/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0616\n",
      "Epoch 120/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0613\n",
      "Epoch 121/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0622\n",
      "Epoch 122/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0622\n",
      "Epoch 123/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0621\n",
      "Epoch 124/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0618\n",
      "Epoch 125/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0620\n",
      "Epoch 126/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0615\n",
      "Epoch 127/10000\n",
      "372032/372032 [==============================] - 5s 15us/sample - loss: 0.0754 - val_loss: 0.0614\n",
      "Epoch 128/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0618\n",
      "Epoch 129/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0613\n",
      "Epoch 130/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0619\n",
      "Epoch 131/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0611\n",
      "Epoch 132/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0617\n",
      "Epoch 133/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0618\n",
      "Epoch 134/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0615\n",
      "Epoch 135/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0614\n",
      "Epoch 136/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0615\n",
      "Epoch 137/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0611\n",
      "Epoch 138/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0616\n",
      "Epoch 139/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0611\n",
      "Epoch 140/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0626\n",
      "Epoch 141/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0621\n",
      "Epoch 142/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0621\n",
      "Epoch 143/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0615\n",
      "Epoch 144/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0617\n",
      "Epoch 145/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0620\n",
      "Epoch 146/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0631\n",
      "Epoch 147/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 148/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 149/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0617\n",
      "Epoch 150/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0624\n",
      "Epoch 151/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 152/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0619\n",
      "Epoch 153/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0621\n",
      "Epoch 154/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0617\n",
      "Epoch 155/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0620\n",
      "Epoch 156/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0616\n",
      "Epoch 157/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0613\n",
      "Epoch 158/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0618\n",
      "Epoch 159/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0615\n",
      "Epoch 160/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0612\n",
      "Epoch 161/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0619\n",
      "Epoch 162/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 163/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0612\n",
      "Epoch 164/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0611\n",
      "Epoch 165/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 166/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0615\n",
      "Epoch 167/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0620\n",
      "Epoch 168/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0612\n",
      "Epoch 169/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0613\n",
      "Epoch 170/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0618\n",
      "Epoch 171/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0616\n",
      "Epoch 172/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0613\n",
      "Epoch 173/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0617\n",
      "Epoch 174/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0615\n",
      "Epoch 175/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0611\n",
      "Epoch 176/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 177/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0622\n",
      "Epoch 178/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 179/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0618\n",
      "Epoch 180/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0614\n",
      "Epoch 181/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0608\n",
      "Epoch 182/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 183/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0615\n",
      "Epoch 184/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0619\n",
      "Epoch 185/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 186/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0617\n",
      "Epoch 187/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0613\n",
      "Epoch 188/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0619\n",
      "Epoch 189/10000\n",
      "372032/372032 [==============================] - 5s 15us/sample - loss: 0.0754 - val_loss: 0.0613\n",
      "Epoch 190/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0617\n",
      "Epoch 191/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0623\n",
      "Epoch 192/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 193/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0630\n",
      "Epoch 194/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0622\n",
      "Epoch 195/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0610\n",
      "Epoch 196/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0616\n",
      "Epoch 197/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0630\n",
      "Epoch 198/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0617\n",
      "Epoch 199/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0613\n",
      "Epoch 200/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0619\n",
      "Epoch 201/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0615\n",
      "Epoch 202/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0616\n",
      "Epoch 203/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 204/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0611\n",
      "Epoch 205/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0624\n",
      "Epoch 206/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0612\n",
      "Epoch 207/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 208/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0615\n",
      "Epoch 209/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 210/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0624\n",
      "Epoch 211/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0618\n",
      "Epoch 212/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0621\n",
      "Epoch 213/10000\n",
      "372032/372032 [==============================] - 5s 15us/sample - loss: 0.0755 - val_loss: 0.0614\n",
      "Epoch 214/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0624\n",
      "Epoch 215/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0610\n",
      "Epoch 216/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0618\n",
      "Epoch 217/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 218/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0615\n",
      "Epoch 219/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0619\n",
      "Epoch 220/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0615\n",
      "Epoch 221/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0610\n",
      "Epoch 222/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0640\n",
      "Epoch 223/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0614\n",
      "Epoch 224/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 225/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0615\n",
      "Epoch 226/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0613\n",
      "Epoch 227/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0612\n",
      "Epoch 228/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0611\n",
      "Epoch 229/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 230/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0617\n",
      "Epoch 231/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0626\n",
      "Epoch 232/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0627\n",
      "Epoch 233/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0618\n",
      "Epoch 234/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0614\n",
      "Epoch 235/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0622\n",
      "Epoch 236/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0611\n",
      "Epoch 237/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0614\n",
      "Epoch 238/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0610\n",
      "Epoch 239/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0615\n",
      "Epoch 240/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0620\n",
      "Epoch 241/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0620\n",
      "Epoch 242/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0614\n",
      "Epoch 243/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0612\n",
      "Epoch 244/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0625\n",
      "Epoch 245/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0614\n",
      "Epoch 246/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0617\n",
      "Epoch 247/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0621\n",
      "Epoch 248/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0615\n",
      "Epoch 249/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0616\n",
      "Epoch 250/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0616\n",
      "Epoch 251/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0621\n",
      "Epoch 252/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0616\n",
      "Epoch 253/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0616\n",
      "Epoch 254/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0612\n",
      "Epoch 255/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0619\n",
      "Epoch 256/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0612\n",
      "Epoch 257/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0612\n",
      "Epoch 258/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0612\n",
      "Epoch 259/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0617\n",
      "Epoch 260/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0620\n",
      "Epoch 261/10000\n",
      "372032/372032 [==============================] - 5s 15us/sample - loss: 0.0753 - val_loss: 0.0613\n",
      "Epoch 262/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0612\n",
      "Epoch 263/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0611\n",
      "Epoch 264/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0610\n",
      "Epoch 265/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0614\n",
      "Epoch 266/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0622\n",
      "Epoch 267/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0616\n",
      "Epoch 268/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 269/10000\n",
      "372032/372032 [==============================] - 5s 15us/sample - loss: 0.0751 - val_loss: 0.0618\n",
      "Epoch 270/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0632\n",
      "Epoch 271/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0615\n",
      "Epoch 272/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0618\n",
      "Epoch 273/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0616\n",
      "Epoch 274/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0616\n",
      "Epoch 275/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0616\n",
      "Epoch 276/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0617\n",
      "Epoch 277/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0618\n",
      "Epoch 278/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0617\n",
      "Epoch 279/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0620\n",
      "Epoch 280/10000\n",
      "372032/372032 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0611\n",
      "Epoch 281/10000\n",
      "372032/372032 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0617\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject10.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject10.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11829, 6, 1)\n",
      "y_test.shape:  (11829, 1)\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:13,200 WARNING Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:13,272 WARNING Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject10.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject11.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11623, 6, 1)\n",
      "y_test.shape:  (11623, 1)\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:20,600 WARNING Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:20,661 WARNING Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject1.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8612, 6, 1)\n",
      "y_test.shape:  (8612, 1)\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:27,860 WARNING Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:27,922 WARNING Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject2.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject2.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7435, 6, 1)\n",
      "y_test.shape:  (7435, 1)\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:34,278 WARNING Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:34,349 WARNING Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject2.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject3.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject3.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8101, 6, 1)\n",
      "y_test.shape:  (8101, 1)\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:40,289 WARNING Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:40,353 WARNING Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject3.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject4.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject4.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11449, 6, 1)\n",
      "y_test.shape:  (11449, 1)\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:46,884 WARNING Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:46,948 WARNING Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject4.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject5.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject5.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10753, 6, 1)\n",
      "y_test.shape:  (10753, 1)\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:54,279 WARNING Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:21:54,366 WARNING Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject5.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject6.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject6.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11541, 6, 1)\n",
      "y_test.shape:  (11541, 1)\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:01,652 WARNING Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:01,722 WARNING Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject6.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject7.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject7.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11389, 6, 1)\n",
      "y_test.shape:  (11389, 1)\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:09,490 WARNING Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:09,541 WARNING Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject7.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject8.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject8.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11340, 6, 1)\n",
      "y_test.shape:  (11340, 1)\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:17,087 WARNING Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:17,150 WARNING Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject8.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject9.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject9.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11745, 6, 1)\n",
      "y_test.shape:  (11745, 1)\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:25,345 WARNING Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:22:25,414 WARNING Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject9.csv\n",
      "2025-01-19 16:22:31,460 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 154\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (362525, 6, 1)\n",
      "y_train.shape:  (362525, 1)\n",
      "x_valid.shape:  (90612, 6, 1)\n",
      "y_valid.shape:  (90612, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:23:35,247 WARNING Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:23:35,311 WARNING Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 16:23:35,415 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 362525 samples, validate on 90612 samples\n",
      "Epoch 1/10000\n",
      "360448/362525 [============================>.] - ETA: 0s - loss: 2.4379"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362525/362525 [==============================] - 8s 23us/sample - loss: 2.4356 - val_loss: 2.1635\n",
      "Epoch 2/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 1.0106 - val_loss: 0.1006\n",
      "Epoch 3/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.1605 - val_loss: 0.0752\n",
      "Epoch 4/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.1300 - val_loss: 0.0693\n",
      "Epoch 5/10000\n",
      "362525/362525 [==============================] - 7s 18us/sample - loss: 0.1183 - val_loss: 0.0685\n",
      "Epoch 6/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.1101 - val_loss: 0.0659\n",
      "Epoch 7/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.1047 - val_loss: 0.0684\n",
      "Epoch 8/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0997 - val_loss: 0.0663\n",
      "Epoch 9/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0967 - val_loss: 0.0666\n",
      "Epoch 10/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0931 - val_loss: 0.0676\n",
      "Epoch 11/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0900 - val_loss: 0.0663\n",
      "Epoch 12/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0862 - val_loss: 0.0676\n",
      "Epoch 13/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0835 - val_loss: 0.0703\n",
      "Epoch 14/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0816 - val_loss: 0.0648\n",
      "Epoch 15/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0796 - val_loss: 0.0655\n",
      "Epoch 16/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0666\n",
      "Epoch 17/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0781 - val_loss: 0.0648\n",
      "Epoch 18/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0776 - val_loss: 0.0651\n",
      "Epoch 19/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0777 - val_loss: 0.0662\n",
      "Epoch 20/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0776 - val_loss: 0.0660\n",
      "Epoch 21/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0776 - val_loss: 0.0649\n",
      "Epoch 22/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0772 - val_loss: 0.0662\n",
      "Epoch 23/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0775 - val_loss: 0.0647\n",
      "Epoch 24/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0772 - val_loss: 0.0651\n",
      "Epoch 25/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0771 - val_loss: 0.0656\n",
      "Epoch 26/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0774 - val_loss: 0.0655\n",
      "Epoch 27/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0768 - val_loss: 0.0640\n",
      "Epoch 28/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0769 - val_loss: 0.0643\n",
      "Epoch 29/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0769 - val_loss: 0.0644\n",
      "Epoch 30/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0770 - val_loss: 0.0689\n",
      "Epoch 31/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0768 - val_loss: 0.0646\n",
      "Epoch 32/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0767 - val_loss: 0.0663\n",
      "Epoch 33/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0769 - val_loss: 0.0642\n",
      "Epoch 34/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0765 - val_loss: 0.0656\n",
      "Epoch 35/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0764 - val_loss: 0.0642\n",
      "Epoch 36/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0763 - val_loss: 0.0663\n",
      "Epoch 37/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0764 - val_loss: 0.0654\n",
      "Epoch 38/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0644\n",
      "Epoch 39/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0762 - val_loss: 0.0680\n",
      "Epoch 40/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0647\n",
      "Epoch 41/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0764 - val_loss: 0.0653\n",
      "Epoch 42/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0636\n",
      "Epoch 43/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0762 - val_loss: 0.0676\n",
      "Epoch 44/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0763 - val_loss: 0.0672\n",
      "Epoch 45/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0763 - val_loss: 0.0637\n",
      "Epoch 46/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0632\n",
      "Epoch 47/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0763 - val_loss: 0.0635\n",
      "Epoch 48/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0635\n",
      "Epoch 49/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0640\n",
      "Epoch 50/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0660\n",
      "Epoch 51/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0633\n",
      "Epoch 52/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0633\n",
      "Epoch 53/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0636\n",
      "Epoch 54/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0630\n",
      "Epoch 55/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0641\n",
      "Epoch 56/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0637\n",
      "Epoch 57/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0634\n",
      "Epoch 58/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0644\n",
      "Epoch 59/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0683\n",
      "Epoch 60/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0635\n",
      "Epoch 61/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0645\n",
      "Epoch 62/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0632\n",
      "Epoch 63/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 64/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0631\n",
      "Epoch 65/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0625\n",
      "Epoch 66/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0631\n",
      "Epoch 67/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0636\n",
      "Epoch 68/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0655\n",
      "Epoch 69/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0653\n",
      "Epoch 70/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0676\n",
      "Epoch 71/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0626\n",
      "Epoch 72/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0633\n",
      "Epoch 73/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0640\n",
      "Epoch 74/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0629\n",
      "Epoch 75/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 76/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0629\n",
      "Epoch 77/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0631\n",
      "Epoch 78/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0630\n",
      "Epoch 79/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 80/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0636\n",
      "Epoch 81/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0634\n",
      "Epoch 82/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0629\n",
      "Epoch 83/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0630\n",
      "Epoch 84/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0638\n",
      "Epoch 85/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0652\n",
      "Epoch 86/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0638\n",
      "Epoch 87/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0648\n",
      "Epoch 88/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0654\n",
      "Epoch 89/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0628\n",
      "Epoch 90/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 91/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0648\n",
      "Epoch 92/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0632\n",
      "Epoch 93/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 94/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0628\n",
      "Epoch 95/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0635\n",
      "Epoch 96/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0625\n",
      "Epoch 97/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 98/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0627\n",
      "Epoch 99/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0643\n",
      "Epoch 100/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0753 - val_loss: 0.0631\n",
      "Epoch 101/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0637\n",
      "Epoch 102/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 103/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0632\n",
      "Epoch 104/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 105/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0635\n",
      "Epoch 106/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0626\n",
      "Epoch 107/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0625\n",
      "Epoch 108/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 109/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0630\n",
      "Epoch 110/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0751 - val_loss: 0.0624\n",
      "Epoch 111/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0626\n",
      "Epoch 112/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0627\n",
      "Epoch 113/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0625\n",
      "Epoch 114/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0645\n",
      "Epoch 115/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0626\n",
      "Epoch 116/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0627\n",
      "Epoch 117/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0647\n",
      "Epoch 118/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0751 - val_loss: 0.0645\n",
      "Epoch 119/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0624\n",
      "Epoch 120/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0653\n",
      "Epoch 121/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0628\n",
      "Epoch 122/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0629\n",
      "Epoch 123/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0629\n",
      "Epoch 124/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0630\n",
      "Epoch 125/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0627\n",
      "Epoch 126/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 127/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0635\n",
      "Epoch 128/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0630\n",
      "Epoch 129/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0630\n",
      "Epoch 130/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0625\n",
      "Epoch 131/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0627\n",
      "Epoch 132/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0645\n",
      "Epoch 133/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0626\n",
      "Epoch 134/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0625\n",
      "Epoch 135/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0644\n",
      "Epoch 136/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0627\n",
      "Epoch 137/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0625\n",
      "Epoch 138/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0629\n",
      "Epoch 139/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0629\n",
      "Epoch 140/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0626\n",
      "Epoch 141/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0625\n",
      "Epoch 142/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0625\n",
      "Epoch 143/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0627\n",
      "Epoch 144/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0624\n",
      "Epoch 145/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0623\n",
      "Epoch 146/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0664\n",
      "Epoch 147/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0679\n",
      "Epoch 148/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0624\n",
      "Epoch 149/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0644\n",
      "Epoch 150/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 151/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0627\n",
      "Epoch 152/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0625\n",
      "Epoch 153/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0621\n",
      "Epoch 154/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0629\n",
      "Epoch 155/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0626\n",
      "Epoch 156/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0633\n",
      "Epoch 157/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0628\n",
      "Epoch 158/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0628\n",
      "Epoch 159/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0632\n",
      "Epoch 160/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0635\n",
      "Epoch 161/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 162/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0631\n",
      "Epoch 163/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0633\n",
      "Epoch 164/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0622\n",
      "Epoch 165/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0640\n",
      "Epoch 166/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0627\n",
      "Epoch 167/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0746 - val_loss: 0.0645\n",
      "Epoch 168/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0652\n",
      "Epoch 169/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0624\n",
      "Epoch 170/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0625\n",
      "Epoch 171/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0750 - val_loss: 0.0626\n",
      "Epoch 172/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 173/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0628\n",
      "Epoch 174/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0627\n",
      "Epoch 175/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0629\n",
      "Epoch 176/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0626\n",
      "Epoch 177/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0621\n",
      "Epoch 178/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0662\n",
      "Epoch 179/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 180/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0621\n",
      "Epoch 181/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0637\n",
      "Epoch 182/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0653\n",
      "Epoch 183/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0636\n",
      "Epoch 184/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0629\n",
      "Epoch 185/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0633\n",
      "Epoch 186/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0623\n",
      "Epoch 187/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 188/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0622\n",
      "Epoch 189/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0626\n",
      "Epoch 190/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 191/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0627\n",
      "Epoch 192/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 193/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0626\n",
      "Epoch 194/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0621\n",
      "Epoch 195/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0628\n",
      "Epoch 196/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0634\n",
      "Epoch 197/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0620\n",
      "Epoch 198/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0640\n",
      "Epoch 199/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0626\n",
      "Epoch 200/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0632\n",
      "Epoch 201/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0631\n",
      "Epoch 202/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0624\n",
      "Epoch 203/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0623\n",
      "Epoch 204/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0627\n",
      "Epoch 205/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0623\n",
      "Epoch 206/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0622\n",
      "Epoch 207/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0623\n",
      "Epoch 208/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0742 - val_loss: 0.0644\n",
      "Epoch 209/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0625\n",
      "Epoch 210/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0625\n",
      "Epoch 211/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0636\n",
      "Epoch 212/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0623\n",
      "Epoch 213/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0621\n",
      "Epoch 214/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0632\n",
      "Epoch 215/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0622\n",
      "Epoch 216/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0636\n",
      "Epoch 217/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0622\n",
      "Epoch 218/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0623\n",
      "Epoch 219/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0623\n",
      "Epoch 220/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0622\n",
      "Epoch 221/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0624\n",
      "Epoch 222/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0622\n",
      "Epoch 223/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0636\n",
      "Epoch 224/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0629\n",
      "Epoch 225/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0633\n",
      "Epoch 226/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0621\n",
      "Epoch 227/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0627\n",
      "Epoch 228/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0637\n",
      "Epoch 229/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0628\n",
      "Epoch 230/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0628\n",
      "Epoch 231/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0639\n",
      "Epoch 232/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0624\n",
      "Epoch 233/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0623\n",
      "Epoch 234/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0630\n",
      "Epoch 235/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0630\n",
      "Epoch 236/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0626\n",
      "Epoch 237/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0626\n",
      "Epoch 238/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0622\n",
      "Epoch 239/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0629\n",
      "Epoch 240/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0634\n",
      "Epoch 241/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0635\n",
      "Epoch 242/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0636\n",
      "Epoch 243/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0629\n",
      "Epoch 244/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0625\n",
      "Epoch 245/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0628\n",
      "Epoch 246/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0625\n",
      "Epoch 247/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0620\n",
      "Epoch 248/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0628\n",
      "Epoch 249/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0747 - val_loss: 0.0638\n",
      "Epoch 250/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0627\n",
      "Epoch 251/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0623\n",
      "Epoch 252/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0626\n",
      "Epoch 253/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0648\n",
      "Epoch 254/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0623\n",
      "Epoch 255/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0625\n",
      "Epoch 256/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0623\n",
      "Epoch 257/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0624\n",
      "Epoch 258/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 259/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0628\n",
      "Epoch 260/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0624\n",
      "Epoch 261/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0628\n",
      "Epoch 262/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0637\n",
      "Epoch 263/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0636\n",
      "Epoch 264/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0622\n",
      "Epoch 265/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0627\n",
      "Epoch 266/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0642\n",
      "Epoch 267/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 268/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0624\n",
      "Epoch 269/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0623\n",
      "Epoch 270/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0633\n",
      "Epoch 271/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0742 - val_loss: 0.0632\n",
      "Epoch 272/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0633\n",
      "Epoch 273/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0619\n",
      "Epoch 274/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 275/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 276/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0746 - val_loss: 0.0623\n",
      "Epoch 277/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0622\n",
      "Epoch 278/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0620\n",
      "Epoch 279/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0625\n",
      "Epoch 280/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0637\n",
      "Epoch 281/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0633\n",
      "Epoch 282/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0631\n",
      "Epoch 283/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0635\n",
      "Epoch 284/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0632\n",
      "Epoch 285/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0743 - val_loss: 0.0630\n",
      "Epoch 286/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0629\n",
      "Epoch 287/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0744 - val_loss: 0.0632\n",
      "Epoch 288/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0744 - val_loss: 0.0625\n",
      "Epoch 289/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0621\n",
      "Epoch 290/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0625\n",
      "Epoch 291/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0629\n",
      "Epoch 292/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0745 - val_loss: 0.0627\n",
      "Epoch 293/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0743 - val_loss: 0.0625\n",
      "Epoch 294/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0626\n",
      "Epoch 295/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0632\n",
      "Epoch 296/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0627\n",
      "Epoch 297/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0633\n",
      "Epoch 298/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0624\n",
      "Epoch 299/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0628\n",
      "Epoch 300/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0741 - val_loss: 0.0628\n",
      "Epoch 301/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0626\n",
      "Epoch 302/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0743 - val_loss: 0.0621\n",
      "Epoch 303/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0653\n",
      "Epoch 304/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0637\n",
      "Epoch 305/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0642\n",
      "Epoch 306/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0621\n",
      "Epoch 307/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0619\n",
      "Epoch 308/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0742 - val_loss: 0.0631\n",
      "Epoch 309/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0623\n",
      "Epoch 310/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0625\n",
      "Epoch 311/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0624\n",
      "Epoch 312/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0627\n",
      "Epoch 313/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0744 - val_loss: 0.0635\n",
      "Epoch 314/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0629\n",
      "Epoch 315/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0624\n",
      "Epoch 316/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0633\n",
      "Epoch 317/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0621\n",
      "Epoch 318/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0632\n",
      "Epoch 319/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0742 - val_loss: 0.0623\n",
      "Epoch 320/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 321/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0646\n",
      "Epoch 322/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0631\n",
      "Epoch 323/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0622\n",
      "Epoch 324/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0747 - val_loss: 0.0626\n",
      "Epoch 325/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0627\n",
      "Epoch 326/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0741 - val_loss: 0.0626\n",
      "Epoch 327/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 328/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0623\n",
      "Epoch 329/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 330/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0746 - val_loss: 0.0623\n",
      "Epoch 331/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0627\n",
      "Epoch 332/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0629\n",
      "Epoch 333/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0644\n",
      "Epoch 334/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0624\n",
      "Epoch 335/10000\n",
      "362525/362525 [==============================] - 6s 16us/sample - loss: 0.0741 - val_loss: 0.0629\n",
      "Epoch 336/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0745 - val_loss: 0.0649\n",
      "Epoch 337/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0625\n",
      "Epoch 338/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0647\n",
      "Epoch 339/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0623\n",
      "Epoch 340/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0739 - val_loss: 0.0625\n",
      "Epoch 341/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0628\n",
      "Epoch 342/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0740 - val_loss: 0.0625\n",
      "Epoch 343/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0634\n",
      "Epoch 344/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0746 - val_loss: 0.0625\n",
      "Epoch 345/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0646\n",
      "Epoch 346/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 347/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0627\n",
      "Epoch 348/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0624\n",
      "Epoch 349/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0745 - val_loss: 0.0629\n",
      "Epoch 350/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0622\n",
      "Epoch 351/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0623\n",
      "Epoch 352/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0623\n",
      "Epoch 353/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0622\n",
      "Epoch 354/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0633\n",
      "Epoch 355/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0622\n",
      "Epoch 356/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0744 - val_loss: 0.0634\n",
      "Epoch 357/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0626\n",
      "Epoch 358/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0639\n",
      "Epoch 359/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0742 - val_loss: 0.0626\n",
      "Epoch 360/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0630\n",
      "Epoch 361/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0743 - val_loss: 0.0630\n",
      "Epoch 362/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0627\n",
      "Epoch 363/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0741 - val_loss: 0.0635\n",
      "Epoch 364/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0745 - val_loss: 0.0626\n",
      "Epoch 365/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 366/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 367/10000\n",
      "362525/362525 [==============================] - 5s 13us/sample - loss: 0.0741 - val_loss: 0.0620\n",
      "Epoch 368/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0741 - val_loss: 0.0629\n",
      "Epoch 369/10000\n",
      "362525/362525 [==============================] - 5s 15us/sample - loss: 0.0742 - val_loss: 0.0626\n",
      "Epoch 370/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0670\n",
      "Epoch 371/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0743 - val_loss: 0.0635\n",
      "Epoch 372/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0742 - val_loss: 0.0626\n",
      "Epoch 373/10000\n",
      "362525/362525 [==============================] - 5s 14us/sample - loss: 0.0740 - val_loss: 0.0630\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject12.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject12.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11783, 6, 1)\n",
      "y_test.shape:  (11783, 1)\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:42,306 WARNING Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:42,366 WARNING Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject12.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject13.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject13.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 40\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10688, 6, 1)\n",
      "y_test.shape:  (10688, 1)\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:50,055 WARNING Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:50,111 WARNING Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject13.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject14.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject14.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11428, 6, 1)\n",
      "y_test.shape:  (11428, 1)\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:57,576 WARNING Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:54:57,630 WARNING Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject14.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject15.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject15.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11857, 6, 1)\n",
      "y_test.shape:  (11857, 1)\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:05,522 WARNING Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:05,569 WARNING Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject15.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject16.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject16.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11736, 6, 1)\n",
      "y_test.shape:  (11736, 1)\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:13,481 WARNING Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:13,543 WARNING Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject16.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject17.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject17.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11929, 6, 1)\n",
      "y_test.shape:  (11929, 1)\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:21,476 WARNING Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:21,559 WARNING Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject17.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject18.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11771, 6, 1)\n",
      "y_test.shape:  (11771, 1)\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:30,031 WARNING Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:30,102 WARNING Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject19.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject19.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11771, 6, 1)\n",
      "y_test.shape:  (11771, 1)\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:38,169 WARNING Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:38,234 WARNING Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject19.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject20.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject20.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11437, 6, 1)\n",
      "y_test.shape:  (11437, 1)\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:46,226 WARNING Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:46,286 WARNING Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject20.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject21.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject21.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 23\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11432, 6, 1)\n",
      "y_test.shape:  (11432, 1)\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:54,218 WARNING Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:55:54,270 WARNING Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject21.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject22.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject22.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11868, 6, 1)\n",
      "y_test.shape:  (11868, 1)\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:56:02,865 WARNING Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:56:02,918 WARNING Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject22.csv\n",
      "2025-01-19 16:56:09,605 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 40\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 23\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 154\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (366707, 6, 1)\n",
      "y_train.shape:  (366707, 1)\n",
      "x_valid.shape:  (91655, 6, 1)\n",
      "y_valid.shape:  (91655, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:57:11,008 WARNING Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 16:57:11,083 WARNING Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 16:57:11,195 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 366707 samples, validate on 91655 samples\n",
      "Epoch 1/10000\n",
      "366592/366707 [============================>.] - ETA: 0s - loss: 0.8276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366707/366707 [==============================] - 9s 24us/sample - loss: 0.8274 - val_loss: 0.1399\n",
      "Epoch 2/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.2609 - val_loss: 0.0855\n",
      "Epoch 3/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.2319 - val_loss: 0.0880\n",
      "Epoch 4/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.2110 - val_loss: 0.0766\n",
      "Epoch 5/10000\n",
      "366707/366707 [==============================] - 7s 20us/sample - loss: 0.1867 - val_loss: 0.0705\n",
      "Epoch 6/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1669 - val_loss: 0.0663\n",
      "Epoch 7/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1497 - val_loss: 0.0642\n",
      "Epoch 8/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1336 - val_loss: 0.0669\n",
      "Epoch 9/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1216 - val_loss: 0.0693\n",
      "Epoch 10/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1111 - val_loss: 0.0720\n",
      "Epoch 11/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.1026 - val_loss: 0.0727\n",
      "Epoch 12/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0969 - val_loss: 0.0640\n",
      "Epoch 13/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0918 - val_loss: 0.0638\n",
      "Epoch 14/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0884 - val_loss: 0.0625\n",
      "Epoch 15/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0859 - val_loss: 0.0637\n",
      "Epoch 16/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0844 - val_loss: 0.0640\n",
      "Epoch 17/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0839 - val_loss: 0.0640\n",
      "Epoch 18/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0828 - val_loss: 0.0665\n",
      "Epoch 19/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0814 - val_loss: 0.0623\n",
      "Epoch 20/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0810 - val_loss: 0.0624\n",
      "Epoch 21/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0806 - val_loss: 0.0643\n",
      "Epoch 22/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0809 - val_loss: 0.0629\n",
      "Epoch 23/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0806 - val_loss: 0.0622\n",
      "Epoch 24/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0804 - val_loss: 0.0640\n",
      "Epoch 25/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0804 - val_loss: 0.0625\n",
      "Epoch 26/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0804 - val_loss: 0.0627\n",
      "Epoch 27/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0804 - val_loss: 0.0618\n",
      "Epoch 28/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0802 - val_loss: 0.0629\n",
      "Epoch 29/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0631\n",
      "Epoch 30/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0800 - val_loss: 0.0615\n",
      "Epoch 31/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0799 - val_loss: 0.0613\n",
      "Epoch 32/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0797 - val_loss: 0.0663\n",
      "Epoch 33/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0797 - val_loss: 0.0648\n",
      "Epoch 34/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0618\n",
      "Epoch 35/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0801 - val_loss: 0.0630\n",
      "Epoch 36/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0796 - val_loss: 0.0667\n",
      "Epoch 37/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0800 - val_loss: 0.0621\n",
      "Epoch 38/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0799 - val_loss: 0.0624\n",
      "Epoch 39/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0796 - val_loss: 0.0618\n",
      "Epoch 40/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0620\n",
      "Epoch 41/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0622\n",
      "Epoch 42/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0796 - val_loss: 0.0621\n",
      "Epoch 43/10000\n",
      "366707/366707 [==============================] - 6s 16us/sample - loss: 0.0796 - val_loss: 0.0618\n",
      "Epoch 44/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0798 - val_loss: 0.0626\n",
      "Epoch 45/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0797 - val_loss: 0.0615\n",
      "Epoch 46/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0796 - val_loss: 0.0646\n",
      "Epoch 47/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0791 - val_loss: 0.0613\n",
      "Epoch 48/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0797 - val_loss: 0.0609\n",
      "Epoch 49/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0791 - val_loss: 0.0616\n",
      "Epoch 50/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0793 - val_loss: 0.0632\n",
      "Epoch 51/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0797 - val_loss: 0.0625\n",
      "Epoch 52/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0792 - val_loss: 0.0613\n",
      "Epoch 53/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0794 - val_loss: 0.0621\n",
      "Epoch 54/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0794 - val_loss: 0.0625\n",
      "Epoch 55/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0793 - val_loss: 0.0616\n",
      "Epoch 56/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0791 - val_loss: 0.0613\n",
      "Epoch 57/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0791 - val_loss: 0.0612\n",
      "Epoch 58/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0794 - val_loss: 0.0609\n",
      "Epoch 59/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0789 - val_loss: 0.0612\n",
      "Epoch 60/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0790 - val_loss: 0.0611\n",
      "Epoch 61/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0792 - val_loss: 0.0615\n",
      "Epoch 62/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0794 - val_loss: 0.0617\n",
      "Epoch 63/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0791 - val_loss: 0.0615\n",
      "Epoch 64/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0796 - val_loss: 0.0618\n",
      "Epoch 65/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0788 - val_loss: 0.0607\n",
      "Epoch 66/10000\n",
      "366707/366707 [==============================] - 4s 12us/sample - loss: 0.0787 - val_loss: 0.0623\n",
      "Epoch 67/10000\n",
      "366707/366707 [==============================] - 4s 11us/sample - loss: 0.0792 - val_loss: 0.0614\n",
      "Epoch 68/10000\n",
      "366707/366707 [==============================] - 4s 11us/sample - loss: 0.0788 - val_loss: 0.0617\n",
      "Epoch 69/10000\n",
      "366707/366707 [==============================] - 4s 11us/sample - loss: 0.0789 - val_loss: 0.0606\n",
      "Epoch 70/10000\n",
      "366707/366707 [==============================] - 4s 11us/sample - loss: 0.0793 - val_loss: 0.0610\n",
      "Epoch 71/10000\n",
      "366707/366707 [==============================] - 4s 11us/sample - loss: 0.0787 - val_loss: 0.0615\n",
      "Epoch 72/10000\n",
      "366707/366707 [==============================] - 4s 12us/sample - loss: 0.0787 - val_loss: 0.0612\n",
      "Epoch 73/10000\n",
      "366707/366707 [==============================] - 4s 12us/sample - loss: 0.0790 - val_loss: 0.0612\n",
      "Epoch 74/10000\n",
      "366707/366707 [==============================] - 4s 12us/sample - loss: 0.0789 - val_loss: 0.0612\n",
      "Epoch 75/10000\n",
      "366707/366707 [==============================] - 5s 12us/sample - loss: 0.0788 - val_loss: 0.0613\n",
      "Epoch 76/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0791 - val_loss: 0.0605\n",
      "Epoch 77/10000\n",
      "366707/366707 [==============================] - 6s 16us/sample - loss: 0.0790 - val_loss: 0.0612\n",
      "Epoch 78/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0793 - val_loss: 0.0626\n",
      "Epoch 79/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0605\n",
      "Epoch 80/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0612\n",
      "Epoch 81/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0791 - val_loss: 0.0620\n",
      "Epoch 82/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0603\n",
      "Epoch 83/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0610\n",
      "Epoch 84/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0601\n",
      "Epoch 85/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0790 - val_loss: 0.0608\n",
      "Epoch 86/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0613\n",
      "Epoch 87/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0789 - val_loss: 0.0617\n",
      "Epoch 88/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0606\n",
      "Epoch 89/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0785 - val_loss: 0.0605\n",
      "Epoch 90/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0795 - val_loss: 0.0632\n",
      "Epoch 91/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0632\n",
      "Epoch 92/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0614\n",
      "Epoch 93/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0608\n",
      "Epoch 94/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0605\n",
      "Epoch 95/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0789 - val_loss: 0.0645\n",
      "Epoch 96/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0791 - val_loss: 0.0611\n",
      "Epoch 97/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0628\n",
      "Epoch 98/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0604\n",
      "Epoch 99/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0787 - val_loss: 0.0606\n",
      "Epoch 100/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0610\n",
      "Epoch 101/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0789 - val_loss: 0.0609\n",
      "Epoch 102/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0605\n",
      "Epoch 103/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0623\n",
      "Epoch 104/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0615\n",
      "Epoch 105/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0788 - val_loss: 0.0631\n",
      "Epoch 106/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0788 - val_loss: 0.0608\n",
      "Epoch 107/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0615\n",
      "Epoch 108/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0785 - val_loss: 0.0619\n",
      "Epoch 109/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0789 - val_loss: 0.0620\n",
      "Epoch 110/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0608\n",
      "Epoch 111/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0790 - val_loss: 0.0601\n",
      "Epoch 112/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0612\n",
      "Epoch 113/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0613\n",
      "Epoch 114/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0620\n",
      "Epoch 115/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0788 - val_loss: 0.0601\n",
      "Epoch 116/10000\n",
      "366707/366707 [==============================] - 6s 16us/sample - loss: 0.0787 - val_loss: 0.0608\n",
      "Epoch 117/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0620\n",
      "Epoch 118/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0615\n",
      "Epoch 119/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0611\n",
      "Epoch 120/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0616\n",
      "Epoch 121/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0607\n",
      "Epoch 122/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0619\n",
      "Epoch 123/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0788 - val_loss: 0.0611\n",
      "Epoch 124/10000\n",
      "366707/366707 [==============================] - 6s 16us/sample - loss: 0.0785 - val_loss: 0.0614\n",
      "Epoch 125/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0785 - val_loss: 0.0611\n",
      "Epoch 126/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0785 - val_loss: 0.0618\n",
      "Epoch 127/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0604\n",
      "Epoch 128/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0788 - val_loss: 0.0598\n",
      "Epoch 129/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0611\n",
      "Epoch 130/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0612\n",
      "Epoch 131/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0787 - val_loss: 0.0606\n",
      "Epoch 132/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0613\n",
      "Epoch 133/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0606\n",
      "Epoch 134/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0604\n",
      "Epoch 135/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0603\n",
      "Epoch 136/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0787 - val_loss: 0.0604\n",
      "Epoch 137/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0624\n",
      "Epoch 138/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0607\n",
      "Epoch 139/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0790 - val_loss: 0.0620\n",
      "Epoch 140/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0617\n",
      "Epoch 141/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0609\n",
      "Epoch 142/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0786 - val_loss: 0.0615\n",
      "Epoch 143/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0616\n",
      "Epoch 144/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0788 - val_loss: 0.0619\n",
      "Epoch 145/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0786 - val_loss: 0.0618\n",
      "Epoch 146/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0609\n",
      "Epoch 147/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0787 - val_loss: 0.0627\n",
      "Epoch 148/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0613\n",
      "Epoch 149/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0609\n",
      "Epoch 150/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0607\n",
      "Epoch 151/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0604\n",
      "Epoch 152/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0786 - val_loss: 0.0605\n",
      "Epoch 153/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0607\n",
      "Epoch 154/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0604\n",
      "Epoch 155/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0606\n",
      "Epoch 156/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0787 - val_loss: 0.0637\n",
      "Epoch 157/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0607\n",
      "Epoch 158/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0786 - val_loss: 0.0605\n",
      "Epoch 159/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0606\n",
      "Epoch 160/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0608\n",
      "Epoch 161/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0604\n",
      "Epoch 162/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0611\n",
      "Epoch 163/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0786 - val_loss: 0.0617\n",
      "Epoch 164/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0606\n",
      "Epoch 165/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0600\n",
      "Epoch 166/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0615\n",
      "Epoch 167/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0632\n",
      "Epoch 168/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0785 - val_loss: 0.0608\n",
      "Epoch 169/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0612\n",
      "Epoch 170/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0620\n",
      "Epoch 171/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0612\n",
      "Epoch 172/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0610\n",
      "Epoch 173/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0606\n",
      "Epoch 174/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0604\n",
      "Epoch 175/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0612\n",
      "Epoch 176/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0612\n",
      "Epoch 177/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0622\n",
      "Epoch 178/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0603\n",
      "Epoch 179/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0610\n",
      "Epoch 180/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0621\n",
      "Epoch 181/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0617\n",
      "Epoch 182/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0606\n",
      "Epoch 183/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0615\n",
      "Epoch 184/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0603\n",
      "Epoch 185/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0606\n",
      "Epoch 186/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0609\n",
      "Epoch 187/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0611\n",
      "Epoch 188/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0608\n",
      "Epoch 189/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0603\n",
      "Epoch 190/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0609\n",
      "Epoch 191/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0615\n",
      "Epoch 192/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0608\n",
      "Epoch 193/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0789 - val_loss: 0.0605\n",
      "Epoch 194/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0615\n",
      "Epoch 195/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0605\n",
      "Epoch 196/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0607\n",
      "Epoch 197/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0785 - val_loss: 0.0603\n",
      "Epoch 198/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0605\n",
      "Epoch 199/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0780 - val_loss: 0.0616\n",
      "Epoch 200/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0786 - val_loss: 0.0605\n",
      "Epoch 201/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0612\n",
      "Epoch 202/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0602\n",
      "Epoch 203/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0613\n",
      "Epoch 204/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0605\n",
      "Epoch 205/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0610\n",
      "Epoch 206/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0779 - val_loss: 0.0618\n",
      "Epoch 207/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0614\n",
      "Epoch 208/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0606\n",
      "Epoch 209/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0605\n",
      "Epoch 210/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0613\n",
      "Epoch 211/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0598\n",
      "Epoch 212/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0611\n",
      "Epoch 213/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0607\n",
      "Epoch 214/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0610\n",
      "Epoch 215/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0613\n",
      "Epoch 216/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0602\n",
      "Epoch 217/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0613\n",
      "Epoch 218/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0628\n",
      "Epoch 219/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0612\n",
      "Epoch 220/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0613\n",
      "Epoch 221/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0603\n",
      "Epoch 222/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0617\n",
      "Epoch 223/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0779 - val_loss: 0.0613\n",
      "Epoch 224/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0604\n",
      "Epoch 225/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0609\n",
      "Epoch 226/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0608\n",
      "Epoch 227/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0615\n",
      "Epoch 228/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0607\n",
      "Epoch 229/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0623\n",
      "Epoch 230/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0780 - val_loss: 0.0622\n",
      "Epoch 231/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0780 - val_loss: 0.0618\n",
      "Epoch 232/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0609\n",
      "Epoch 233/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0602\n",
      "Epoch 234/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0786 - val_loss: 0.0605\n",
      "Epoch 235/10000\n",
      "366707/366707 [==============================] - 7s 18us/sample - loss: 0.0782 - val_loss: 0.0622\n",
      "Epoch 236/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0600\n",
      "Epoch 237/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0612\n",
      "Epoch 238/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0783 - val_loss: 0.0600\n",
      "Epoch 239/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0602\n",
      "Epoch 240/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0781 - val_loss: 0.0604\n",
      "Epoch 241/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0615\n",
      "Epoch 242/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0782 - val_loss: 0.0619\n",
      "Epoch 243/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0783 - val_loss: 0.0608\n",
      "Epoch 244/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0779 - val_loss: 0.0619\n",
      "Epoch 245/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0612\n",
      "Epoch 246/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0600\n",
      "Epoch 247/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0604\n",
      "Epoch 248/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0786 - val_loss: 0.0612\n",
      "Epoch 249/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0610\n",
      "Epoch 250/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0604\n",
      "Epoch 251/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0622\n",
      "Epoch 252/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0779 - val_loss: 0.0610\n",
      "Epoch 253/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0602\n",
      "Epoch 254/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0607\n",
      "Epoch 255/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0617\n",
      "Epoch 256/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0612\n",
      "Epoch 257/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0602\n",
      "Epoch 258/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0605\n",
      "Epoch 259/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0623\n",
      "Epoch 260/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0604\n",
      "Epoch 261/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0621\n",
      "Epoch 262/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0780 - val_loss: 0.0607\n",
      "Epoch 263/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0605\n",
      "Epoch 264/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0605\n",
      "Epoch 265/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0779 - val_loss: 0.0613\n",
      "Epoch 266/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0609\n",
      "Epoch 267/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0778 - val_loss: 0.0603\n",
      "Epoch 268/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0613\n",
      "Epoch 269/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0606\n",
      "Epoch 270/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0606\n",
      "Epoch 271/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0601\n",
      "Epoch 272/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0603\n",
      "Epoch 273/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0608\n",
      "Epoch 274/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0607\n",
      "Epoch 275/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0612\n",
      "Epoch 276/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0602\n",
      "Epoch 277/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0605\n",
      "Epoch 278/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0608\n",
      "Epoch 279/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0608\n",
      "Epoch 280/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0783 - val_loss: 0.0615\n",
      "Epoch 281/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0620\n",
      "Epoch 282/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0785 - val_loss: 0.0610\n",
      "Epoch 283/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0784 - val_loss: 0.0608\n",
      "Epoch 284/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0611\n",
      "Epoch 285/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0778 - val_loss: 0.0609\n",
      "Epoch 286/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0778 - val_loss: 0.0613\n",
      "Epoch 287/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0784 - val_loss: 0.0608\n",
      "Epoch 288/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0604\n",
      "Epoch 289/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0787 - val_loss: 0.0608\n",
      "Epoch 290/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0616\n",
      "Epoch 291/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0782 - val_loss: 0.0604\n",
      "Epoch 292/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0785 - val_loss: 0.0613\n",
      "Epoch 293/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0778 - val_loss: 0.0614\n",
      "Epoch 294/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0607\n",
      "Epoch 295/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0615\n",
      "Epoch 296/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0605\n",
      "Epoch 297/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0611\n",
      "Epoch 298/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0621\n",
      "Epoch 299/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0609\n",
      "Epoch 300/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0614\n",
      "Epoch 301/10000\n",
      "366707/366707 [==============================] - 6s 15us/sample - loss: 0.0781 - val_loss: 0.0607\n",
      "Epoch 302/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0783 - val_loss: 0.0613\n",
      "Epoch 303/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0781 - val_loss: 0.0599\n",
      "Epoch 304/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0780 - val_loss: 0.0611\n",
      "Epoch 305/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0779 - val_loss: 0.0610\n",
      "Epoch 306/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0779 - val_loss: 0.0621\n",
      "Epoch 307/10000\n",
      "366707/366707 [==============================] - 5s 14us/sample - loss: 0.0780 - val_loss: 0.0619\n",
      "Epoch 308/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0784 - val_loss: 0.0611\n",
      "Epoch 309/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0782 - val_loss: 0.0610\n",
      "Epoch 310/10000\n",
      "366707/366707 [==============================] - 5s 13us/sample - loss: 0.0781 - val_loss: 0.0606\n",
      "Epoch 311/10000\n",
      "366707/366707 [==============================] - 5s 15us/sample - loss: 0.0780 - val_loss: 0.0607\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject23.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject23.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11617, 6, 1)\n",
      "y_test.shape:  (11617, 1)\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:34,465 WARNING Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:34,540 WARNING Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject23.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject24.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9961, 6, 1)\n",
      "y_test.shape:  (9961, 1)\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:42,697 WARNING Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:42,749 WARNING Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject25.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11796, 6, 1)\n",
      "y_test.shape:  (11796, 1)\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:50,583 WARNING Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:50,653 WARNING Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject25.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject26.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject26.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11691, 6, 1)\n",
      "y_test.shape:  (11691, 1)\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:58,821 WARNING Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:23:58,905 WARNING Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject26.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject27.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject27.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11879, 6, 1)\n",
      "y_test.shape:  (11879, 1)\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:07,442 WARNING Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:07,502 WARNING Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject27.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject28.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject28.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11810, 6, 1)\n",
      "y_test.shape:  (11810, 1)\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:15,949 WARNING Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:16,011 WARNING Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject28.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject29.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject29.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6579, 6, 1)\n",
      "y_test.shape:  (6579, 1)\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:24,152 WARNING Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:24,210 WARNING Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject29.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject30.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject30.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11790, 6, 1)\n",
      "y_test.shape:  (11790, 1)\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:31,394 WARNING Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:31,451 WARNING Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject30.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject31.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject31.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11941, 6, 1)\n",
      "y_test.shape:  (11941, 1)\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:40,305 WARNING Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:40,359 WARNING Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject31.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject32.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject32.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11706, 6, 1)\n",
      "y_test.shape:  (11706, 1)\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:48,949 WARNING Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:49,048 WARNING Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject32.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject33.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject33.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11705, 6, 1)\n",
      "y_test.shape:  (11705, 1)\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:57,823 WARNING Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:24:57,884 WARNING Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject33.csv\n",
      "2025-01-19 17:25:04,902 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 40\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 23\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 154\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (364206, 6, 1)\n",
      "y_train.shape:  (364206, 1)\n",
      "x_valid.shape:  (91032, 6, 1)\n",
      "y_valid.shape:  (91032, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:26:07,642 WARNING Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:26:07,700 WARNING Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 17:26:07,840 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 364206 samples, validate on 91032 samples\n",
      "Epoch 1/10000\n",
      "361472/364206 [============================>.] - ETA: 0s - loss: 2.2633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364206/364206 [==============================] - 9s 25us/sample - loss: 2.2484 - val_loss: 0.1487\n",
      "Epoch 2/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.2080 - val_loss: 0.0828\n",
      "Epoch 3/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.1630 - val_loss: 0.0712\n",
      "Epoch 4/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.1419 - val_loss: 0.0685\n",
      "Epoch 5/10000\n",
      "364206/364206 [==============================] - 7s 20us/sample - loss: 0.1359 - val_loss: 0.0801\n",
      "Epoch 6/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.1311 - val_loss: 0.0675\n",
      "Epoch 7/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.1264 - val_loss: 0.0681\n",
      "Epoch 8/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.1221 - val_loss: 0.0644\n",
      "Epoch 9/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.1181 - val_loss: 0.0639\n",
      "Epoch 10/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.1141 - val_loss: 0.0673\n",
      "Epoch 11/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.1092 - val_loss: 0.0647\n",
      "Epoch 12/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.1046 - val_loss: 0.0632\n",
      "Epoch 13/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.1006 - val_loss: 0.0644\n",
      "Epoch 14/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0960 - val_loss: 0.0638\n",
      "Epoch 15/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0939 - val_loss: 0.0642\n",
      "Epoch 16/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0925 - val_loss: 0.0670\n",
      "Epoch 17/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0922 - val_loss: 0.0661\n",
      "Epoch 18/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0918 - val_loss: 0.0676\n",
      "Epoch 19/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0916 - val_loss: 0.0655\n",
      "Epoch 20/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0894 - val_loss: 0.0655\n",
      "Epoch 21/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0853 - val_loss: 0.0635\n",
      "Epoch 22/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0830 - val_loss: 0.0630\n",
      "Epoch 23/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0823 - val_loss: 0.0639\n",
      "Epoch 24/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0817 - val_loss: 0.0641\n",
      "Epoch 25/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0814 - val_loss: 0.0631\n",
      "Epoch 26/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0812 - val_loss: 0.0629\n",
      "Epoch 27/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0812 - val_loss: 0.0633\n",
      "Epoch 28/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0812 - val_loss: 0.0636\n",
      "Epoch 29/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0812 - val_loss: 0.0625\n",
      "Epoch 30/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0807 - val_loss: 0.0633\n",
      "Epoch 31/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0810 - val_loss: 0.0625\n",
      "Epoch 32/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0806 - val_loss: 0.0626\n",
      "Epoch 33/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0804 - val_loss: 0.0626\n",
      "Epoch 34/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0804 - val_loss: 0.0621\n",
      "Epoch 35/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0807 - val_loss: 0.0639\n",
      "Epoch 36/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0803 - val_loss: 0.0623\n",
      "Epoch 37/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0803 - val_loss: 0.0623\n",
      "Epoch 38/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0807 - val_loss: 0.0622\n",
      "Epoch 39/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0804 - val_loss: 0.0654\n",
      "Epoch 40/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0803 - val_loss: 0.0619\n",
      "Epoch 41/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0804 - val_loss: 0.0622\n",
      "Epoch 42/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0804 - val_loss: 0.0626\n",
      "Epoch 43/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0804 - val_loss: 0.0614\n",
      "Epoch 44/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0800 - val_loss: 0.0624\n",
      "Epoch 45/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0805 - val_loss: 0.0620\n",
      "Epoch 46/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0801 - val_loss: 0.0619\n",
      "Epoch 47/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0798 - val_loss: 0.0622\n",
      "Epoch 48/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0802 - val_loss: 0.0620\n",
      "Epoch 49/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0620\n",
      "Epoch 50/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0799 - val_loss: 0.0627\n",
      "Epoch 51/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0799 - val_loss: 0.0632\n",
      "Epoch 52/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0627\n",
      "Epoch 53/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0797 - val_loss: 0.0617\n",
      "Epoch 54/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0799 - val_loss: 0.0619\n",
      "Epoch 55/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0799 - val_loss: 0.0616\n",
      "Epoch 56/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0801 - val_loss: 0.0618\n",
      "Epoch 57/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0617\n",
      "Epoch 58/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0800 - val_loss: 0.0622\n",
      "Epoch 59/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0797 - val_loss: 0.0619\n",
      "Epoch 60/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0798 - val_loss: 0.0617\n",
      "Epoch 61/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0801 - val_loss: 0.0619\n",
      "Epoch 62/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0800 - val_loss: 0.0621\n",
      "Epoch 63/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0797 - val_loss: 0.0619\n",
      "Epoch 64/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0623\n",
      "Epoch 65/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0798 - val_loss: 0.0615\n",
      "Epoch 66/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0803 - val_loss: 0.0611\n",
      "Epoch 67/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0797 - val_loss: 0.0614\n",
      "Epoch 68/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0797 - val_loss: 0.0617\n",
      "Epoch 69/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0799 - val_loss: 0.0616\n",
      "Epoch 70/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0794 - val_loss: 0.0619\n",
      "Epoch 71/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0796 - val_loss: 0.0619\n",
      "Epoch 72/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0796 - val_loss: 0.0614\n",
      "Epoch 73/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0798 - val_loss: 0.0619\n",
      "Epoch 74/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0795 - val_loss: 0.0619\n",
      "Epoch 75/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0795 - val_loss: 0.0614\n",
      "Epoch 76/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0799 - val_loss: 0.0617\n",
      "Epoch 77/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0796 - val_loss: 0.0615\n",
      "Epoch 78/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0798 - val_loss: 0.0614\n",
      "Epoch 79/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0793 - val_loss: 0.0623\n",
      "Epoch 80/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0797 - val_loss: 0.0619\n",
      "Epoch 81/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0796 - val_loss: 0.0620\n",
      "Epoch 82/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0797 - val_loss: 0.0618\n",
      "Epoch 83/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0798 - val_loss: 0.0618\n",
      "Epoch 84/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0794 - val_loss: 0.0619\n",
      "Epoch 85/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0795 - val_loss: 0.0616\n",
      "Epoch 86/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0795 - val_loss: 0.0618\n",
      "Epoch 87/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0797 - val_loss: 0.0617\n",
      "Epoch 88/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0794 - val_loss: 0.0624\n",
      "Epoch 89/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0798 - val_loss: 0.0612\n",
      "Epoch 90/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0791 - val_loss: 0.0615\n",
      "Epoch 91/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0785 - val_loss: 0.0608\n",
      "Epoch 92/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0768 - val_loss: 0.0608\n",
      "Epoch 93/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0765 - val_loss: 0.0612\n",
      "Epoch 94/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0764 - val_loss: 0.0614\n",
      "Epoch 95/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0764 - val_loss: 0.0611\n",
      "Epoch 96/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0764 - val_loss: 0.0613\n",
      "Epoch 97/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0762 - val_loss: 0.0608\n",
      "Epoch 98/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0764 - val_loss: 0.0612\n",
      "Epoch 99/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0763 - val_loss: 0.0606\n",
      "Epoch 100/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0765 - val_loss: 0.0613\n",
      "Epoch 101/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0761 - val_loss: 0.0611\n",
      "Epoch 102/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0609\n",
      "Epoch 103/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0620\n",
      "Epoch 104/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0765 - val_loss: 0.0610\n",
      "Epoch 105/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0616\n",
      "Epoch 106/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0760 - val_loss: 0.0614\n",
      "Epoch 107/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0761 - val_loss: 0.0619\n",
      "Epoch 108/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0760 - val_loss: 0.0617\n",
      "Epoch 109/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0760 - val_loss: 0.0613\n",
      "Epoch 110/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0607\n",
      "Epoch 111/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0760 - val_loss: 0.0605\n",
      "Epoch 112/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0763 - val_loss: 0.0612\n",
      "Epoch 113/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 114/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0761 - val_loss: 0.0610\n",
      "Epoch 115/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0763 - val_loss: 0.0610\n",
      "Epoch 116/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0761 - val_loss: 0.0616\n",
      "Epoch 117/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0614\n",
      "Epoch 118/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0617\n",
      "Epoch 119/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0607\n",
      "Epoch 120/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0762 - val_loss: 0.0613\n",
      "Epoch 121/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0761 - val_loss: 0.0615\n",
      "Epoch 122/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0609\n",
      "Epoch 123/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0762 - val_loss: 0.0615\n",
      "Epoch 124/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0608\n",
      "Epoch 125/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0607\n",
      "Epoch 126/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0607\n",
      "Epoch 127/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0616\n",
      "Epoch 128/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0760 - val_loss: 0.0608\n",
      "Epoch 129/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0760 - val_loss: 0.0614\n",
      "Epoch 130/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0617\n",
      "Epoch 131/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0625\n",
      "Epoch 132/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0607\n",
      "Epoch 133/10000\n",
      "364206/364206 [==============================] - 7s 18us/sample - loss: 0.0759 - val_loss: 0.0610\n",
      "Epoch 134/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0762 - val_loss: 0.0608\n",
      "Epoch 135/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0759 - val_loss: 0.0609\n",
      "Epoch 136/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0611\n",
      "Epoch 137/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0620\n",
      "Epoch 138/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0759 - val_loss: 0.0611\n",
      "Epoch 139/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0611\n",
      "Epoch 140/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0760 - val_loss: 0.0609\n",
      "Epoch 141/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0760 - val_loss: 0.0607\n",
      "Epoch 142/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0607\n",
      "Epoch 143/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0613\n",
      "Epoch 144/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0762 - val_loss: 0.0620\n",
      "Epoch 145/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 146/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0618\n",
      "Epoch 147/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0759 - val_loss: 0.0608\n",
      "Epoch 148/10000\n",
      "364206/364206 [==============================] - 6s 17us/sample - loss: 0.0761 - val_loss: 0.0618\n",
      "Epoch 149/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 150/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0761 - val_loss: 0.0605\n",
      "Epoch 151/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0760 - val_loss: 0.0606\n",
      "Epoch 152/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0610\n",
      "Epoch 153/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 154/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0609\n",
      "Epoch 155/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0759 - val_loss: 0.0610\n",
      "Epoch 156/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0608\n",
      "Epoch 157/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0611\n",
      "Epoch 158/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0761 - val_loss: 0.0621\n",
      "Epoch 159/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0759 - val_loss: 0.0613\n",
      "Epoch 160/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0759 - val_loss: 0.0609\n",
      "Epoch 161/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0759 - val_loss: 0.0614\n",
      "Epoch 162/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0616\n",
      "Epoch 163/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0762 - val_loss: 0.0611\n",
      "Epoch 164/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0760 - val_loss: 0.0615\n",
      "Epoch 165/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 166/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0612\n",
      "Epoch 167/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0607\n",
      "Epoch 168/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0609\n",
      "Epoch 169/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 170/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0759 - val_loss: 0.0612\n",
      "Epoch 171/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0616\n",
      "Epoch 172/10000\n",
      "364206/364206 [==============================] - 4s 10us/sample - loss: 0.0757 - val_loss: 0.0616\n",
      "Epoch 173/10000\n",
      "364206/364206 [==============================] - 5s 12us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 174/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0605\n",
      "Epoch 175/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0624\n",
      "Epoch 176/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0612\n",
      "Epoch 177/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0610\n",
      "Epoch 178/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0612\n",
      "Epoch 179/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0612\n",
      "Epoch 180/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0617\n",
      "Epoch 181/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0607\n",
      "Epoch 182/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0608\n",
      "Epoch 183/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0608\n",
      "Epoch 184/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0608\n",
      "Epoch 185/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0604\n",
      "Epoch 186/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0617\n",
      "Epoch 187/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0616\n",
      "Epoch 188/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 189/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0604\n",
      "Epoch 190/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0611\n",
      "Epoch 191/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 192/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0759 - val_loss: 0.0604\n",
      "Epoch 193/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 194/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0610\n",
      "Epoch 195/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0612\n",
      "Epoch 196/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0606\n",
      "Epoch 197/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 198/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0614\n",
      "Epoch 199/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0609\n",
      "Epoch 200/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0605\n",
      "Epoch 201/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0611\n",
      "Epoch 202/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 203/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0613\n",
      "Epoch 204/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 205/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0604\n",
      "Epoch 206/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0605\n",
      "Epoch 207/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0613\n",
      "Epoch 208/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0606\n",
      "Epoch 209/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0609\n",
      "Epoch 210/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0611\n",
      "Epoch 211/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 212/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0605\n",
      "Epoch 213/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 214/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 215/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0615\n",
      "Epoch 216/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0606\n",
      "Epoch 217/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 218/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0606\n",
      "Epoch 219/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0606\n",
      "Epoch 220/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0609\n",
      "Epoch 221/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 222/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0611\n",
      "Epoch 223/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0612\n",
      "Epoch 224/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0608\n",
      "Epoch 225/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0614\n",
      "Epoch 226/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0758 - val_loss: 0.0606\n",
      "Epoch 227/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 228/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0605\n",
      "Epoch 229/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0609\n",
      "Epoch 230/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 231/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 232/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0613\n",
      "Epoch 233/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0610\n",
      "Epoch 234/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 235/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0607\n",
      "Epoch 236/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0614\n",
      "Epoch 237/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0606\n",
      "Epoch 238/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 239/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 240/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0606\n",
      "Epoch 241/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0607\n",
      "Epoch 242/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0610\n",
      "Epoch 243/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0617\n",
      "Epoch 244/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0614\n",
      "Epoch 245/10000\n",
      "364206/364206 [==============================] - 5s 12us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 246/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0618\n",
      "Epoch 247/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0605\n",
      "Epoch 248/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0611\n",
      "Epoch 249/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0604\n",
      "Epoch 250/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0608\n",
      "Epoch 251/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 252/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 253/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0616\n",
      "Epoch 254/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0754 - val_loss: 0.0605\n",
      "Epoch 255/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0609\n",
      "Epoch 256/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 257/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0605\n",
      "Epoch 258/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0757 - val_loss: 0.0611\n",
      "Epoch 259/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0624\n",
      "Epoch 260/10000\n",
      "364206/364206 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 261/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0757 - val_loss: 0.0608\n",
      "Epoch 262/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0607\n",
      "Epoch 263/10000\n",
      "364206/364206 [==============================] - 6s 18us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 264/10000\n",
      "364206/364206 [==============================] - 6s 17us/sample - loss: 0.0756 - val_loss: 0.0618\n",
      "Epoch 265/10000\n",
      "364206/364206 [==============================] - 7s 19us/sample - loss: 0.0757 - val_loss: 0.0606\n",
      "Epoch 266/10000\n",
      "364206/364206 [==============================] - 6s 17us/sample - loss: 0.0753 - val_loss: 0.0614\n",
      "Epoch 267/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0610\n",
      "Epoch 268/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 269/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0755 - val_loss: 0.0608\n",
      "Epoch 270/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 271/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 272/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0621\n",
      "Epoch 273/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0616\n",
      "Epoch 274/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 275/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0608\n",
      "Epoch 276/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0605\n",
      "Epoch 277/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0614\n",
      "Epoch 278/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0604\n",
      "Epoch 279/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0609\n",
      "Epoch 280/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 281/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0609\n",
      "Epoch 282/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0610\n",
      "Epoch 283/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0606\n",
      "Epoch 284/10000\n",
      "364206/364206 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 285/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 286/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0755 - val_loss: 0.0608\n",
      "Epoch 287/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0755 - val_loss: 0.0610\n",
      "Epoch 288/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0627\n",
      "Epoch 289/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 290/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0758 - val_loss: 0.0614\n",
      "Epoch 291/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 292/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 293/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0753 - val_loss: 0.0608\n",
      "Epoch 294/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0607\n",
      "Epoch 295/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0754 - val_loss: 0.0616\n",
      "Epoch 296/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0754 - val_loss: 0.0607\n",
      "Epoch 297/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 298/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 299/10000\n",
      "364206/364206 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 300/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0757 - val_loss: 0.0609\n",
      "Epoch 301/10000\n",
      "364206/364206 [==============================] - 5s 15us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 302/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0755 - val_loss: 0.0614\n",
      "Epoch 303/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0608\n",
      "Epoch 304/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 305/10000\n",
      "364206/364206 [==============================] - 5s 14us/sample - loss: 0.0756 - val_loss: 0.0606\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject34.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject34.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11523, 6, 1)\n",
      "y_test.shape:  (11523, 1)\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:26,911 WARNING Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:26,963 WARNING Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject34.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject35.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject35.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11734, 6, 1)\n",
      "y_test.shape:  (11734, 1)\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:35,935 WARNING Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:35,993 WARNING Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject35.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject36.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject36.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11313, 6, 1)\n",
      "y_test.shape:  (11313, 1)\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:45,059 WARNING Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:45,123 WARNING Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject36.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject37.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject37.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11596, 6, 1)\n",
      "y_test.shape:  (11596, 1)\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:54,224 WARNING Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:52:54,286 WARNING Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject37.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject38.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject38.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11621, 6, 1)\n",
      "y_test.shape:  (11621, 1)\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:03,644 WARNING Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:03,710 WARNING Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject38.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject39.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject39.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11293, 6, 1)\n",
      "y_test.shape:  (11293, 1)\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:13,091 WARNING Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:13,141 WARNING Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject39.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject40.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject40.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11563, 6, 1)\n",
      "y_test.shape:  (11563, 1)\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:22,563 WARNING Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:22,628 WARNING Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject40.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject41.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject41.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11671, 6, 1)\n",
      "y_test.shape:  (11671, 1)\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:32,206 WARNING Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:32,265 WARNING Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject41.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject42.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject42.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11591, 6, 1)\n",
      "y_test.shape:  (11591, 1)\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:42,341 WARNING Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:42,413 WARNING Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject42.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject43.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject43.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11514, 6, 1)\n",
      "y_test.shape:  (11514, 1)\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:52,233 WARNING Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:53:52,279 WARNING Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject43.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject44.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject44.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10180, 6, 1)\n",
      "y_test.shape:  (10180, 1)\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:54:01,777 WARNING Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:54:01,863 WARNING Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject44.csv\n",
      "2025-01-19 17:54:09,635 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 40\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 23\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 53\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 279\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (393290, 6, 1)\n",
      "y_train.shape:  (393290, 1)\n",
      "x_valid.shape:  (98301, 6, 1)\n",
      "y_valid.shape:  (98301, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:55:19,216 WARNING Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 17:55:19,282 WARNING Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 17:55:19,396 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 393290 samples, validate on 98301 samples\n",
      "Epoch 1/10000\n",
      "392192/393290 [============================>.] - ETA: 0s - loss: 0.6706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393290/393290 [==============================] - 11s 27us/sample - loss: 0.6693 - val_loss: 0.1287\n",
      "Epoch 2/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.1946 - val_loss: 0.0795\n",
      "Epoch 3/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.1726 - val_loss: 0.0788\n",
      "Epoch 4/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.1544 - val_loss: 0.0839\n",
      "Epoch 5/10000\n",
      "393290/393290 [==============================] - 8s 21us/sample - loss: 0.1400 - val_loss: 0.0695\n",
      "Epoch 6/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.1286 - val_loss: 0.0691\n",
      "Epoch 7/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.1192 - val_loss: 0.0668\n",
      "Epoch 8/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.1106 - val_loss: 0.0705\n",
      "Epoch 9/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.1035 - val_loss: 0.0677\n",
      "Epoch 10/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0976 - val_loss: 0.0669\n",
      "Epoch 11/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0930 - val_loss: 0.0679\n",
      "Epoch 12/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0887 - val_loss: 0.0724\n",
      "Epoch 13/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0863 - val_loss: 0.0670\n",
      "Epoch 14/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0842 - val_loss: 0.0797\n",
      "Epoch 15/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0828 - val_loss: 0.0661\n",
      "Epoch 16/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0822 - val_loss: 0.0663\n",
      "Epoch 17/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0820 - val_loss: 0.0665\n",
      "Epoch 18/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0816 - val_loss: 0.0685\n",
      "Epoch 19/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0800 - val_loss: 0.0656\n",
      "Epoch 20/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0789 - val_loss: 0.0664\n",
      "Epoch 21/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0784 - val_loss: 0.0656\n",
      "Epoch 22/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0780 - val_loss: 0.0653\n",
      "Epoch 23/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0778 - val_loss: 0.0665\n",
      "Epoch 24/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0785 - val_loss: 0.0679\n",
      "Epoch 25/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0776 - val_loss: 0.0655\n",
      "Epoch 26/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0781 - val_loss: 0.0656\n",
      "Epoch 27/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0778 - val_loss: 0.0664\n",
      "Epoch 28/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0778 - val_loss: 0.0653\n",
      "Epoch 29/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0778 - val_loss: 0.0651\n",
      "Epoch 30/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0777 - val_loss: 0.0653\n",
      "Epoch 31/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0778 - val_loss: 0.0651\n",
      "Epoch 32/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0776 - val_loss: 0.0659\n",
      "Epoch 33/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0776 - val_loss: 0.0672\n",
      "Epoch 34/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0775 - val_loss: 0.0654\n",
      "Epoch 35/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0777 - val_loss: 0.0646\n",
      "Epoch 36/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0773 - val_loss: 0.0662\n",
      "Epoch 37/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0774 - val_loss: 0.0647\n",
      "Epoch 38/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0772 - val_loss: 0.0651\n",
      "Epoch 39/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0772 - val_loss: 0.0659\n",
      "Epoch 40/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0772 - val_loss: 0.0654\n",
      "Epoch 41/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0773 - val_loss: 0.0669\n",
      "Epoch 42/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0771 - val_loss: 0.0647\n",
      "Epoch 43/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0771 - val_loss: 0.0650\n",
      "Epoch 44/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0767 - val_loss: 0.0666\n",
      "Epoch 45/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0772 - val_loss: 0.0647\n",
      "Epoch 46/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0771 - val_loss: 0.0663\n",
      "Epoch 47/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0767 - val_loss: 0.0690\n",
      "Epoch 48/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0767 - val_loss: 0.0641\n",
      "Epoch 49/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0770 - val_loss: 0.0645\n",
      "Epoch 50/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0767 - val_loss: 0.0646\n",
      "Epoch 51/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0767 - val_loss: 0.0651\n",
      "Epoch 52/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0768 - val_loss: 0.0643\n",
      "Epoch 53/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0645\n",
      "Epoch 54/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0768 - val_loss: 0.0646\n",
      "Epoch 55/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0768 - val_loss: 0.0642\n",
      "Epoch 56/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0765 - val_loss: 0.0642\n",
      "Epoch 57/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0642\n",
      "Epoch 58/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0638\n",
      "Epoch 59/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0642\n",
      "Epoch 60/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0640\n",
      "Epoch 61/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0664\n",
      "Epoch 62/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0658\n",
      "Epoch 63/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0765 - val_loss: 0.0648\n",
      "Epoch 64/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0656\n",
      "Epoch 65/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0659\n",
      "Epoch 66/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0766 - val_loss: 0.0640\n",
      "Epoch 67/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0764 - val_loss: 0.0641\n",
      "Epoch 68/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0638\n",
      "Epoch 69/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0650\n",
      "Epoch 70/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0763 - val_loss: 0.0648\n",
      "Epoch 71/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0765 - val_loss: 0.0650\n",
      "Epoch 72/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0640\n",
      "Epoch 73/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0765 - val_loss: 0.0653\n",
      "Epoch 74/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0763 - val_loss: 0.0645\n",
      "Epoch 75/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0647\n",
      "Epoch 76/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0766 - val_loss: 0.0643\n",
      "Epoch 77/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0648\n",
      "Epoch 78/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0670\n",
      "Epoch 79/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0760 - val_loss: 0.0653\n",
      "Epoch 80/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0643\n",
      "Epoch 81/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0762 - val_loss: 0.0637\n",
      "Epoch 82/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0762 - val_loss: 0.0649\n",
      "Epoch 83/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0636\n",
      "Epoch 84/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0764 - val_loss: 0.0647\n",
      "Epoch 85/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0765 - val_loss: 0.0655\n",
      "Epoch 86/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0639\n",
      "Epoch 87/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0641\n",
      "Epoch 88/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0762 - val_loss: 0.0651\n",
      "Epoch 89/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0762 - val_loss: 0.0681\n",
      "Epoch 90/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0763 - val_loss: 0.0640\n",
      "Epoch 91/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0763 - val_loss: 0.0640\n",
      "Epoch 92/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0639\n",
      "Epoch 93/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0761 - val_loss: 0.0655\n",
      "Epoch 94/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0760 - val_loss: 0.0645\n",
      "Epoch 95/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0641\n",
      "Epoch 96/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0644\n",
      "Epoch 97/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0762 - val_loss: 0.0649\n",
      "Epoch 98/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0760 - val_loss: 0.0645\n",
      "Epoch 99/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0643\n",
      "Epoch 100/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0644\n",
      "Epoch 101/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0762 - val_loss: 0.0637\n",
      "Epoch 102/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0636\n",
      "Epoch 103/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0760 - val_loss: 0.0637\n",
      "Epoch 104/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0760 - val_loss: 0.0648\n",
      "Epoch 105/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0632\n",
      "Epoch 106/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0641\n",
      "Epoch 107/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0760 - val_loss: 0.0640\n",
      "Epoch 108/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0654\n",
      "Epoch 109/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0763 - val_loss: 0.0642\n",
      "Epoch 110/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0640\n",
      "Epoch 111/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0638\n",
      "Epoch 112/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0760 - val_loss: 0.0634\n",
      "Epoch 113/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0761 - val_loss: 0.0643\n",
      "Epoch 114/10000\n",
      "393290/393290 [==============================] - 6s 17us/sample - loss: 0.0759 - val_loss: 0.0642\n",
      "Epoch 115/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0632\n",
      "Epoch 116/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0758 - val_loss: 0.0636\n",
      "Epoch 117/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0760 - val_loss: 0.0639\n",
      "Epoch 118/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0761 - val_loss: 0.0644\n",
      "Epoch 119/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0759 - val_loss: 0.0651\n",
      "Epoch 120/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0758 - val_loss: 0.0648\n",
      "Epoch 121/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0759 - val_loss: 0.0648\n",
      "Epoch 122/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0757 - val_loss: 0.0660\n",
      "Epoch 123/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0760 - val_loss: 0.0634\n",
      "Epoch 124/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0758 - val_loss: 0.0643\n",
      "Epoch 125/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0758 - val_loss: 0.0641\n",
      "Epoch 126/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0756 - val_loss: 0.0633\n",
      "Epoch 127/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0756 - val_loss: 0.0642\n",
      "Epoch 128/10000\n",
      "393290/393290 [==============================] - 6s 14us/sample - loss: 0.0757 - val_loss: 0.0642\n",
      "Epoch 129/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0639\n",
      "Epoch 130/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0648\n",
      "Epoch 131/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0645\n",
      "Epoch 132/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0640\n",
      "Epoch 133/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0636\n",
      "Epoch 134/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0633\n",
      "Epoch 135/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0645\n",
      "Epoch 136/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0637\n",
      "Epoch 137/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0641\n",
      "Epoch 138/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0759 - val_loss: 0.0635\n",
      "Epoch 139/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0640\n",
      "Epoch 140/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0652\n",
      "Epoch 141/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0640\n",
      "Epoch 142/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0760 - val_loss: 0.0636\n",
      "Epoch 143/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0634\n",
      "Epoch 144/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0657\n",
      "Epoch 145/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0651\n",
      "Epoch 146/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0637\n",
      "Epoch 147/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0759 - val_loss: 0.0634\n",
      "Epoch 148/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0636\n",
      "Epoch 149/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0635\n",
      "Epoch 150/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0760 - val_loss: 0.0633\n",
      "Epoch 151/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0657\n",
      "Epoch 152/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0637\n",
      "Epoch 153/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0632\n",
      "Epoch 154/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0634\n",
      "Epoch 155/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0642\n",
      "Epoch 156/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0633\n",
      "Epoch 157/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0759 - val_loss: 0.0633\n",
      "Epoch 158/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0639\n",
      "Epoch 159/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0636\n",
      "Epoch 160/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0632\n",
      "Epoch 161/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 162/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0759 - val_loss: 0.0636\n",
      "Epoch 163/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0641\n",
      "Epoch 164/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0758 - val_loss: 0.0636\n",
      "Epoch 165/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0674\n",
      "Epoch 166/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0635\n",
      "Epoch 167/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 168/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0757 - val_loss: 0.0632\n",
      "Epoch 169/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0633\n",
      "Epoch 170/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0653\n",
      "Epoch 171/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0635\n",
      "Epoch 172/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0640\n",
      "Epoch 173/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0756 - val_loss: 0.0650\n",
      "Epoch 174/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0638\n",
      "Epoch 175/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0639\n",
      "Epoch 176/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0637\n",
      "Epoch 177/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0639\n",
      "Epoch 178/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0636\n",
      "Epoch 179/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0635\n",
      "Epoch 180/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0633\n",
      "Epoch 181/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0636\n",
      "Epoch 182/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0631\n",
      "Epoch 183/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 184/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0635\n",
      "Epoch 185/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 186/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0633\n",
      "Epoch 187/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0639\n",
      "Epoch 188/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0638\n",
      "Epoch 189/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 190/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 191/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0635\n",
      "Epoch 192/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0636\n",
      "Epoch 193/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0641\n",
      "Epoch 194/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0640\n",
      "Epoch 195/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0635\n",
      "Epoch 196/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0633\n",
      "Epoch 197/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0631\n",
      "Epoch 198/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0640\n",
      "Epoch 199/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0637\n",
      "Epoch 200/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 201/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 202/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0644\n",
      "Epoch 203/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0632\n",
      "Epoch 204/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0641\n",
      "Epoch 205/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0758 - val_loss: 0.0662\n",
      "Epoch 206/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0631\n",
      "Epoch 207/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0637\n",
      "Epoch 208/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0643\n",
      "Epoch 209/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0638\n",
      "Epoch 210/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0641\n",
      "Epoch 211/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0642\n",
      "Epoch 212/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 213/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 214/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0633\n",
      "Epoch 215/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0639\n",
      "Epoch 216/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0640\n",
      "Epoch 217/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0756 - val_loss: 0.0634\n",
      "Epoch 218/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 219/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0639\n",
      "Epoch 220/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0631\n",
      "Epoch 221/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0642\n",
      "Epoch 222/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0633\n",
      "Epoch 223/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0635\n",
      "Epoch 224/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 225/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0638\n",
      "Epoch 226/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0656\n",
      "Epoch 227/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 228/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0757 - val_loss: 0.0638\n",
      "Epoch 229/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0756 - val_loss: 0.0631\n",
      "Epoch 230/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0641\n",
      "Epoch 231/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0636\n",
      "Epoch 232/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0637\n",
      "Epoch 233/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0630\n",
      "Epoch 234/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 235/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0638\n",
      "Epoch 236/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0633\n",
      "Epoch 237/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0642\n",
      "Epoch 238/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 239/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0651\n",
      "Epoch 240/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 241/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0639\n",
      "Epoch 242/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0635\n",
      "Epoch 243/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0643\n",
      "Epoch 244/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0754 - val_loss: 0.0641\n",
      "Epoch 245/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0644\n",
      "Epoch 246/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0631\n",
      "Epoch 247/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0642\n",
      "Epoch 248/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0639\n",
      "Epoch 249/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0636\n",
      "Epoch 250/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0633\n",
      "Epoch 251/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0635\n",
      "Epoch 252/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0632\n",
      "Epoch 253/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0640\n",
      "Epoch 254/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 255/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 256/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0636\n",
      "Epoch 257/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0648\n",
      "Epoch 258/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0631\n",
      "Epoch 259/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 260/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0754 - val_loss: 0.0640\n",
      "Epoch 261/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0639\n",
      "Epoch 262/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0635\n",
      "Epoch 263/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 264/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0629\n",
      "Epoch 265/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 266/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0633\n",
      "Epoch 267/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0636\n",
      "Epoch 268/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0644\n",
      "Epoch 269/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 270/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0632\n",
      "Epoch 271/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0637\n",
      "Epoch 272/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0634\n",
      "Epoch 273/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0638\n",
      "Epoch 274/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 275/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0755 - val_loss: 0.0631\n",
      "Epoch 276/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0630\n",
      "Epoch 277/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0645\n",
      "Epoch 278/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0638\n",
      "Epoch 279/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0630\n",
      "Epoch 280/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 281/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0641\n",
      "Epoch 282/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 283/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0638\n",
      "Epoch 284/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 285/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0632\n",
      "Epoch 286/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0755 - val_loss: 0.0634\n",
      "Epoch 287/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0643\n",
      "Epoch 288/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0753 - val_loss: 0.0630\n",
      "Epoch 289/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0634\n",
      "Epoch 290/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 291/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 292/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0752 - val_loss: 0.0638\n",
      "Epoch 293/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0753 - val_loss: 0.0639\n",
      "Epoch 294/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0641\n",
      "Epoch 295/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0645\n",
      "Epoch 296/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0634\n",
      "Epoch 297/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0753 - val_loss: 0.0656\n",
      "Epoch 298/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0637\n",
      "Epoch 299/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 300/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0636\n",
      "Epoch 301/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0630\n",
      "Epoch 302/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 303/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0638\n",
      "Epoch 304/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 305/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 306/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0644\n",
      "Epoch 307/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0641\n",
      "Epoch 308/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 309/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0634\n",
      "Epoch 310/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 311/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 312/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0632\n",
      "Epoch 313/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 314/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0754 - val_loss: 0.0640\n",
      "Epoch 315/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0754 - val_loss: 0.0650\n",
      "Epoch 316/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0754 - val_loss: 0.0635\n",
      "Epoch 317/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0635\n",
      "Epoch 318/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0641\n",
      "Epoch 319/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0634\n",
      "Epoch 320/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0639\n",
      "Epoch 321/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 322/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 323/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0647\n",
      "Epoch 324/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0632\n",
      "Epoch 325/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 326/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0642\n",
      "Epoch 327/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0636\n",
      "Epoch 328/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0753 - val_loss: 0.0636\n",
      "Epoch 329/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0640\n",
      "Epoch 330/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0748 - val_loss: 0.0637\n",
      "Epoch 331/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0632\n",
      "Epoch 332/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0637\n",
      "Epoch 333/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 334/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0633\n",
      "Epoch 335/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0631\n",
      "Epoch 336/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 337/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0635\n",
      "Epoch 338/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0644\n",
      "Epoch 339/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0647\n",
      "Epoch 340/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 341/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0654\n",
      "Epoch 342/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 343/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0631\n",
      "Epoch 344/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0641\n",
      "Epoch 345/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0632\n",
      "Epoch 346/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0637\n",
      "Epoch 347/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 348/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0630\n",
      "Epoch 349/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0633\n",
      "Epoch 350/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0631\n",
      "Epoch 351/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0642\n",
      "Epoch 352/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0632\n",
      "Epoch 353/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0752 - val_loss: 0.0634\n",
      "Epoch 354/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0637\n",
      "Epoch 355/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 356/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0638\n",
      "Epoch 357/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0748 - val_loss: 0.0635\n",
      "Epoch 358/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 359/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0753 - val_loss: 0.0637\n",
      "Epoch 360/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 361/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 362/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0646\n",
      "Epoch 363/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0652\n",
      "Epoch 364/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0635\n",
      "Epoch 365/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0629\n",
      "Epoch 366/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0639\n",
      "Epoch 367/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 368/10000\n",
      "393290/393290 [==============================] - 5s 14us/sample - loss: 0.0749 - val_loss: 0.0635\n",
      "Epoch 369/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0748 - val_loss: 0.0631\n",
      "Epoch 370/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0636\n",
      "Epoch 371/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0639\n",
      "Epoch 372/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0751 - val_loss: 0.0640\n",
      "Epoch 373/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0636\n",
      "Epoch 374/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 375/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0752 - val_loss: 0.0630\n",
      "Epoch 376/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 377/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0639\n",
      "Epoch 378/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 379/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 380/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 381/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0646\n",
      "Epoch 382/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0748 - val_loss: 0.0635\n",
      "Epoch 383/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0753 - val_loss: 0.0638\n",
      "Epoch 384/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0749 - val_loss: 0.0640\n",
      "Epoch 385/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0638\n",
      "Epoch 386/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0642\n",
      "Epoch 387/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0641\n",
      "Epoch 388/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 389/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 390/10000\n",
      "393290/393290 [==============================] - 5s 12us/sample - loss: 0.0749 - val_loss: 0.0644\n",
      "Epoch 391/10000\n",
      "393290/393290 [==============================] - 5s 13us/sample - loss: 0.0753 - val_loss: 0.0635\n",
      "Epoch 392/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0635\n",
      "Epoch 393/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0647\n",
      "Epoch 394/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 395/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0752 - val_loss: 0.0638\n",
      "Epoch 396/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0640\n",
      "Epoch 397/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0633\n",
      "Epoch 398/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0638\n",
      "Epoch 399/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 400/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0631\n",
      "Epoch 401/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0633\n",
      "Epoch 402/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0631\n",
      "Epoch 403/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 404/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 405/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0634\n",
      "Epoch 406/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0751 - val_loss: 0.0637\n",
      "Epoch 407/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0753 - val_loss: 0.0640\n",
      "Epoch 408/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0635\n",
      "Epoch 409/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0637\n",
      "Epoch 410/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0635\n",
      "Epoch 411/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 412/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0637\n",
      "Epoch 413/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 414/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0643\n",
      "Epoch 415/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0749 - val_loss: 0.0629\n",
      "Epoch 416/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 417/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0754 - val_loss: 0.0634\n",
      "Epoch 418/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0634\n",
      "Epoch 419/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0644\n",
      "Epoch 420/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0748 - val_loss: 0.0639\n",
      "Epoch 421/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0748 - val_loss: 0.0638\n",
      "Epoch 422/10000\n",
      "393290/393290 [==============================] - 6s 17us/sample - loss: 0.0749 - val_loss: 0.0641\n",
      "Epoch 423/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 424/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0750 - val_loss: 0.0633\n",
      "Epoch 425/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0632\n",
      "Epoch 426/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0749 - val_loss: 0.0639\n",
      "Epoch 427/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0643\n",
      "Epoch 428/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0631\n",
      "Epoch 429/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0751 - val_loss: 0.0633\n",
      "Epoch 430/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0750 - val_loss: 0.0647\n",
      "Epoch 431/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0751 - val_loss: 0.0632\n",
      "Epoch 432/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0747 - val_loss: 0.0631\n",
      "Epoch 433/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0749 - val_loss: 0.0640\n",
      "Epoch 434/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0633\n",
      "Epoch 435/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0749 - val_loss: 0.0634\n",
      "Epoch 436/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0749 - val_loss: 0.0630\n",
      "Epoch 437/10000\n",
      "393290/393290 [==============================] - 7s 17us/sample - loss: 0.0747 - val_loss: 0.0634\n",
      "Epoch 438/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0631\n",
      "Epoch 439/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0750 - val_loss: 0.0634\n",
      "Epoch 440/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0630\n",
      "Epoch 441/10000\n",
      "393290/393290 [==============================] - 6s 16us/sample - loss: 0.0748 - val_loss: 0.0637\n",
      "Epoch 442/10000\n",
      "393290/393290 [==============================] - 6s 15us/sample - loss: 0.0749 - val_loss: 0.0640\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject45.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject45.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11315, 6, 1)\n",
      "y_test.shape:  (11315, 1)\n",
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:37:46,193 WARNING Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:37:46,666 WARNING Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject45.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject46.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject46.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11499, 6, 1)\n",
      "y_test.shape:  (11499, 1)\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:37:58,881 WARNING Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:37:59,070 WARNING Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject46.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject47.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject47.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11715, 6, 1)\n",
      "y_test.shape:  (11715, 1)\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:09,146 WARNING Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:09,273 WARNING Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject47.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject48.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject48.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11756, 6, 1)\n",
      "y_test.shape:  (11756, 1)\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:19,081 WARNING Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:19,214 WARNING Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject48.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject49.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject49.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10747, 6, 1)\n",
      "y_test.shape:  (10747, 1)\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:29,997 WARNING Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:30,082 WARNING Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject49.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject50.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject50.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 154\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6287, 6, 1)\n",
      "y_test.shape:  (6287, 1)\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:39,470 WARNING Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:39,615 WARNING Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject50.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject51.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject51.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8945, 6, 1)\n",
      "y_test.shape:  (8945, 1)\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:47,815 WARNING Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:47,927 WARNING Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject51.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject53.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject53.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8889, 6, 1)\n",
      "y_test.shape:  (8889, 1)\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:56,727 WARNING Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:38:56,803 WARNING Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject53.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject54.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject54.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8093, 6, 1)\n",
      "y_test.shape:  (8093, 1)\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:39:05,731 WARNING Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 6, 32)\n",
      "x.shape =  (?, 6, 32)\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:39:07,353 WARNING Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject54.csv\n",
      "2025-01-19 18:39:15,920 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (366985, 12, 1)\n",
      "y_train.shape:  (366985, 1)\n",
      "x_valid.shape:  (91727, 12, 1)\n",
      "y_valid.shape:  (91727, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:40:14,593 WARNING Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:40:14,692 WARNING Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 18:40:14,827 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 366985 samples, validate on 91727 samples\n",
      "Epoch 1/10000\n",
      "366592/366985 [============================>.] - ETA: 0s - loss: 0.8764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366985/366985 [==============================] - 13s 35us/sample - loss: 0.8760 - val_loss: 0.1787\n",
      "Epoch 2/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.3593 - val_loss: 0.1049\n",
      "Epoch 3/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.2648 - val_loss: 0.0834\n",
      "Epoch 4/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.2132 - val_loss: 0.0884\n",
      "Epoch 5/10000\n",
      "366985/366985 [==============================] - 10s 28us/sample - loss: 0.1826 - val_loss: 0.0840\n",
      "Epoch 6/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1615 - val_loss: 0.0669\n",
      "Epoch 7/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1433 - val_loss: 0.0695\n",
      "Epoch 8/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1301 - val_loss: 0.0661\n",
      "Epoch 9/10000\n",
      "366985/366985 [==============================] - 8s 20us/sample - loss: 0.1211 - val_loss: 0.0717\n",
      "Epoch 10/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1182 - val_loss: 0.0702\n",
      "Epoch 11/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1164 - val_loss: 0.0712\n",
      "Epoch 12/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1158 - val_loss: 0.0724\n",
      "Epoch 13/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1163 - val_loss: 0.0719\n",
      "Epoch 14/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1165 - val_loss: 0.0764\n",
      "Epoch 15/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1165 - val_loss: 0.0739\n",
      "Epoch 16/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1158 - val_loss: 0.0733\n",
      "Epoch 17/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1160 - val_loss: 0.0742\n",
      "Epoch 18/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1159 - val_loss: 0.0721\n",
      "Epoch 19/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1153 - val_loss: 0.0735\n",
      "Epoch 20/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1157 - val_loss: 0.0757\n",
      "Epoch 21/10000\n",
      "366985/366985 [==============================] - 8s 20us/sample - loss: 0.1157 - val_loss: 0.0770\n",
      "Epoch 22/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1159 - val_loss: 0.0714\n",
      "Epoch 23/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1150 - val_loss: 0.0728\n",
      "Epoch 24/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1154 - val_loss: 0.0735\n",
      "Epoch 25/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1161 - val_loss: 0.0736\n",
      "Epoch 26/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1154 - val_loss: 0.0723\n",
      "Epoch 27/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1155 - val_loss: 0.0788\n",
      "Epoch 28/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1157 - val_loss: 0.0737\n",
      "Epoch 29/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1156 - val_loss: 0.0753\n",
      "Epoch 30/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1148 - val_loss: 0.0706\n",
      "Epoch 31/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1152 - val_loss: 0.0716\n",
      "Epoch 32/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1153 - val_loss: 0.0706\n",
      "Epoch 33/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1151 - val_loss: 0.0746\n",
      "Epoch 34/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1157 - val_loss: 0.0718\n",
      "Epoch 35/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1138 - val_loss: 0.0713\n",
      "Epoch 36/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1151 - val_loss: 0.0730\n",
      "Epoch 37/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1149 - val_loss: 0.0707\n",
      "Epoch 38/10000\n",
      "366985/366985 [==============================] - 9s 23us/sample - loss: 0.1151 - val_loss: 0.0726\n",
      "Epoch 39/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1158 - val_loss: 0.0754\n",
      "Epoch 40/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1147 - val_loss: 0.0725\n",
      "Epoch 41/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1148 - val_loss: 0.0717\n",
      "Epoch 42/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1146 - val_loss: 0.0718\n",
      "Epoch 43/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1140 - val_loss: 0.0746\n",
      "Epoch 44/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1145 - val_loss: 0.0723\n",
      "Epoch 45/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1150 - val_loss: 0.0730\n",
      "Epoch 46/10000\n",
      "366985/366985 [==============================] - 10s 27us/sample - loss: 0.1151 - val_loss: 0.0728\n",
      "Epoch 47/10000\n",
      "366985/366985 [==============================] - 9s 25us/sample - loss: 0.1145 - val_loss: 0.0749\n",
      "Epoch 48/10000\n",
      "366985/366985 [==============================] - 10s 26us/sample - loss: 0.1150 - val_loss: 0.0746\n",
      "Epoch 49/10000\n",
      "366985/366985 [==============================] - 9s 25us/sample - loss: 0.1142 - val_loss: 0.0706\n",
      "Epoch 50/10000\n",
      "366985/366985 [==============================] - 10s 26us/sample - loss: 0.1150 - val_loss: 0.0744\n",
      "Epoch 51/10000\n",
      "366985/366985 [==============================] - 11s 29us/sample - loss: 0.1143 - val_loss: 0.0710\n",
      "Epoch 52/10000\n",
      "366985/366985 [==============================] - 10s 26us/sample - loss: 0.1144 - val_loss: 0.0740\n",
      "Epoch 53/10000\n",
      "366985/366985 [==============================] - 12s 32us/sample - loss: 0.1148 - val_loss: 0.0717\n",
      "Epoch 54/10000\n",
      "366985/366985 [==============================] - 12s 34us/sample - loss: 0.1144 - val_loss: 0.0742\n",
      "Epoch 55/10000\n",
      "366985/366985 [==============================] - 12s 34us/sample - loss: 0.1146 - val_loss: 0.0744\n",
      "Epoch 56/10000\n",
      "366985/366985 [==============================] - 12s 31us/sample - loss: 0.1149 - val_loss: 0.0714\n",
      "Epoch 57/10000\n",
      "366985/366985 [==============================] - 12s 32us/sample - loss: 0.1141 - val_loss: 0.0736\n",
      "Epoch 58/10000\n",
      "366985/366985 [==============================] - 11s 29us/sample - loss: 0.1150 - val_loss: 0.0746\n",
      "Epoch 59/10000\n",
      "366985/366985 [==============================] - 10s 28us/sample - loss: 0.1150 - val_loss: 0.0738\n",
      "Epoch 60/10000\n",
      "366985/366985 [==============================] - 9s 24us/sample - loss: 0.1146 - val_loss: 0.0732\n",
      "Epoch 61/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1137 - val_loss: 0.0709\n",
      "Epoch 62/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1156 - val_loss: 0.0756\n",
      "Epoch 63/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1142 - val_loss: 0.0719\n",
      "Epoch 64/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1136 - val_loss: 0.0726\n",
      "Epoch 65/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1158 - val_loss: 0.0716\n",
      "Epoch 66/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1146 - val_loss: 0.0720\n",
      "Epoch 67/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1143 - val_loss: 0.0722\n",
      "Epoch 68/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1144 - val_loss: 0.0720\n",
      "Epoch 69/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1147 - val_loss: 0.0727\n",
      "Epoch 70/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1141 - val_loss: 0.0708\n",
      "Epoch 71/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1140 - val_loss: 0.0713\n",
      "Epoch 72/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1138 - val_loss: 0.0751\n",
      "Epoch 73/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1152 - val_loss: 0.0712\n",
      "Epoch 74/10000\n",
      "366985/366985 [==============================] - 8s 20us/sample - loss: 0.1143 - val_loss: 0.0732\n",
      "Epoch 75/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1136 - val_loss: 0.0712\n",
      "Epoch 76/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1143 - val_loss: 0.0707\n",
      "Epoch 77/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1146 - val_loss: 0.0724\n",
      "Epoch 78/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1135 - val_loss: 0.0715\n",
      "Epoch 79/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1146 - val_loss: 0.0738\n",
      "Epoch 80/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1138 - val_loss: 0.0724\n",
      "Epoch 81/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1150 - val_loss: 0.0750\n",
      "Epoch 82/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1141 - val_loss: 0.0707\n",
      "Epoch 83/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1146 - val_loss: 0.0710\n",
      "Epoch 84/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1141 - val_loss: 0.0723\n",
      "Epoch 85/10000\n",
      "366985/366985 [==============================] - 9s 24us/sample - loss: 0.1143 - val_loss: 0.0710\n",
      "Epoch 86/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1135 - val_loss: 0.0736\n",
      "Epoch 87/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1145 - val_loss: 0.0733\n",
      "Epoch 88/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1136 - val_loss: 0.0742\n",
      "Epoch 89/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1139 - val_loss: 0.0713\n",
      "Epoch 90/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1140 - val_loss: 0.0725\n",
      "Epoch 91/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1144 - val_loss: 0.0739\n",
      "Epoch 92/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1138 - val_loss: 0.0705\n",
      "Epoch 93/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1140 - val_loss: 0.0709\n",
      "Epoch 94/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1138 - val_loss: 0.0735\n",
      "Epoch 95/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1140 - val_loss: 0.0710\n",
      "Epoch 96/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1144 - val_loss: 0.0735\n",
      "Epoch 97/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1138 - val_loss: 0.0738\n",
      "Epoch 98/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1140 - val_loss: 0.0722\n",
      "Epoch 99/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1142 - val_loss: 0.0719\n",
      "Epoch 100/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1142 - val_loss: 0.0719\n",
      "Epoch 101/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1138 - val_loss: 0.0722\n",
      "Epoch 102/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1140 - val_loss: 0.0712\n",
      "Epoch 103/10000\n",
      "366985/366985 [==============================] - 8s 23us/sample - loss: 0.1143 - val_loss: 0.0713\n",
      "Epoch 104/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1139 - val_loss: 0.0722\n",
      "Epoch 105/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1140 - val_loss: 0.0747\n",
      "Epoch 106/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1136 - val_loss: 0.0713\n",
      "Epoch 107/10000\n",
      "366985/366985 [==============================] - 8s 21us/sample - loss: 0.1144 - val_loss: 0.0738\n",
      "Epoch 108/10000\n",
      "366985/366985 [==============================] - 8s 22us/sample - loss: 0.1138 - val_loss: 0.0701\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject10.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject10.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11769, 12, 1)\n",
      "y_test.shape:  (11769, 1)\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:20,826 WARNING Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:21,271 WARNING Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject10.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject11.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11515, 12, 1)\n",
      "y_test.shape:  (11515, 1)\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:35,350 WARNING Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:35,553 WARNING Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject1.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 131\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7779, 12, 1)\n",
      "y_test.shape:  (7779, 1)\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:45,892 WARNING Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:46,028 WARNING Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject2.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject2.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 159\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6325, 12, 1)\n",
      "y_test.shape:  (6325, 1)\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:55,188 WARNING Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:55:55,296 WARNING Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject2.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject3.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject3.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 151\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7071, 12, 1)\n",
      "y_test.shape:  (7071, 1)\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:03,472 WARNING Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:03,562 WARNING Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject3.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject4.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject4.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11347, 12, 1)\n",
      "y_test.shape:  (11347, 1)\n",
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:12,747 WARNING Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:12,844 WARNING Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject4.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject5.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject5.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10489, 12, 1)\n",
      "y_test.shape:  (10489, 1)\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:22,871 WARNING Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:22,944 WARNING Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject5.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject6.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject6.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11416, 12, 1)\n",
      "y_test.shape:  (11416, 1)\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:33,409 WARNING Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:33,508 WARNING Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject6.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject7.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject7.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11317, 12, 1)\n",
      "y_test.shape:  (11317, 1)\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:43,795 WARNING Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:43,919 WARNING Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject7.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject8.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject8.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11216, 12, 1)\n",
      "y_test.shape:  (11216, 1)\n",
      "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:54,644 WARNING Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:56:54,725 WARNING Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject8.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject9.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject9.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11685, 12, 1)\n",
      "y_test.shape:  (11685, 1)\n",
      "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:57:05,887 WARNING Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:57:05,960 WARNING Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject9.csv\n",
      "2025-01-19 18:57:15,856 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 131\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 159\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (355150, 12, 1)\n",
      "y_train.shape:  (355150, 1)\n",
      "x_valid.shape:  (88769, 12, 1)\n",
      "y_valid.shape:  (88769, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:58:10,957 WARNING Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 18:58:11,027 WARNING Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 18:58:11,480 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 355150 samples, validate on 88769 samples\n",
      "Epoch 1/10000\n",
      "355150/355150 [==============================] - ETA: 0s - loss: 1.3599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355150/355150 [==============================] - 15s 42us/sample - loss: 1.3599 - val_loss: 0.2094\n",
      "Epoch 2/10000\n",
      "355150/355150 [==============================] - 9s 25us/sample - loss: 0.2525 - val_loss: 0.0953\n",
      "Epoch 3/10000\n",
      "355150/355150 [==============================] - 9s 26us/sample - loss: 0.1996 - val_loss: 0.0803\n",
      "Epoch 4/10000\n",
      "355150/355150 [==============================] - 9s 25us/sample - loss: 0.1752 - val_loss: 0.0747\n",
      "Epoch 5/10000\n",
      "355150/355150 [==============================] - 10s 29us/sample - loss: 0.1571 - val_loss: 0.0747\n",
      "Epoch 6/10000\n",
      "355150/355150 [==============================] - 9s 24us/sample - loss: 0.1447 - val_loss: 0.0668\n",
      "Epoch 7/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.1344 - val_loss: 0.0678\n",
      "Epoch 8/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.1247 - val_loss: 0.0660\n",
      "Epoch 9/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.1162 - val_loss: 0.0686\n",
      "Epoch 10/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.1093 - val_loss: 0.0696\n",
      "Epoch 11/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.1021 - val_loss: 0.0645\n",
      "Epoch 12/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0964 - val_loss: 0.0641\n",
      "Epoch 13/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0915 - val_loss: 0.0646\n",
      "Epoch 14/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0870 - val_loss: 0.0654\n",
      "Epoch 15/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0845 - val_loss: 0.0641\n",
      "Epoch 16/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0817 - val_loss: 0.0643\n",
      "Epoch 17/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0806 - val_loss: 0.0643\n",
      "Epoch 18/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0801 - val_loss: 0.0640\n",
      "Epoch 19/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0793 - val_loss: 0.0647\n",
      "Epoch 20/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0789 - val_loss: 0.0639\n",
      "Epoch 21/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0789 - val_loss: 0.0639\n",
      "Epoch 22/10000\n",
      "355150/355150 [==============================] - 8s 21us/sample - loss: 0.0786 - val_loss: 0.0658\n",
      "Epoch 23/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0789 - val_loss: 0.0655\n",
      "Epoch 24/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0790 - val_loss: 0.0643\n",
      "Epoch 25/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0786 - val_loss: 0.0648\n",
      "Epoch 26/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0788 - val_loss: 0.0639\n",
      "Epoch 27/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0787 - val_loss: 0.0636\n",
      "Epoch 28/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0786 - val_loss: 0.0635\n",
      "Epoch 29/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0787 - val_loss: 0.0629\n",
      "Epoch 30/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0784 - val_loss: 0.0637\n",
      "Epoch 31/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0785 - val_loss: 0.0637\n",
      "Epoch 32/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0784 - val_loss: 0.0638\n",
      "Epoch 33/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0634\n",
      "Epoch 34/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0638\n",
      "Epoch 35/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0643\n",
      "Epoch 36/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0633\n",
      "Epoch 37/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0784 - val_loss: 0.0630\n",
      "Epoch 38/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0784 - val_loss: 0.0635\n",
      "Epoch 39/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0634\n",
      "Epoch 40/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0640\n",
      "Epoch 41/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0636\n",
      "Epoch 42/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0638\n",
      "Epoch 43/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0635\n",
      "Epoch 44/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0640\n",
      "Epoch 45/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0638\n",
      "Epoch 46/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0780 - val_loss: 0.0629\n",
      "Epoch 47/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0783 - val_loss: 0.0655\n",
      "Epoch 48/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0641\n",
      "Epoch 49/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0627\n",
      "Epoch 50/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0629\n",
      "Epoch 51/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0629\n",
      "Epoch 52/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0630\n",
      "Epoch 53/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0777 - val_loss: 0.0629\n",
      "Epoch 54/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0633\n",
      "Epoch 55/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0633\n",
      "Epoch 56/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0640\n",
      "Epoch 57/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0774 - val_loss: 0.0632\n",
      "Epoch 58/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0777 - val_loss: 0.0633\n",
      "Epoch 59/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0626\n",
      "Epoch 60/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0633\n",
      "Epoch 61/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0629\n",
      "Epoch 62/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0772 - val_loss: 0.0633\n",
      "Epoch 63/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0636\n",
      "Epoch 64/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0627\n",
      "Epoch 65/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0632\n",
      "Epoch 66/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0630\n",
      "Epoch 67/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0621\n",
      "Epoch 68/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0647\n",
      "Epoch 69/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0643\n",
      "Epoch 70/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0629\n",
      "Epoch 71/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 72/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0627\n",
      "Epoch 73/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0775 - val_loss: 0.0623\n",
      "Epoch 74/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0774 - val_loss: 0.0629\n",
      "Epoch 75/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0626\n",
      "Epoch 76/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0644\n",
      "Epoch 77/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0633\n",
      "Epoch 78/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0622\n",
      "Epoch 79/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0773 - val_loss: 0.0636\n",
      "Epoch 80/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0618\n",
      "Epoch 81/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0770 - val_loss: 0.0643\n",
      "Epoch 82/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0774 - val_loss: 0.0622\n",
      "Epoch 83/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0773 - val_loss: 0.0621\n",
      "Epoch 84/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0771 - val_loss: 0.0629\n",
      "Epoch 85/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0627\n",
      "Epoch 86/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0620\n",
      "Epoch 87/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0623\n",
      "Epoch 88/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0626\n",
      "Epoch 89/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0624\n",
      "Epoch 90/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0625\n",
      "Epoch 91/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0630\n",
      "Epoch 92/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0768 - val_loss: 0.0627\n",
      "Epoch 93/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0631\n",
      "Epoch 94/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0769 - val_loss: 0.0625\n",
      "Epoch 95/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0637\n",
      "Epoch 96/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0628\n",
      "Epoch 97/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 98/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0633\n",
      "Epoch 99/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0631\n",
      "Epoch 100/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0635\n",
      "Epoch 101/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 102/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0624\n",
      "Epoch 103/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0626\n",
      "Epoch 104/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0631\n",
      "Epoch 105/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0627\n",
      "Epoch 106/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 107/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0765 - val_loss: 0.0619\n",
      "Epoch 108/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0769 - val_loss: 0.0633\n",
      "Epoch 109/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0770 - val_loss: 0.0620\n",
      "Epoch 110/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0766 - val_loss: 0.0627\n",
      "Epoch 111/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0619\n",
      "Epoch 112/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0624\n",
      "Epoch 113/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0630\n",
      "Epoch 114/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0632\n",
      "Epoch 115/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0769 - val_loss: 0.0623\n",
      "Epoch 116/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0770 - val_loss: 0.0619\n",
      "Epoch 117/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 118/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0768 - val_loss: 0.0622\n",
      "Epoch 119/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 120/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0768 - val_loss: 0.0627\n",
      "Epoch 121/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0768 - val_loss: 0.0626\n",
      "Epoch 122/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0627\n",
      "Epoch 123/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0764 - val_loss: 0.0629\n",
      "Epoch 124/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0628\n",
      "Epoch 125/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0767 - val_loss: 0.0626\n",
      "Epoch 126/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0766 - val_loss: 0.0619\n",
      "Epoch 127/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0767 - val_loss: 0.0623\n",
      "Epoch 128/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0763 - val_loss: 0.0625\n",
      "Epoch 129/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0769 - val_loss: 0.0619\n",
      "Epoch 130/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0624\n",
      "Epoch 131/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0765 - val_loss: 0.0632\n",
      "Epoch 132/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0618\n",
      "Epoch 133/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 134/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0623\n",
      "Epoch 135/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0629\n",
      "Epoch 136/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0617\n",
      "Epoch 137/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0764 - val_loss: 0.0622\n",
      "Epoch 138/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0765 - val_loss: 0.0621\n",
      "Epoch 139/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0765 - val_loss: 0.0615\n",
      "Epoch 140/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0627\n",
      "Epoch 141/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0618\n",
      "Epoch 142/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 143/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0766 - val_loss: 0.0624\n",
      "Epoch 144/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 145/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0627\n",
      "Epoch 146/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0765 - val_loss: 0.0625\n",
      "Epoch 147/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0763 - val_loss: 0.0621\n",
      "Epoch 148/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0764 - val_loss: 0.0621\n",
      "Epoch 149/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0753 - val_loss: 0.0622\n",
      "Epoch 150/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0616\n",
      "Epoch 151/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0734 - val_loss: 0.0619\n",
      "Epoch 152/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0735 - val_loss: 0.0614\n",
      "Epoch 153/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0614\n",
      "Epoch 154/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0626\n",
      "Epoch 155/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0621\n",
      "Epoch 156/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0617\n",
      "Epoch 157/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0616\n",
      "Epoch 158/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0621\n",
      "Epoch 159/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0626\n",
      "Epoch 160/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0623\n",
      "Epoch 161/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0731 - val_loss: 0.0626\n",
      "Epoch 162/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0616\n",
      "Epoch 163/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0733 - val_loss: 0.0617\n",
      "Epoch 164/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0615\n",
      "Epoch 165/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0614\n",
      "Epoch 166/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0622\n",
      "Epoch 167/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0612\n",
      "Epoch 168/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0620\n",
      "Epoch 169/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0619\n",
      "Epoch 170/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0614\n",
      "Epoch 171/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0626\n",
      "Epoch 172/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0617\n",
      "Epoch 173/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0620\n",
      "Epoch 174/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0622\n",
      "Epoch 175/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0623\n",
      "Epoch 176/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0731 - val_loss: 0.0621\n",
      "Epoch 177/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0620\n",
      "Epoch 178/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0615\n",
      "Epoch 179/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0626\n",
      "Epoch 180/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0617\n",
      "Epoch 181/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0619\n",
      "Epoch 182/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0628\n",
      "Epoch 183/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0733 - val_loss: 0.0616\n",
      "Epoch 184/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0728 - val_loss: 0.0614\n",
      "Epoch 185/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0619\n",
      "Epoch 186/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0731 - val_loss: 0.0620\n",
      "Epoch 187/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0731 - val_loss: 0.0625\n",
      "Epoch 188/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0730 - val_loss: 0.0617\n",
      "Epoch 189/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0615\n",
      "Epoch 190/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0620\n",
      "Epoch 191/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0727 - val_loss: 0.0616\n",
      "Epoch 192/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0615\n",
      "Epoch 193/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0619\n",
      "Epoch 194/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0615\n",
      "Epoch 195/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0728 - val_loss: 0.0616\n",
      "Epoch 196/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0619\n",
      "Epoch 197/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0618\n",
      "Epoch 198/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0614\n",
      "Epoch 199/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0619\n",
      "Epoch 200/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0621\n",
      "Epoch 201/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0621\n",
      "Epoch 202/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0621\n",
      "Epoch 203/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0614\n",
      "Epoch 204/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0730 - val_loss: 0.0613\n",
      "Epoch 205/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0631\n",
      "Epoch 206/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0620\n",
      "Epoch 207/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0727 - val_loss: 0.0620\n",
      "Epoch 208/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0730 - val_loss: 0.0623\n",
      "Epoch 209/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0614\n",
      "Epoch 210/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0618\n",
      "Epoch 211/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0618\n",
      "Epoch 212/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0628\n",
      "Epoch 213/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0616\n",
      "Epoch 214/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0620\n",
      "Epoch 215/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0726 - val_loss: 0.0621\n",
      "Epoch 216/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0632\n",
      "Epoch 217/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0620\n",
      "Epoch 218/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0730 - val_loss: 0.0618\n",
      "Epoch 219/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0728 - val_loss: 0.0619\n",
      "Epoch 220/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0727 - val_loss: 0.0622\n",
      "Epoch 221/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0727 - val_loss: 0.0628\n",
      "Epoch 222/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0728 - val_loss: 0.0615\n",
      "Epoch 223/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0620\n",
      "Epoch 224/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0725 - val_loss: 0.0624\n",
      "Epoch 225/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0615\n",
      "Epoch 226/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0617\n",
      "Epoch 227/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0624\n",
      "Epoch 228/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0621\n",
      "Epoch 229/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0729 - val_loss: 0.0618\n",
      "Epoch 230/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0615\n",
      "Epoch 231/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0725 - val_loss: 0.0616\n",
      "Epoch 232/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0632\n",
      "Epoch 233/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0616\n",
      "Epoch 234/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0628\n",
      "Epoch 235/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0620\n",
      "Epoch 236/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0727 - val_loss: 0.0621\n",
      "Epoch 237/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0615\n",
      "Epoch 238/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0620\n",
      "Epoch 239/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0618\n",
      "Epoch 240/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0725 - val_loss: 0.0620\n",
      "Epoch 241/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0624\n",
      "Epoch 242/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0620\n",
      "Epoch 243/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0623\n",
      "Epoch 244/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0618\n",
      "Epoch 245/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0726 - val_loss: 0.0618\n",
      "Epoch 246/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0728 - val_loss: 0.0619\n",
      "Epoch 247/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0726 - val_loss: 0.0616\n",
      "Epoch 248/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0711 - val_loss: 0.0615\n",
      "Epoch 249/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0707 - val_loss: 0.0614\n",
      "Epoch 250/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0706 - val_loss: 0.0613\n",
      "Epoch 251/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0706 - val_loss: 0.0619\n",
      "Epoch 252/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0704 - val_loss: 0.0618\n",
      "Epoch 253/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0704 - val_loss: 0.0615\n",
      "Epoch 254/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0704 - val_loss: 0.0622\n",
      "Epoch 255/10000\n",
      "355150/355150 [==============================] - 7s 19us/sample - loss: 0.0704 - val_loss: 0.0614\n",
      "Epoch 256/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0703 - val_loss: 0.0615\n",
      "Epoch 257/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0702 - val_loss: 0.0613\n",
      "Epoch 258/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0708 - val_loss: 0.0615\n",
      "Epoch 259/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0705 - val_loss: 0.0615\n",
      "Epoch 260/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0704 - val_loss: 0.0616\n",
      "Epoch 261/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0701 - val_loss: 0.0616\n",
      "Epoch 262/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0704 - val_loss: 0.0625\n",
      "Epoch 263/10000\n",
      "355150/355150 [==============================] - 7s 21us/sample - loss: 0.0705 - val_loss: 0.0615\n",
      "Epoch 264/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0705 - val_loss: 0.0617\n",
      "Epoch 265/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0705 - val_loss: 0.0614\n",
      "Epoch 266/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0705 - val_loss: 0.0615\n",
      "Epoch 267/10000\n",
      "355150/355150 [==============================] - 7s 20us/sample - loss: 0.0704 - val_loss: 0.0617\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject12.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject12.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11720, 12, 1)\n",
      "y_test.shape:  (11720, 1)\n",
      "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:29:55,864 WARNING Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:29:55,956 WARNING Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject12.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject13.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject13.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10456, 12, 1)\n",
      "y_test.shape:  (10456, 1)\n",
      "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:06,098 WARNING Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:06,169 WARNING Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject13.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject14.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject14.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11323, 12, 1)\n",
      "y_test.shape:  (11323, 1)\n",
      "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:15,924 WARNING Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:15,989 WARNING Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject14.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject15.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject15.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11815, 12, 1)\n",
      "y_test.shape:  (11815, 1)\n",
      "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:26,192 WARNING Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:26,265 WARNING Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject15.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject16.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject16.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11652, 12, 1)\n",
      "y_test.shape:  (11652, 1)\n",
      "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:36,254 WARNING Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:36,311 WARNING Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject16.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject17.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject17.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11893, 12, 1)\n",
      "y_test.shape:  (11893, 1)\n",
      "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:46,793 WARNING Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:46,883 WARNING Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject17.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject18.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11717, 12, 1)\n",
      "y_test.shape:  (11717, 1)\n",
      "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:56,644 WARNING Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:30:56,706 WARNING Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject19.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject19.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11711, 12, 1)\n",
      "y_test.shape:  (11711, 1)\n",
      "WARNING:tensorflow:Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:06,246 WARNING Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:06,313 WARNING Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject19.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject20.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject20.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11317, 12, 1)\n",
      "y_test.shape:  (11317, 1)\n",
      "WARNING:tensorflow:Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:16,065 WARNING Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:16,124 WARNING Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject20.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject21.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject21.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11299, 12, 1)\n",
      "y_test.shape:  (11299, 1)\n",
      "WARNING:tensorflow:Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:25,615 WARNING Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:25,681 WARNING Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject21.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject22.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject22.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11819, 12, 1)\n",
      "y_test.shape:  (11819, 1)\n",
      "WARNING:tensorflow:Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:35,657 WARNING Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:31:35,728 WARNING Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject22.csv\n",
      "2025-01-19 19:31:44,699 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 131\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 159\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (360473, 12, 1)\n",
      "y_train.shape:  (360473, 1)\n",
      "x_valid.shape:  (90101, 12, 1)\n",
      "y_valid.shape:  (90101, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:32:32,818 WARNING Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 19:32:32,882 WARNING Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 19:32:32,984 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 360473 samples, validate on 90101 samples\n",
      "Epoch 1/10000\n",
      "360448/360473 [============================>.] - ETA: 0s - loss: 1.5788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360473/360473 [==============================] - 12s 33us/sample - loss: 1.5787 - val_loss: 0.1858\n",
      "Epoch 2/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.2006 - val_loss: 0.0822\n",
      "Epoch 3/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.1579 - val_loss: 0.0927\n",
      "Epoch 4/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.1391 - val_loss: 0.0736\n",
      "Epoch 5/10000\n",
      "360473/360473 [==============================] - 10s 29us/sample - loss: 0.1285 - val_loss: 0.0672\n",
      "Epoch 6/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.1208 - val_loss: 0.0646\n",
      "Epoch 7/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.1130 - val_loss: 0.0627\n",
      "Epoch 8/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.1061 - val_loss: 0.0662\n",
      "Epoch 9/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.1001 - val_loss: 0.0621\n",
      "Epoch 10/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0951 - val_loss: 0.0666\n",
      "Epoch 11/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0929 - val_loss: 0.0679\n",
      "Epoch 12/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0921 - val_loss: 0.0653\n",
      "Epoch 13/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0916 - val_loss: 0.0696\n",
      "Epoch 14/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0916 - val_loss: 0.0677\n",
      "Epoch 15/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0914 - val_loss: 0.0661\n",
      "Epoch 16/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0914 - val_loss: 0.0665\n",
      "Epoch 17/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0916 - val_loss: 0.0721\n",
      "Epoch 18/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0913 - val_loss: 0.0670\n",
      "Epoch 19/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0915 - val_loss: 0.0706\n",
      "Epoch 20/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0915 - val_loss: 0.0645\n",
      "Epoch 21/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0912 - val_loss: 0.0659\n",
      "Epoch 22/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0908 - val_loss: 0.0641\n",
      "Epoch 23/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0916 - val_loss: 0.0653\n",
      "Epoch 24/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0913 - val_loss: 0.0650\n",
      "Epoch 25/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0910 - val_loss: 0.0698\n",
      "Epoch 26/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0914 - val_loss: 0.0669\n",
      "Epoch 27/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0912 - val_loss: 0.0711\n",
      "Epoch 28/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0910 - val_loss: 0.0726\n",
      "Epoch 29/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0908 - val_loss: 0.0663\n",
      "Epoch 30/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0909 - val_loss: 0.0648\n",
      "Epoch 31/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0911 - val_loss: 0.0700\n",
      "Epoch 32/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0907 - val_loss: 0.0647\n",
      "Epoch 33/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0905 - val_loss: 0.0658\n",
      "Epoch 34/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0904 - val_loss: 0.0723\n",
      "Epoch 35/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0903 - val_loss: 0.0637\n",
      "Epoch 36/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0906 - val_loss: 0.0651\n",
      "Epoch 37/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0902 - val_loss: 0.0638\n",
      "Epoch 38/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0903 - val_loss: 0.0657\n",
      "Epoch 39/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0905 - val_loss: 0.0753\n",
      "Epoch 40/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0903 - val_loss: 0.0647\n",
      "Epoch 41/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0902 - val_loss: 0.0671\n",
      "Epoch 42/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0902 - val_loss: 0.0663\n",
      "Epoch 43/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0899 - val_loss: 0.0696\n",
      "Epoch 44/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0905 - val_loss: 0.0658\n",
      "Epoch 45/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0899 - val_loss: 0.0639\n",
      "Epoch 46/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0900 - val_loss: 0.0637\n",
      "Epoch 47/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0899 - val_loss: 0.0631\n",
      "Epoch 48/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0904 - val_loss: 0.0706\n",
      "Epoch 49/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0902 - val_loss: 0.0648\n",
      "Epoch 50/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0902 - val_loss: 0.0632\n",
      "Epoch 51/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0899 - val_loss: 0.0640\n",
      "Epoch 52/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0898 - val_loss: 0.0634\n",
      "Epoch 53/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0896 - val_loss: 0.0659\n",
      "Epoch 54/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0899 - val_loss: 0.0635\n",
      "Epoch 55/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0901 - val_loss: 0.0647\n",
      "Epoch 56/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0889 - val_loss: 0.0627\n",
      "Epoch 57/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0819 - val_loss: 0.0622\n",
      "Epoch 58/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0798 - val_loss: 0.0643\n",
      "Epoch 59/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0790 - val_loss: 0.0633\n",
      "Epoch 60/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0789 - val_loss: 0.0621\n",
      "Epoch 61/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0788 - val_loss: 0.0604\n",
      "Epoch 62/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0789 - val_loss: 0.0606\n",
      "Epoch 63/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0785 - val_loss: 0.0607\n",
      "Epoch 64/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0788 - val_loss: 0.0609\n",
      "Epoch 65/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0613\n",
      "Epoch 66/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0786 - val_loss: 0.0606\n",
      "Epoch 67/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0617\n",
      "Epoch 68/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0784 - val_loss: 0.0606\n",
      "Epoch 69/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0602\n",
      "Epoch 70/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0787 - val_loss: 0.0607\n",
      "Epoch 71/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0608\n",
      "Epoch 72/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0781 - val_loss: 0.0618\n",
      "Epoch 73/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0786 - val_loss: 0.0685\n",
      "Epoch 74/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0785 - val_loss: 0.0609\n",
      "Epoch 75/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0614\n",
      "Epoch 76/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0783 - val_loss: 0.0606\n",
      "Epoch 77/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0605\n",
      "Epoch 78/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0602\n",
      "Epoch 79/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0780 - val_loss: 0.0630\n",
      "Epoch 80/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0621\n",
      "Epoch 81/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0636\n",
      "Epoch 82/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0602\n",
      "Epoch 83/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0777 - val_loss: 0.0603\n",
      "Epoch 84/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0621\n",
      "Epoch 85/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0610\n",
      "Epoch 86/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0598\n",
      "Epoch 87/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0614\n",
      "Epoch 88/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0603\n",
      "Epoch 89/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0595\n",
      "Epoch 90/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0599\n",
      "Epoch 91/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0605\n",
      "Epoch 92/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0621\n",
      "Epoch 93/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0614\n",
      "Epoch 94/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0778 - val_loss: 0.0620\n",
      "Epoch 95/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0599\n",
      "Epoch 96/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0779 - val_loss: 0.0605\n",
      "Epoch 97/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0780 - val_loss: 0.0596\n",
      "Epoch 98/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0594\n",
      "Epoch 99/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0608\n",
      "Epoch 100/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0611\n",
      "Epoch 101/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0778 - val_loss: 0.0601\n",
      "Epoch 102/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0611\n",
      "Epoch 103/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0776 - val_loss: 0.0593\n",
      "Epoch 104/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0609\n",
      "Epoch 105/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0778 - val_loss: 0.0604\n",
      "Epoch 106/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0776 - val_loss: 0.0606\n",
      "Epoch 107/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0606\n",
      "Epoch 108/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0781 - val_loss: 0.0626\n",
      "Epoch 109/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0626\n",
      "Epoch 110/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0774 - val_loss: 0.0597\n",
      "Epoch 111/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0603\n",
      "Epoch 112/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0604\n",
      "Epoch 113/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0597\n",
      "Epoch 114/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0600\n",
      "Epoch 115/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0616\n",
      "Epoch 116/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0600\n",
      "Epoch 117/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0605\n",
      "Epoch 118/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0610\n",
      "Epoch 119/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0599\n",
      "Epoch 120/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0611\n",
      "Epoch 121/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0592\n",
      "Epoch 122/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0607\n",
      "Epoch 123/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0777 - val_loss: 0.0594\n",
      "Epoch 124/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0605\n",
      "Epoch 125/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0595\n",
      "Epoch 126/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0606\n",
      "Epoch 127/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0597\n",
      "Epoch 128/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0777 - val_loss: 0.0602\n",
      "Epoch 129/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0613\n",
      "Epoch 130/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0597\n",
      "Epoch 131/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0777 - val_loss: 0.0595\n",
      "Epoch 132/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0607\n",
      "Epoch 133/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0607\n",
      "Epoch 134/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0599\n",
      "Epoch 135/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0604\n",
      "Epoch 136/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0599\n",
      "Epoch 137/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0604\n",
      "Epoch 138/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0775 - val_loss: 0.0598\n",
      "Epoch 139/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0776 - val_loss: 0.0598\n",
      "Epoch 140/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0599\n",
      "Epoch 141/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0778 - val_loss: 0.0621\n",
      "Epoch 142/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0774 - val_loss: 0.0602\n",
      "Epoch 143/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0600\n",
      "Epoch 144/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0603\n",
      "Epoch 145/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0597\n",
      "Epoch 146/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0607\n",
      "Epoch 147/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0602\n",
      "Epoch 148/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0774 - val_loss: 0.0593\n",
      "Epoch 149/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0769 - val_loss: 0.0598\n",
      "Epoch 150/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0772 - val_loss: 0.0608\n",
      "Epoch 151/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0592\n",
      "Epoch 152/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0598\n",
      "Epoch 153/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0597\n",
      "Epoch 154/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0616\n",
      "Epoch 155/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0635\n",
      "Epoch 156/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0612\n",
      "Epoch 157/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0597\n",
      "Epoch 158/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0604\n",
      "Epoch 159/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0595\n",
      "Epoch 160/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0608\n",
      "Epoch 161/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0775 - val_loss: 0.0603\n",
      "Epoch 162/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0771 - val_loss: 0.0605\n",
      "Epoch 163/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0776 - val_loss: 0.0606\n",
      "Epoch 164/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0772 - val_loss: 0.0604\n",
      "Epoch 165/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0623\n",
      "Epoch 166/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0614\n",
      "Epoch 167/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0597\n",
      "Epoch 168/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0780 - val_loss: 0.0604\n",
      "Epoch 169/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0777 - val_loss: 0.0611\n",
      "Epoch 170/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0611\n",
      "Epoch 171/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0778 - val_loss: 0.0604\n",
      "Epoch 172/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0776 - val_loss: 0.0593\n",
      "Epoch 173/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0775 - val_loss: 0.0596\n",
      "Epoch 174/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0596\n",
      "Epoch 175/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0779 - val_loss: 0.0602\n",
      "Epoch 176/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0609\n",
      "Epoch 177/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0645\n",
      "Epoch 178/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0600\n",
      "Epoch 179/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0610\n",
      "Epoch 180/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0605\n",
      "Epoch 181/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0773 - val_loss: 0.0604\n",
      "Epoch 182/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0605\n",
      "Epoch 183/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0600\n",
      "Epoch 184/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0614\n",
      "Epoch 185/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0605\n",
      "Epoch 186/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0598\n",
      "Epoch 187/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0608\n",
      "Epoch 188/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0599\n",
      "Epoch 189/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0600\n",
      "Epoch 190/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0610\n",
      "Epoch 191/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0596\n",
      "Epoch 192/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0599\n",
      "Epoch 193/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0599\n",
      "Epoch 194/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0657\n",
      "Epoch 195/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0773 - val_loss: 0.0603\n",
      "Epoch 196/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0770 - val_loss: 0.0602\n",
      "Epoch 197/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0773 - val_loss: 0.0600\n",
      "Epoch 198/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0772 - val_loss: 0.0595\n",
      "Epoch 199/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0598\n",
      "Epoch 200/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0600\n",
      "Epoch 201/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0610\n",
      "Epoch 202/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0604\n",
      "Epoch 203/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0774 - val_loss: 0.0605\n",
      "Epoch 204/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0776 - val_loss: 0.0606\n",
      "Epoch 205/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0773 - val_loss: 0.0595\n",
      "Epoch 206/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0776 - val_loss: 0.0602\n",
      "Epoch 207/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0601\n",
      "Epoch 208/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0612\n",
      "Epoch 209/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0598\n",
      "Epoch 210/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0771 - val_loss: 0.0608\n",
      "Epoch 211/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0599\n",
      "Epoch 212/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0592\n",
      "Epoch 213/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0773 - val_loss: 0.0615\n",
      "Epoch 214/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0600\n",
      "Epoch 215/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0606\n",
      "Epoch 216/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0774 - val_loss: 0.0613\n",
      "Epoch 217/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0611\n",
      "Epoch 218/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0601\n",
      "Epoch 219/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0609\n",
      "Epoch 220/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0609\n",
      "Epoch 221/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0596\n",
      "Epoch 222/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0614\n",
      "Epoch 223/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0602\n",
      "Epoch 224/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0607\n",
      "Epoch 225/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0641\n",
      "Epoch 226/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0600\n",
      "Epoch 227/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0599\n",
      "Epoch 228/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0600\n",
      "Epoch 229/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0768 - val_loss: 0.0603\n",
      "Epoch 230/10000\n",
      "360473/360473 [==============================] - 7s 19us/sample - loss: 0.0770 - val_loss: 0.0603\n",
      "Epoch 231/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0608\n",
      "Epoch 232/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0621\n",
      "Epoch 233/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0602\n",
      "Epoch 234/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0594\n",
      "Epoch 235/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0769 - val_loss: 0.0602\n",
      "Epoch 236/10000\n",
      "360473/360473 [==============================] - 8s 22us/sample - loss: 0.0769 - val_loss: 0.0607\n",
      "Epoch 237/10000\n",
      "360473/360473 [==============================] - 8s 21us/sample - loss: 0.0769 - val_loss: 0.0601\n",
      "Epoch 238/10000\n",
      "360473/360473 [==============================] - 8s 22us/sample - loss: 0.0770 - val_loss: 0.0603\n",
      "Epoch 239/10000\n",
      "360473/360473 [==============================] - 7s 21us/sample - loss: 0.0771 - val_loss: 0.0625\n",
      "Epoch 240/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0599\n",
      "Epoch 241/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0606\n",
      "Epoch 242/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0602\n",
      "Epoch 243/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0611\n",
      "Epoch 244/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0770 - val_loss: 0.0600\n",
      "Epoch 245/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0608\n",
      "Epoch 246/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0601\n",
      "Epoch 247/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0772 - val_loss: 0.0615\n",
      "Epoch 248/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0771 - val_loss: 0.0605\n",
      "Epoch 249/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0608\n",
      "Epoch 250/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0767 - val_loss: 0.0606\n",
      "Epoch 251/10000\n",
      "360473/360473 [==============================] - 7s 20us/sample - loss: 0.0768 - val_loss: 0.0597\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject23.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject23.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11505, 12, 1)\n",
      "y_test.shape:  (11505, 1)\n",
      "WARNING:tensorflow:Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:01,237 WARNING Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:01,292 WARNING Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject23.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject24.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9648, 12, 1)\n",
      "y_test.shape:  (9648, 1)\n",
      "WARNING:tensorflow:Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:11,035 WARNING Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:11,100 WARNING Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject25.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11748, 12, 1)\n",
      "y_test.shape:  (11748, 1)\n",
      "WARNING:tensorflow:Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:20,800 WARNING Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:20,858 WARNING Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject25.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject26.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject26.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11613, 12, 1)\n",
      "y_test.shape:  (11613, 1)\n",
      "WARNING:tensorflow:Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:30,950 WARNING Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:31,017 WARNING Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject26.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject27.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject27.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11831, 12, 1)\n",
      "y_test.shape:  (11831, 1)\n",
      "WARNING:tensorflow:Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:41,268 WARNING Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:41,346 WARNING Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject27.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject28.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject28.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11762, 12, 1)\n",
      "y_test.shape:  (11762, 1)\n",
      "WARNING:tensorflow:Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:51,549 WARNING Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:03:51,601 WARNING Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject28.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject29.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject29.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5079, 12, 1)\n",
      "y_test.shape:  (5079, 1)\n",
      "WARNING:tensorflow:Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:02,336 WARNING Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:02,399 WARNING Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject29.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject30.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject30.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11724, 12, 1)\n",
      "y_test.shape:  (11724, 1)\n",
      "WARNING:tensorflow:Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:10,085 WARNING Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:10,143 WARNING Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject30.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject31.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject31.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11911, 12, 1)\n",
      "y_test.shape:  (11911, 1)\n",
      "WARNING:tensorflow:Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:20,036 WARNING Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:20,097 WARNING Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject31.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject32.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject32.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11607, 12, 1)\n",
      "y_test.shape:  (11607, 1)\n",
      "WARNING:tensorflow:Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:30,047 WARNING Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:30,102 WARNING Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject32.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject33.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject33.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11639, 12, 1)\n",
      "y_test.shape:  (11639, 1)\n",
      "WARNING:tensorflow:Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:40,534 WARNING Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:04:40,609 WARNING Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject33.csv\n",
      "2025-01-19 20:04:49,552 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 131\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 159\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (357116, 12, 1)\n",
      "y_train.shape:  (357116, 1)\n",
      "x_valid.shape:  (89259, 12, 1)\n",
      "y_valid.shape:  (89259, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:05:38,341 WARNING Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:05:38,411 WARNING Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 20:05:38,504 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 357116 samples, validate on 89259 samples\n",
      "Epoch 1/10000\n",
      "356352/357116 [============================>.] - ETA: 0s - loss: 2.0366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357116/357116 [==============================] - 13s 35us/sample - loss: 2.0328 - val_loss: 0.1565\n",
      "Epoch 2/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.1814 - val_loss: 0.0761\n",
      "Epoch 3/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.1433 - val_loss: 0.0683\n",
      "Epoch 4/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.1320 - val_loss: 0.0702\n",
      "Epoch 5/10000\n",
      "357116/357116 [==============================] - 10s 29us/sample - loss: 0.1242 - val_loss: 0.0677\n",
      "Epoch 6/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.1161 - val_loss: 0.0671\n",
      "Epoch 7/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.1103 - val_loss: 0.0694\n",
      "Epoch 8/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.1042 - val_loss: 0.0629\n",
      "Epoch 9/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0992 - val_loss: 0.0637\n",
      "Epoch 10/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0946 - val_loss: 0.0640\n",
      "Epoch 11/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0904 - val_loss: 0.0641\n",
      "Epoch 12/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0874 - val_loss: 0.0631\n",
      "Epoch 13/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0840 - val_loss: 0.0627\n",
      "Epoch 14/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0818 - val_loss: 0.0624\n",
      "Epoch 15/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0800 - val_loss: 0.0630\n",
      "Epoch 16/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0789 - val_loss: 0.0620\n",
      "Epoch 17/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0782 - val_loss: 0.0627\n",
      "Epoch 18/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0777 - val_loss: 0.0623\n",
      "Epoch 19/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0769 - val_loss: 0.0632\n",
      "Epoch 20/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0766 - val_loss: 0.0618\n",
      "Epoch 21/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0764 - val_loss: 0.0617\n",
      "Epoch 22/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0762 - val_loss: 0.0615\n",
      "Epoch 23/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0761 - val_loss: 0.0623\n",
      "Epoch 24/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0758 - val_loss: 0.0610\n",
      "Epoch 25/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 26/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0756 - val_loss: 0.0623\n",
      "Epoch 27/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0754 - val_loss: 0.0614\n",
      "Epoch 28/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0756 - val_loss: 0.0611\n",
      "Epoch 29/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0755 - val_loss: 0.0618\n",
      "Epoch 30/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0754 - val_loss: 0.0612\n",
      "Epoch 31/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0752 - val_loss: 0.0614\n",
      "Epoch 32/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0749 - val_loss: 0.0609\n",
      "Epoch 33/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0753 - val_loss: 0.0610\n",
      "Epoch 34/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0754 - val_loss: 0.0620\n",
      "Epoch 35/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0751 - val_loss: 0.0605\n",
      "Epoch 36/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0750 - val_loss: 0.0607\n",
      "Epoch 37/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0751 - val_loss: 0.0607\n",
      "Epoch 38/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0752 - val_loss: 0.0616\n",
      "Epoch 39/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0752 - val_loss: 0.0608\n",
      "Epoch 40/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0750 - val_loss: 0.0610\n",
      "Epoch 41/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0748 - val_loss: 0.0606\n",
      "Epoch 42/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0750 - val_loss: 0.0609\n",
      "Epoch 43/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0750 - val_loss: 0.0604\n",
      "Epoch 44/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0747 - val_loss: 0.0608\n",
      "Epoch 45/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0749 - val_loss: 0.0603\n",
      "Epoch 46/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0746 - val_loss: 0.0603\n",
      "Epoch 47/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0748 - val_loss: 0.0606\n",
      "Epoch 48/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0747 - val_loss: 0.0609\n",
      "Epoch 49/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0749 - val_loss: 0.0608\n",
      "Epoch 50/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0745 - val_loss: 0.0608\n",
      "Epoch 51/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0745 - val_loss: 0.0604\n",
      "Epoch 52/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0747 - val_loss: 0.0612\n",
      "Epoch 53/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0747 - val_loss: 0.0607\n",
      "Epoch 54/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0747 - val_loss: 0.0630\n",
      "Epoch 55/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0746 - val_loss: 0.0625\n",
      "Epoch 56/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0744 - val_loss: 0.0606\n",
      "Epoch 57/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0746 - val_loss: 0.0625\n",
      "Epoch 58/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0744 - val_loss: 0.0604\n",
      "Epoch 59/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0745 - val_loss: 0.0604\n",
      "Epoch 60/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0745 - val_loss: 0.0609\n",
      "Epoch 61/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0746 - val_loss: 0.0607\n",
      "Epoch 62/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0745 - val_loss: 0.0604\n",
      "Epoch 63/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0745 - val_loss: 0.0602\n",
      "Epoch 64/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0747 - val_loss: 0.0606\n",
      "Epoch 65/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0599\n",
      "Epoch 66/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0742 - val_loss: 0.0601\n",
      "Epoch 67/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0742 - val_loss: 0.0603\n",
      "Epoch 68/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0746 - val_loss: 0.0602\n",
      "Epoch 69/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0746 - val_loss: 0.0604\n",
      "Epoch 70/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0604\n",
      "Epoch 71/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0743 - val_loss: 0.0609\n",
      "Epoch 72/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0746 - val_loss: 0.0605\n",
      "Epoch 73/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0742 - val_loss: 0.0606\n",
      "Epoch 74/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0622\n",
      "Epoch 75/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0603\n",
      "Epoch 76/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0604\n",
      "Epoch 77/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0744 - val_loss: 0.0619\n",
      "Epoch 78/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0601\n",
      "Epoch 79/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0745 - val_loss: 0.0609\n",
      "Epoch 80/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0743 - val_loss: 0.0605\n",
      "Epoch 81/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0743 - val_loss: 0.0603\n",
      "Epoch 82/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0739 - val_loss: 0.0607\n",
      "Epoch 83/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0741 - val_loss: 0.0599\n",
      "Epoch 84/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0611\n",
      "Epoch 85/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0742 - val_loss: 0.0603\n",
      "Epoch 86/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0745 - val_loss: 0.0601\n",
      "Epoch 87/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0742 - val_loss: 0.0601\n",
      "Epoch 88/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0743 - val_loss: 0.0609\n",
      "Epoch 89/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0743 - val_loss: 0.0603\n",
      "Epoch 90/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0742 - val_loss: 0.0601\n",
      "Epoch 91/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0742 - val_loss: 0.0611\n",
      "Epoch 92/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0740 - val_loss: 0.0605\n",
      "Epoch 93/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0741 - val_loss: 0.0604\n",
      "Epoch 94/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0739 - val_loss: 0.0603\n",
      "Epoch 95/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0608\n",
      "Epoch 96/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0742 - val_loss: 0.0602\n",
      "Epoch 97/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0601\n",
      "Epoch 98/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0608\n",
      "Epoch 99/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0604\n",
      "Epoch 100/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0603\n",
      "Epoch 101/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0740 - val_loss: 0.0603\n",
      "Epoch 102/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0605\n",
      "Epoch 103/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0602\n",
      "Epoch 104/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0605\n",
      "Epoch 105/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0611\n",
      "Epoch 106/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0740 - val_loss: 0.0612\n",
      "Epoch 107/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0607\n",
      "Epoch 108/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0740 - val_loss: 0.0601\n",
      "Epoch 109/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0605\n",
      "Epoch 110/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0740 - val_loss: 0.0606\n",
      "Epoch 111/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0737 - val_loss: 0.0599\n",
      "Epoch 112/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0603\n",
      "Epoch 113/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0613\n",
      "Epoch 114/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0739 - val_loss: 0.0613\n",
      "Epoch 115/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0741 - val_loss: 0.0604\n",
      "Epoch 116/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0602\n",
      "Epoch 117/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0605\n",
      "Epoch 118/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0740 - val_loss: 0.0599\n",
      "Epoch 119/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0738 - val_loss: 0.0606\n",
      "Epoch 120/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0738 - val_loss: 0.0603\n",
      "Epoch 121/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0601\n",
      "Epoch 122/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0606\n",
      "Epoch 123/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0599\n",
      "Epoch 124/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0601\n",
      "Epoch 125/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0613\n",
      "Epoch 126/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0741 - val_loss: 0.0601\n",
      "Epoch 127/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0608\n",
      "Epoch 128/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0601\n",
      "Epoch 129/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0741 - val_loss: 0.0600\n",
      "Epoch 130/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0601\n",
      "Epoch 131/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0601\n",
      "Epoch 132/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0600\n",
      "Epoch 133/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0602\n",
      "Epoch 134/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0737 - val_loss: 0.0603\n",
      "Epoch 135/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0612\n",
      "Epoch 136/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0738 - val_loss: 0.0617\n",
      "Epoch 137/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0611\n",
      "Epoch 138/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0598\n",
      "Epoch 139/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0608\n",
      "Epoch 140/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 141/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0600\n",
      "Epoch 142/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0601\n",
      "Epoch 143/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0601\n",
      "Epoch 144/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Epoch 145/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0737 - val_loss: 0.0604\n",
      "Epoch 146/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0601\n",
      "Epoch 147/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0600\n",
      "Epoch 148/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0602\n",
      "Epoch 149/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0600\n",
      "Epoch 150/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0604\n",
      "Epoch 151/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0602\n",
      "Epoch 152/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0739 - val_loss: 0.0601\n",
      "Epoch 153/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0739 - val_loss: 0.0602\n",
      "Epoch 154/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0612\n",
      "Epoch 155/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0736 - val_loss: 0.0601\n",
      "Epoch 156/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0600\n",
      "Epoch 157/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0602\n",
      "Epoch 158/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0607\n",
      "Epoch 159/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0599\n",
      "Epoch 160/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0737 - val_loss: 0.0600\n",
      "Epoch 161/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0736 - val_loss: 0.0599\n",
      "Epoch 162/10000\n",
      "357116/357116 [==============================] - 8s 22us/sample - loss: 0.0736 - val_loss: 0.0609\n",
      "Epoch 163/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0734 - val_loss: 0.0606\n",
      "Epoch 164/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0601\n",
      "Epoch 165/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0738 - val_loss: 0.0603\n",
      "Epoch 166/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0607\n",
      "Epoch 167/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0606\n",
      "Epoch 168/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0736 - val_loss: 0.0597\n",
      "Epoch 169/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0604\n",
      "Epoch 170/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0606\n",
      "Epoch 171/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 172/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 173/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0606\n",
      "Epoch 174/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 175/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0600\n",
      "Epoch 176/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0602\n",
      "Epoch 177/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0736 - val_loss: 0.0600\n",
      "Epoch 178/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0601\n",
      "Epoch 179/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0604\n",
      "Epoch 180/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0609\n",
      "Epoch 181/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0600\n",
      "Epoch 182/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0737 - val_loss: 0.0608\n",
      "Epoch 183/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 184/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0602\n",
      "Epoch 185/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0734 - val_loss: 0.0603\n",
      "Epoch 186/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0608\n",
      "Epoch 187/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0732 - val_loss: 0.0603\n",
      "Epoch 188/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0607\n",
      "Epoch 189/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0606\n",
      "Epoch 190/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Epoch 191/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 192/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0601\n",
      "Epoch 193/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0603\n",
      "Epoch 194/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0732 - val_loss: 0.0602\n",
      "Epoch 195/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0602\n",
      "Epoch 196/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 197/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0603\n",
      "Epoch 198/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0600\n",
      "Epoch 199/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0599\n",
      "Epoch 200/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0602\n",
      "Epoch 201/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Epoch 202/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0603\n",
      "Epoch 203/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0601\n",
      "Epoch 204/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0602\n",
      "Epoch 205/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0615\n",
      "Epoch 206/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0604\n",
      "Epoch 207/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0603\n",
      "Epoch 208/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0607\n",
      "Epoch 209/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0733 - val_loss: 0.0601\n",
      "Epoch 210/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0607\n",
      "Epoch 211/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0603\n",
      "Epoch 212/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0607\n",
      "Epoch 213/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0731 - val_loss: 0.0601\n",
      "Epoch 214/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0598\n",
      "Epoch 215/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0608\n",
      "Epoch 216/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0599\n",
      "Epoch 217/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0609\n",
      "Epoch 218/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0602\n",
      "Epoch 219/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0601\n",
      "Epoch 220/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0730 - val_loss: 0.0600\n",
      "Epoch 221/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0733 - val_loss: 0.0605\n",
      "Epoch 222/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0604\n",
      "Epoch 223/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0600\n",
      "Epoch 224/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0604\n",
      "Epoch 225/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0600\n",
      "Epoch 226/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0736 - val_loss: 0.0607\n",
      "Epoch 227/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0733 - val_loss: 0.0614\n",
      "Epoch 228/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0734 - val_loss: 0.0606\n",
      "Epoch 229/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0601\n",
      "Epoch 230/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0731 - val_loss: 0.0597\n",
      "Epoch 231/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0735 - val_loss: 0.0604\n",
      "Epoch 232/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0602\n",
      "Epoch 233/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0603\n",
      "Epoch 234/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0605\n",
      "Epoch 235/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0602\n",
      "Epoch 236/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0730 - val_loss: 0.0602\n",
      "Epoch 237/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0599\n",
      "Epoch 238/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0603\n",
      "Epoch 239/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0602\n",
      "Epoch 240/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0734 - val_loss: 0.0600\n",
      "Epoch 241/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0601\n",
      "Epoch 242/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0606\n",
      "Epoch 243/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0601\n",
      "Epoch 244/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0600\n",
      "Epoch 245/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0600\n",
      "Epoch 246/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0733 - val_loss: 0.0601\n",
      "Epoch 247/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0617\n",
      "Epoch 248/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0615\n",
      "Epoch 249/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0601\n",
      "Epoch 250/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0603\n",
      "Epoch 251/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0604\n",
      "Epoch 252/10000\n",
      "357116/357116 [==============================] - 7s 19us/sample - loss: 0.0731 - val_loss: 0.0599\n",
      "Epoch 253/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0732 - val_loss: 0.0600\n",
      "Epoch 254/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0599\n",
      "Epoch 255/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0601\n",
      "Epoch 256/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0734 - val_loss: 0.0599\n",
      "Epoch 257/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0606\n",
      "Epoch 258/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0732 - val_loss: 0.0601\n",
      "Epoch 259/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0733 - val_loss: 0.0600\n",
      "Epoch 260/10000\n",
      "357116/357116 [==============================] - 8s 21us/sample - loss: 0.0731 - val_loss: 0.0603\n",
      "Epoch 261/10000\n",
      "357116/357116 [==============================] - 8s 22us/sample - loss: 0.0730 - val_loss: 0.0603\n",
      "Epoch 262/10000\n",
      "357116/357116 [==============================] - 8s 22us/sample - loss: 0.0732 - val_loss: 0.0598\n",
      "Epoch 263/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0730 - val_loss: 0.0607\n",
      "Epoch 264/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0604\n",
      "Epoch 265/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0729 - val_loss: 0.0602\n",
      "Epoch 266/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0731 - val_loss: 0.0600\n",
      "Epoch 267/10000\n",
      "357116/357116 [==============================] - 7s 20us/sample - loss: 0.0730 - val_loss: 0.0599\n",
      "Epoch 268/10000\n",
      "357116/357116 [==============================] - 7s 21us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject34.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject34.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11469, 12, 1)\n",
      "y_test.shape:  (11469, 1)\n",
      "WARNING:tensorflow:Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:10,338 WARNING Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:10,386 WARNING Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject34.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject35.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject35.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11692, 12, 1)\n",
      "y_test.shape:  (11692, 1)\n",
      "WARNING:tensorflow:Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:21,563 WARNING Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:21,614 WARNING Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject35.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject36.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject36.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11157, 12, 1)\n",
      "y_test.shape:  (11157, 1)\n",
      "WARNING:tensorflow:Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:32,610 WARNING Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:32,664 WARNING Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject36.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject37.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject37.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11500, 12, 1)\n",
      "y_test.shape:  (11500, 1)\n",
      "WARNING:tensorflow:Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:42,600 WARNING Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:42,667 WARNING Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject37.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject38.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject38.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11531, 12, 1)\n",
      "y_test.shape:  (11531, 1)\n",
      "WARNING:tensorflow:Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:53,876 WARNING Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:38:53,958 WARNING Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject38.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject39.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject39.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11071, 12, 1)\n",
      "y_test.shape:  (11071, 1)\n",
      "WARNING:tensorflow:Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:04,322 WARNING Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:04,383 WARNING Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject39.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject40.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject40.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11431, 12, 1)\n",
      "y_test.shape:  (11431, 1)\n",
      "WARNING:tensorflow:Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:14,427 WARNING Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:14,485 WARNING Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject40.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject41.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject41.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11515, 12, 1)\n",
      "y_test.shape:  (11515, 1)\n",
      "WARNING:tensorflow:Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:25,430 WARNING Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:25,490 WARNING Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject41.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject42.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject42.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11435, 12, 1)\n",
      "y_test.shape:  (11435, 1)\n",
      "WARNING:tensorflow:Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:36,222 WARNING Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:36,299 WARNING Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject42.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject43.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject43.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11396, 12, 1)\n",
      "y_test.shape:  (11396, 1)\n",
      "WARNING:tensorflow:Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:46,817 WARNING Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:46,873 WARNING Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject43.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject44.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject44.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10069, 12, 1)\n",
      "y_test.shape:  (10069, 1)\n",
      "WARNING:tensorflow:Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:57,501 WARNING Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:39:57,562 WARNING Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject44.csv\n",
      "2025-01-19 20:40:06,141 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 131\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 159\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (386404, 12, 1)\n",
      "y_train.shape:  (386404, 1)\n",
      "x_valid.shape:  (96580, 12, 1)\n",
      "y_valid.shape:  (96580, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:40:57,825 WARNING Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 20:40:57,876 WARNING Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 20:40:57,961 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 386404 samples, validate on 96580 samples\n",
      "Epoch 1/10000\n",
      "386404/386404 [==============================] - ETA: 0s - loss: 1.7649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386404/386404 [==============================] - 14s 36us/sample - loss: 1.7649 - val_loss: 0.2162\n",
      "Epoch 2/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.3341 - val_loss: 0.0854\n",
      "Epoch 3/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.1550 - val_loss: 0.0754\n",
      "Epoch 4/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.1299 - val_loss: 0.0720\n",
      "Epoch 5/10000\n",
      "386404/386404 [==============================] - 11s 30us/sample - loss: 0.1177 - val_loss: 0.0678\n",
      "Epoch 6/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.1106 - val_loss: 0.0662\n",
      "Epoch 7/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.1040 - val_loss: 0.0677\n",
      "Epoch 8/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0988 - val_loss: 0.0663\n",
      "Epoch 9/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0947 - val_loss: 0.0664\n",
      "Epoch 10/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0927 - val_loss: 0.0698\n",
      "Epoch 11/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0910 - val_loss: 0.0688\n",
      "Epoch 12/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0902 - val_loss: 0.0681\n",
      "Epoch 13/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0893 - val_loss: 0.0711\n",
      "Epoch 14/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0893 - val_loss: 0.0696\n",
      "Epoch 15/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0883 - val_loss: 0.0672\n",
      "Epoch 16/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0879 - val_loss: 0.0679\n",
      "Epoch 17/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0867 - val_loss: 0.0671\n",
      "Epoch 18/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0860 - val_loss: 0.0667\n",
      "Epoch 19/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0861 - val_loss: 0.0669\n",
      "Epoch 20/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0854 - val_loss: 0.0703\n",
      "Epoch 21/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0857 - val_loss: 0.0682\n",
      "Epoch 22/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0856 - val_loss: 0.0684\n",
      "Epoch 23/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0853 - val_loss: 0.0671\n",
      "Epoch 24/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0854 - val_loss: 0.0668\n",
      "Epoch 25/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0852 - val_loss: 0.0668\n",
      "Epoch 26/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0851 - val_loss: 0.0664\n",
      "Epoch 27/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0849 - val_loss: 0.0672\n",
      "Epoch 28/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0850 - val_loss: 0.0679\n",
      "Epoch 29/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0854 - val_loss: 0.0669\n",
      "Epoch 30/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0844 - val_loss: 0.0661\n",
      "Epoch 31/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0849 - val_loss: 0.0684\n",
      "Epoch 32/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0845 - val_loss: 0.0666\n",
      "Epoch 33/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0845 - val_loss: 0.0671\n",
      "Epoch 34/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0846 - val_loss: 0.0663\n",
      "Epoch 35/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0849 - val_loss: 0.0664\n",
      "Epoch 36/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0847 - val_loss: 0.0665\n",
      "Epoch 37/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0848 - val_loss: 0.0655\n",
      "Epoch 38/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0846 - val_loss: 0.0665\n",
      "Epoch 39/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0846 - val_loss: 0.0661\n",
      "Epoch 40/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0843 - val_loss: 0.0665\n",
      "Epoch 41/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0845 - val_loss: 0.0655\n",
      "Epoch 42/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0845 - val_loss: 0.0668\n",
      "Epoch 43/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0846 - val_loss: 0.0664\n",
      "Epoch 44/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0845 - val_loss: 0.0658\n",
      "Epoch 45/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0842 - val_loss: 0.0658\n",
      "Epoch 46/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0841 - val_loss: 0.0656\n",
      "Epoch 47/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0843 - val_loss: 0.0659\n",
      "Epoch 48/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0841 - val_loss: 0.0678\n",
      "Epoch 49/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0846 - val_loss: 0.0663\n",
      "Epoch 50/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0846 - val_loss: 0.0662\n",
      "Epoch 51/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0844 - val_loss: 0.0660\n",
      "Epoch 52/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0844 - val_loss: 0.0654\n",
      "Epoch 53/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0840 - val_loss: 0.0660\n",
      "Epoch 54/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0843 - val_loss: 0.0659\n",
      "Epoch 55/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0841 - val_loss: 0.0664\n",
      "Epoch 56/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0842 - val_loss: 0.0654\n",
      "Epoch 57/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0841 - val_loss: 0.0662\n",
      "Epoch 58/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0842 - val_loss: 0.0664\n",
      "Epoch 59/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0843 - val_loss: 0.0659\n",
      "Epoch 60/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0840 - val_loss: 0.0660\n",
      "Epoch 61/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0841 - val_loss: 0.0655\n",
      "Epoch 62/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0841 - val_loss: 0.0663\n",
      "Epoch 63/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0840 - val_loss: 0.0658\n",
      "Epoch 64/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0841 - val_loss: 0.0665\n",
      "Epoch 65/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0839 - val_loss: 0.0659\n",
      "Epoch 66/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0843 - val_loss: 0.0653\n",
      "Epoch 67/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0842 - val_loss: 0.0653\n",
      "Epoch 68/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0840 - val_loss: 0.0665\n",
      "Epoch 69/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0664\n",
      "Epoch 70/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0838 - val_loss: 0.0663\n",
      "Epoch 71/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0651\n",
      "Epoch 72/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0654\n",
      "Epoch 73/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0842 - val_loss: 0.0652\n",
      "Epoch 74/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0837 - val_loss: 0.0649\n",
      "Epoch 75/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0840 - val_loss: 0.0652\n",
      "Epoch 76/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0836 - val_loss: 0.0665\n",
      "Epoch 77/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0839 - val_loss: 0.0656\n",
      "Epoch 78/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0840 - val_loss: 0.0666\n",
      "Epoch 79/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0655\n",
      "Epoch 80/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0649\n",
      "Epoch 81/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0837 - val_loss: 0.0677\n",
      "Epoch 82/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0840 - val_loss: 0.0651\n",
      "Epoch 83/10000\n",
      "386404/386404 [==============================] - 8s 19us/sample - loss: 0.0834 - val_loss: 0.0662\n",
      "Epoch 84/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0836 - val_loss: 0.0658\n",
      "Epoch 85/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0839 - val_loss: 0.0672\n",
      "Epoch 86/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0839 - val_loss: 0.0649\n",
      "Epoch 87/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0654\n",
      "Epoch 88/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0660\n",
      "Epoch 89/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0841 - val_loss: 0.0659\n",
      "Epoch 90/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0837 - val_loss: 0.0654\n",
      "Epoch 91/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0655\n",
      "Epoch 92/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0838 - val_loss: 0.0663\n",
      "Epoch 93/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0656\n",
      "Epoch 94/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0653\n",
      "Epoch 95/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0837 - val_loss: 0.0656\n",
      "Epoch 96/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0666\n",
      "Epoch 97/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0657\n",
      "Epoch 98/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0836 - val_loss: 0.0659\n",
      "Epoch 99/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0656\n",
      "Epoch 100/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0660\n",
      "Epoch 101/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0838 - val_loss: 0.0669\n",
      "Epoch 102/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 103/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0659\n",
      "Epoch 104/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 105/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0662\n",
      "Epoch 106/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0653\n",
      "Epoch 107/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0651\n",
      "Epoch 108/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0837 - val_loss: 0.0657\n",
      "Epoch 109/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0651\n",
      "Epoch 110/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0655\n",
      "Epoch 111/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0836 - val_loss: 0.0662\n",
      "Epoch 112/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0656\n",
      "Epoch 113/10000\n",
      "386404/386404 [==============================] - 7s 19us/sample - loss: 0.0836 - val_loss: 0.0649\n",
      "Epoch 114/10000\n",
      "386404/386404 [==============================] - 7s 19us/sample - loss: 0.0836 - val_loss: 0.0653\n",
      "Epoch 115/10000\n",
      "386404/386404 [==============================] - 7s 19us/sample - loss: 0.0835 - val_loss: 0.0657\n",
      "Epoch 116/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0653\n",
      "Epoch 117/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0836 - val_loss: 0.0651\n",
      "Epoch 118/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0836 - val_loss: 0.0648\n",
      "Epoch 119/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0839 - val_loss: 0.0651\n",
      "Epoch 120/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0652\n",
      "Epoch 121/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0652\n",
      "Epoch 122/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0652\n",
      "Epoch 123/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0838 - val_loss: 0.0665\n",
      "Epoch 124/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0659\n",
      "Epoch 125/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0654\n",
      "Epoch 126/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0657\n",
      "Epoch 127/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0659\n",
      "Epoch 128/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0648\n",
      "Epoch 129/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0651\n",
      "Epoch 130/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0652\n",
      "Epoch 131/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0650\n",
      "Epoch 132/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0648\n",
      "Epoch 133/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0657\n",
      "Epoch 134/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0652\n",
      "Epoch 135/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0649\n",
      "Epoch 136/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 137/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0834 - val_loss: 0.0672\n",
      "Epoch 138/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0649\n",
      "Epoch 139/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0839 - val_loss: 0.0656\n",
      "Epoch 140/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0655\n",
      "Epoch 141/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0834 - val_loss: 0.0658\n",
      "Epoch 142/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0661\n",
      "Epoch 143/10000\n",
      "386404/386404 [==============================] - 7s 19us/sample - loss: 0.0835 - val_loss: 0.0653\n",
      "Epoch 144/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0834 - val_loss: 0.0650\n",
      "Epoch 145/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0650\n",
      "Epoch 146/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0837 - val_loss: 0.0679\n",
      "Epoch 147/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0654\n",
      "Epoch 148/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0650\n",
      "Epoch 149/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0837 - val_loss: 0.0652\n",
      "Epoch 150/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 151/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0834 - val_loss: 0.0649\n",
      "Epoch 152/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0833 - val_loss: 0.0658\n",
      "Epoch 153/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0836 - val_loss: 0.0661\n",
      "Epoch 154/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0657\n",
      "Epoch 155/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0651\n",
      "Epoch 156/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0652\n",
      "Epoch 157/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0657\n",
      "Epoch 158/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0665\n",
      "Epoch 159/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0660\n",
      "Epoch 160/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 161/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0835 - val_loss: 0.0655\n",
      "Epoch 162/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0650\n",
      "Epoch 163/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0662\n",
      "Epoch 164/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0659\n",
      "Epoch 165/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0666\n",
      "Epoch 166/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0657\n",
      "Epoch 167/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0657\n",
      "Epoch 168/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0662\n",
      "Epoch 169/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0650\n",
      "Epoch 170/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0661\n",
      "Epoch 171/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0649\n",
      "Epoch 172/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0652\n",
      "Epoch 173/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0662\n",
      "Epoch 174/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0657\n",
      "Epoch 175/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0657\n",
      "Epoch 176/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0655\n",
      "Epoch 177/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0835 - val_loss: 0.0656\n",
      "Epoch 178/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0659\n",
      "Epoch 179/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0664\n",
      "Epoch 180/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0653\n",
      "Epoch 181/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0656\n",
      "Epoch 182/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0832 - val_loss: 0.0654\n",
      "Epoch 183/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0831 - val_loss: 0.0656\n",
      "Epoch 184/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0653\n",
      "Epoch 185/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0655\n",
      "Epoch 186/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0651\n",
      "Epoch 187/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0649\n",
      "Epoch 188/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0648\n",
      "Epoch 189/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0648\n",
      "Epoch 190/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0653\n",
      "Epoch 191/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0659\n",
      "Epoch 192/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0653\n",
      "Epoch 193/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0653\n",
      "Epoch 194/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0657\n",
      "Epoch 195/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0668\n",
      "Epoch 196/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0650\n",
      "Epoch 197/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0664\n",
      "Epoch 198/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0661\n",
      "Epoch 199/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0829 - val_loss: 0.0655\n",
      "Epoch 200/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0832 - val_loss: 0.0664\n",
      "Epoch 201/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0657\n",
      "Epoch 202/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0655\n",
      "Epoch 203/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0657\n",
      "Epoch 204/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0829 - val_loss: 0.0663\n",
      "Epoch 205/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0826 - val_loss: 0.0652\n",
      "Epoch 206/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0667\n",
      "Epoch 207/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0657\n",
      "Epoch 208/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0659\n",
      "Epoch 209/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0660\n",
      "Epoch 210/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0658\n",
      "Epoch 211/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0660\n",
      "Epoch 212/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0649\n",
      "Epoch 213/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0828 - val_loss: 0.0653\n",
      "Epoch 214/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0659\n",
      "Epoch 215/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0834 - val_loss: 0.0650\n",
      "Epoch 216/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0650\n",
      "Epoch 217/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0651\n",
      "Epoch 218/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0649\n",
      "Epoch 219/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0657\n",
      "Epoch 220/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0660\n",
      "Epoch 221/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0655\n",
      "Epoch 222/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0651\n",
      "Epoch 223/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0657\n",
      "Epoch 224/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0684\n",
      "Epoch 225/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0649\n",
      "Epoch 226/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0833 - val_loss: 0.0659\n",
      "Epoch 227/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0829 - val_loss: 0.0655\n",
      "Epoch 228/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0665\n",
      "Epoch 229/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0655\n",
      "Epoch 230/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0652\n",
      "Epoch 231/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0650\n",
      "Epoch 232/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0827 - val_loss: 0.0660\n",
      "Epoch 233/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0650\n",
      "Epoch 234/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0827 - val_loss: 0.0652\n",
      "Epoch 235/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0829 - val_loss: 0.0653\n",
      "Epoch 236/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0827 - val_loss: 0.0660\n",
      "Epoch 237/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0833 - val_loss: 0.0655\n",
      "Epoch 238/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0654\n",
      "Epoch 239/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0650\n",
      "Epoch 240/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0653\n",
      "Epoch 241/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0651\n",
      "Epoch 242/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0662\n",
      "Epoch 243/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0829 - val_loss: 0.0654\n",
      "Epoch 244/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0650\n",
      "Epoch 245/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0832 - val_loss: 0.0655\n",
      "Epoch 246/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0660\n",
      "Epoch 247/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0828 - val_loss: 0.0653\n",
      "Epoch 248/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0650\n",
      "Epoch 249/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0825 - val_loss: 0.0663\n",
      "Epoch 250/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0830 - val_loss: 0.0668\n",
      "Epoch 251/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0654\n",
      "Epoch 252/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0653\n",
      "Epoch 253/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0826 - val_loss: 0.0661\n",
      "Epoch 254/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0656\n",
      "Epoch 255/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0650\n",
      "Epoch 256/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0831 - val_loss: 0.0653\n",
      "Epoch 257/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0829 - val_loss: 0.0658\n",
      "Epoch 258/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0654\n",
      "Epoch 259/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0650\n",
      "Epoch 260/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0656\n",
      "Epoch 261/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0663\n",
      "Epoch 262/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0830 - val_loss: 0.0650\n",
      "Epoch 263/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0654\n",
      "Epoch 264/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0652\n",
      "Epoch 265/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0825 - val_loss: 0.0652\n",
      "Epoch 266/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0831 - val_loss: 0.0652\n",
      "Epoch 267/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0656\n",
      "Epoch 268/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0675\n",
      "Epoch 269/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0655\n",
      "Epoch 270/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0651\n",
      "Epoch 271/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0653\n",
      "Epoch 272/10000\n",
      "386404/386404 [==============================] - 8s 22us/sample - loss: 0.0831 - val_loss: 0.0664\n",
      "Epoch 273/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0670\n",
      "Epoch 274/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0651\n",
      "Epoch 275/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0828 - val_loss: 0.0664\n",
      "Epoch 276/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0663\n",
      "Epoch 277/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0652\n",
      "Epoch 278/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0672\n",
      "Epoch 279/10000\n",
      "386404/386404 [==============================] - 8s 20us/sample - loss: 0.0825 - val_loss: 0.0657\n",
      "Epoch 280/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0828 - val_loss: 0.0659\n",
      "Epoch 281/10000\n",
      "386404/386404 [==============================] - 9s 22us/sample - loss: 0.0828 - val_loss: 0.0649\n",
      "Epoch 282/10000\n",
      "386404/386404 [==============================] - 9s 23us/sample - loss: 0.0828 - val_loss: 0.0657\n",
      "Epoch 283/10000\n",
      "386404/386404 [==============================] - 9s 22us/sample - loss: 0.0826 - val_loss: 0.0650\n",
      "Epoch 284/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0661\n",
      "Epoch 285/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0829 - val_loss: 0.0657\n",
      "Epoch 286/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0827 - val_loss: 0.0655\n",
      "Epoch 287/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0825 - val_loss: 0.0651\n",
      "Epoch 288/10000\n",
      "386404/386404 [==============================] - 8s 21us/sample - loss: 0.0826 - val_loss: 0.0655\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject45.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject45.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11105, 12, 1)\n",
      "y_test.shape:  (11105, 1)\n",
      "WARNING:tensorflow:Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:26,483 WARNING Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:26,543 WARNING Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject45.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject46.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject46.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11395, 12, 1)\n",
      "y_test.shape:  (11395, 1)\n",
      "WARNING:tensorflow:Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:37,553 WARNING Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:37,620 WARNING Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject46.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject47.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject47.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11655, 12, 1)\n",
      "y_test.shape:  (11655, 1)\n",
      "WARNING:tensorflow:Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:48,744 WARNING Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:19:48,802 WARNING Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject47.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject48.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject48.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11726, 12, 1)\n",
      "y_test.shape:  (11726, 1)\n",
      "WARNING:tensorflow:Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:00,166 WARNING Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:00,212 WARNING Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject48.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject49.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject49.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10707, 12, 1)\n",
      "y_test.shape:  (10707, 1)\n",
      "WARNING:tensorflow:Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:11,005 WARNING Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:11,060 WARNING Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject49.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject50.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject50.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5496, 12, 1)\n",
      "y_test.shape:  (5496, 1)\n",
      "WARNING:tensorflow:Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:21,300 WARNING Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:21,355 WARNING Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject50.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject51.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject51.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8813, 12, 1)\n",
      "y_test.shape:  (8813, 1)\n",
      "WARNING:tensorflow:Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:29,965 WARNING Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:30,015 WARNING Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject51.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject53.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject53.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8847, 12, 1)\n",
      "y_test.shape:  (8847, 1)\n",
      "WARNING:tensorflow:Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:39,750 WARNING Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:39,808 WARNING Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject53.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject54.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject54.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7913, 12, 1)\n",
      "y_test.shape:  (7913, 1)\n",
      "WARNING:tensorflow:Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:49,478 WARNING Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:20:49,532 WARNING Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject54.csv\n",
      "2025-01-19 21:20:58,137 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\all does not exist.\n",
      "2025-01-19 21:20:58,137 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 47\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 88\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (362495, 18, 1)\n",
      "y_train.shape:  (362495, 1)\n",
      "x_valid.shape:  (90606, 18, 1)\n",
      "y_valid.shape:  (90606, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:21:47,442 WARNING Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 21:21:47,490 WARNING Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 21:21:47,576 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 362495 samples, validate on 90606 samples\n",
      "Epoch 1/10000\n",
      "361472/362495 [============================>.] - ETA: 0s - loss: 0.6710"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362495/362495 [==============================] - 17s 46us/sample - loss: 0.6697 - val_loss: 0.1050\n",
      "Epoch 2/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.2066 - val_loss: 0.0819\n",
      "Epoch 3/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.1719 - val_loss: 0.0739\n",
      "Epoch 4/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.1558 - val_loss: 0.0681\n",
      "Epoch 5/10000\n",
      "362495/362495 [==============================] - 14s 38us/sample - loss: 0.1405 - val_loss: 0.0647\n",
      "Epoch 6/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.1295 - val_loss: 0.0639\n",
      "Epoch 7/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.1198 - val_loss: 0.0653\n",
      "Epoch 8/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.1115 - val_loss: 0.0634\n",
      "Epoch 9/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.1042 - val_loss: 0.0647\n",
      "Epoch 10/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0975 - val_loss: 0.0641\n",
      "Epoch 11/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0934 - val_loss: 0.0645\n",
      "Epoch 12/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0918 - val_loss: 0.0665\n",
      "Epoch 13/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0919 - val_loss: 0.0662\n",
      "Epoch 14/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0918 - val_loss: 0.0673\n",
      "Epoch 15/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0912 - val_loss: 0.0661\n",
      "Epoch 16/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0909 - val_loss: 0.0700\n",
      "Epoch 17/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0912 - val_loss: 0.0685\n",
      "Epoch 18/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0912 - val_loss: 0.0668\n",
      "Epoch 19/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0909 - val_loss: 0.0682\n",
      "Epoch 20/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0916 - val_loss: 0.0683\n",
      "Epoch 21/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0907 - val_loss: 0.0658\n",
      "Epoch 22/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0911 - val_loss: 0.0661\n",
      "Epoch 23/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0911 - val_loss: 0.0662\n",
      "Epoch 24/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0905 - val_loss: 0.0700\n",
      "Epoch 25/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0911 - val_loss: 0.0656\n",
      "Epoch 26/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0907 - val_loss: 0.0686\n",
      "Epoch 27/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0907 - val_loss: 0.0667\n",
      "Epoch 28/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0905 - val_loss: 0.0659\n",
      "Epoch 29/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0907 - val_loss: 0.0666\n",
      "Epoch 30/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0908 - val_loss: 0.0688\n",
      "Epoch 31/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0905 - val_loss: 0.0666\n",
      "Epoch 32/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0905 - val_loss: 0.0673\n",
      "Epoch 33/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0900 - val_loss: 0.0670\n",
      "Epoch 34/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0904 - val_loss: 0.0660\n",
      "Epoch 35/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0904 - val_loss: 0.0658\n",
      "Epoch 36/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0904 - val_loss: 0.0655\n",
      "Epoch 37/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0905 - val_loss: 0.0652\n",
      "Epoch 38/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0907 - val_loss: 0.0653\n",
      "Epoch 39/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0905 - val_loss: 0.0655\n",
      "Epoch 40/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0900 - val_loss: 0.0653\n",
      "Epoch 41/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0902 - val_loss: 0.0659\n",
      "Epoch 42/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0900 - val_loss: 0.0665\n",
      "Epoch 43/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0899 - val_loss: 0.0650\n",
      "Epoch 44/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0900 - val_loss: 0.0661\n",
      "Epoch 45/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0901 - val_loss: 0.0655\n",
      "Epoch 46/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0897 - val_loss: 0.0653\n",
      "Epoch 47/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0902 - val_loss: 0.0658\n",
      "Epoch 48/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0899 - val_loss: 0.0675\n",
      "Epoch 49/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0899 - val_loss: 0.0671\n",
      "Epoch 50/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0900 - val_loss: 0.0666\n",
      "Epoch 51/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0899 - val_loss: 0.0653\n",
      "Epoch 52/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0899 - val_loss: 0.0673\n",
      "Epoch 53/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0899 - val_loss: 0.0651\n",
      "Epoch 54/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0896 - val_loss: 0.0666\n",
      "Epoch 55/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0897 - val_loss: 0.0650\n",
      "Epoch 56/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0896 - val_loss: 0.0647\n",
      "Epoch 57/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0894 - val_loss: 0.0658\n",
      "Epoch 58/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0898 - val_loss: 0.0669\n",
      "Epoch 59/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0895 - val_loss: 0.0654\n",
      "Epoch 60/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0896 - val_loss: 0.0649\n",
      "Epoch 61/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0898 - val_loss: 0.0652\n",
      "Epoch 62/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0899 - val_loss: 0.0653\n",
      "Epoch 63/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0898 - val_loss: 0.0656\n",
      "Epoch 64/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0896 - val_loss: 0.0649\n",
      "Epoch 65/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0896 - val_loss: 0.0658\n",
      "Epoch 66/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0897 - val_loss: 0.0659\n",
      "Epoch 67/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0895 - val_loss: 0.0652\n",
      "Epoch 68/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0892 - val_loss: 0.0656\n",
      "Epoch 69/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0896 - val_loss: 0.0647\n",
      "Epoch 70/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0894 - val_loss: 0.0658\n",
      "Epoch 71/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0890 - val_loss: 0.0651\n",
      "Epoch 72/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0891 - val_loss: 0.0651\n",
      "Epoch 73/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0894 - val_loss: 0.0670\n",
      "Epoch 74/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0895 - val_loss: 0.0650\n",
      "Epoch 75/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0890 - val_loss: 0.0654\n",
      "Epoch 76/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0889 - val_loss: 0.0670\n",
      "Epoch 77/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0894 - val_loss: 0.0649\n",
      "Epoch 78/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0899 - val_loss: 0.0648\n",
      "Epoch 79/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0892 - val_loss: 0.0652\n",
      "Epoch 80/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0892 - val_loss: 0.0664\n",
      "Epoch 81/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0892 - val_loss: 0.0642\n",
      "Epoch 82/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0828 - val_loss: 0.0612\n",
      "Epoch 83/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0789 - val_loss: 0.0620\n",
      "Epoch 84/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0781 - val_loss: 0.0617\n",
      "Epoch 85/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0781 - val_loss: 0.0620\n",
      "Epoch 86/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0778 - val_loss: 0.0613\n",
      "Epoch 87/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0777 - val_loss: 0.0622\n",
      "Epoch 88/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0611\n",
      "Epoch 89/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0781 - val_loss: 0.0622\n",
      "Epoch 90/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0778 - val_loss: 0.0614\n",
      "Epoch 91/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0779 - val_loss: 0.0620\n",
      "Epoch 92/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0777 - val_loss: 0.0608\n",
      "Epoch 93/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0777 - val_loss: 0.0616\n",
      "Epoch 94/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0776 - val_loss: 0.0622\n",
      "Epoch 95/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0776 - val_loss: 0.0610\n",
      "Epoch 96/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0776 - val_loss: 0.0619\n",
      "Epoch 97/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0777 - val_loss: 0.0629\n",
      "Epoch 98/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0778 - val_loss: 0.0620\n",
      "Epoch 99/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0778 - val_loss: 0.0612\n",
      "Epoch 100/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0608\n",
      "Epoch 101/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0777 - val_loss: 0.0608\n",
      "Epoch 102/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0773 - val_loss: 0.0610\n",
      "Epoch 103/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0777 - val_loss: 0.0614\n",
      "Epoch 104/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0775 - val_loss: 0.0621\n",
      "Epoch 105/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0778 - val_loss: 0.0606\n",
      "Epoch 106/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0775 - val_loss: 0.0614\n",
      "Epoch 107/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0777 - val_loss: 0.0612\n",
      "Epoch 108/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0775 - val_loss: 0.0607\n",
      "Epoch 109/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0776 - val_loss: 0.0609\n",
      "Epoch 110/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0775 - val_loss: 0.0622\n",
      "Epoch 111/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0775 - val_loss: 0.0610\n",
      "Epoch 112/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0777 - val_loss: 0.0620\n",
      "Epoch 113/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0630\n",
      "Epoch 114/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0773 - val_loss: 0.0612\n",
      "Epoch 115/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0773 - val_loss: 0.0609\n",
      "Epoch 116/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0775 - val_loss: 0.0623\n",
      "Epoch 117/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0776 - val_loss: 0.0613\n",
      "Epoch 118/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0778 - val_loss: 0.0622\n",
      "Epoch 119/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0777 - val_loss: 0.0615\n",
      "Epoch 120/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0776 - val_loss: 0.0614\n",
      "Epoch 121/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0616\n",
      "Epoch 122/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0773 - val_loss: 0.0612\n",
      "Epoch 123/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0772 - val_loss: 0.0615\n",
      "Epoch 124/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0620\n",
      "Epoch 125/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0771 - val_loss: 0.0616\n",
      "Epoch 126/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0772 - val_loss: 0.0613\n",
      "Epoch 127/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0615\n",
      "Epoch 128/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0773 - val_loss: 0.0626\n",
      "Epoch 129/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0772 - val_loss: 0.0614\n",
      "Epoch 130/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0776 - val_loss: 0.0616\n",
      "Epoch 131/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0778 - val_loss: 0.0620\n",
      "Epoch 132/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0772 - val_loss: 0.0612\n",
      "Epoch 133/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0777 - val_loss: 0.0610\n",
      "Epoch 134/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0771 - val_loss: 0.0612\n",
      "Epoch 135/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0769 - val_loss: 0.0608\n",
      "Epoch 136/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0775 - val_loss: 0.0609\n",
      "Epoch 137/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0605\n",
      "Epoch 138/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0773 - val_loss: 0.0616\n",
      "Epoch 139/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0774 - val_loss: 0.0614\n",
      "Epoch 140/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0773 - val_loss: 0.0609\n",
      "Epoch 141/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0771 - val_loss: 0.0619\n",
      "Epoch 142/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0615\n",
      "Epoch 143/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0606\n",
      "Epoch 144/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0776 - val_loss: 0.0614\n",
      "Epoch 145/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0610\n",
      "Epoch 146/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0616\n",
      "Epoch 147/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0615\n",
      "Epoch 148/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0607\n",
      "Epoch 149/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0770 - val_loss: 0.0610\n",
      "Epoch 150/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0773 - val_loss: 0.0609\n",
      "Epoch 151/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0772 - val_loss: 0.0615\n",
      "Epoch 152/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0770 - val_loss: 0.0609\n",
      "Epoch 153/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0770 - val_loss: 0.0615\n",
      "Epoch 154/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0772 - val_loss: 0.0608\n",
      "Epoch 155/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0767 - val_loss: 0.0616\n",
      "Epoch 156/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0615\n",
      "Epoch 157/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0614\n",
      "Epoch 158/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0771 - val_loss: 0.0613\n",
      "Epoch 159/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0769 - val_loss: 0.0612\n",
      "Epoch 160/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0773 - val_loss: 0.0614\n",
      "Epoch 161/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0622\n",
      "Epoch 162/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0771 - val_loss: 0.0617\n",
      "Epoch 163/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 164/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0606\n",
      "Epoch 165/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0768 - val_loss: 0.0616\n",
      "Epoch 166/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0613\n",
      "Epoch 167/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0769 - val_loss: 0.0606\n",
      "Epoch 168/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0611\n",
      "Epoch 169/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0771 - val_loss: 0.0613\n",
      "Epoch 170/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0769 - val_loss: 0.0618\n",
      "Epoch 171/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0607\n",
      "Epoch 172/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0617\n",
      "Epoch 173/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0606\n",
      "Epoch 174/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0771 - val_loss: 0.0606\n",
      "Epoch 175/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0772 - val_loss: 0.0611\n",
      "Epoch 176/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0767 - val_loss: 0.0612\n",
      "Epoch 177/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0769 - val_loss: 0.0611\n",
      "Epoch 178/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0617\n",
      "Epoch 179/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0769 - val_loss: 0.0607\n",
      "Epoch 180/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0769 - val_loss: 0.0610\n",
      "Epoch 181/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0769 - val_loss: 0.0611\n",
      "Epoch 182/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0771 - val_loss: 0.0612\n",
      "Epoch 183/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0768 - val_loss: 0.0604\n",
      "Epoch 184/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0768 - val_loss: 0.0610\n",
      "Epoch 185/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0768 - val_loss: 0.0611\n",
      "Epoch 186/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0618\n",
      "Epoch 187/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0615\n",
      "Epoch 188/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0626\n",
      "Epoch 189/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0610\n",
      "Epoch 190/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0614\n",
      "Epoch 191/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0765 - val_loss: 0.0611\n",
      "Epoch 192/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0762 - val_loss: 0.0611\n",
      "Epoch 193/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0614\n",
      "Epoch 194/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0613\n",
      "Epoch 195/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0610\n",
      "Epoch 196/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0610\n",
      "Epoch 197/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0610\n",
      "Epoch 198/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0611\n",
      "Epoch 199/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0766 - val_loss: 0.0614\n",
      "Epoch 200/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0765 - val_loss: 0.0611\n",
      "Epoch 201/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0605\n",
      "Epoch 202/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0767 - val_loss: 0.0607\n",
      "Epoch 203/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0607\n",
      "Epoch 204/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0617\n",
      "Epoch 205/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0768 - val_loss: 0.0611\n",
      "Epoch 206/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0622\n",
      "Epoch 207/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0615\n",
      "Epoch 208/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0766 - val_loss: 0.0613\n",
      "Epoch 209/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0610\n",
      "Epoch 210/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0765 - val_loss: 0.0612\n",
      "Epoch 211/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0766 - val_loss: 0.0610\n",
      "Epoch 212/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0770 - val_loss: 0.0610\n",
      "Epoch 213/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0619\n",
      "Epoch 214/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0767 - val_loss: 0.0612\n",
      "Epoch 215/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0766 - val_loss: 0.0615\n",
      "Epoch 216/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0610\n",
      "Epoch 217/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0764 - val_loss: 0.0612\n",
      "Epoch 218/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0764 - val_loss: 0.0614\n",
      "Epoch 219/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0612\n",
      "Epoch 220/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0771 - val_loss: 0.0610\n",
      "Epoch 221/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0766 - val_loss: 0.0616\n",
      "Epoch 222/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0769 - val_loss: 0.0615\n",
      "Epoch 223/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0763 - val_loss: 0.0613\n",
      "Epoch 224/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0768 - val_loss: 0.0631\n",
      "Epoch 225/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0769 - val_loss: 0.0616\n",
      "Epoch 226/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0764 - val_loss: 0.0614\n",
      "Epoch 227/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0617\n",
      "Epoch 228/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0761 - val_loss: 0.0610\n",
      "Epoch 229/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0767 - val_loss: 0.0619\n",
      "Epoch 230/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0609\n",
      "Epoch 231/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0761 - val_loss: 0.0616\n",
      "Epoch 232/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0616\n",
      "Epoch 233/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0766 - val_loss: 0.0612\n",
      "Epoch 234/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0610\n",
      "Epoch 235/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0610\n",
      "Epoch 236/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0620\n",
      "Epoch 237/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0613\n",
      "Epoch 238/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0762 - val_loss: 0.0608\n",
      "Epoch 239/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0614\n",
      "Epoch 240/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0609\n",
      "Epoch 241/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0762 - val_loss: 0.0611\n",
      "Epoch 242/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0766 - val_loss: 0.0617\n",
      "Epoch 243/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0764 - val_loss: 0.0609\n",
      "Epoch 244/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0627\n",
      "Epoch 245/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0762 - val_loss: 0.0619\n",
      "Epoch 246/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0761 - val_loss: 0.0607\n",
      "Epoch 247/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0765 - val_loss: 0.0619\n",
      "Epoch 248/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0614\n",
      "Epoch 249/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0765 - val_loss: 0.0610\n",
      "Epoch 250/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0763 - val_loss: 0.0610\n",
      "Epoch 251/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0619\n",
      "Epoch 252/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0617\n",
      "Epoch 253/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0760 - val_loss: 0.0611\n",
      "Epoch 254/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0761 - val_loss: 0.0618\n",
      "Epoch 255/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 256/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0765 - val_loss: 0.0616\n",
      "Epoch 257/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0623\n",
      "Epoch 258/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0761 - val_loss: 0.0617\n",
      "Epoch 259/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0613\n",
      "Epoch 260/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0765 - val_loss: 0.0614\n",
      "Epoch 261/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0766 - val_loss: 0.0615\n",
      "Epoch 262/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0764 - val_loss: 0.0624\n",
      "Epoch 263/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0762 - val_loss: 0.0616\n",
      "Epoch 264/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0760 - val_loss: 0.0618\n",
      "Epoch 265/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0762 - val_loss: 0.0612\n",
      "Epoch 266/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0758 - val_loss: 0.0613\n",
      "Epoch 267/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0739 - val_loss: 0.0613\n",
      "Epoch 268/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0733 - val_loss: 0.0612\n",
      "Epoch 269/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0732 - val_loss: 0.0615\n",
      "Epoch 270/10000\n",
      "362495/362495 [==============================] - 10s 27us/sample - loss: 0.0730 - val_loss: 0.0608\n",
      "Epoch 271/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0729 - val_loss: 0.0613\n",
      "Epoch 272/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0730 - val_loss: 0.0613\n",
      "Epoch 273/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0729 - val_loss: 0.0614\n",
      "Epoch 274/10000\n",
      "362495/362495 [==============================] - 11s 30us/sample - loss: 0.0731 - val_loss: 0.0614\n",
      "Epoch 275/10000\n",
      "362495/362495 [==============================] - 11s 29us/sample - loss: 0.0728 - val_loss: 0.0609\n",
      "Epoch 276/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0729 - val_loss: 0.0609\n",
      "Epoch 277/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0728 - val_loss: 0.0617\n",
      "Epoch 278/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0728 - val_loss: 0.0618\n",
      "Epoch 279/10000\n",
      "362495/362495 [==============================] - 10s 28us/sample - loss: 0.0728 - val_loss: 0.0610\n",
      "Epoch 280/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0727 - val_loss: 0.0611\n",
      "Epoch 281/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0728 - val_loss: 0.0615\n",
      "Epoch 282/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0727 - val_loss: 0.0618\n",
      "Epoch 283/10000\n",
      "362495/362495 [==============================] - 10s 29us/sample - loss: 0.0728 - val_loss: 0.0619\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject10.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject10.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11709, 18, 1)\n",
      "y_test.shape:  (11709, 1)\n",
      "WARNING:tensorflow:Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:10:40,916 WARNING Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:10:40,974 WARNING Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject10.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject11.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11407, 18, 1)\n",
      "y_test.shape:  (11407, 1)\n",
      "WARNING:tensorflow:Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:10:54,226 WARNING Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:10:54,286 WARNING Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject1.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 119\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7024, 18, 1)\n",
      "y_test.shape:  (7024, 1)\n",
      "WARNING:tensorflow:Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:06,560 WARNING Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:06,620 WARNING Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject2.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject2.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 136\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5426, 18, 1)\n",
      "y_test.shape:  (5426, 1)\n",
      "WARNING:tensorflow:Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:16,653 WARNING Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:16,706 WARNING Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject2.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject3.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject3.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 118\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6240, 18, 1)\n",
      "y_test.shape:  (6240, 1)\n",
      "WARNING:tensorflow:Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:25,846 WARNING Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:25,899 WARNING Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject3.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject4.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject4.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11250, 18, 1)\n",
      "y_test.shape:  (11250, 1)\n",
      "WARNING:tensorflow:Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:36,102 WARNING Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:36,168 WARNING Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject4.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject5.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject5.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10225, 18, 1)\n",
      "y_test.shape:  (10225, 1)\n",
      "WARNING:tensorflow:Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:48,610 WARNING Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:11:48,665 WARNING Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject5.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject6.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject6.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11296, 18, 1)\n",
      "y_test.shape:  (11296, 1)\n",
      "WARNING:tensorflow:Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:00,635 WARNING Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:00,691 WARNING Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject6.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject7.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject7.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11245, 18, 1)\n",
      "y_test.shape:  (11245, 1)\n",
      "WARNING:tensorflow:Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:13,041 WARNING Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:13,101 WARNING Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject7.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject8.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject8.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11096, 18, 1)\n",
      "y_test.shape:  (11096, 1)\n",
      "WARNING:tensorflow:Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:25,337 WARNING Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:25,406 WARNING Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject8.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject9.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject9.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11625, 18, 1)\n",
      "y_test.shape:  (11625, 1)\n",
      "WARNING:tensorflow:Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:37,419 WARNING Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:12:37,474 WARNING Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject9.csv\n",
      "2025-01-19 22:12:48,847 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 119\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 136\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 47\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 118\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 88\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (348696, 18, 1)\n",
      "y_train.shape:  (348696, 1)\n",
      "x_valid.shape:  (87158, 18, 1)\n",
      "y_valid.shape:  (87158, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:13:37,263 WARNING Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:13:37,314 WARNING Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 22:13:37,400 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 348696 samples, validate on 87158 samples\n",
      "Epoch 1/10000\n",
      "348696/348696 [==============================] - ETA: 0s - loss: 1.1143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348696/348696 [==============================] - 17s 48us/sample - loss: 1.1143 - val_loss: 0.2025\n",
      "Epoch 2/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.2120 - val_loss: 0.0842\n",
      "Epoch 3/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.1667 - val_loss: 0.0733\n",
      "Epoch 4/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.1514 - val_loss: 0.0706\n",
      "Epoch 5/10000\n",
      "348696/348696 [==============================] - 14s 39us/sample - loss: 0.1391 - val_loss: 0.0694\n",
      "Epoch 6/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.1294 - val_loss: 0.0660\n",
      "Epoch 7/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.1205 - val_loss: 0.0680\n",
      "Epoch 8/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.1114 - val_loss: 0.0696\n",
      "Epoch 9/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.1038 - val_loss: 0.0645\n",
      "Epoch 10/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0984 - val_loss: 0.0646\n",
      "Epoch 11/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0932 - val_loss: 0.0662\n",
      "Epoch 12/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0899 - val_loss: 0.0674\n",
      "Epoch 13/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0873 - val_loss: 0.0668\n",
      "Epoch 14/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0859 - val_loss: 0.0666\n",
      "Epoch 15/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0850 - val_loss: 0.0661\n",
      "Epoch 16/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0841 - val_loss: 0.0658\n",
      "Epoch 17/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0673\n",
      "Epoch 18/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0790 - val_loss: 0.0639\n",
      "Epoch 19/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0775 - val_loss: 0.0647\n",
      "Epoch 20/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0760 - val_loss: 0.0651\n",
      "Epoch 21/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0741 - val_loss: 0.0637\n",
      "Epoch 22/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0650\n",
      "Epoch 23/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0730 - val_loss: 0.0636\n",
      "Epoch 24/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0731 - val_loss: 0.0652\n",
      "Epoch 25/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0728 - val_loss: 0.0632\n",
      "Epoch 26/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0726 - val_loss: 0.0628\n",
      "Epoch 27/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0724 - val_loss: 0.0637\n",
      "Epoch 28/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0723 - val_loss: 0.0632\n",
      "Epoch 29/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0722 - val_loss: 0.0627\n",
      "Epoch 30/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0723 - val_loss: 0.0620\n",
      "Epoch 31/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0720 - val_loss: 0.0630\n",
      "Epoch 32/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0723 - val_loss: 0.0625\n",
      "Epoch 33/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0720 - val_loss: 0.0624\n",
      "Epoch 34/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0722 - val_loss: 0.0622\n",
      "Epoch 35/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0719 - val_loss: 0.0624\n",
      "Epoch 36/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0720 - val_loss: 0.0626\n",
      "Epoch 37/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0719 - val_loss: 0.0626\n",
      "Epoch 38/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0717 - val_loss: 0.0624\n",
      "Epoch 39/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0716 - val_loss: 0.0624\n",
      "Epoch 40/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0717 - val_loss: 0.0624\n",
      "Epoch 41/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0714 - val_loss: 0.0621\n",
      "Epoch 42/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0715 - val_loss: 0.0617\n",
      "Epoch 43/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0716 - val_loss: 0.0622\n",
      "Epoch 44/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0714 - val_loss: 0.0621\n",
      "Epoch 45/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0715 - val_loss: 0.0640\n",
      "Epoch 46/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0714 - val_loss: 0.0616\n",
      "Epoch 47/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0714 - val_loss: 0.0623\n",
      "Epoch 48/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0713 - val_loss: 0.0621\n",
      "Epoch 49/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0714 - val_loss: 0.0622\n",
      "Epoch 50/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0714 - val_loss: 0.0617\n",
      "Epoch 51/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0716 - val_loss: 0.0619\n",
      "Epoch 52/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0715 - val_loss: 0.0630\n",
      "Epoch 53/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0711 - val_loss: 0.0624\n",
      "Epoch 54/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0713 - val_loss: 0.0616\n",
      "Epoch 55/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0711 - val_loss: 0.0639\n",
      "Epoch 56/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0711 - val_loss: 0.0617\n",
      "Epoch 57/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0710 - val_loss: 0.0615\n",
      "Epoch 58/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0711 - val_loss: 0.0623\n",
      "Epoch 59/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0700 - val_loss: 0.0619\n",
      "Epoch 60/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0697 - val_loss: 0.0616\n",
      "Epoch 61/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0697 - val_loss: 0.0620\n",
      "Epoch 62/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0694 - val_loss: 0.0615\n",
      "Epoch 63/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0696 - val_loss: 0.0622\n",
      "Epoch 64/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0695 - val_loss: 0.0621\n",
      "Epoch 65/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0695 - val_loss: 0.0616\n",
      "Epoch 66/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0697 - val_loss: 0.0632\n",
      "Epoch 67/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0695 - val_loss: 0.0623\n",
      "Epoch 68/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0696 - val_loss: 0.0623\n",
      "Epoch 69/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0694 - val_loss: 0.0619\n",
      "Epoch 70/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0696 - val_loss: 0.0623\n",
      "Epoch 71/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0693 - val_loss: 0.0620\n",
      "Epoch 72/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0693 - val_loss: 0.0615\n",
      "Epoch 73/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0692 - val_loss: 0.0622\n",
      "Epoch 74/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0617\n",
      "Epoch 75/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0693 - val_loss: 0.0621\n",
      "Epoch 76/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0693 - val_loss: 0.0615\n",
      "Epoch 77/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0694 - val_loss: 0.0624\n",
      "Epoch 78/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0615\n",
      "Epoch 79/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0693 - val_loss: 0.0614\n",
      "Epoch 80/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0695 - val_loss: 0.0625\n",
      "Epoch 81/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0622\n",
      "Epoch 82/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0694 - val_loss: 0.0614\n",
      "Epoch 83/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0692 - val_loss: 0.0614\n",
      "Epoch 84/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0624\n",
      "Epoch 85/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0617\n",
      "Epoch 86/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0692 - val_loss: 0.0613\n",
      "Epoch 87/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0691 - val_loss: 0.0617\n",
      "Epoch 88/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0692 - val_loss: 0.0616\n",
      "Epoch 89/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0691 - val_loss: 0.0614\n",
      "Epoch 90/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0693 - val_loss: 0.0619\n",
      "Epoch 91/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0689 - val_loss: 0.0617\n",
      "Epoch 92/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0690 - val_loss: 0.0621\n",
      "Epoch 93/10000\n",
      "348696/348696 [==============================] - 11s 30us/sample - loss: 0.0693 - val_loss: 0.0617\n",
      "Epoch 94/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0690 - val_loss: 0.0617\n",
      "Epoch 95/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0689 - val_loss: 0.0623\n",
      "Epoch 96/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0620\n",
      "Epoch 97/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0690 - val_loss: 0.0620\n",
      "Epoch 98/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0617\n",
      "Epoch 99/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0692 - val_loss: 0.0622\n",
      "Epoch 100/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0690 - val_loss: 0.0619\n",
      "Epoch 101/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0690 - val_loss: 0.0615\n",
      "Epoch 102/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0692 - val_loss: 0.0616\n",
      "Epoch 103/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0690 - val_loss: 0.0616\n",
      "Epoch 104/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0692 - val_loss: 0.0616\n",
      "Epoch 105/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0691 - val_loss: 0.0625\n",
      "Epoch 106/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0690 - val_loss: 0.0619\n",
      "Epoch 107/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0689 - val_loss: 0.0618\n",
      "Epoch 108/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0689 - val_loss: 0.0624\n",
      "Epoch 109/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0687 - val_loss: 0.0617\n",
      "Epoch 110/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0624\n",
      "Epoch 111/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0689 - val_loss: 0.0632\n",
      "Epoch 112/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0693 - val_loss: 0.0617\n",
      "Epoch 113/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0689 - val_loss: 0.0617\n",
      "Epoch 114/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0690 - val_loss: 0.0617\n",
      "Epoch 115/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0689 - val_loss: 0.0619\n",
      "Epoch 116/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0688 - val_loss: 0.0616\n",
      "Epoch 117/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0687 - val_loss: 0.0624\n",
      "Epoch 118/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0688 - val_loss: 0.0614\n",
      "Epoch 119/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0690 - val_loss: 0.0616\n",
      "Epoch 120/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0618\n",
      "Epoch 121/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0688 - val_loss: 0.0615\n",
      "Epoch 122/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0617\n",
      "Epoch 123/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0688 - val_loss: 0.0615\n",
      "Epoch 124/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0688 - val_loss: 0.0623\n",
      "Epoch 125/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0620\n",
      "Epoch 126/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0687 - val_loss: 0.0618\n",
      "Epoch 127/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0687 - val_loss: 0.0617\n",
      "Epoch 128/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0686 - val_loss: 0.0619\n",
      "Epoch 129/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0625\n",
      "Epoch 130/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0617\n",
      "Epoch 131/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0687 - val_loss: 0.0620\n",
      "Epoch 132/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0687 - val_loss: 0.0617\n",
      "Epoch 133/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0689 - val_loss: 0.0616\n",
      "Epoch 134/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0622\n",
      "Epoch 135/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0687 - val_loss: 0.0626\n",
      "Epoch 136/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0686 - val_loss: 0.0615\n",
      "Epoch 137/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0690 - val_loss: 0.0619\n",
      "Epoch 138/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0618\n",
      "Epoch 139/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0627\n",
      "Epoch 140/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0685 - val_loss: 0.0615\n",
      "Epoch 141/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0688 - val_loss: 0.0617\n",
      "Epoch 142/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0684 - val_loss: 0.0620\n",
      "Epoch 143/10000\n",
      "348696/348696 [==============================] - 10s 30us/sample - loss: 0.0687 - val_loss: 0.0617\n",
      "Epoch 144/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0616\n",
      "Epoch 145/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0685 - val_loss: 0.0617\n",
      "Epoch 146/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0688 - val_loss: 0.0619\n",
      "Epoch 147/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0632\n",
      "Epoch 148/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0620\n",
      "Epoch 149/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0685 - val_loss: 0.0616\n",
      "Epoch 150/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0685 - val_loss: 0.0616\n",
      "Epoch 151/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0624\n",
      "Epoch 152/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0685 - val_loss: 0.0621\n",
      "Epoch 153/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0622\n",
      "Epoch 154/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0686 - val_loss: 0.0620\n",
      "Epoch 155/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0617\n",
      "Epoch 156/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0621\n",
      "Epoch 157/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0685 - val_loss: 0.0615\n",
      "Epoch 158/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0685 - val_loss: 0.0620\n",
      "Epoch 159/10000\n",
      "348696/348696 [==============================] - 10s 27us/sample - loss: 0.0685 - val_loss: 0.0629\n",
      "Epoch 160/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0684 - val_loss: 0.0616\n",
      "Epoch 161/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0686 - val_loss: 0.0623\n",
      "Epoch 162/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0683 - val_loss: 0.0619\n",
      "Epoch 163/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0615\n",
      "Epoch 164/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0617\n",
      "Epoch 165/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0622\n",
      "Epoch 166/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0687 - val_loss: 0.0614\n",
      "Epoch 167/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0685 - val_loss: 0.0620\n",
      "Epoch 168/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0685 - val_loss: 0.0618\n",
      "Epoch 169/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0683 - val_loss: 0.0619\n",
      "Epoch 170/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0681 - val_loss: 0.0622\n",
      "Epoch 171/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0682 - val_loss: 0.0619\n",
      "Epoch 172/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0686 - val_loss: 0.0616\n",
      "Epoch 173/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0614\n",
      "Epoch 174/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0681 - val_loss: 0.0627\n",
      "Epoch 175/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0616\n",
      "Epoch 176/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0682 - val_loss: 0.0622\n",
      "Epoch 177/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0684 - val_loss: 0.0618\n",
      "Epoch 178/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0682 - val_loss: 0.0615\n",
      "Epoch 179/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0683 - val_loss: 0.0621\n",
      "Epoch 180/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0682 - val_loss: 0.0621\n",
      "Epoch 181/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0683 - val_loss: 0.0621\n",
      "Epoch 182/10000\n",
      "348696/348696 [==============================] - 10s 29us/sample - loss: 0.0683 - val_loss: 0.0617\n",
      "Epoch 183/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0681 - val_loss: 0.0627\n",
      "Epoch 184/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0680 - val_loss: 0.0620\n",
      "Epoch 185/10000\n",
      "348696/348696 [==============================] - 9s 27us/sample - loss: 0.0684 - val_loss: 0.0619\n",
      "Epoch 186/10000\n",
      "348696/348696 [==============================] - 10s 28us/sample - loss: 0.0679 - val_loss: 0.0617\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject12.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject12.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11660, 18, 1)\n",
      "y_test.shape:  (11660, 1)\n",
      "WARNING:tensorflow:Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:33,598 WARNING Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:33,650 WARNING Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject12.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject13.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject13.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10246, 18, 1)\n",
      "y_test.shape:  (10246, 1)\n",
      "WARNING:tensorflow:Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:46,425 WARNING Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:46,481 WARNING Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject13.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject14.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject14.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11227, 18, 1)\n",
      "y_test.shape:  (11227, 1)\n",
      "WARNING:tensorflow:Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:58,570 WARNING Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:44:58,625 WARNING Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject14.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject15.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject15.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11773, 18, 1)\n",
      "y_test.shape:  (11773, 1)\n",
      "WARNING:tensorflow:Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:12,011 WARNING Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:12,071 WARNING Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject15.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject16.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject16.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11568, 18, 1)\n",
      "y_test.shape:  (11568, 1)\n",
      "WARNING:tensorflow:Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:25,022 WARNING Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:25,096 WARNING Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject16.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject17.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject17.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11857, 18, 1)\n",
      "y_test.shape:  (11857, 1)\n",
      "WARNING:tensorflow:Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:39,387 WARNING Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:39,466 WARNING Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject17.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject18.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11663, 18, 1)\n",
      "y_test.shape:  (11663, 1)\n",
      "WARNING:tensorflow:Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:53,601 WARNING Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:45:53,679 WARNING Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject19.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject19.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11651, 18, 1)\n",
      "y_test.shape:  (11651, 1)\n",
      "WARNING:tensorflow:Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:06,525 WARNING Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:06,590 WARNING Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject19.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject20.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject20.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11203, 18, 1)\n",
      "y_test.shape:  (11203, 1)\n",
      "WARNING:tensorflow:Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:20,190 WARNING Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:20,252 WARNING Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject20.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject21.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject21.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11171, 18, 1)\n",
      "y_test.shape:  (11171, 1)\n",
      "WARNING:tensorflow:Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:33,703 WARNING Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:33,763 WARNING Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject21.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject22.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject22.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11771, 18, 1)\n",
      "y_test.shape:  (11771, 1)\n",
      "WARNING:tensorflow:Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:47,999 WARNING Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:46:48,067 WARNING Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject22.csv\n",
      "2025-01-19 22:47:01,229 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 119\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 136\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 118\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 88\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (354881, 18, 1)\n",
      "y_train.shape:  (354881, 1)\n",
      "x_valid.shape:  (88704, 18, 1)\n",
      "y_valid.shape:  (88704, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:47:49,914 WARNING Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 22:47:49,968 WARNING Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 22:47:50,043 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 354881 samples, validate on 88704 samples\n",
      "Epoch 1/10000\n",
      "353280/354881 [============================>.] - ETA: 0s - loss: 0.6790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354881/354881 [==============================] - 17s 48us/sample - loss: 0.6770 - val_loss: 0.1014\n",
      "Epoch 2/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.2081 - val_loss: 0.0859\n",
      "Epoch 3/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.1817 - val_loss: 0.0694\n",
      "Epoch 4/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.1620 - val_loss: 0.0706\n",
      "Epoch 5/10000\n",
      "354881/354881 [==============================] - 14s 41us/sample - loss: 0.1470 - val_loss: 0.0653\n",
      "Epoch 6/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.1330 - val_loss: 0.0652\n",
      "Epoch 7/10000\n",
      "354881/354881 [==============================] - 11s 31us/sample - loss: 0.1205 - val_loss: 0.0624\n",
      "Epoch 8/10000\n",
      "354881/354881 [==============================] - 11s 31us/sample - loss: 0.1116 - val_loss: 0.0653\n",
      "Epoch 9/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.1023 - val_loss: 0.0640\n",
      "Epoch 10/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0969 - val_loss: 0.0651\n",
      "Epoch 11/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0927 - val_loss: 0.0644\n",
      "Epoch 12/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0878 - val_loss: 0.0626\n",
      "Epoch 13/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0843 - val_loss: 0.0626\n",
      "Epoch 14/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0816 - val_loss: 0.0618\n",
      "Epoch 15/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0796 - val_loss: 0.0619\n",
      "Epoch 16/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0785 - val_loss: 0.0641\n",
      "Epoch 17/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0778 - val_loss: 0.0624\n",
      "Epoch 18/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0768 - val_loss: 0.0621\n",
      "Epoch 19/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0766 - val_loss: 0.0625\n",
      "Epoch 20/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0617\n",
      "Epoch 21/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0763 - val_loss: 0.0616\n",
      "Epoch 22/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0764 - val_loss: 0.0612\n",
      "Epoch 23/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0762 - val_loss: 0.0619\n",
      "Epoch 24/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0763 - val_loss: 0.0614\n",
      "Epoch 25/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0761 - val_loss: 0.0626\n",
      "Epoch 26/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0760 - val_loss: 0.0618\n",
      "Epoch 27/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0760 - val_loss: 0.0631\n",
      "Epoch 28/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0759 - val_loss: 0.0603\n",
      "Epoch 29/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0760 - val_loss: 0.0610\n",
      "Epoch 30/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0758 - val_loss: 0.0611\n",
      "Epoch 31/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0758 - val_loss: 0.0615\n",
      "Epoch 32/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0760 - val_loss: 0.0610\n",
      "Epoch 33/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0757 - val_loss: 0.0627\n",
      "Epoch 34/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0754 - val_loss: 0.0603\n",
      "Epoch 35/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0755 - val_loss: 0.0610\n",
      "Epoch 36/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0755 - val_loss: 0.0612\n",
      "Epoch 37/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0755 - val_loss: 0.0607\n",
      "Epoch 38/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0752 - val_loss: 0.0601\n",
      "Epoch 39/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0754 - val_loss: 0.0602\n",
      "Epoch 40/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0753 - val_loss: 0.0608\n",
      "Epoch 41/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0751 - val_loss: 0.0606\n",
      "Epoch 42/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0752 - val_loss: 0.0607\n",
      "Epoch 43/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0751 - val_loss: 0.0608\n",
      "Epoch 44/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0753 - val_loss: 0.0600\n",
      "Epoch 45/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0747 - val_loss: 0.0600\n",
      "Epoch 46/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0751 - val_loss: 0.0598\n",
      "Epoch 47/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0750 - val_loss: 0.0604\n",
      "Epoch 48/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0747 - val_loss: 0.0601\n",
      "Epoch 49/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0752 - val_loss: 0.0610\n",
      "Epoch 50/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0746 - val_loss: 0.0603\n",
      "Epoch 51/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0748 - val_loss: 0.0603\n",
      "Epoch 52/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0748 - val_loss: 0.0598\n",
      "Epoch 53/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0745 - val_loss: 0.0619\n",
      "Epoch 54/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0747 - val_loss: 0.0605\n",
      "Epoch 55/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0746 - val_loss: 0.0606\n",
      "Epoch 56/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0746 - val_loss: 0.0613\n",
      "Epoch 57/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0600\n",
      "Epoch 58/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0746 - val_loss: 0.0600\n",
      "Epoch 59/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0745 - val_loss: 0.0602\n",
      "Epoch 60/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0745 - val_loss: 0.0606\n",
      "Epoch 61/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0606\n",
      "Epoch 62/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0747 - val_loss: 0.0611\n",
      "Epoch 63/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0746 - val_loss: 0.0598\n",
      "Epoch 64/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0748 - val_loss: 0.0600\n",
      "Epoch 65/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0745 - val_loss: 0.0603\n",
      "Epoch 66/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0749 - val_loss: 0.0600\n",
      "Epoch 67/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0594\n",
      "Epoch 68/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0743 - val_loss: 0.0611\n",
      "Epoch 69/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0749 - val_loss: 0.0597\n",
      "Epoch 70/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0746 - val_loss: 0.0600\n",
      "Epoch 71/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0744 - val_loss: 0.0602\n",
      "Epoch 72/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0745 - val_loss: 0.0598\n",
      "Epoch 73/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0746 - val_loss: 0.0598\n",
      "Epoch 74/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0599\n",
      "Epoch 75/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0607\n",
      "Epoch 76/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0745 - val_loss: 0.0599\n",
      "Epoch 77/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0746 - val_loss: 0.0601\n",
      "Epoch 78/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0743 - val_loss: 0.0609\n",
      "Epoch 79/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0609\n",
      "Epoch 80/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0742 - val_loss: 0.0597\n",
      "Epoch 81/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0597\n",
      "Epoch 82/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0608\n",
      "Epoch 83/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0745 - val_loss: 0.0597\n",
      "Epoch 84/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0611\n",
      "Epoch 85/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0608\n",
      "Epoch 86/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0744 - val_loss: 0.0602\n",
      "Epoch 87/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0744 - val_loss: 0.0597\n",
      "Epoch 88/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0745 - val_loss: 0.0607\n",
      "Epoch 89/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0597\n",
      "Epoch 90/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0593\n",
      "Epoch 91/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0744 - val_loss: 0.0598\n",
      "Epoch 92/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0589\n",
      "Epoch 93/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0742 - val_loss: 0.0591\n",
      "Epoch 94/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0603\n",
      "Epoch 95/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0741 - val_loss: 0.0606\n",
      "Epoch 96/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0599\n",
      "Epoch 97/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0595\n",
      "Epoch 98/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0742 - val_loss: 0.0594\n",
      "Epoch 99/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0593\n",
      "Epoch 100/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0741 - val_loss: 0.0593\n",
      "Epoch 101/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0588\n",
      "Epoch 102/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0599\n",
      "Epoch 103/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0612\n",
      "Epoch 104/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0598\n",
      "Epoch 105/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0594\n",
      "Epoch 106/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0601\n",
      "Epoch 107/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0595\n",
      "Epoch 108/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0742 - val_loss: 0.0597\n",
      "Epoch 109/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0742 - val_loss: 0.0592\n",
      "Epoch 110/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0596\n",
      "Epoch 111/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0742 - val_loss: 0.0601\n",
      "Epoch 112/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0739 - val_loss: 0.0603\n",
      "Epoch 113/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0739 - val_loss: 0.0590\n",
      "Epoch 114/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0743 - val_loss: 0.0598\n",
      "Epoch 115/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0594\n",
      "Epoch 116/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0741 - val_loss: 0.0602\n",
      "Epoch 117/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0738 - val_loss: 0.0596\n",
      "Epoch 118/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0740 - val_loss: 0.0595\n",
      "Epoch 119/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0739 - val_loss: 0.0599\n",
      "Epoch 120/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0739 - val_loss: 0.0609\n",
      "Epoch 121/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0597\n",
      "Epoch 122/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0738 - val_loss: 0.0595\n",
      "Epoch 123/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0741 - val_loss: 0.0598\n",
      "Epoch 124/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0740 - val_loss: 0.0597\n",
      "Epoch 125/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0591\n",
      "Epoch 126/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0606\n",
      "Epoch 127/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0595\n",
      "Epoch 128/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0602\n",
      "Epoch 129/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0598\n",
      "Epoch 130/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0739 - val_loss: 0.0595\n",
      "Epoch 131/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0595\n",
      "Epoch 132/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0593\n",
      "Epoch 133/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0602\n",
      "Epoch 134/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0596\n",
      "Epoch 135/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0598\n",
      "Epoch 136/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0594\n",
      "Epoch 137/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0595\n",
      "Epoch 138/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0591\n",
      "Epoch 139/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0737 - val_loss: 0.0595\n",
      "Epoch 140/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0598\n",
      "Epoch 141/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0739 - val_loss: 0.0597\n",
      "Epoch 142/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0595\n",
      "Epoch 143/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0596\n",
      "Epoch 144/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0604\n",
      "Epoch 145/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 146/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0740 - val_loss: 0.0598\n",
      "Epoch 147/10000\n",
      "354881/354881 [==============================] - 11s 31us/sample - loss: 0.0739 - val_loss: 0.0605\n",
      "Epoch 148/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0740 - val_loss: 0.0590\n",
      "Epoch 149/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0596\n",
      "Epoch 150/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0592\n",
      "Epoch 151/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0736 - val_loss: 0.0594\n",
      "Epoch 152/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0593\n",
      "Epoch 153/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0599\n",
      "Epoch 154/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0590\n",
      "Epoch 155/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0597\n",
      "Epoch 156/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0741 - val_loss: 0.0603\n",
      "Epoch 157/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0602\n",
      "Epoch 158/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0738 - val_loss: 0.0590\n",
      "Epoch 159/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0739 - val_loss: 0.0600\n",
      "Epoch 160/10000\n",
      "354881/354881 [==============================] - 10s 30us/sample - loss: 0.0737 - val_loss: 0.0595\n",
      "Epoch 161/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0603\n",
      "Epoch 162/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0591\n",
      "Epoch 163/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 164/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0594\n",
      "Epoch 165/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0735 - val_loss: 0.0599\n",
      "Epoch 166/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0591\n",
      "Epoch 167/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 168/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0597\n",
      "Epoch 169/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0735 - val_loss: 0.0602\n",
      "Epoch 170/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0735 - val_loss: 0.0594\n",
      "Epoch 171/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0599\n",
      "Epoch 172/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0599\n",
      "Epoch 173/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0594\n",
      "Epoch 174/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0590\n",
      "Epoch 175/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0594\n",
      "Epoch 176/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0602\n",
      "Epoch 177/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0736 - val_loss: 0.0603\n",
      "Epoch 178/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0595\n",
      "Epoch 179/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0595\n",
      "Epoch 180/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0737 - val_loss: 0.0593\n",
      "Epoch 181/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0596\n",
      "Epoch 182/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0737 - val_loss: 0.0590\n",
      "Epoch 183/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0734 - val_loss: 0.0600\n",
      "Epoch 184/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0594\n",
      "Epoch 185/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0734 - val_loss: 0.0592\n",
      "Epoch 186/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0733 - val_loss: 0.0596\n",
      "Epoch 187/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0736 - val_loss: 0.0594\n",
      "Epoch 188/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0732 - val_loss: 0.0604\n",
      "Epoch 189/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0730 - val_loss: 0.0598\n",
      "Epoch 190/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0596\n",
      "Epoch 191/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0731 - val_loss: 0.0591\n",
      "Epoch 192/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0734 - val_loss: 0.0597\n",
      "Epoch 193/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0734 - val_loss: 0.0597\n",
      "Epoch 194/10000\n",
      "354881/354881 [==============================] - 11s 30us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Epoch 195/10000\n",
      "354881/354881 [==============================] - 10s 28us/sample - loss: 0.0734 - val_loss: 0.0593\n",
      "Epoch 196/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0735 - val_loss: 0.0598\n",
      "Epoch 197/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0596\n",
      "Epoch 198/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0734 - val_loss: 0.0601\n",
      "Epoch 199/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0734 - val_loss: 0.0599\n",
      "Epoch 200/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0732 - val_loss: 0.0594\n",
      "Epoch 201/10000\n",
      "354881/354881 [==============================] - 10s 29us/sample - loss: 0.0733 - val_loss: 0.0603\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject23.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject23.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11397, 18, 1)\n",
      "y_test.shape:  (11397, 1)\n",
      "WARNING:tensorflow:Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:22:47,474 WARNING Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:22:47,526 WARNING Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject23.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject24.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 47\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9345, 18, 1)\n",
      "y_test.shape:  (9345, 1)\n",
      "WARNING:tensorflow:Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:01,920 WARNING Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:01,969 WARNING Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject25.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11700, 18, 1)\n",
      "y_test.shape:  (11700, 1)\n",
      "WARNING:tensorflow:Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:14,403 WARNING Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:14,472 WARNING Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject25.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject26.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject26.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11535, 18, 1)\n",
      "y_test.shape:  (11535, 1)\n",
      "WARNING:tensorflow:Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:28,022 WARNING Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:28,074 WARNING Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject26.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject27.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject27.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11783, 18, 1)\n",
      "y_test.shape:  (11783, 1)\n",
      "WARNING:tensorflow:Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:41,710 WARNING Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:41,762 WARNING Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject27.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject28.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject28.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11714, 18, 1)\n",
      "y_test.shape:  (11714, 1)\n",
      "WARNING:tensorflow:Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:56,016 WARNING Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:23:56,067 WARNING Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject28.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject29.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject29.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 151\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (3950, 18, 1)\n",
      "y_test.shape:  (3950, 1)\n",
      "WARNING:tensorflow:Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:09,460 WARNING Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:09,507 WARNING Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject29.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject30.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject30.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11658, 18, 1)\n",
      "y_test.shape:  (11658, 1)\n",
      "WARNING:tensorflow:Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:19,641 WARNING Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:19,721 WARNING Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject30.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject31.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject31.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11881, 18, 1)\n",
      "y_test.shape:  (11881, 1)\n",
      "WARNING:tensorflow:Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:33,431 WARNING Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:33,506 WARNING Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject31.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject32.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject32.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11523, 18, 1)\n",
      "y_test.shape:  (11523, 1)\n",
      "WARNING:tensorflow:Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:47,556 WARNING Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:24:47,618 WARNING Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject32.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject33.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject33.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11573, 18, 1)\n",
      "y_test.shape:  (11573, 1)\n",
      "WARNING:tensorflow:Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:25:01,337 WARNING Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:25:01,386 WARNING Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject33.csv\n",
      "2025-01-19 23:25:13,941 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 119\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 136\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 47\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 118\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 88\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (350956, 18, 1)\n",
      "y_train.shape:  (350956, 1)\n",
      "x_valid.shape:  (87723, 18, 1)\n",
      "y_valid.shape:  (87723, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:26:04,057 WARNING Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:26:04,107 WARNING Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 23:26:04,198 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 350956 samples, validate on 87723 samples\n",
      "Epoch 1/10000\n",
      "349184/350956 [============================>.] - ETA: 0s - loss: 1.0041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350956/350956 [==============================] - 18s 52us/sample - loss: 1.0003 - val_loss: 0.1121\n",
      "Epoch 2/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.2216 - val_loss: 0.0828\n",
      "Epoch 3/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.2011 - val_loss: 0.0793\n",
      "Epoch 4/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1860 - val_loss: 0.0717\n",
      "Epoch 5/10000\n",
      "350956/350956 [==============================] - 15s 42us/sample - loss: 0.1739 - val_loss: 0.0670\n",
      "Epoch 6/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1612 - val_loss: 0.0648\n",
      "Epoch 7/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1490 - val_loss: 0.0681\n",
      "Epoch 8/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1379 - val_loss: 0.0682\n",
      "Epoch 9/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1284 - val_loss: 0.0640\n",
      "Epoch 10/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.1182 - val_loss: 0.0646\n",
      "Epoch 11/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.1107 - val_loss: 0.0632\n",
      "Epoch 12/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.1037 - val_loss: 0.0643\n",
      "Epoch 13/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0980 - val_loss: 0.0642\n",
      "Epoch 14/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0933 - val_loss: 0.0641\n",
      "Epoch 15/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0895 - val_loss: 0.0635\n",
      "Epoch 16/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0869 - val_loss: 0.0652\n",
      "Epoch 17/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0851 - val_loss: 0.0631\n",
      "Epoch 18/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0846 - val_loss: 0.0625\n",
      "Epoch 19/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0843 - val_loss: 0.0631\n",
      "Epoch 20/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0839 - val_loss: 0.0649\n",
      "Epoch 21/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0839 - val_loss: 0.0623\n",
      "Epoch 22/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0838 - val_loss: 0.0626\n",
      "Epoch 23/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0832 - val_loss: 0.0633\n",
      "Epoch 24/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0836 - val_loss: 0.0626\n",
      "Epoch 25/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0836 - val_loss: 0.0630\n",
      "Epoch 26/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0835 - val_loss: 0.0615\n",
      "Epoch 27/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0831 - val_loss: 0.0637\n",
      "Epoch 28/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0833 - val_loss: 0.0625\n",
      "Epoch 29/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0833 - val_loss: 0.0656\n",
      "Epoch 30/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0832 - val_loss: 0.0626\n",
      "Epoch 31/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0832 - val_loss: 0.0617\n",
      "Epoch 32/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0833 - val_loss: 0.0627\n",
      "Epoch 33/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0831 - val_loss: 0.0619\n",
      "Epoch 34/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0828 - val_loss: 0.0612\n",
      "Epoch 35/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0826 - val_loss: 0.0617\n",
      "Epoch 36/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0827 - val_loss: 0.0610\n",
      "Epoch 37/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0824 - val_loss: 0.0622\n",
      "Epoch 38/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0823 - val_loss: 0.0623\n",
      "Epoch 39/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0826 - val_loss: 0.0620\n",
      "Epoch 40/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0829 - val_loss: 0.0621\n",
      "Epoch 41/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0824 - val_loss: 0.0620\n",
      "Epoch 42/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0822 - val_loss: 0.0612\n",
      "Epoch 43/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0608\n",
      "Epoch 44/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0824 - val_loss: 0.0614\n",
      "Epoch 45/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0617\n",
      "Epoch 46/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0820 - val_loss: 0.0622\n",
      "Epoch 47/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0822 - val_loss: 0.0609\n",
      "Epoch 48/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0613\n",
      "Epoch 49/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0606\n",
      "Epoch 50/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0614\n",
      "Epoch 51/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0816 - val_loss: 0.0611\n",
      "Epoch 52/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0608\n",
      "Epoch 53/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0818 - val_loss: 0.0612\n",
      "Epoch 54/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0610\n",
      "Epoch 55/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0818 - val_loss: 0.0611\n",
      "Epoch 56/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0816 - val_loss: 0.0607\n",
      "Epoch 57/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0816 - val_loss: 0.0623\n",
      "Epoch 58/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0818 - val_loss: 0.0607\n",
      "Epoch 59/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0813 - val_loss: 0.0613\n",
      "Epoch 60/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0816 - val_loss: 0.0613\n",
      "Epoch 61/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0816 - val_loss: 0.0606\n",
      "Epoch 62/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0820 - val_loss: 0.0627\n",
      "Epoch 63/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0817 - val_loss: 0.0608\n",
      "Epoch 64/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0816 - val_loss: 0.0618\n",
      "Epoch 65/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0819 - val_loss: 0.0617\n",
      "Epoch 66/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0814 - val_loss: 0.0613\n",
      "Epoch 67/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0611\n",
      "Epoch 68/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0817 - val_loss: 0.0611\n",
      "Epoch 69/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0604\n",
      "Epoch 70/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0816 - val_loss: 0.0615\n",
      "Epoch 71/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0819 - val_loss: 0.0603\n",
      "Epoch 72/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0817 - val_loss: 0.0611\n",
      "Epoch 73/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0815 - val_loss: 0.0612\n",
      "Epoch 74/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0616\n",
      "Epoch 75/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0815 - val_loss: 0.0620\n",
      "Epoch 76/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0811 - val_loss: 0.0609\n",
      "Epoch 77/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0602\n",
      "Epoch 78/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0609\n",
      "Epoch 79/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0609\n",
      "Epoch 80/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0812 - val_loss: 0.0610\n",
      "Epoch 81/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0815 - val_loss: 0.0604\n",
      "Epoch 82/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0816 - val_loss: 0.0614\n",
      "Epoch 83/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 84/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0614\n",
      "Epoch 85/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0606\n",
      "Epoch 86/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0607\n",
      "Epoch 87/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0605\n",
      "Epoch 88/10000\n",
      "350956/350956 [==============================] - 11s 31us/sample - loss: 0.0813 - val_loss: 0.0608\n",
      "Epoch 89/10000\n",
      "350956/350956 [==============================] - 11s 31us/sample - loss: 0.0811 - val_loss: 0.0610\n",
      "Epoch 90/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0811 - val_loss: 0.0609\n",
      "Epoch 91/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0602\n",
      "Epoch 92/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0613\n",
      "Epoch 93/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0817 - val_loss: 0.0608\n",
      "Epoch 94/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0810 - val_loss: 0.0611\n",
      "Epoch 95/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0607\n",
      "Epoch 96/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0607\n",
      "Epoch 97/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0605\n",
      "Epoch 98/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0609\n",
      "Epoch 99/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0603\n",
      "Epoch 100/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0605\n",
      "Epoch 101/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0608\n",
      "Epoch 102/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0813 - val_loss: 0.0623\n",
      "Epoch 103/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0607\n",
      "Epoch 104/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0814 - val_loss: 0.0609\n",
      "Epoch 105/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0609\n",
      "Epoch 106/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0610\n",
      "Epoch 107/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0609\n",
      "Epoch 108/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0604\n",
      "Epoch 109/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0608\n",
      "Epoch 110/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0813 - val_loss: 0.0605\n",
      "Epoch 111/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0612\n",
      "Epoch 112/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0809 - val_loss: 0.0610\n",
      "Epoch 113/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0611\n",
      "Epoch 114/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0626\n",
      "Epoch 115/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0611\n",
      "Epoch 116/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0808 - val_loss: 0.0611\n",
      "Epoch 117/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0616\n",
      "Epoch 118/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0604\n",
      "Epoch 119/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0811 - val_loss: 0.0621\n",
      "Epoch 120/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 121/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0808 - val_loss: 0.0611\n",
      "Epoch 122/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0807 - val_loss: 0.0614\n",
      "Epoch 123/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0813 - val_loss: 0.0622\n",
      "Epoch 124/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0810 - val_loss: 0.0613\n",
      "Epoch 125/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0612\n",
      "Epoch 126/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0807 - val_loss: 0.0615\n",
      "Epoch 127/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0605\n",
      "Epoch 128/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0807 - val_loss: 0.0618\n",
      "Epoch 129/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 130/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0607\n",
      "Epoch 131/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0809 - val_loss: 0.0606\n",
      "Epoch 132/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0609\n",
      "Epoch 133/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0809 - val_loss: 0.0609\n",
      "Epoch 134/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 135/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0807 - val_loss: 0.0608\n",
      "Epoch 136/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0809 - val_loss: 0.0607\n",
      "Epoch 137/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0812 - val_loss: 0.0613\n",
      "Epoch 138/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0809 - val_loss: 0.0612\n",
      "Epoch 139/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0608\n",
      "Epoch 140/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0808 - val_loss: 0.0613\n",
      "Epoch 141/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0808 - val_loss: 0.0607\n",
      "Epoch 142/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 143/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0810 - val_loss: 0.0613\n",
      "Epoch 144/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0804 - val_loss: 0.0608\n",
      "Epoch 145/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0804 - val_loss: 0.0607\n",
      "Epoch 146/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0602\n",
      "Epoch 147/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0809 - val_loss: 0.0612\n",
      "Epoch 148/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0803 - val_loss: 0.0611\n",
      "Epoch 149/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0809 - val_loss: 0.0609\n",
      "Epoch 150/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0610\n",
      "Epoch 151/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0608\n",
      "Epoch 152/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0808 - val_loss: 0.0619\n",
      "Epoch 153/10000\n",
      "350956/350956 [==============================] - 10s 28us/sample - loss: 0.0804 - val_loss: 0.0607\n",
      "Epoch 154/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0810 - val_loss: 0.0609\n",
      "Epoch 155/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0613\n",
      "Epoch 156/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 157/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0614\n",
      "Epoch 158/10000\n",
      "350956/350956 [==============================] - 11s 30us/sample - loss: 0.0809 - val_loss: 0.0608\n",
      "Epoch 159/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0807 - val_loss: 0.0611\n",
      "Epoch 160/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0806 - val_loss: 0.0613\n",
      "Epoch 161/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0617\n",
      "Epoch 162/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0804 - val_loss: 0.0611\n",
      "Epoch 163/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0805 - val_loss: 0.0625\n",
      "Epoch 164/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0807 - val_loss: 0.0616\n",
      "Epoch 165/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0610\n",
      "Epoch 166/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0809 - val_loss: 0.0604\n",
      "Epoch 167/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 168/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0806 - val_loss: 0.0605\n",
      "Epoch 169/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0803 - val_loss: 0.0610\n",
      "Epoch 170/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0610\n",
      "Epoch 171/10000\n",
      "350956/350956 [==============================] - 10s 30us/sample - loss: 0.0803 - val_loss: 0.0603\n",
      "Epoch 172/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0611\n",
      "Epoch 173/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0805 - val_loss: 0.0609\n",
      "Epoch 174/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0807 - val_loss: 0.0613\n",
      "Epoch 175/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0805 - val_loss: 0.0607\n",
      "Epoch 176/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0809 - val_loss: 0.0615\n",
      "Epoch 177/10000\n",
      "350956/350956 [==============================] - 10s 29us/sample - loss: 0.0805 - val_loss: 0.0620\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject34.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject34.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11415, 18, 1)\n",
      "y_test.shape:  (11415, 1)\n",
      "WARNING:tensorflow:Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:56:41,059 WARNING Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:56:41,109 WARNING Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject34.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject35.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject35.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11650, 18, 1)\n",
      "y_test.shape:  (11650, 1)\n",
      "WARNING:tensorflow:Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:56:55,145 WARNING Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:56:55,195 WARNING Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject35.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject36.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject36.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11001, 18, 1)\n",
      "y_test.shape:  (11001, 1)\n",
      "WARNING:tensorflow:Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:09,205 WARNING Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:09,255 WARNING Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject36.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject37.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject37.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11413, 18, 1)\n",
      "y_test.shape:  (11413, 1)\n",
      "WARNING:tensorflow:Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:23,080 WARNING Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:23,136 WARNING Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject37.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject38.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject38.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11441, 18, 1)\n",
      "y_test.shape:  (11441, 1)\n",
      "WARNING:tensorflow:Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:38,010 WARNING Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:38,059 WARNING Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject38.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject39.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject39.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10849, 18, 1)\n",
      "y_test.shape:  (10849, 1)\n",
      "WARNING:tensorflow:Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:52,987 WARNING Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:57:53,035 WARNING Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject39.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject40.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject40.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11299, 18, 1)\n",
      "y_test.shape:  (11299, 1)\n",
      "WARNING:tensorflow:Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:07,254 WARNING Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:07,306 WARNING Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject40.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject41.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject41.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 25\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11363, 18, 1)\n",
      "y_test.shape:  (11363, 1)\n",
      "WARNING:tensorflow:Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:22,243 WARNING Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:22,292 WARNING Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject41.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject42.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject42.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11279, 18, 1)\n",
      "y_test.shape:  (11279, 1)\n",
      "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:37,449 WARNING Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:37,499 WARNING Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject42.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject43.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject43.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11290, 18, 1)\n",
      "y_test.shape:  (11290, 1)\n",
      "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:52,507 WARNING Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:58:52,555 WARNING Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject43.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject44.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject44.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9965, 18, 1)\n",
      "y_test.shape:  (9965, 1)\n",
      "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:59:06,740 WARNING Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 23:59:06,790 WARNING Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject44.csv\n",
      "2025-01-19 23:59:19,224 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 119\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 136\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 47\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 151\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 118\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (380300, 18, 1)\n",
      "y_train.shape:  (380300, 1)\n",
      "x_valid.shape:  (95057, 18, 1)\n",
      "y_valid.shape:  (95057, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:00:12,811 WARNING Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:00:12,869 WARNING Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 00:00:12,976 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 380300 samples, validate on 95057 samples\n",
      "Epoch 1/10000\n",
      "380300/380300 [==============================] - ETA: 0s - loss: 1.1107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380300/380300 [==============================] - 19s 51us/sample - loss: 1.1107 - val_loss: 0.1559\n",
      "Epoch 2/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.2340 - val_loss: 0.0911\n",
      "Epoch 3/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.2002 - val_loss: 0.0758\n",
      "Epoch 4/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.1810 - val_loss: 0.0841\n",
      "Epoch 5/10000\n",
      "380300/380300 [==============================] - 16s 42us/sample - loss: 0.1678 - val_loss: 0.0731\n",
      "Epoch 6/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.1544 - val_loss: 0.0682\n",
      "Epoch 7/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.1422 - val_loss: 0.0682\n",
      "Epoch 8/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.1311 - val_loss: 0.0675\n",
      "Epoch 9/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.1213 - val_loss: 0.0689\n",
      "Epoch 10/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.1126 - val_loss: 0.0692\n",
      "Epoch 11/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.1042 - val_loss: 0.0659\n",
      "Epoch 12/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0980 - val_loss: 0.0657\n",
      "Epoch 13/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0920 - val_loss: 0.0668\n",
      "Epoch 14/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0882 - val_loss: 0.0662\n",
      "Epoch 15/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0848 - val_loss: 0.0653\n",
      "Epoch 16/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0825 - val_loss: 0.0667\n",
      "Epoch 17/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0818 - val_loss: 0.0651\n",
      "Epoch 18/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0809 - val_loss: 0.0646\n",
      "Epoch 19/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0807 - val_loss: 0.0652\n",
      "Epoch 20/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0800 - val_loss: 0.0647\n",
      "Epoch 21/10000\n",
      "380300/380300 [==============================] - 11s 28us/sample - loss: 0.0798 - val_loss: 0.0642\n",
      "Epoch 22/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0800 - val_loss: 0.0652\n",
      "Epoch 23/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0797 - val_loss: 0.0637\n",
      "Epoch 24/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0795 - val_loss: 0.0652\n",
      "Epoch 25/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0795 - val_loss: 0.0642\n",
      "Epoch 26/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0791 - val_loss: 0.0643\n",
      "Epoch 27/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0795 - val_loss: 0.0639\n",
      "Epoch 28/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0794 - val_loss: 0.0655\n",
      "Epoch 29/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0790 - val_loss: 0.0632\n",
      "Epoch 30/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0792 - val_loss: 0.0644\n",
      "Epoch 31/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0790 - val_loss: 0.0634\n",
      "Epoch 32/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0791 - val_loss: 0.0633\n",
      "Epoch 33/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0790 - val_loss: 0.0668\n",
      "Epoch 34/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0792 - val_loss: 0.0640\n",
      "Epoch 35/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0789 - val_loss: 0.0646\n",
      "Epoch 36/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0790 - val_loss: 0.0636\n",
      "Epoch 37/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0787 - val_loss: 0.0638\n",
      "Epoch 38/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0785 - val_loss: 0.0643\n",
      "Epoch 39/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0787 - val_loss: 0.0639\n",
      "Epoch 40/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0787 - val_loss: 0.0630\n",
      "Epoch 41/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0786 - val_loss: 0.0633\n",
      "Epoch 42/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0787 - val_loss: 0.0639\n",
      "Epoch 43/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0786 - val_loss: 0.0639\n",
      "Epoch 44/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0782 - val_loss: 0.0637\n",
      "Epoch 45/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0785 - val_loss: 0.0643\n",
      "Epoch 46/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0785 - val_loss: 0.0630\n",
      "Epoch 47/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0785 - val_loss: 0.0630\n",
      "Epoch 48/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0784 - val_loss: 0.0636\n",
      "Epoch 49/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0783 - val_loss: 0.0634\n",
      "Epoch 50/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0788 - val_loss: 0.0644\n",
      "Epoch 51/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0785 - val_loss: 0.0634\n",
      "Epoch 52/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0785 - val_loss: 0.0637\n",
      "Epoch 53/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0633\n",
      "Epoch 54/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0639\n",
      "Epoch 55/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0784 - val_loss: 0.0634\n",
      "Epoch 56/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0783 - val_loss: 0.0641\n",
      "Epoch 57/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0784 - val_loss: 0.0631\n",
      "Epoch 58/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0638\n",
      "Epoch 59/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0784 - val_loss: 0.0647\n",
      "Epoch 60/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0786 - val_loss: 0.0647\n",
      "Epoch 61/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0783 - val_loss: 0.0625\n",
      "Epoch 62/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0784 - val_loss: 0.0631\n",
      "Epoch 63/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0784 - val_loss: 0.0632\n",
      "Epoch 64/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0780 - val_loss: 0.0635\n",
      "Epoch 65/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0781 - val_loss: 0.0632\n",
      "Epoch 66/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0782 - val_loss: 0.0628\n",
      "Epoch 67/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0783 - val_loss: 0.0634\n",
      "Epoch 68/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0785 - val_loss: 0.0635\n",
      "Epoch 69/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0626\n",
      "Epoch 70/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0778 - val_loss: 0.0629\n",
      "Epoch 71/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0780 - val_loss: 0.0626\n",
      "Epoch 72/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0636\n",
      "Epoch 73/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0780 - val_loss: 0.0628\n",
      "Epoch 74/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0783 - val_loss: 0.0633\n",
      "Epoch 75/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0629\n",
      "Epoch 76/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0782 - val_loss: 0.0631\n",
      "Epoch 77/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0779 - val_loss: 0.0632\n",
      "Epoch 78/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0633\n",
      "Epoch 79/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0780 - val_loss: 0.0634\n",
      "Epoch 80/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0779 - val_loss: 0.0627\n",
      "Epoch 81/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0779 - val_loss: 0.0630\n",
      "Epoch 82/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0778 - val_loss: 0.0635\n",
      "Epoch 83/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0782 - val_loss: 0.0638\n",
      "Epoch 84/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0779 - val_loss: 0.0631\n",
      "Epoch 85/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0777 - val_loss: 0.0626\n",
      "Epoch 86/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0780 - val_loss: 0.0634\n",
      "Epoch 87/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0627\n",
      "Epoch 88/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0778 - val_loss: 0.0636\n",
      "Epoch 89/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0779 - val_loss: 0.0631\n",
      "Epoch 90/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0778 - val_loss: 0.0623\n",
      "Epoch 91/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0779 - val_loss: 0.0632\n",
      "Epoch 92/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0781 - val_loss: 0.0629\n",
      "Epoch 93/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0778 - val_loss: 0.0628\n",
      "Epoch 94/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0781 - val_loss: 0.0625\n",
      "Epoch 95/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0779 - val_loss: 0.0631\n",
      "Epoch 96/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0779 - val_loss: 0.0637\n",
      "Epoch 97/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0778 - val_loss: 0.0630\n",
      "Epoch 98/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0630\n",
      "Epoch 99/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0775 - val_loss: 0.0624\n",
      "Epoch 100/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0636\n",
      "Epoch 101/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0777 - val_loss: 0.0643\n",
      "Epoch 102/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0778 - val_loss: 0.0629\n",
      "Epoch 103/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0632\n",
      "Epoch 104/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0778 - val_loss: 0.0631\n",
      "Epoch 105/10000\n",
      "380300/380300 [==============================] - 11s 28us/sample - loss: 0.0779 - val_loss: 0.0626\n",
      "Epoch 106/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0776 - val_loss: 0.0644\n",
      "Epoch 107/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0634\n",
      "Epoch 108/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0779 - val_loss: 0.0628\n",
      "Epoch 109/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0777 - val_loss: 0.0625\n",
      "Epoch 110/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0775 - val_loss: 0.0630\n",
      "Epoch 111/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0778 - val_loss: 0.0631\n",
      "Epoch 112/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0776 - val_loss: 0.0630\n",
      "Epoch 113/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0639\n",
      "Epoch 114/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0630\n",
      "Epoch 115/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0773 - val_loss: 0.0626\n",
      "Epoch 116/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0777 - val_loss: 0.0625\n",
      "Epoch 117/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0776 - val_loss: 0.0637\n",
      "Epoch 118/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0628\n",
      "Epoch 119/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0779 - val_loss: 0.0628\n",
      "Epoch 120/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0773 - val_loss: 0.0635\n",
      "Epoch 121/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0634\n",
      "Epoch 122/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0776 - val_loss: 0.0634\n",
      "Epoch 123/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0775 - val_loss: 0.0631\n",
      "Epoch 124/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0629\n",
      "Epoch 125/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0633\n",
      "Epoch 126/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0773 - val_loss: 0.0633\n",
      "Epoch 127/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0627\n",
      "Epoch 128/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0775 - val_loss: 0.0630\n",
      "Epoch 129/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0634\n",
      "Epoch 130/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0630\n",
      "Epoch 131/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0773 - val_loss: 0.0630\n",
      "Epoch 132/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0773 - val_loss: 0.0629\n",
      "Epoch 133/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0632\n",
      "Epoch 134/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0632\n",
      "Epoch 135/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0633\n",
      "Epoch 136/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0775 - val_loss: 0.0646\n",
      "Epoch 137/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0773 - val_loss: 0.0626\n",
      "Epoch 138/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0775 - val_loss: 0.0624\n",
      "Epoch 139/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0777 - val_loss: 0.0630\n",
      "Epoch 140/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0771 - val_loss: 0.0623\n",
      "Epoch 141/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0776 - val_loss: 0.0628\n",
      "Epoch 142/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0776 - val_loss: 0.0632\n",
      "Epoch 143/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0771 - val_loss: 0.0627\n",
      "Epoch 144/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0772 - val_loss: 0.0627\n",
      "Epoch 145/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0626\n",
      "Epoch 146/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0772 - val_loss: 0.0627\n",
      "Epoch 147/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0774 - val_loss: 0.0629\n",
      "Epoch 148/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0631\n",
      "Epoch 149/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0629\n",
      "Epoch 150/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0630\n",
      "Epoch 151/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0631\n",
      "Epoch 152/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0772 - val_loss: 0.0627\n",
      "Epoch 153/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0772 - val_loss: 0.0638\n",
      "Epoch 154/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0769 - val_loss: 0.0631\n",
      "Epoch 155/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0774 - val_loss: 0.0626\n",
      "Epoch 156/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0632\n",
      "Epoch 157/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0627\n",
      "Epoch 158/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0771 - val_loss: 0.0645\n",
      "Epoch 159/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0773 - val_loss: 0.0623\n",
      "Epoch 160/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0772 - val_loss: 0.0626\n",
      "Epoch 161/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 162/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0770 - val_loss: 0.0629\n",
      "Epoch 163/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0638\n",
      "Epoch 164/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0773 - val_loss: 0.0627\n",
      "Epoch 165/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0637\n",
      "Epoch 166/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0624\n",
      "Epoch 167/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0775 - val_loss: 0.0623\n",
      "Epoch 168/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0771 - val_loss: 0.0635\n",
      "Epoch 169/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0633\n",
      "Epoch 170/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0773 - val_loss: 0.0629\n",
      "Epoch 171/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0642\n",
      "Epoch 172/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0771 - val_loss: 0.0625\n",
      "Epoch 173/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0631\n",
      "Epoch 174/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0631\n",
      "Epoch 175/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0634\n",
      "Epoch 176/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0630\n",
      "Epoch 177/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0627\n",
      "Epoch 178/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0626\n",
      "Epoch 179/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0772 - val_loss: 0.0624\n",
      "Epoch 180/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0630\n",
      "Epoch 181/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0767 - val_loss: 0.0632\n",
      "Epoch 182/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0771 - val_loss: 0.0636\n",
      "Epoch 183/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0630\n",
      "Epoch 184/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0767 - val_loss: 0.0625\n",
      "Epoch 185/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0622\n",
      "Epoch 186/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0770 - val_loss: 0.0636\n",
      "Epoch 187/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0628\n",
      "Epoch 188/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0767 - val_loss: 0.0623\n",
      "Epoch 189/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0768 - val_loss: 0.0623\n",
      "Epoch 190/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0629\n",
      "Epoch 191/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0772 - val_loss: 0.0625\n",
      "Epoch 192/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0632\n",
      "Epoch 193/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0633\n",
      "Epoch 194/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0629\n",
      "Epoch 195/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0767 - val_loss: 0.0627\n",
      "Epoch 196/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0765 - val_loss: 0.0629\n",
      "Epoch 197/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0633\n",
      "Epoch 198/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0626\n",
      "Epoch 199/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0631\n",
      "Epoch 200/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0769 - val_loss: 0.0629\n",
      "Epoch 201/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0767 - val_loss: 0.0630\n",
      "Epoch 202/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 203/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0629\n",
      "Epoch 204/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 205/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0625\n",
      "Epoch 206/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0631\n",
      "Epoch 207/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0645\n",
      "Epoch 208/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0769 - val_loss: 0.0634\n",
      "Epoch 209/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 210/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0629\n",
      "Epoch 211/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0632\n",
      "Epoch 212/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0624\n",
      "Epoch 213/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0629\n",
      "Epoch 214/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0625\n",
      "Epoch 215/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0625\n",
      "Epoch 216/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0768 - val_loss: 0.0633\n",
      "Epoch 217/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0769 - val_loss: 0.0627\n",
      "Epoch 218/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0766 - val_loss: 0.0633\n",
      "Epoch 219/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0630\n",
      "Epoch 220/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0768 - val_loss: 0.0633\n",
      "Epoch 221/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0627\n",
      "Epoch 222/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0767 - val_loss: 0.0630\n",
      "Epoch 223/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0770 - val_loss: 0.0625\n",
      "Epoch 224/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0633\n",
      "Epoch 225/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0765 - val_loss: 0.0628\n",
      "Epoch 226/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0768 - val_loss: 0.0631\n",
      "Epoch 227/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0634\n",
      "Epoch 228/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0634\n",
      "Epoch 229/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0627\n",
      "Epoch 230/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0632\n",
      "Epoch 231/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0637\n",
      "Epoch 232/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0632\n",
      "Epoch 233/10000\n",
      "380300/380300 [==============================] - 11s 28us/sample - loss: 0.0765 - val_loss: 0.0628\n",
      "Epoch 234/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0763 - val_loss: 0.0626\n",
      "Epoch 235/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0768 - val_loss: 0.0627\n",
      "Epoch 236/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0764 - val_loss: 0.0625\n",
      "Epoch 237/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0632\n",
      "Epoch 238/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0766 - val_loss: 0.0631\n",
      "Epoch 239/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0640\n",
      "Epoch 240/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0626\n",
      "Epoch 241/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0767 - val_loss: 0.0629\n",
      "Epoch 242/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0637\n",
      "Epoch 243/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0630\n",
      "Epoch 244/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0763 - val_loss: 0.0628\n",
      "Epoch 245/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0764 - val_loss: 0.0628\n",
      "Epoch 246/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0761 - val_loss: 0.0630\n",
      "Epoch 247/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0634\n",
      "Epoch 248/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0763 - val_loss: 0.0627\n",
      "Epoch 249/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 250/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0762 - val_loss: 0.0639\n",
      "Epoch 251/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0629\n",
      "Epoch 252/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0626\n",
      "Epoch 253/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0763 - val_loss: 0.0634\n",
      "Epoch 254/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0762 - val_loss: 0.0632\n",
      "Epoch 255/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0763 - val_loss: 0.0628\n",
      "Epoch 256/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0635\n",
      "Epoch 257/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0763 - val_loss: 0.0633\n",
      "Epoch 258/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0635\n",
      "Epoch 259/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0634\n",
      "Epoch 260/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0638\n",
      "Epoch 261/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0635\n",
      "Epoch 262/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0627\n",
      "Epoch 263/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0765 - val_loss: 0.0635\n",
      "Epoch 264/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0633\n",
      "Epoch 265/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0630\n",
      "Epoch 266/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0763 - val_loss: 0.0632\n",
      "Epoch 267/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0631\n",
      "Epoch 268/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0761 - val_loss: 0.0632\n",
      "Epoch 269/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0760 - val_loss: 0.0631\n",
      "Epoch 270/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0764 - val_loss: 0.0631\n",
      "Epoch 271/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0761 - val_loss: 0.0637\n",
      "Epoch 272/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0760 - val_loss: 0.0634\n",
      "Epoch 273/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0761 - val_loss: 0.0629\n",
      "Epoch 274/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0632\n",
      "Epoch 275/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0762 - val_loss: 0.0636\n",
      "Epoch 276/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0760 - val_loss: 0.0633\n",
      "Epoch 277/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0760 - val_loss: 0.0632\n",
      "Epoch 278/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0766 - val_loss: 0.0633\n",
      "Epoch 279/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0759 - val_loss: 0.0639\n",
      "Epoch 280/10000\n",
      "380300/380300 [==============================] - 12s 30us/sample - loss: 0.0762 - val_loss: 0.0637\n",
      "Epoch 281/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0762 - val_loss: 0.0632\n",
      "Epoch 282/10000\n",
      "380300/380300 [==============================] - 12s 31us/sample - loss: 0.0760 - val_loss: 0.0632\n",
      "Epoch 283/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0628\n",
      "Epoch 284/10000\n",
      "380300/380300 [==============================] - 11s 30us/sample - loss: 0.0761 - val_loss: 0.0628\n",
      "Epoch 285/10000\n",
      "380300/380300 [==============================] - 11s 29us/sample - loss: 0.0760 - val_loss: 0.0627\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject45.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject45.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10895, 18, 1)\n",
      "y_test.shape:  (10895, 1)\n",
      "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:09,042 WARNING Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:09,119 WARNING Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject45.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject46.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject46.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11293, 18, 1)\n",
      "y_test.shape:  (11293, 1)\n",
      "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:23,956 WARNING Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:24,006 WARNING Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject46.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject47.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject47.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11595, 18, 1)\n",
      "y_test.shape:  (11595, 1)\n",
      "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:39,013 WARNING Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:39,067 WARNING Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject47.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject48.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject48.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11696, 18, 1)\n",
      "y_test.shape:  (11696, 1)\n",
      "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:54,967 WARNING Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:54:55,019 WARNING Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject48.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject49.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject49.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10671, 18, 1)\n",
      "y_test.shape:  (10671, 1)\n",
      "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:09,587 WARNING Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:09,639 WARNING Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject49.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject50.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject50.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 88\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4918, 18, 1)\n",
      "y_test.shape:  (4918, 1)\n",
      "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:23,558 WARNING Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:23,614 WARNING Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject50.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject51.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject51.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8681, 18, 1)\n",
      "y_test.shape:  (8681, 1)\n",
      "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:34,865 WARNING Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:34,918 WARNING Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject51.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject53.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject53.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8805, 18, 1)\n",
      "y_test.shape:  (8805, 1)\n",
      "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:48,220 WARNING Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:55:48,269 WARNING Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject53.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject54.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject54.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7733, 18, 1)\n",
      "y_test.shape:  (7733, 1)\n",
      "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:56:01,487 WARNING Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 18, 32)\n",
      "x.shape =  (?, 18, 32)\n",
      "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:56:01,536 WARNING Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject54.csv\n",
      "2025-01-20 00:56:13,194 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\all does not exist.\n",
      "2025-01-20 00:56:13,195 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 34\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 46\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 112\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 24\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 78\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (358389, 24, 1)\n",
      "y_train.shape:  (358389, 1)\n",
      "x_valid.shape:  (89575, 24, 1)\n",
      "y_valid.shape:  (89575, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:57:04,250 WARNING Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 00:57:04,298 WARNING Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 00:57:04,397 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 358389 samples, validate on 89575 samples\n",
      "Epoch 1/10000\n",
      "357376/358389 [============================>.] - ETA: 0s - loss: 1.4024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358389/358389 [==============================] - 22s 60us/sample - loss: 1.3991 - val_loss: 0.1324\n",
      "Epoch 2/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1876 - val_loss: 0.0906\n",
      "Epoch 3/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1568 - val_loss: 0.0725\n",
      "Epoch 4/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1406 - val_loss: 0.0808\n",
      "Epoch 5/10000\n",
      "358389/358389 [==============================] - 18s 51us/sample - loss: 0.1305 - val_loss: 0.0671\n",
      "Epoch 6/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.1235 - val_loss: 0.0751\n",
      "Epoch 7/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1171 - val_loss: 0.0644\n",
      "Epoch 8/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1109 - val_loss: 0.0652\n",
      "Epoch 9/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.1059 - val_loss: 0.0633\n",
      "Epoch 10/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.1003 - val_loss: 0.0669\n",
      "Epoch 11/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0949 - val_loss: 0.0636\n",
      "Epoch 12/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0910 - val_loss: 0.0633\n",
      "Epoch 13/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0871 - val_loss: 0.0641\n",
      "Epoch 14/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0846 - val_loss: 0.0632\n",
      "Epoch 15/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0819 - val_loss: 0.0636\n",
      "Epoch 16/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0799 - val_loss: 0.0630\n",
      "Epoch 17/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0789 - val_loss: 0.0649\n",
      "Epoch 18/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0778 - val_loss: 0.0642\n",
      "Epoch 19/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0773 - val_loss: 0.0629\n",
      "Epoch 20/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0771 - val_loss: 0.0630\n",
      "Epoch 21/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0769 - val_loss: 0.0625\n",
      "Epoch 22/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0768 - val_loss: 0.0631\n",
      "Epoch 23/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0767 - val_loss: 0.0625\n",
      "Epoch 24/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0765 - val_loss: 0.0627\n",
      "Epoch 25/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0767 - val_loss: 0.0628\n",
      "Epoch 26/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0765 - val_loss: 0.0620\n",
      "Epoch 27/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0764 - val_loss: 0.0616\n",
      "Epoch 28/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0764 - val_loss: 0.0638\n",
      "Epoch 29/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0763 - val_loss: 0.0617\n",
      "Epoch 30/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0763 - val_loss: 0.0627\n",
      "Epoch 31/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0761 - val_loss: 0.0626\n",
      "Epoch 32/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0759 - val_loss: 0.0614\n",
      "Epoch 33/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0756 - val_loss: 0.0624\n",
      "Epoch 34/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0759 - val_loss: 0.0614\n",
      "Epoch 35/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0756 - val_loss: 0.0628\n",
      "Epoch 36/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0752 - val_loss: 0.0619\n",
      "Epoch 37/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0755 - val_loss: 0.0615\n",
      "Epoch 38/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0753 - val_loss: 0.0616\n",
      "Epoch 39/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0754 - val_loss: 0.0621\n",
      "Epoch 40/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0751 - val_loss: 0.0611\n",
      "Epoch 41/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0748 - val_loss: 0.0613\n",
      "Epoch 42/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0751 - val_loss: 0.0607\n",
      "Epoch 43/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0754 - val_loss: 0.0619\n",
      "Epoch 44/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0752 - val_loss: 0.0611\n",
      "Epoch 45/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0751 - val_loss: 0.0611\n",
      "Epoch 46/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0753 - val_loss: 0.0612\n",
      "Epoch 47/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0747 - val_loss: 0.0619\n",
      "Epoch 48/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0748 - val_loss: 0.0618\n",
      "Epoch 49/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0747 - val_loss: 0.0610\n",
      "Epoch 50/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0747 - val_loss: 0.0607\n",
      "Epoch 51/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0728 - val_loss: 0.0610\n",
      "Epoch 52/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0722 - val_loss: 0.0604\n",
      "Epoch 53/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0723 - val_loss: 0.0610\n",
      "Epoch 54/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0721 - val_loss: 0.0604\n",
      "Epoch 55/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0720 - val_loss: 0.0605\n",
      "Epoch 56/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0720 - val_loss: 0.0607\n",
      "Epoch 57/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0719 - val_loss: 0.0614\n",
      "Epoch 58/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0718 - val_loss: 0.0615\n",
      "Epoch 59/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0718 - val_loss: 0.0608\n",
      "Epoch 60/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0718 - val_loss: 0.0611\n",
      "Epoch 61/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0715 - val_loss: 0.0610\n",
      "Epoch 62/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0714 - val_loss: 0.0602\n",
      "Epoch 63/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0720 - val_loss: 0.0616\n",
      "Epoch 64/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0716 - val_loss: 0.0605\n",
      "Epoch 65/10000\n",
      "358389/358389 [==============================] - 13s 35us/sample - loss: 0.0717 - val_loss: 0.0609\n",
      "Epoch 66/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0717 - val_loss: 0.0603\n",
      "Epoch 67/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0717 - val_loss: 0.0606\n",
      "Epoch 68/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0718 - val_loss: 0.0600\n",
      "Epoch 69/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0716 - val_loss: 0.0603\n",
      "Epoch 70/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0716 - val_loss: 0.0601\n",
      "Epoch 71/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0715 - val_loss: 0.0603\n",
      "Epoch 72/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0718 - val_loss: 0.0602\n",
      "Epoch 73/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0713 - val_loss: 0.0626\n",
      "Epoch 74/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0715 - val_loss: 0.0606\n",
      "Epoch 75/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0715 - val_loss: 0.0599\n",
      "Epoch 76/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0716 - val_loss: 0.0605\n",
      "Epoch 77/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0713 - val_loss: 0.0609\n",
      "Epoch 78/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0713 - val_loss: 0.0604\n",
      "Epoch 79/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0714 - val_loss: 0.0604\n",
      "Epoch 80/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0716 - val_loss: 0.0600\n",
      "Epoch 81/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0712 - val_loss: 0.0606\n",
      "Epoch 82/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0716 - val_loss: 0.0605\n",
      "Epoch 83/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0711 - val_loss: 0.0607\n",
      "Epoch 84/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0713 - val_loss: 0.0604\n",
      "Epoch 85/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0714 - val_loss: 0.0606\n",
      "Epoch 86/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0711 - val_loss: 0.0603\n",
      "Epoch 87/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0709 - val_loss: 0.0601\n",
      "Epoch 88/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0712 - val_loss: 0.0607\n",
      "Epoch 89/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0712 - val_loss: 0.0610\n",
      "Epoch 90/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0710 - val_loss: 0.0603\n",
      "Epoch 91/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0714 - val_loss: 0.0602\n",
      "Epoch 92/10000\n",
      "358389/358389 [==============================] - 14s 39us/sample - loss: 0.0711 - val_loss: 0.0597\n",
      "Epoch 93/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0713 - val_loss: 0.0601\n",
      "Epoch 94/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0712 - val_loss: 0.0605\n",
      "Epoch 95/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0709 - val_loss: 0.0607\n",
      "Epoch 96/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0714 - val_loss: 0.0623\n",
      "Epoch 97/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0710 - val_loss: 0.0600\n",
      "Epoch 98/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0712 - val_loss: 0.0602\n",
      "Epoch 99/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0707 - val_loss: 0.0596\n",
      "Epoch 100/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0710 - val_loss: 0.0597\n",
      "Epoch 101/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0712 - val_loss: 0.0600\n",
      "Epoch 102/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0710 - val_loss: 0.0601\n",
      "Epoch 103/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0700 - val_loss: 0.0601\n",
      "Epoch 104/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0695 - val_loss: 0.0597\n",
      "Epoch 105/10000\n",
      "358389/358389 [==============================] - 13s 35us/sample - loss: 0.0694 - val_loss: 0.0602\n",
      "Epoch 106/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0693 - val_loss: 0.0609\n",
      "Epoch 107/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0695 - val_loss: 0.0597\n",
      "Epoch 108/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0695 - val_loss: 0.0602\n",
      "Epoch 109/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0692 - val_loss: 0.0599\n",
      "Epoch 110/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0694 - val_loss: 0.0598\n",
      "Epoch 111/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0693 - val_loss: 0.0600\n",
      "Epoch 112/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0692 - val_loss: 0.0601\n",
      "Epoch 113/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0608\n",
      "Epoch 114/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0694 - val_loss: 0.0599\n",
      "Epoch 115/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0601\n",
      "Epoch 116/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0692 - val_loss: 0.0597\n",
      "Epoch 117/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0614\n",
      "Epoch 118/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0605\n",
      "Epoch 119/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0693 - val_loss: 0.0599\n",
      "Epoch 120/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0600\n",
      "Epoch 121/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0599\n",
      "Epoch 122/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0691 - val_loss: 0.0599\n",
      "Epoch 123/10000\n",
      "358389/358389 [==============================] - 13s 35us/sample - loss: 0.0691 - val_loss: 0.0601\n",
      "Epoch 124/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0693 - val_loss: 0.0596\n",
      "Epoch 125/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0690 - val_loss: 0.0600\n",
      "Epoch 126/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0691 - val_loss: 0.0596\n",
      "Epoch 127/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0691 - val_loss: 0.0599\n",
      "Epoch 128/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0692 - val_loss: 0.0601\n",
      "Epoch 129/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0693 - val_loss: 0.0604\n",
      "Epoch 130/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0691 - val_loss: 0.0611\n",
      "Epoch 131/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0602\n",
      "Epoch 132/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0601\n",
      "Epoch 133/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0598\n",
      "Epoch 134/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0604\n",
      "Epoch 135/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0600\n",
      "Epoch 136/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0603\n",
      "Epoch 137/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0600\n",
      "Epoch 138/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0607\n",
      "Epoch 139/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0603\n",
      "Epoch 140/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0691 - val_loss: 0.0597\n",
      "Epoch 141/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0690 - val_loss: 0.0603\n",
      "Epoch 142/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0601\n",
      "Epoch 143/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0596\n",
      "Epoch 144/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0608\n",
      "Epoch 145/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0688 - val_loss: 0.0608\n",
      "Epoch 146/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0689 - val_loss: 0.0601\n",
      "Epoch 147/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0687 - val_loss: 0.0602\n",
      "Epoch 148/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0604\n",
      "Epoch 149/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0687 - val_loss: 0.0602\n",
      "Epoch 150/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0602\n",
      "Epoch 151/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0685 - val_loss: 0.0607\n",
      "Epoch 152/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0603\n",
      "Epoch 153/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0687 - val_loss: 0.0599\n",
      "Epoch 154/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0608\n",
      "Epoch 155/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0688 - val_loss: 0.0601\n",
      "Epoch 156/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0687 - val_loss: 0.0607\n",
      "Epoch 157/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0690 - val_loss: 0.0602\n",
      "Epoch 158/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0686 - val_loss: 0.0598\n",
      "Epoch 159/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0687 - val_loss: 0.0600\n",
      "Epoch 160/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0688 - val_loss: 0.0601\n",
      "Epoch 161/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0601\n",
      "Epoch 162/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0599\n",
      "Epoch 163/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0689 - val_loss: 0.0600\n",
      "Epoch 164/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0686 - val_loss: 0.0599\n",
      "Epoch 165/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0598\n",
      "Epoch 166/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0688 - val_loss: 0.0599\n",
      "Epoch 167/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0686 - val_loss: 0.0604\n",
      "Epoch 168/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0685 - val_loss: 0.0607\n",
      "Epoch 169/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0601\n",
      "Epoch 170/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0685 - val_loss: 0.0601\n",
      "Epoch 171/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0687 - val_loss: 0.0605\n",
      "Epoch 172/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0603\n",
      "Epoch 173/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0686 - val_loss: 0.0602\n",
      "Epoch 174/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0606\n",
      "Epoch 175/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0685 - val_loss: 0.0602\n",
      "Epoch 176/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0684 - val_loss: 0.0604\n",
      "Epoch 177/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0684 - val_loss: 0.0604\n",
      "Epoch 178/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0684 - val_loss: 0.0600\n",
      "Epoch 179/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0612\n",
      "Epoch 180/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0607\n",
      "Epoch 181/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0684 - val_loss: 0.0603\n",
      "Epoch 182/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0686 - val_loss: 0.0597\n",
      "Epoch 183/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0685 - val_loss: 0.0609\n",
      "Epoch 184/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0684 - val_loss: 0.0607\n",
      "Epoch 185/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0606\n",
      "Epoch 186/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0603\n",
      "Epoch 187/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0681 - val_loss: 0.0604\n",
      "Epoch 188/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0681 - val_loss: 0.0599\n",
      "Epoch 189/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0601\n",
      "Epoch 190/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0685 - val_loss: 0.0600\n",
      "Epoch 191/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0681 - val_loss: 0.0597\n",
      "Epoch 192/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0602\n",
      "Epoch 193/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0604\n",
      "Epoch 194/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0607\n",
      "Epoch 195/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0682 - val_loss: 0.0601\n",
      "Epoch 196/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0683 - val_loss: 0.0607\n",
      "Epoch 197/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0606\n",
      "Epoch 198/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0604\n",
      "Epoch 199/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0603\n",
      "Epoch 200/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0681 - val_loss: 0.0603\n",
      "Epoch 201/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0605\n",
      "Epoch 202/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0601\n",
      "Epoch 203/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0681 - val_loss: 0.0604\n",
      "Epoch 204/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0604\n",
      "Epoch 205/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0683 - val_loss: 0.0601\n",
      "Epoch 206/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0607\n",
      "Epoch 207/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0682 - val_loss: 0.0605\n",
      "Epoch 208/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0607\n",
      "Epoch 209/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0679 - val_loss: 0.0610\n",
      "Epoch 210/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0684 - val_loss: 0.0607\n",
      "Epoch 211/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0681 - val_loss: 0.0606\n",
      "Epoch 212/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0682 - val_loss: 0.0601\n",
      "Epoch 213/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0679 - val_loss: 0.0607\n",
      "Epoch 214/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0679 - val_loss: 0.0603\n",
      "Epoch 215/10000\n",
      "358389/358389 [==============================] - 13s 36us/sample - loss: 0.0679 - val_loss: 0.0611\n",
      "Epoch 216/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0604\n",
      "Epoch 217/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0677 - val_loss: 0.0600\n",
      "Epoch 218/10000\n",
      "358389/358389 [==============================] - 13s 38us/sample - loss: 0.0681 - val_loss: 0.0601\n",
      "Epoch 219/10000\n",
      "358389/358389 [==============================] - 14s 38us/sample - loss: 0.0679 - val_loss: 0.0609\n",
      "Epoch 220/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0679 - val_loss: 0.0609\n",
      "Epoch 221/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0679 - val_loss: 0.0606\n",
      "Epoch 222/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0608\n",
      "Epoch 223/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0608\n",
      "Epoch 224/10000\n",
      "358389/358389 [==============================] - 13s 37us/sample - loss: 0.0680 - val_loss: 0.0605\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject10.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject10.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11649, 24, 1)\n",
      "y_test.shape:  (11649, 1)\n",
      "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:46:43,671 WARNING Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:46:43,727 WARNING Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject10.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject11.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11304, 24, 1)\n",
      "y_test.shape:  (11304, 1)\n",
      "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:01,115 WARNING Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:01,165 WARNING Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject1.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 106\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6347, 24, 1)\n",
      "y_test.shape:  (6347, 1)\n",
      "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:16,801 WARNING Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:16,850 WARNING Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject2.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject2.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 108\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4674, 24, 1)\n",
      "y_test.shape:  (4674, 1)\n",
      "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:30,066 WARNING Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:30,114 WARNING Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject2.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject3.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject3.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 102\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5580, 24, 1)\n",
      "y_test.shape:  (5580, 1)\n",
      "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:41,962 WARNING Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:42,057 WARNING Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject3.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject4.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject4.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11154, 24, 1)\n",
      "y_test.shape:  (11154, 1)\n",
      "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:54,914 WARNING Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:47:54,962 WARNING Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject4.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject5.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject5.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9961, 24, 1)\n",
      "y_test.shape:  (9961, 1)\n",
      "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:10,968 WARNING Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:11,019 WARNING Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject5.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject6.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject6.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11176, 24, 1)\n",
      "y_test.shape:  (11176, 1)\n",
      "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:26,488 WARNING Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:26,543 WARNING Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject6.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject7.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject7.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11173, 24, 1)\n",
      "y_test.shape:  (11173, 1)\n",
      "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:43,322 WARNING Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:43,382 WARNING Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject7.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject8.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject8.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10978, 24, 1)\n",
      "y_test.shape:  (10978, 1)\n",
      "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:59,712 WARNING Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:48:59,761 WARNING Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject8.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject9.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject9.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11565, 24, 1)\n",
      "y_test.shape:  (11565, 1)\n",
      "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:49:15,681 WARNING Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:49:15,729 WARNING Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject9.csv\n",
      "2025-01-20 01:49:31,384 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 106\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 108\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 46\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 112\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 102\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 24\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 78\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (342943, 24, 1)\n",
      "y_train.shape:  (342943, 1)\n",
      "x_valid.shape:  (85712, 24, 1)\n",
      "y_valid.shape:  (85712, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:50:21,466 WARNING Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 01:50:21,516 WARNING Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 01:50:21,626 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 342943 samples, validate on 85712 samples\n",
      "Epoch 1/10000\n",
      "342943/342943 [==============================] - ETA: 0s - loss: 0.5368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342943/342943 [==============================] - 22s 64us/sample - loss: 0.5368 - val_loss: 0.1047\n",
      "Epoch 2/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.2479 - val_loss: 0.0966\n",
      "Epoch 3/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.2001 - val_loss: 0.0790\n",
      "Epoch 4/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.1614 - val_loss: 0.0840\n",
      "Epoch 5/10000\n",
      "342943/342943 [==============================] - 18s 53us/sample - loss: 0.1442 - val_loss: 0.0735\n",
      "Epoch 6/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.1304 - val_loss: 0.0657\n",
      "Epoch 7/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.1193 - val_loss: 0.0649\n",
      "Epoch 8/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.1095 - val_loss: 0.0669\n",
      "Epoch 9/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.1030 - val_loss: 0.0685\n",
      "Epoch 10/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0996 - val_loss: 0.0684\n",
      "Epoch 11/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0989 - val_loss: 0.0711\n",
      "Epoch 12/10000\n",
      "342943/342943 [==============================] - 13s 39us/sample - loss: 0.0981 - val_loss: 0.0699\n",
      "Epoch 13/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0980 - val_loss: 0.0727\n",
      "Epoch 14/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0979 - val_loss: 0.0695\n",
      "Epoch 15/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0982 - val_loss: 0.0701\n",
      "Epoch 16/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0979 - val_loss: 0.0692\n",
      "Epoch 17/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0979 - val_loss: 0.0700\n",
      "Epoch 18/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0976 - val_loss: 0.0704\n",
      "Epoch 19/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0976 - val_loss: 0.0689\n",
      "Epoch 20/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0978 - val_loss: 0.0690\n",
      "Epoch 21/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0975 - val_loss: 0.0722\n",
      "Epoch 22/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0972 - val_loss: 0.0688\n",
      "Epoch 23/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0976 - val_loss: 0.0702\n",
      "Epoch 24/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0974 - val_loss: 0.0704\n",
      "Epoch 25/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0969 - val_loss: 0.0697\n",
      "Epoch 26/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0970 - val_loss: 0.0699\n",
      "Epoch 27/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0967 - val_loss: 0.0700\n",
      "Epoch 28/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0971 - val_loss: 0.0691\n",
      "Epoch 29/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0971 - val_loss: 0.0702\n",
      "Epoch 30/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0972 - val_loss: 0.0690\n",
      "Epoch 31/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0972 - val_loss: 0.0690\n",
      "Epoch 32/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0970 - val_loss: 0.0687\n",
      "Epoch 33/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0972 - val_loss: 0.0683\n",
      "Epoch 34/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0968 - val_loss: 0.0706\n",
      "Epoch 35/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0967 - val_loss: 0.0709\n",
      "Epoch 36/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0968 - val_loss: 0.0691\n",
      "Epoch 37/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0969 - val_loss: 0.0702\n",
      "Epoch 38/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0971 - val_loss: 0.0693\n",
      "Epoch 39/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0965 - val_loss: 0.0683\n",
      "Epoch 40/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0969 - val_loss: 0.0726\n",
      "Epoch 41/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0967 - val_loss: 0.0710\n",
      "Epoch 42/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0972 - val_loss: 0.0731\n",
      "Epoch 43/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0966 - val_loss: 0.0687\n",
      "Epoch 44/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0970 - val_loss: 0.0697\n",
      "Epoch 45/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0961 - val_loss: 0.0704\n",
      "Epoch 46/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0709\n",
      "Epoch 47/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0966 - val_loss: 0.0690\n",
      "Epoch 48/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0960 - val_loss: 0.0691\n",
      "Epoch 49/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0963 - val_loss: 0.0698\n",
      "Epoch 50/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0965 - val_loss: 0.0691\n",
      "Epoch 51/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0965 - val_loss: 0.0690\n",
      "Epoch 52/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0963 - val_loss: 0.0700\n",
      "Epoch 53/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0698\n",
      "Epoch 54/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0961 - val_loss: 0.0689\n",
      "Epoch 55/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0958 - val_loss: 0.0683\n",
      "Epoch 56/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0962 - val_loss: 0.0678\n",
      "Epoch 57/10000\n",
      "342943/342943 [==============================] - 13s 39us/sample - loss: 0.0960 - val_loss: 0.0693\n",
      "Epoch 58/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0960 - val_loss: 0.0698\n",
      "Epoch 59/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0965 - val_loss: 0.0684\n",
      "Epoch 60/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0706\n",
      "Epoch 61/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0690\n",
      "Epoch 62/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0963 - val_loss: 0.0697\n",
      "Epoch 63/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0956 - val_loss: 0.0686\n",
      "Epoch 64/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0961 - val_loss: 0.0692\n",
      "Epoch 65/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0960 - val_loss: 0.0687\n",
      "Epoch 66/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0962 - val_loss: 0.0677\n",
      "Epoch 67/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0694\n",
      "Epoch 68/10000\n",
      "342943/342943 [==============================] - 13s 39us/sample - loss: 0.0959 - val_loss: 0.0681\n",
      "Epoch 69/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0960 - val_loss: 0.0685\n",
      "Epoch 70/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0681\n",
      "Epoch 71/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0960 - val_loss: 0.0686\n",
      "Epoch 72/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0965 - val_loss: 0.0681\n",
      "Epoch 73/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0959 - val_loss: 0.0682\n",
      "Epoch 74/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0962 - val_loss: 0.0681\n",
      "Epoch 75/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0961 - val_loss: 0.0693\n",
      "Epoch 76/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0684\n",
      "Epoch 77/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0953 - val_loss: 0.0697\n",
      "Epoch 78/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0964 - val_loss: 0.0687\n",
      "Epoch 79/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0956 - val_loss: 0.0680\n",
      "Epoch 80/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0962 - val_loss: 0.0695\n",
      "Epoch 81/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0684\n",
      "Epoch 82/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0960 - val_loss: 0.0676\n",
      "Epoch 83/10000\n",
      "342943/342943 [==============================] - 13s 36us/sample - loss: 0.0960 - val_loss: 0.0681\n",
      "Epoch 84/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0962 - val_loss: 0.0702\n",
      "Epoch 85/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0691\n",
      "Epoch 86/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0955 - val_loss: 0.0700\n",
      "Epoch 87/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0959 - val_loss: 0.0678\n",
      "Epoch 88/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0958 - val_loss: 0.0692\n",
      "Epoch 89/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0960 - val_loss: 0.0675\n",
      "Epoch 90/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0688\n",
      "Epoch 91/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0958 - val_loss: 0.0693\n",
      "Epoch 92/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0681\n",
      "Epoch 93/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0686\n",
      "Epoch 94/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0952 - val_loss: 0.0684\n",
      "Epoch 95/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0953 - val_loss: 0.0683\n",
      "Epoch 96/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0957 - val_loss: 0.0695\n",
      "Epoch 97/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0963 - val_loss: 0.0686\n",
      "Epoch 98/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0955 - val_loss: 0.0706\n",
      "Epoch 99/10000\n",
      "342943/342943 [==============================] - 13s 38us/sample - loss: 0.0960 - val_loss: 0.0673\n",
      "Epoch 100/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0962 - val_loss: 0.0720\n",
      "Epoch 101/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0956 - val_loss: 0.0686\n",
      "Epoch 102/10000\n",
      "342943/342943 [==============================] - 12s 36us/sample - loss: 0.0957 - val_loss: 0.0685\n",
      "Epoch 103/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0959 - val_loss: 0.0677\n",
      "Epoch 104/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0955 - val_loss: 0.0672\n",
      "Epoch 105/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0954 - val_loss: 0.0686\n",
      "Epoch 106/10000\n",
      "342943/342943 [==============================] - 13s 37us/sample - loss: 0.0958 - val_loss: 0.0691\n",
      "Epoch 107/10000\n",
      "342943/342943 [==============================] - 13s 39us/sample - loss: 0.0959 - val_loss: 0.0680\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject12.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject12.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11600, 24, 1)\n",
      "y_test.shape:  (11600, 1)\n",
      "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:13:30,450 WARNING Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:13:30,515 WARNING Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject12.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject13.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject13.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 34\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10038, 24, 1)\n",
      "y_test.shape:  (10038, 1)\n",
      "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:13:48,205 WARNING Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:13:48,256 WARNING Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject13.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject14.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject14.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11131, 24, 1)\n",
      "y_test.shape:  (11131, 1)\n",
      "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:04,647 WARNING Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:04,696 WARNING Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject14.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject15.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject15.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11731, 24, 1)\n",
      "y_test.shape:  (11731, 1)\n",
      "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:21,712 WARNING Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:21,789 WARNING Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject15.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject16.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject16.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11488, 24, 1)\n",
      "y_test.shape:  (11488, 1)\n",
      "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:39,469 WARNING Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:39,519 WARNING Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject16.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject17.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject17.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11821, 24, 1)\n",
      "y_test.shape:  (11821, 1)\n",
      "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:57,360 WARNING Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:14:57,412 WARNING Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject17.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject18.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11612, 24, 1)\n",
      "y_test.shape:  (11612, 1)\n",
      "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:14,150 WARNING Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:14,200 WARNING Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject19.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject19.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11591, 24, 1)\n",
      "y_test.shape:  (11591, 1)\n",
      "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:31,355 WARNING Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:31,405 WARNING Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject19.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject20.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject20.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11089, 24, 1)\n",
      "y_test.shape:  (11089, 1)\n",
      "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:48,330 WARNING Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:15:48,382 WARNING Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject20.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject21.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject21.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 20\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11046, 24, 1)\n",
      "y_test.shape:  (11046, 1)\n",
      "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:16:05,153 WARNING Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:16:05,202 WARNING Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject21.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject22.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject22.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11723, 24, 1)\n",
      "y_test.shape:  (11723, 1)\n",
      "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:16:21,789 WARNING Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:16:21,839 WARNING Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject22.csv\n",
      "2025-01-20 02:16:37,798 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 106\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 34\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 108\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 102\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 24\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 78\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (349705, 24, 1)\n",
      "y_train.shape:  (349705, 1)\n",
      "x_valid.shape:  (87403, 24, 1)\n",
      "y_valid.shape:  (87403, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:17:28,420 WARNING Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 02:17:28,473 WARNING Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 02:17:28,568 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 349705 samples, validate on 87403 samples\n",
      "Epoch 1/10000\n",
      "349184/349705 [============================>.] - ETA: 0s - loss: 1.2827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349705/349705 [==============================] - 23s 65us/sample - loss: 1.2818 - val_loss: 0.6164\n",
      "Epoch 2/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.3975 - val_loss: 0.0981\n",
      "Epoch 3/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.2102 - val_loss: 0.0847\n",
      "Epoch 4/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1860 - val_loss: 0.0754\n",
      "Epoch 5/10000\n",
      "349705/349705 [==============================] - 19s 53us/sample - loss: 0.1688 - val_loss: 0.0701\n",
      "Epoch 6/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1528 - val_loss: 0.0688\n",
      "Epoch 7/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1394 - val_loss: 0.0673\n",
      "Epoch 8/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1281 - val_loss: 0.0639\n",
      "Epoch 9/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1179 - val_loss: 0.0628\n",
      "Epoch 10/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1100 - val_loss: 0.0656\n",
      "Epoch 11/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.1049 - val_loss: 0.0662\n",
      "Epoch 12/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.1012 - val_loss: 0.0656\n",
      "Epoch 13/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1007 - val_loss: 0.0670\n",
      "Epoch 14/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0999 - val_loss: 0.0754\n",
      "Epoch 15/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.1003 - val_loss: 0.0672\n",
      "Epoch 16/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.1004 - val_loss: 0.0674\n",
      "Epoch 17/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.1001 - val_loss: 0.0688\n",
      "Epoch 18/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0998 - val_loss: 0.0673\n",
      "Epoch 19/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0993 - val_loss: 0.0664\n",
      "Epoch 20/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.1004 - val_loss: 0.0671\n",
      "Epoch 21/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0999 - val_loss: 0.0693\n",
      "Epoch 22/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0989 - val_loss: 0.0679\n",
      "Epoch 23/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0970 - val_loss: 0.0632\n",
      "Epoch 24/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0895 - val_loss: 0.0643\n",
      "Epoch 25/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0860 - val_loss: 0.0620\n",
      "Epoch 26/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0849 - val_loss: 0.0648\n",
      "Epoch 27/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0844 - val_loss: 0.0630\n",
      "Epoch 28/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0842 - val_loss: 0.0616\n",
      "Epoch 29/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0839 - val_loss: 0.0617\n",
      "Epoch 30/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0839 - val_loss: 0.0627\n",
      "Epoch 31/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0845 - val_loss: 0.0632\n",
      "Epoch 32/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0838 - val_loss: 0.0631\n",
      "Epoch 33/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0840 - val_loss: 0.0627\n",
      "Epoch 34/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0838 - val_loss: 0.0617\n",
      "Epoch 35/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0836 - val_loss: 0.0624\n",
      "Epoch 36/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0836 - val_loss: 0.0625\n",
      "Epoch 37/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0831 - val_loss: 0.0613\n",
      "Epoch 38/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0838 - val_loss: 0.0624\n",
      "Epoch 39/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0832 - val_loss: 0.0607\n",
      "Epoch 40/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0836 - val_loss: 0.0627\n",
      "Epoch 41/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0832 - val_loss: 0.0614\n",
      "Epoch 42/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0831 - val_loss: 0.0623\n",
      "Epoch 43/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0826 - val_loss: 0.0616\n",
      "Epoch 44/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0831 - val_loss: 0.0620\n",
      "Epoch 45/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0823 - val_loss: 0.0631\n",
      "Epoch 46/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0832 - val_loss: 0.0611\n",
      "Epoch 47/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0832 - val_loss: 0.0611\n",
      "Epoch 48/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0826 - val_loss: 0.0625\n",
      "Epoch 49/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0826 - val_loss: 0.0611\n",
      "Epoch 50/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0825 - val_loss: 0.0610\n",
      "Epoch 51/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0824 - val_loss: 0.0615\n",
      "Epoch 52/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0824 - val_loss: 0.0631\n",
      "Epoch 53/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0823 - val_loss: 0.0614\n",
      "Epoch 54/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0822 - val_loss: 0.0613\n",
      "Epoch 55/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0826 - val_loss: 0.0618\n",
      "Epoch 56/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0825 - val_loss: 0.0608\n",
      "Epoch 57/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0827 - val_loss: 0.0609\n",
      "Epoch 58/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0827 - val_loss: 0.0617\n",
      "Epoch 59/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0823 - val_loss: 0.0616\n",
      "Epoch 60/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0822 - val_loss: 0.0601\n",
      "Epoch 61/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0823 - val_loss: 0.0621\n",
      "Epoch 62/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0822 - val_loss: 0.0612\n",
      "Epoch 63/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0823 - val_loss: 0.0609\n",
      "Epoch 64/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0819 - val_loss: 0.0610\n",
      "Epoch 65/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0824 - val_loss: 0.0627\n",
      "Epoch 66/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0823 - val_loss: 0.0608\n",
      "Epoch 67/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0820 - val_loss: 0.0607\n",
      "Epoch 68/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0818 - val_loss: 0.0607\n",
      "Epoch 69/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0819 - val_loss: 0.0614\n",
      "Epoch 70/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0600\n",
      "Epoch 71/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0818 - val_loss: 0.0620\n",
      "Epoch 72/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0602\n",
      "Epoch 73/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0820 - val_loss: 0.0603\n",
      "Epoch 74/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0825 - val_loss: 0.0610\n",
      "Epoch 75/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0817 - val_loss: 0.0604\n",
      "Epoch 76/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0613\n",
      "Epoch 77/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0619\n",
      "Epoch 78/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0609\n",
      "Epoch 79/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0607\n",
      "Epoch 80/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0821 - val_loss: 0.0617\n",
      "Epoch 81/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0818 - val_loss: 0.0608\n",
      "Epoch 82/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0814 - val_loss: 0.0620\n",
      "Epoch 83/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0610\n",
      "Epoch 84/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0815 - val_loss: 0.0600\n",
      "Epoch 85/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0816 - val_loss: 0.0598\n",
      "Epoch 86/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0818 - val_loss: 0.0614\n",
      "Epoch 87/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0817 - val_loss: 0.0604\n",
      "Epoch 88/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0822 - val_loss: 0.0609\n",
      "Epoch 89/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0605\n",
      "Epoch 90/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0606\n",
      "Epoch 91/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0817 - val_loss: 0.0609\n",
      "Epoch 92/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0605\n",
      "Epoch 93/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0817 - val_loss: 0.0604\n",
      "Epoch 94/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0610\n",
      "Epoch 95/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0599\n",
      "Epoch 96/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0612\n",
      "Epoch 97/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0817 - val_loss: 0.0595\n",
      "Epoch 98/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0613\n",
      "Epoch 99/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 100/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0616\n",
      "Epoch 101/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0817 - val_loss: 0.0597\n",
      "Epoch 102/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0814 - val_loss: 0.0605\n",
      "Epoch 103/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0815 - val_loss: 0.0609\n",
      "Epoch 104/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0813 - val_loss: 0.0605\n",
      "Epoch 105/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0608\n",
      "Epoch 106/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0599\n",
      "Epoch 107/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0817 - val_loss: 0.0612\n",
      "Epoch 108/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0815 - val_loss: 0.0612\n",
      "Epoch 109/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0611\n",
      "Epoch 110/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0812 - val_loss: 0.0609\n",
      "Epoch 111/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0613\n",
      "Epoch 112/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0614\n",
      "Epoch 113/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0604\n",
      "Epoch 114/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0614\n",
      "Epoch 115/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0607\n",
      "Epoch 116/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0812 - val_loss: 0.0606\n",
      "Epoch 117/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0809 - val_loss: 0.0612\n",
      "Epoch 118/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0604\n",
      "Epoch 119/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0603\n",
      "Epoch 120/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0814 - val_loss: 0.0609\n",
      "Epoch 121/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0812 - val_loss: 0.0611\n",
      "Epoch 122/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0815 - val_loss: 0.0606\n",
      "Epoch 123/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0612\n",
      "Epoch 124/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0809 - val_loss: 0.0603\n",
      "Epoch 125/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0811 - val_loss: 0.0599\n",
      "Epoch 126/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0809 - val_loss: 0.0598\n",
      "Epoch 127/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0811 - val_loss: 0.0610\n",
      "Epoch 128/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 129/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0608\n",
      "Epoch 130/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0598\n",
      "Epoch 131/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 132/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0604\n",
      "Epoch 133/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0601\n",
      "Epoch 134/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0615\n",
      "Epoch 135/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0812 - val_loss: 0.0596\n",
      "Epoch 136/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0811 - val_loss: 0.0605\n",
      "Epoch 137/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0790 - val_loss: 0.0602\n",
      "Epoch 138/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0770 - val_loss: 0.0602\n",
      "Epoch 139/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0774 - val_loss: 0.0604\n",
      "Epoch 140/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0764 - val_loss: 0.0598\n",
      "Epoch 141/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0767 - val_loss: 0.0597\n",
      "Epoch 142/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0766 - val_loss: 0.0605\n",
      "Epoch 143/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0767 - val_loss: 0.0605\n",
      "Epoch 144/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0767 - val_loss: 0.0602\n",
      "Epoch 145/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0767 - val_loss: 0.0607\n",
      "Epoch 146/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0766 - val_loss: 0.0601\n",
      "Epoch 147/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0768 - val_loss: 0.0621\n",
      "Epoch 148/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0767 - val_loss: 0.0600\n",
      "Epoch 149/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0765 - val_loss: 0.0608\n",
      "Epoch 150/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0601\n",
      "Epoch 151/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0765 - val_loss: 0.0600\n",
      "Epoch 152/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0764 - val_loss: 0.0600\n",
      "Epoch 153/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0600\n",
      "Epoch 154/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0766 - val_loss: 0.0597\n",
      "Epoch 155/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0767 - val_loss: 0.0603\n",
      "Epoch 156/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0600\n",
      "Epoch 157/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0761 - val_loss: 0.0603\n",
      "Epoch 158/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0767 - val_loss: 0.0600\n",
      "Epoch 159/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0763 - val_loss: 0.0594\n",
      "Epoch 160/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0766 - val_loss: 0.0599\n",
      "Epoch 161/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0763 - val_loss: 0.0604\n",
      "Epoch 162/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0765 - val_loss: 0.0607\n",
      "Epoch 163/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0766 - val_loss: 0.0597\n",
      "Epoch 164/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0602\n",
      "Epoch 165/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0766 - val_loss: 0.0601\n",
      "Epoch 166/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0768 - val_loss: 0.0617\n",
      "Epoch 167/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0602\n",
      "Epoch 168/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0603\n",
      "Epoch 169/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0615\n",
      "Epoch 170/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0603\n",
      "Epoch 171/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0597\n",
      "Epoch 172/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0763 - val_loss: 0.0600\n",
      "Epoch 173/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0601\n",
      "Epoch 174/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0762 - val_loss: 0.0594\n",
      "Epoch 175/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0764 - val_loss: 0.0597\n",
      "Epoch 176/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0761 - val_loss: 0.0599\n",
      "Epoch 177/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0761 - val_loss: 0.0616\n",
      "Epoch 178/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0762 - val_loss: 0.0605\n",
      "Epoch 179/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0612\n",
      "Epoch 180/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0761 - val_loss: 0.0605\n",
      "Epoch 181/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0759 - val_loss: 0.0603\n",
      "Epoch 182/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0609\n",
      "Epoch 183/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0600\n",
      "Epoch 184/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0763 - val_loss: 0.0600\n",
      "Epoch 185/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0761 - val_loss: 0.0601\n",
      "Epoch 186/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0606\n",
      "Epoch 187/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0599\n",
      "Epoch 188/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0764 - val_loss: 0.0603\n",
      "Epoch 189/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0759 - val_loss: 0.0612\n",
      "Epoch 190/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0766 - val_loss: 0.0609\n",
      "Epoch 191/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0760 - val_loss: 0.0605\n",
      "Epoch 192/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0759 - val_loss: 0.0598\n",
      "Epoch 193/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0762 - val_loss: 0.0602\n",
      "Epoch 194/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0762 - val_loss: 0.0609\n",
      "Epoch 195/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0758 - val_loss: 0.0600\n",
      "Epoch 196/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0763 - val_loss: 0.0596\n",
      "Epoch 197/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0761 - val_loss: 0.0615\n",
      "Epoch 198/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0760 - val_loss: 0.0602\n",
      "Epoch 199/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0759 - val_loss: 0.0600\n",
      "Epoch 200/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0598\n",
      "Epoch 201/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0606\n",
      "Epoch 202/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0759 - val_loss: 0.0608\n",
      "Epoch 203/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0606\n",
      "Epoch 204/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0759 - val_loss: 0.0606\n",
      "Epoch 205/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0760 - val_loss: 0.0610\n",
      "Epoch 206/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0764 - val_loss: 0.0603\n",
      "Epoch 207/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0758 - val_loss: 0.0612\n",
      "Epoch 208/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0758 - val_loss: 0.0609\n",
      "Epoch 209/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0760 - val_loss: 0.0602\n",
      "Epoch 210/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0607\n",
      "Epoch 211/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0759 - val_loss: 0.0603\n",
      "Epoch 212/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0754 - val_loss: 0.0605\n",
      "Epoch 213/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0759 - val_loss: 0.0611\n",
      "Epoch 214/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0758 - val_loss: 0.0604\n",
      "Epoch 215/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0599\n",
      "Epoch 216/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0755 - val_loss: 0.0602\n",
      "Epoch 217/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0612\n",
      "Epoch 218/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0606\n",
      "Epoch 219/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0616\n",
      "Epoch 220/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0763 - val_loss: 0.0604\n",
      "Epoch 221/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0605\n",
      "Epoch 222/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0758 - val_loss: 0.0608\n",
      "Epoch 223/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0613\n",
      "Epoch 224/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0601\n",
      "Epoch 225/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0596\n",
      "Epoch 226/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0603\n",
      "Epoch 227/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0610\n",
      "Epoch 228/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0603\n",
      "Epoch 229/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0759 - val_loss: 0.0597\n",
      "Epoch 230/10000\n",
      "349705/349705 [==============================] - 13s 36us/sample - loss: 0.0755 - val_loss: 0.0606\n",
      "Epoch 231/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0756 - val_loss: 0.0611\n",
      "Epoch 232/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0604\n",
      "Epoch 233/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0605\n",
      "Epoch 234/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0757 - val_loss: 0.0605\n",
      "Epoch 235/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0757 - val_loss: 0.0605\n",
      "Epoch 236/10000\n",
      "349705/349705 [==============================] - 13s 39us/sample - loss: 0.0759 - val_loss: 0.0609\n",
      "Epoch 237/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0758 - val_loss: 0.0597\n",
      "Epoch 238/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0600\n",
      "Epoch 239/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0598\n",
      "Epoch 240/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0751 - val_loss: 0.0608\n",
      "Epoch 241/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0758 - val_loss: 0.0610\n",
      "Epoch 242/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0754 - val_loss: 0.0601\n",
      "Epoch 243/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0759 - val_loss: 0.0604\n",
      "Epoch 244/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0754 - val_loss: 0.0604\n",
      "Epoch 245/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0617\n",
      "Epoch 246/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0756 - val_loss: 0.0609\n",
      "Epoch 247/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0755 - val_loss: 0.0602\n",
      "Epoch 248/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0755 - val_loss: 0.0612\n",
      "Epoch 249/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0754 - val_loss: 0.0604\n",
      "Epoch 250/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0755 - val_loss: 0.0614\n",
      "Epoch 251/10000\n",
      "349705/349705 [==============================] - 13s 37us/sample - loss: 0.0753 - val_loss: 0.0615\n",
      "Epoch 252/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0614\n",
      "Epoch 253/10000\n",
      "349705/349705 [==============================] - 14s 39us/sample - loss: 0.0756 - val_loss: 0.0612\n",
      "Epoch 254/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0753 - val_loss: 0.0604\n",
      "Epoch 255/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0611\n",
      "Epoch 256/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0753 - val_loss: 0.0605\n",
      "Epoch 257/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0755 - val_loss: 0.0602\n",
      "Epoch 258/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0751 - val_loss: 0.0604\n",
      "Epoch 259/10000\n",
      "349705/349705 [==============================] - 13s 38us/sample - loss: 0.0752 - val_loss: 0.0610\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject23.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject23.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11297, 24, 1)\n",
      "y_test.shape:  (11297, 1)\n",
      "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:14:46,523 WARNING Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:14:46,574 WARNING Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject23.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject24.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 46\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9064, 24, 1)\n",
      "y_test.shape:  (9064, 1)\n",
      "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:03,914 WARNING Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:03,967 WARNING Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject25.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11652, 24, 1)\n",
      "y_test.shape:  (11652, 1)\n",
      "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:19,555 WARNING Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:19,604 WARNING Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject25.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject26.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject26.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11457, 24, 1)\n",
      "y_test.shape:  (11457, 1)\n",
      "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:36,819 WARNING Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:36,869 WARNING Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject26.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject27.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject27.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11735, 24, 1)\n",
      "y_test.shape:  (11735, 1)\n",
      "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:54,257 WARNING Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:15:54,305 WARNING Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject27.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject28.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject28.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11666, 24, 1)\n",
      "y_test.shape:  (11666, 1)\n",
      "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:11,692 WARNING Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:11,741 WARNING Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject28.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject29.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject29.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 112\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (3151, 24, 1)\n",
      "y_test.shape:  (3151, 1)\n",
      "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:28,679 WARNING Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:28,733 WARNING Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject29.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject30.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject30.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11592, 24, 1)\n",
      "y_test.shape:  (11592, 1)\n",
      "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:41,053 WARNING Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:41,126 WARNING Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject30.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject31.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject31.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11851, 24, 1)\n",
      "y_test.shape:  (11851, 1)\n",
      "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:58,557 WARNING Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:16:58,624 WARNING Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject31.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject32.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject32.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11439, 24, 1)\n",
      "y_test.shape:  (11439, 1)\n",
      "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:17:16,188 WARNING Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:17:16,257 WARNING Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject32.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject33.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject33.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11513, 24, 1)\n",
      "y_test.shape:  (11513, 1)\n",
      "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:17:34,364 WARNING Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:17:34,416 WARNING Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject33.csv\n",
      "2025-01-20 03:17:51,558 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 106\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 34\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 108\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 46\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 112\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 102\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 78\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (345487, 24, 1)\n",
      "y_train.shape:  (345487, 1)\n",
      "x_valid.shape:  (86351, 24, 1)\n",
      "y_valid.shape:  (86351, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:18:44,161 WARNING Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 03:18:44,214 WARNING Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 03:18:44,308 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 345487 samples, validate on 86351 samples\n",
      "Epoch 1/10000\n",
      "345487/345487 [==============================] - ETA: 0s - loss: 1.7839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345487/345487 [==============================] - 23s 66us/sample - loss: 1.7839 - val_loss: 0.6609\n",
      "Epoch 2/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.6687 - val_loss: 0.2392\n",
      "Epoch 3/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.2687 - val_loss: 0.0938\n",
      "Epoch 4/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.2244 - val_loss: 0.0827\n",
      "Epoch 5/10000\n",
      "345487/345487 [==============================] - 18s 53us/sample - loss: 0.2073 - val_loss: 0.0780\n",
      "Epoch 6/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.1897 - val_loss: 0.0722\n",
      "Epoch 7/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.1778 - val_loss: 0.0701\n",
      "Epoch 8/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.1636 - val_loss: 0.0670\n",
      "Epoch 9/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.1507 - val_loss: 0.0655\n",
      "Epoch 10/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.1381 - val_loss: 0.0655\n",
      "Epoch 11/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.1281 - val_loss: 0.0673\n",
      "Epoch 12/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.1177 - val_loss: 0.0663\n",
      "Epoch 13/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.1090 - val_loss: 0.0670\n",
      "Epoch 14/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.1033 - val_loss: 0.0659\n",
      "Epoch 15/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0982 - val_loss: 0.0648\n",
      "Epoch 16/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0937 - val_loss: 0.0630\n",
      "Epoch 17/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0913 - val_loss: 0.0646\n",
      "Epoch 18/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0885 - val_loss: 0.0631\n",
      "Epoch 19/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0873 - val_loss: 0.0624\n",
      "Epoch 20/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0869 - val_loss: 0.0652\n",
      "Epoch 21/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0858 - val_loss: 0.0629\n",
      "Epoch 22/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0857 - val_loss: 0.0636\n",
      "Epoch 23/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0857 - val_loss: 0.0622\n",
      "Epoch 24/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0858 - val_loss: 0.0639\n",
      "Epoch 25/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0843 - val_loss: 0.0621\n",
      "Epoch 26/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0834 - val_loss: 0.0635\n",
      "Epoch 27/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0828 - val_loss: 0.0621\n",
      "Epoch 28/10000\n",
      "345487/345487 [==============================] - 14s 39us/sample - loss: 0.0826 - val_loss: 0.0621\n",
      "Epoch 29/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0825 - val_loss: 0.0622\n",
      "Epoch 30/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0829 - val_loss: 0.0640\n",
      "Epoch 31/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0827 - val_loss: 0.0620\n",
      "Epoch 32/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0831 - val_loss: 0.0619\n",
      "Epoch 33/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0821 - val_loss: 0.0615\n",
      "Epoch 34/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0824 - val_loss: 0.0645\n",
      "Epoch 35/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0827 - val_loss: 0.0642\n",
      "Epoch 36/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0826 - val_loss: 0.0638\n",
      "Epoch 37/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0825 - val_loss: 0.0622\n",
      "Epoch 38/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0825 - val_loss: 0.0615\n",
      "Epoch 39/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0823 - val_loss: 0.0623\n",
      "Epoch 40/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0821 - val_loss: 0.0627\n",
      "Epoch 41/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0820 - val_loss: 0.0616\n",
      "Epoch 42/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0821 - val_loss: 0.0621\n",
      "Epoch 43/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0822 - val_loss: 0.0611\n",
      "Epoch 44/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0821 - val_loss: 0.0625\n",
      "Epoch 45/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0817 - val_loss: 0.0625\n",
      "Epoch 46/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0820 - val_loss: 0.0616\n",
      "Epoch 47/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0819 - val_loss: 0.0622\n",
      "Epoch 48/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0820 - val_loss: 0.0626\n",
      "Epoch 49/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0820 - val_loss: 0.0617\n",
      "Epoch 50/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0819 - val_loss: 0.0620\n",
      "Epoch 51/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0645\n",
      "Epoch 52/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0820 - val_loss: 0.0623\n",
      "Epoch 53/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0622\n",
      "Epoch 54/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0819 - val_loss: 0.0609\n",
      "Epoch 55/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0819 - val_loss: 0.0611\n",
      "Epoch 56/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0816 - val_loss: 0.0626\n",
      "Epoch 57/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0615\n",
      "Epoch 58/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0811 - val_loss: 0.0614\n",
      "Epoch 59/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0815 - val_loss: 0.0633\n",
      "Epoch 60/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0813 - val_loss: 0.0625\n",
      "Epoch 61/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0817 - val_loss: 0.0620\n",
      "Epoch 62/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0814 - val_loss: 0.0615\n",
      "Epoch 63/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0814 - val_loss: 0.0616\n",
      "Epoch 64/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0623\n",
      "Epoch 65/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0817 - val_loss: 0.0611\n",
      "Epoch 66/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0816 - val_loss: 0.0612\n",
      "Epoch 67/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0619\n",
      "Epoch 68/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0811 - val_loss: 0.0608\n",
      "Epoch 69/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0620\n",
      "Epoch 70/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0818 - val_loss: 0.0631\n",
      "Epoch 71/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0609\n",
      "Epoch 72/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0621\n",
      "Epoch 73/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0815 - val_loss: 0.0611\n",
      "Epoch 74/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0811 - val_loss: 0.0610\n",
      "Epoch 75/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0623\n",
      "Epoch 76/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0811 - val_loss: 0.0608\n",
      "Epoch 77/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0810 - val_loss: 0.0613\n",
      "Epoch 78/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0815 - val_loss: 0.0613\n",
      "Epoch 79/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0809 - val_loss: 0.0608\n",
      "Epoch 80/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0811 - val_loss: 0.0609\n",
      "Epoch 81/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0606\n",
      "Epoch 82/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0809 - val_loss: 0.0612\n",
      "Epoch 83/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0605\n",
      "Epoch 84/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0609\n",
      "Epoch 85/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0608\n",
      "Epoch 86/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0614\n",
      "Epoch 87/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0613\n",
      "Epoch 88/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0813 - val_loss: 0.0611\n",
      "Epoch 89/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0811 - val_loss: 0.0617\n",
      "Epoch 90/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0811 - val_loss: 0.0608\n",
      "Epoch 91/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0811 - val_loss: 0.0610\n",
      "Epoch 92/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0610\n",
      "Epoch 93/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0809 - val_loss: 0.0629\n",
      "Epoch 94/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0812 - val_loss: 0.0623\n",
      "Epoch 95/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0810 - val_loss: 0.0611\n",
      "Epoch 96/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0804 - val_loss: 0.0616\n",
      "Epoch 97/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0808 - val_loss: 0.0611\n",
      "Epoch 98/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0809 - val_loss: 0.0615\n",
      "Epoch 99/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0607\n",
      "Epoch 100/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0607\n",
      "Epoch 101/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0814 - val_loss: 0.0606\n",
      "Epoch 102/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0614\n",
      "Epoch 103/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0807 - val_loss: 0.0606\n",
      "Epoch 104/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0604\n",
      "Epoch 105/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0614\n",
      "Epoch 106/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0612\n",
      "Epoch 107/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0606\n",
      "Epoch 108/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0614\n",
      "Epoch 109/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0612\n",
      "Epoch 110/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0619\n",
      "Epoch 111/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0615\n",
      "Epoch 112/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 113/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0604\n",
      "Epoch 114/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0808 - val_loss: 0.0619\n",
      "Epoch 115/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0807 - val_loss: 0.0612\n",
      "Epoch 116/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0802 - val_loss: 0.0610\n",
      "Epoch 117/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0811 - val_loss: 0.0614\n",
      "Epoch 118/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0618\n",
      "Epoch 119/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0609\n",
      "Epoch 120/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0611\n",
      "Epoch 121/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0616\n",
      "Epoch 122/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0804 - val_loss: 0.0603\n",
      "Epoch 123/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0624\n",
      "Epoch 124/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0804 - val_loss: 0.0620\n",
      "Epoch 125/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0608\n",
      "Epoch 126/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0606\n",
      "Epoch 127/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0804 - val_loss: 0.0624\n",
      "Epoch 128/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0610\n",
      "Epoch 129/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0626\n",
      "Epoch 130/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0606\n",
      "Epoch 131/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 132/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0802 - val_loss: 0.0605\n",
      "Epoch 133/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0609\n",
      "Epoch 134/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0806 - val_loss: 0.0605\n",
      "Epoch 135/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0801 - val_loss: 0.0600\n",
      "Epoch 136/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0608\n",
      "Epoch 137/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0610\n",
      "Epoch 138/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0802 - val_loss: 0.0608\n",
      "Epoch 139/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0801 - val_loss: 0.0615\n",
      "Epoch 140/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0802 - val_loss: 0.0604\n",
      "Epoch 141/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0807 - val_loss: 0.0611\n",
      "Epoch 142/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0625\n",
      "Epoch 143/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0802 - val_loss: 0.0610\n",
      "Epoch 144/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0807 - val_loss: 0.0605\n",
      "Epoch 145/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0804 - val_loss: 0.0604\n",
      "Epoch 146/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0802 - val_loss: 0.0607\n",
      "Epoch 147/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0613\n",
      "Epoch 148/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0808 - val_loss: 0.0603\n",
      "Epoch 149/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0610\n",
      "Epoch 150/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0804 - val_loss: 0.0607\n",
      "Epoch 151/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0799 - val_loss: 0.0609\n",
      "Epoch 152/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0799 - val_loss: 0.0614\n",
      "Epoch 153/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0805 - val_loss: 0.0618\n",
      "Epoch 154/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0602\n",
      "Epoch 155/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0806 - val_loss: 0.0622\n",
      "Epoch 156/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0802 - val_loss: 0.0612\n",
      "Epoch 157/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0808 - val_loss: 0.0598\n",
      "Epoch 158/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0799 - val_loss: 0.0615\n",
      "Epoch 159/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0806 - val_loss: 0.0616\n",
      "Epoch 160/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0613\n",
      "Epoch 161/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0612\n",
      "Epoch 162/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0608\n",
      "Epoch 163/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0804 - val_loss: 0.0607\n",
      "Epoch 164/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0606\n",
      "Epoch 165/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0801 - val_loss: 0.0615\n",
      "Epoch 166/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0804 - val_loss: 0.0605\n",
      "Epoch 167/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0803 - val_loss: 0.0612\n",
      "Epoch 168/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0608\n",
      "Epoch 169/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0801 - val_loss: 0.0615\n",
      "Epoch 170/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0803 - val_loss: 0.0618\n",
      "Epoch 171/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0797 - val_loss: 0.0619\n",
      "Epoch 172/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0798 - val_loss: 0.0606\n",
      "Epoch 173/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0805 - val_loss: 0.0614\n",
      "Epoch 174/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0619\n",
      "Epoch 175/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0603\n",
      "Epoch 176/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0800 - val_loss: 0.0612\n",
      "Epoch 177/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0610\n",
      "Epoch 178/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0802 - val_loss: 0.0617\n",
      "Epoch 179/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0801 - val_loss: 0.0611\n",
      "Epoch 180/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0611\n",
      "Epoch 181/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0798 - val_loss: 0.0610\n",
      "Epoch 182/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0612\n",
      "Epoch 183/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0605\n",
      "Epoch 184/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0800 - val_loss: 0.0612\n",
      "Epoch 185/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0799 - val_loss: 0.0607\n",
      "Epoch 186/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0608\n",
      "Epoch 187/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0610\n",
      "Epoch 188/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0797 - val_loss: 0.0611\n",
      "Epoch 189/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0802 - val_loss: 0.0606\n",
      "Epoch 190/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0803 - val_loss: 0.0608\n",
      "Epoch 191/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0611\n",
      "Epoch 192/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0802 - val_loss: 0.0606\n",
      "Epoch 193/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0797 - val_loss: 0.0610\n",
      "Epoch 194/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0798 - val_loss: 0.0613\n",
      "Epoch 195/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0799 - val_loss: 0.0610\n",
      "Epoch 196/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0801 - val_loss: 0.0617\n",
      "Epoch 197/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0612\n",
      "Epoch 198/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0800 - val_loss: 0.0604\n",
      "Epoch 199/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0794 - val_loss: 0.0605\n",
      "Epoch 200/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0617\n",
      "Epoch 201/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0608\n",
      "Epoch 202/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0798 - val_loss: 0.0622\n",
      "Epoch 203/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0798 - val_loss: 0.0622\n",
      "Epoch 204/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0796 - val_loss: 0.0613\n",
      "Epoch 205/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0798 - val_loss: 0.0610\n",
      "Epoch 206/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0795 - val_loss: 0.0607\n",
      "Epoch 207/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0605\n",
      "Epoch 208/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0794 - val_loss: 0.0611\n",
      "Epoch 209/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0614\n",
      "Epoch 210/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0794 - val_loss: 0.0612\n",
      "Epoch 211/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0611\n",
      "Epoch 212/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0607\n",
      "Epoch 213/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0797 - val_loss: 0.0610\n",
      "Epoch 214/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0606\n",
      "Epoch 215/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0613\n",
      "Epoch 216/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0793 - val_loss: 0.0614\n",
      "Epoch 217/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0798 - val_loss: 0.0610\n",
      "Epoch 218/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0611\n",
      "Epoch 219/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0622\n",
      "Epoch 220/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0797 - val_loss: 0.0611\n",
      "Epoch 221/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0795 - val_loss: 0.0606\n",
      "Epoch 222/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0798 - val_loss: 0.0614\n",
      "Epoch 223/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0798 - val_loss: 0.0611\n",
      "Epoch 224/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0796 - val_loss: 0.0612\n",
      "Epoch 225/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0797 - val_loss: 0.0608\n",
      "Epoch 226/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0794 - val_loss: 0.0613\n",
      "Epoch 227/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0609\n",
      "Epoch 228/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0619\n",
      "Epoch 229/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0795 - val_loss: 0.0616\n",
      "Epoch 230/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0796 - val_loss: 0.0614\n",
      "Epoch 231/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0618\n",
      "Epoch 232/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0799 - val_loss: 0.0609\n",
      "Epoch 233/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0607\n",
      "Epoch 234/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0611\n",
      "Epoch 235/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0798 - val_loss: 0.0609\n",
      "Epoch 236/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0615\n",
      "Epoch 237/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0798 - val_loss: 0.0614\n",
      "Epoch 238/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0610\n",
      "Epoch 239/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0799 - val_loss: 0.0613\n",
      "Epoch 240/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0797 - val_loss: 0.0609\n",
      "Epoch 241/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0623\n",
      "Epoch 242/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0791 - val_loss: 0.0605\n",
      "Epoch 243/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0798 - val_loss: 0.0607\n",
      "Epoch 244/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0797 - val_loss: 0.0616\n",
      "Epoch 245/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0613\n",
      "Epoch 246/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0612\n",
      "Epoch 247/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0797 - val_loss: 0.0618\n",
      "Epoch 248/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0794 - val_loss: 0.0606\n",
      "Epoch 249/10000\n",
      "345487/345487 [==============================] - 14s 40us/sample - loss: 0.0795 - val_loss: 0.0619\n",
      "Epoch 250/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0613\n",
      "Epoch 251/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0613\n",
      "Epoch 252/10000\n",
      "345487/345487 [==============================] - 13s 37us/sample - loss: 0.0789 - val_loss: 0.0610\n",
      "Epoch 253/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0796 - val_loss: 0.0612\n",
      "Epoch 254/10000\n",
      "345487/345487 [==============================] - 13s 39us/sample - loss: 0.0791 - val_loss: 0.0611\n",
      "Epoch 255/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0614\n",
      "Epoch 256/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0792 - val_loss: 0.0619\n",
      "Epoch 257/10000\n",
      "345487/345487 [==============================] - 13s 38us/sample - loss: 0.0795 - val_loss: 0.0612\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject34.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject34.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11361, 24, 1)\n",
      "y_test.shape:  (11361, 1)\n",
      "WARNING:tensorflow:Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:20,289 WARNING Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:20,348 WARNING Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject34.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject35.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject35.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11608, 24, 1)\n",
      "y_test.shape:  (11608, 1)\n",
      "WARNING:tensorflow:Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:38,692 WARNING Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:38,740 WARNING Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject35.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject36.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject36.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 25\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10847, 24, 1)\n",
      "y_test.shape:  (10847, 1)\n",
      "WARNING:tensorflow:Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:56,637 WARNING Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:15:56,686 WARNING Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject36.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject37.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject37.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11329, 24, 1)\n",
      "y_test.shape:  (11329, 1)\n",
      "WARNING:tensorflow:Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:13,672 WARNING Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:13,721 WARNING Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject37.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject38.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject38.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11351, 24, 1)\n",
      "y_test.shape:  (11351, 1)\n",
      "WARNING:tensorflow:Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:31,586 WARNING Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:31,637 WARNING Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject38.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject39.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject39.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10627, 24, 1)\n",
      "y_test.shape:  (10627, 1)\n",
      "WARNING:tensorflow:Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:49,439 WARNING Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:16:49,490 WARNING Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject39.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject40.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject40.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11169, 24, 1)\n",
      "y_test.shape:  (11169, 1)\n",
      "WARNING:tensorflow:Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:06,917 WARNING Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:06,971 WARNING Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject40.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject41.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject41.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 24\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11214, 24, 1)\n",
      "y_test.shape:  (11214, 1)\n",
      "WARNING:tensorflow:Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:24,895 WARNING Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:24,966 WARNING Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject41.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject42.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject42.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11123, 24, 1)\n",
      "y_test.shape:  (11123, 1)\n",
      "WARNING:tensorflow:Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:43,719 WARNING Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:17:43,769 WARNING Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject42.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject43.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject43.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11189, 24, 1)\n",
      "y_test.shape:  (11189, 1)\n",
      "WARNING:tensorflow:Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:18:02,436 WARNING Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:18:02,486 WARNING Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject43.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject44.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject44.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (9869, 24, 1)\n",
      "y_test.shape:  (9869, 1)\n",
      "WARNING:tensorflow:Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:18:20,675 WARNING Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:18:20,725 WARNING Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject44.csv\n",
      "2025-01-20 04:18:37,628 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 106\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 34\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 108\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 46\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 112\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 102\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 25\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 21\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 24\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 20\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (374848, 24, 1)\n",
      "y_train.shape:  (374848, 1)\n",
      "x_valid.shape:  (93687, 24, 1)\n",
      "y_valid.shape:  (93687, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:19:33,196 WARNING Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 04:19:33,251 WARNING Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-20 04:19:33,343 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 374848 samples, validate on 93687 samples\n",
      "Epoch 1/10000\n",
      "374784/374848 [============================>.] - ETA: 0s - loss: 0.5736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374848/374848 [==============================] - 24s 65us/sample - loss: 0.5735 - val_loss: 0.1081\n",
      "Epoch 2/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.2139 - val_loss: 0.0960\n",
      "Epoch 3/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.1796 - val_loss: 0.0733\n",
      "Epoch 4/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.1586 - val_loss: 0.0686\n",
      "Epoch 5/10000\n",
      "374848/374848 [==============================] - 20s 54us/sample - loss: 0.1426 - val_loss: 0.0715\n",
      "Epoch 6/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.1294 - val_loss: 0.0655\n",
      "Epoch 7/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.1176 - val_loss: 0.0686\n",
      "Epoch 8/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.1091 - val_loss: 0.0690\n",
      "Epoch 9/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.1014 - val_loss: 0.0675\n",
      "Epoch 10/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0935 - val_loss: 0.0702\n",
      "Epoch 11/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0866 - val_loss: 0.0678\n",
      "Epoch 12/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0843 - val_loss: 0.0661\n",
      "Epoch 13/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0823 - val_loss: 0.0652\n",
      "Epoch 14/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0808 - val_loss: 0.0660\n",
      "Epoch 15/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0806 - val_loss: 0.0655\n",
      "Epoch 16/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0801 - val_loss: 0.0648\n",
      "Epoch 17/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0802 - val_loss: 0.0654\n",
      "Epoch 18/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0801 - val_loss: 0.0647\n",
      "Epoch 19/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0799 - val_loss: 0.0650\n",
      "Epoch 20/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0798 - val_loss: 0.0652\n",
      "Epoch 21/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0794 - val_loss: 0.0651\n",
      "Epoch 22/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0793 - val_loss: 0.0658\n",
      "Epoch 23/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0796 - val_loss: 0.0656\n",
      "Epoch 24/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0792 - val_loss: 0.0667\n",
      "Epoch 25/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0791 - val_loss: 0.0661\n",
      "Epoch 26/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0792 - val_loss: 0.0635\n",
      "Epoch 27/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0789 - val_loss: 0.0640\n",
      "Epoch 28/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0791 - val_loss: 0.0652\n",
      "Epoch 29/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0790 - val_loss: 0.0636\n",
      "Epoch 30/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0791 - val_loss: 0.0668\n",
      "Epoch 31/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0789 - val_loss: 0.0637\n",
      "Epoch 32/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0787 - val_loss: 0.0634\n",
      "Epoch 33/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0785 - val_loss: 0.0647\n",
      "Epoch 34/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0784 - val_loss: 0.0633\n",
      "Epoch 35/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0783 - val_loss: 0.0635\n",
      "Epoch 36/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0782 - val_loss: 0.0632\n",
      "Epoch 37/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0783 - val_loss: 0.0634\n",
      "Epoch 38/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0785 - val_loss: 0.0646\n",
      "Epoch 39/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0781 - val_loss: 0.0670\n",
      "Epoch 40/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0782 - val_loss: 0.0659\n",
      "Epoch 41/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0786 - val_loss: 0.0645\n",
      "Epoch 42/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0781 - val_loss: 0.0634\n",
      "Epoch 43/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0784 - val_loss: 0.0630\n",
      "Epoch 44/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0779 - val_loss: 0.0636\n",
      "Epoch 45/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0782 - val_loss: 0.0655\n",
      "Epoch 46/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0781 - val_loss: 0.0641\n",
      "Epoch 47/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0779 - val_loss: 0.0632\n",
      "Epoch 48/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0775 - val_loss: 0.0646\n",
      "Epoch 49/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0779 - val_loss: 0.0635\n",
      "Epoch 50/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0781 - val_loss: 0.0640\n",
      "Epoch 51/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0776 - val_loss: 0.0629\n",
      "Epoch 52/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0779 - val_loss: 0.0642\n",
      "Epoch 53/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0777 - val_loss: 0.0659\n",
      "Epoch 54/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0778 - val_loss: 0.0635\n",
      "Epoch 55/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0780 - val_loss: 0.0638\n",
      "Epoch 56/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0777 - val_loss: 0.0652\n",
      "Epoch 57/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0775 - val_loss: 0.0636\n",
      "Epoch 58/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0777 - val_loss: 0.0625\n",
      "Epoch 59/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0779 - val_loss: 0.0630\n",
      "Epoch 60/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0777 - val_loss: 0.0661\n",
      "Epoch 61/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0777 - val_loss: 0.0629\n",
      "Epoch 62/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0772 - val_loss: 0.0633\n",
      "Epoch 63/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0777 - val_loss: 0.0634\n",
      "Epoch 64/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0775 - val_loss: 0.0634\n",
      "Epoch 65/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0636\n",
      "Epoch 66/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0772 - val_loss: 0.0642\n",
      "Epoch 67/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0776 - val_loss: 0.0627\n",
      "Epoch 68/10000\n",
      "374848/374848 [==============================] - 14s 37us/sample - loss: 0.0773 - val_loss: 0.0638\n",
      "Epoch 69/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0774 - val_loss: 0.0634\n",
      "Epoch 70/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0775 - val_loss: 0.0652\n",
      "Epoch 71/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0772 - val_loss: 0.0637\n",
      "Epoch 72/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0775 - val_loss: 0.0644\n",
      "Epoch 73/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0771 - val_loss: 0.0628\n",
      "Epoch 74/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0774 - val_loss: 0.0628\n",
      "Epoch 75/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0773 - val_loss: 0.0636\n",
      "Epoch 76/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0772 - val_loss: 0.0640\n",
      "Epoch 77/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0774 - val_loss: 0.0626\n",
      "Epoch 78/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0773 - val_loss: 0.0628\n",
      "Epoch 79/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0628\n",
      "Epoch 80/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0772 - val_loss: 0.0638\n",
      "Epoch 81/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0774 - val_loss: 0.0628\n",
      "Epoch 82/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0772 - val_loss: 0.0635\n",
      "Epoch 83/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0773 - val_loss: 0.0629\n",
      "Epoch 84/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0773 - val_loss: 0.0627\n",
      "Epoch 85/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0773 - val_loss: 0.0630\n",
      "Epoch 86/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0774 - val_loss: 0.0637\n",
      "Epoch 87/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0634\n",
      "Epoch 88/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0769 - val_loss: 0.0625\n",
      "Epoch 89/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0771 - val_loss: 0.0626\n",
      "Epoch 90/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 91/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0770 - val_loss: 0.0623\n",
      "Epoch 92/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0774 - val_loss: 0.0624\n",
      "Epoch 93/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0625\n",
      "Epoch 94/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0768 - val_loss: 0.0641\n",
      "Epoch 95/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0769 - val_loss: 0.0633\n",
      "Epoch 96/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0772 - val_loss: 0.0628\n",
      "Epoch 97/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0642\n",
      "Epoch 98/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0770 - val_loss: 0.0637\n",
      "Epoch 99/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0633\n",
      "Epoch 100/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0771 - val_loss: 0.0637\n",
      "Epoch 101/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0765 - val_loss: 0.0623\n",
      "Epoch 102/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0770 - val_loss: 0.0633\n",
      "Epoch 103/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0771 - val_loss: 0.0625\n",
      "Epoch 104/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0768 - val_loss: 0.0635\n",
      "Epoch 105/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0770 - val_loss: 0.0628\n",
      "Epoch 106/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0640\n",
      "Epoch 107/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0768 - val_loss: 0.0637\n",
      "Epoch 108/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0770 - val_loss: 0.0638\n",
      "Epoch 109/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0769 - val_loss: 0.0628\n",
      "Epoch 110/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0771 - val_loss: 0.0627\n",
      "Epoch 111/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0770 - val_loss: 0.0624\n",
      "Epoch 112/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0769 - val_loss: 0.0646\n",
      "Epoch 113/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0769 - val_loss: 0.0626\n",
      "Epoch 114/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0768 - val_loss: 0.0646\n",
      "Epoch 115/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0622\n",
      "Epoch 116/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0768 - val_loss: 0.0629\n",
      "Epoch 117/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0768 - val_loss: 0.0626\n",
      "Epoch 118/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0770 - val_loss: 0.0625\n",
      "Epoch 119/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0767 - val_loss: 0.0626\n",
      "Epoch 120/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0765 - val_loss: 0.0620\n",
      "Epoch 121/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0765 - val_loss: 0.0626\n",
      "Epoch 122/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0764 - val_loss: 0.0640\n",
      "Epoch 123/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0621\n",
      "Epoch 124/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0765 - val_loss: 0.0630\n",
      "Epoch 125/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0767 - val_loss: 0.0627\n",
      "Epoch 126/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0766 - val_loss: 0.0642\n",
      "Epoch 127/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0765 - val_loss: 0.0622\n",
      "Epoch 128/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0764 - val_loss: 0.0622\n",
      "Epoch 129/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0764 - val_loss: 0.0624\n",
      "Epoch 130/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0764 - val_loss: 0.0638\n",
      "Epoch 131/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0765 - val_loss: 0.0634\n",
      "Epoch 132/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0628\n",
      "Epoch 133/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0764 - val_loss: 0.0635\n",
      "Epoch 134/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0762 - val_loss: 0.0633\n",
      "Epoch 135/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0763 - val_loss: 0.0624\n",
      "Epoch 136/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0765 - val_loss: 0.0623\n",
      "Epoch 137/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0763 - val_loss: 0.0630\n",
      "Epoch 138/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0766 - val_loss: 0.0631\n",
      "Epoch 139/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0763 - val_loss: 0.0629\n",
      "Epoch 140/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0764 - val_loss: 0.0630\n",
      "Epoch 141/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0761 - val_loss: 0.0623\n",
      "Epoch 142/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0764 - val_loss: 0.0626\n",
      "Epoch 143/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0763 - val_loss: 0.0636\n",
      "Epoch 144/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0767 - val_loss: 0.0627\n",
      "Epoch 145/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0764 - val_loss: 0.0621\n",
      "Epoch 146/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0762 - val_loss: 0.0634\n",
      "Epoch 147/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0763 - val_loss: 0.0635\n",
      "Epoch 148/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0764 - val_loss: 0.0625\n",
      "Epoch 149/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0762 - val_loss: 0.0625\n",
      "Epoch 150/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0765 - val_loss: 0.0628\n",
      "Epoch 151/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0762 - val_loss: 0.0647\n",
      "Epoch 152/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0763 - val_loss: 0.0631\n",
      "Epoch 153/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0761 - val_loss: 0.0630\n",
      "Epoch 154/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0764 - val_loss: 0.0622\n",
      "Epoch 155/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0763 - val_loss: 0.0653\n",
      "Epoch 156/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0761 - val_loss: 0.0628\n",
      "Epoch 157/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0652\n",
      "Epoch 158/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0758 - val_loss: 0.0632\n",
      "Epoch 159/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0762 - val_loss: 0.0642\n",
      "Epoch 160/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0761 - val_loss: 0.0629\n",
      "Epoch 161/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0763 - val_loss: 0.0632\n",
      "Epoch 162/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0760 - val_loss: 0.0630\n",
      "Epoch 163/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0762 - val_loss: 0.0634\n",
      "Epoch 164/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0758 - val_loss: 0.0633\n",
      "Epoch 165/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0758 - val_loss: 0.0630\n",
      "Epoch 166/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0761 - val_loss: 0.0631\n",
      "Epoch 167/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0760 - val_loss: 0.0631\n",
      "Epoch 168/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0758 - val_loss: 0.0632\n",
      "Epoch 169/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0760 - val_loss: 0.0629\n",
      "Epoch 170/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0762 - val_loss: 0.0639\n",
      "Epoch 171/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0758 - val_loss: 0.0628\n",
      "Epoch 172/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0627\n",
      "Epoch 173/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0762 - val_loss: 0.0636\n",
      "Epoch 174/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0760 - val_loss: 0.0649\n",
      "Epoch 175/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0758 - val_loss: 0.0628\n",
      "Epoch 176/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0636\n",
      "Epoch 177/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0758 - val_loss: 0.0624\n",
      "Epoch 178/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0760 - val_loss: 0.0631\n",
      "Epoch 179/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0628\n",
      "Epoch 180/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0760 - val_loss: 0.0628\n",
      "Epoch 181/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0639\n",
      "Epoch 182/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0631\n",
      "Epoch 183/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0755 - val_loss: 0.0632\n",
      "Epoch 184/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0757 - val_loss: 0.0634\n",
      "Epoch 185/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0626\n",
      "Epoch 186/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0756 - val_loss: 0.0628\n",
      "Epoch 187/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0760 - val_loss: 0.0628\n",
      "Epoch 188/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0760 - val_loss: 0.0633\n",
      "Epoch 189/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0757 - val_loss: 0.0626\n",
      "Epoch 190/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0757 - val_loss: 0.0628\n",
      "Epoch 191/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0756 - val_loss: 0.0628\n",
      "Epoch 192/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0757 - val_loss: 0.0632\n",
      "Epoch 193/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0760 - val_loss: 0.0634\n",
      "Epoch 194/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0758 - val_loss: 0.0629\n",
      "Epoch 195/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0756 - val_loss: 0.0635\n",
      "Epoch 196/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0755 - val_loss: 0.0631\n",
      "Epoch 197/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0756 - val_loss: 0.0630\n",
      "Epoch 198/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0754 - val_loss: 0.0636\n",
      "Epoch 199/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0759 - val_loss: 0.0637\n",
      "Epoch 200/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0757 - val_loss: 0.0630\n",
      "Epoch 201/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0758 - val_loss: 0.0634\n",
      "Epoch 202/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0755 - val_loss: 0.0630\n",
      "Epoch 203/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0757 - val_loss: 0.0635\n",
      "Epoch 204/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0759 - val_loss: 0.0634\n",
      "Epoch 205/10000\n",
      "374848/374848 [==============================] - 15s 40us/sample - loss: 0.0754 - val_loss: 0.0633\n",
      "Epoch 206/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0755 - val_loss: 0.0634\n",
      "Epoch 207/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0757 - val_loss: 0.0635\n",
      "Epoch 208/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0753 - val_loss: 0.0632\n",
      "Epoch 209/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0753 - val_loss: 0.0639\n",
      "Epoch 210/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0756 - val_loss: 0.0634\n",
      "Epoch 211/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0757 - val_loss: 0.0631\n",
      "Epoch 212/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0756 - val_loss: 0.0633\n",
      "Epoch 213/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0754 - val_loss: 0.0638\n",
      "Epoch 214/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0752 - val_loss: 0.0635\n",
      "Epoch 215/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0753 - val_loss: 0.0636\n",
      "Epoch 216/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0753 - val_loss: 0.0634\n",
      "Epoch 217/10000\n",
      "374848/374848 [==============================] - 14s 38us/sample - loss: 0.0755 - val_loss: 0.0650\n",
      "Epoch 218/10000\n",
      "374848/374848 [==============================] - 14s 39us/sample - loss: 0.0754 - val_loss: 0.0647\n",
      "Epoch 219/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0752 - val_loss: 0.0658\n",
      "Epoch 220/10000\n",
      "374848/374848 [==============================] - 15s 39us/sample - loss: 0.0755 - val_loss: 0.0634\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject45.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject45.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10685, 24, 1)\n",
      "y_test.shape:  (10685, 1)\n",
      "WARNING:tensorflow:Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:19,998 WARNING Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:20,047 WARNING Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject45.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject46.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject46.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11191, 24, 1)\n",
      "y_test.shape:  (11191, 1)\n",
      "WARNING:tensorflow:Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:38,829 WARNING Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:38,879 WARNING Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject46.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject47.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject47.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11535, 24, 1)\n",
      "y_test.shape:  (11535, 1)\n",
      "WARNING:tensorflow:Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:58,029 WARNING Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:13:58,076 WARNING Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject47.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject48.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject48.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11666, 24, 1)\n",
      "y_test.shape:  (11666, 1)\n",
      "WARNING:tensorflow:Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:16,926 WARNING Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:16,998 WARNING Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject48.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject49.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject49.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10635, 24, 1)\n",
      "y_test.shape:  (10635, 1)\n",
      "WARNING:tensorflow:Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:35,750 WARNING Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:35,799 WARNING Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject49.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject50.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject50.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 78\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4413, 24, 1)\n",
      "y_test.shape:  (4413, 1)\n",
      "WARNING:tensorflow:Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:54,007 WARNING Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:14:54,057 WARNING Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject50.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject51.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject51.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8549, 24, 1)\n",
      "y_test.shape:  (8549, 1)\n",
      "WARNING:tensorflow:Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:07,735 WARNING Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:07,783 WARNING Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject51.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject53.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject53.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8763, 24, 1)\n",
      "y_test.shape:  (8763, 1)\n",
      "WARNING:tensorflow:Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:24,006 WARNING Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:24,055 WARNING Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject53.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject54.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject54.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7553, 24, 1)\n",
      "y_test.shape:  (7553, 1)\n",
      "WARNING:tensorflow:Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:40,473 WARNING Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 24, 32)\n",
      "x.shape =  (?, 24, 32)\n",
      "WARNING:tensorflow:Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-20 05:15:40,523 WARNING Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  processed_cgm_data_Subject54.csv\n"
     ]
    }
   ],
   "source": [
    "for turning_sh in [30, 60, 90, 120]:\n",
    "    for fold_number in range(1, 6):\n",
    "        yaml_filepath = f\"./vandoorn_diatrend_experiments_{turning_sh}min/all_final_experiment_fold{fold_number}.yaml\"\n",
    "        mode = \"train\"\n",
    "\n",
    "        cfgs = load_cfgs(yaml_filepath)\n",
    "        print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "        for cfg in cfgs:\n",
    "            seed = int(cfg['train']['seed'])\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Print the configuration - just to make sure that you loaded what you\n",
    "            # wanted to load\n",
    "\n",
    "            module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "            module_model         = load_module(cfg['model']['script_path'])\n",
    "            module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "            module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "            module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "            pp = pprint.PrettyPrinter(indent=4)\n",
    "            pp.pprint(cfg)\n",
    "\n",
    "            #print(\"loading dataset ...\")\n",
    "            #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "            #nb_past_steps_tmp = 36\n",
    "            #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "            x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "            #x_train = x_train[:,-nb_past_steps:,:]\n",
    "            #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "            #x_test = x_test[:,-nb_past_steps:,:]\n",
    "            print(\"x_train.shape: \", x_train.shape)\n",
    "            print(\"y_train.shape: \", y_train.shape)\n",
    "            print(\"x_valid.shape: \", x_valid.shape)\n",
    "            print(\"y_valid.shape: \", y_valid.shape)\n",
    "            print(\"x_test.shape: \", x_test.shape)\n",
    "            print(\"y_test.shape: \", y_test.shape)\n",
    "            \n",
    "            #print(\"loading optimizer ...\")\n",
    "            optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "            #print(\"loading loss function ...\")\n",
    "            loss_function = module_loss_function.load()\n",
    "            #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "            #print(\"loading model ...\")\n",
    "            if 'tf_nll' in loss_function.__name__:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1]*2,\n",
    "                    cfg['model']\n",
    "                )\n",
    "            else:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1],\n",
    "                    cfg['model']\n",
    "                )\n",
    "\n",
    "            if 'initial_weights_path' in cfg['train']:\n",
    "                #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function\n",
    "            )\n",
    "\n",
    "            #print(model.summary())\n",
    "\n",
    "            # training mode\n",
    "            if mode == 'train':\n",
    "                #print(\"training model ...\")\n",
    "                train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "            if mode == 'plot_nll':\n",
    "                plot_nll(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_noise_experiment':\n",
    "                plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_seg':\n",
    "                plot_seg(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_dist':\n",
    "                plot_target_distribution(y_test, cfg)\n",
    "\n",
    "            # evaluation mode\n",
    "            if mode == 'evaluate':\n",
    "                evaluate(model, x_test, y_test, cfg)\n",
    "\n",
    "                \n",
    "        yaml_files = glob.glob(f\"./vandoorn_diatrend_experiments_{turning_sh}min/fold{fold_number}_eval/*.yaml\")\n",
    "        mode = \"evaluate\"\n",
    "        for yaml_filepath in yaml_files:\n",
    "            cfgs = load_cfgs(yaml_filepath)\n",
    "            print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "            for cfg in cfgs:\n",
    "                seed = int(cfg['train']['seed'])\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                # Print the configuration - just to make sure that you loaded what you\n",
    "                # wanted to load\n",
    "\n",
    "                module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "                module_model         = load_module(cfg['model']['script_path'])\n",
    "                module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "                module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "                module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "                pp = pprint.PrettyPrinter(indent=4)\n",
    "                pp.pprint(cfg)\n",
    "\n",
    "                #print(\"loading dataset ...\")\n",
    "                #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "                #nb_past_steps_tmp = 36\n",
    "                #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "                x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "                #x_train = x_train[:,-nb_past_steps:,:]\n",
    "                #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "                #x_test = x_test[:,-nb_past_steps:,:]\n",
    "                print(\"x_train.shape: \", x_train.shape)\n",
    "                print(\"y_train.shape: \", y_train.shape)\n",
    "                print(\"x_valid.shape: \", x_valid.shape)\n",
    "                print(\"y_valid.shape: \", y_valid.shape)\n",
    "                print(\"x_test.shape: \", x_test.shape)\n",
    "                print(\"y_test.shape: \", y_test.shape)\n",
    "                #print(\"loading optimizer ...\")\n",
    "                optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "                #print(\"loading loss function ...\")\n",
    "                loss_function = module_loss_function.load()\n",
    "                #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "                #print(\"loading model ...\")\n",
    "                if 'tf_nll' in loss_function.__name__:\n",
    "                    model = module_model.load(\n",
    "                        x_train.shape[1:],\n",
    "                        y_train.shape[1]*2,\n",
    "                        cfg['model']\n",
    "                    )\n",
    "                else:\n",
    "                    model = module_model.load(\n",
    "                        x_train.shape[1:],\n",
    "                        y_train.shape[1],\n",
    "                        cfg['model']\n",
    "                    )\n",
    "\n",
    "                if 'initial_weights_path' in cfg['train']:\n",
    "                    #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                    model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "                model.compile(\n",
    "                    optimizer=optimizer,\n",
    "                    loss=loss_function\n",
    "                )\n",
    "\n",
    "                #print(model.summary())\n",
    "\n",
    "                # training mode\n",
    "                if mode == 'train':\n",
    "                    #print(\"training model ...\")\n",
    "                    train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "                if mode == 'plot_nll':\n",
    "                    plot_nll(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_noise_experiment':\n",
    "                    plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_seg':\n",
    "                    plot_seg(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_dist':\n",
    "                    plot_target_distribution(y_test, cfg)\n",
    "\n",
    "                # evaluation mode\n",
    "                if mode == 'evaluate':\n",
    "                    evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./vandoorn_diatrend_experiments_60min/fold1_eval\\\\processed_cgm_data_Subject10_evaluation.yaml'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./vandoorn_diatrend_experiments_60min/all_final_experiment_fold1.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-19 15:28:02,451 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\fold1_training\\all does not exist.\n",
      "2025-01-19 15:28:02,452 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_12sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [30, 1827, 283, 14, 1809, 1883, 1987, 1154, 1, 109, 1189, 1619]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 48\n",
      "Segment lengths: [394, 455, 803, 241, 843, 205, 155, 54, 764, 42, 282, 443, 53, 56, 466, 104, 126, 54, 650, 85, 126, 128, 1440, 161, 259, 4, 27, 15, 47, 121, 14, 4, 14, 1940, 1, 4, 1, 2, 47, 1, 181, 3, 93, 17, 48, 17, 60, 98]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [14, 532, 491, 1427, 17, 1662, 892, 64, 2211, 166, 60, 67, 101, 1451, 233, 622, 175, 1441]\n",
      "Segments after filtering: 16\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1986, 2282, 1, 556, 2855, 2844, 521, 890]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 18\n",
      "Segment lengths: [1587, 1375, 840, 548, 7, 6, 25, 1196, 520, 1, 875, 7, 68, 153, 2368, 330, 842, 1163]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 6\n",
      "Segment lengths: [417, 2298, 2856, 2703, 2236, 1485]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [421, 2161, 78, 26, 2597, 1307, 1311, 2856, 1113]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [2747, 2617, 136, 89, 2077, 469, 5, 156, 71, 2846, 673]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 25\n",
      "Segment lengths: [842, 337, 1, 1291, 452, 1, 8, 60, 49, 17, 41, 223, 2, 2755, 39, 2606, 306, 3, 250, 248, 829, 539, 269, 218, 286]\n",
      "Segments after filtering: 19\n",
      "nb_future_steps  6\n",
      "Total segments found: 27\n",
      "Segment lengths: [40, 326, 326, 28, 2856, 2402, 347, 93, 1450, 48, 33, 12, 328, 2, 272, 285, 8, 201, 1, 3, 361, 32, 168, 1439, 327, 19, 292]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2177, 390, 12, 2, 2472, 349, 2591, 2741, 96, 1139]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [27, 1276, 577, 695, 71, 2759, 44, 15, 23, 802, 1760, 136, 44, 925, 632, 1, 3, 414, 547, 74, 1005]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 59\n",
      "Segment lengths: [23, 1038, 71, 18, 124, 75, 28, 103, 112, 43, 12, 174, 74, 5, 92, 32, 348, 152, 728, 122, 106, 939, 212, 241, 76, 164, 576, 37, 240, 23, 181, 201, 104, 175, 46, 214, 21, 21, 2, 3, 2, 388, 537, 484, 500, 64, 6, 111, 136, 2, 50, 100, 214, 210, 205, 59, 294, 160, 86]\n",
      "Segments after filtering: 52\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [2806, 1050, 1627, 170, 1, 35, 2802, 2792, 602]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 14\n",
      "Segment lengths: [33, 656, 43, 666, 1346, 524, 1238, 1404, 2824, 3, 1720, 734, 305, 341]\n",
      "Segments after filtering: 13\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [2433, 2324, 502, 1057, 1281, 485, 2856, 1029]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [289, 1376, 2189, 515, 439, 2360, 2856, 1874]\n",
      "Segments after filtering: 8\n",
      "nb_future_steps  6\n",
      "Total segments found: 548\n",
      "Segment lengths: [10, 17, 24, 3, 8, 1, 3, 4, 1, 58, 1, 3, 171, 2, 19, 2, 1, 25, 96, 8, 37, 25, 19, 20, 46, 26, 27, 18, 20, 9, 1, 2, 1, 2, 6, 5, 2, 2, 1, 7, 4, 9, 13, 2, 1, 4, 1, 3, 2, 8, 13, 1, 28, 17, 5, 23, 7, 3, 23, 20, 42, 32, 5, 1, 20, 18, 37, 16, 16, 1, 1, 11, 11, 3, 2, 4, 3, 2, 3, 1, 15, 2, 12, 19, 30, 23, 10, 5, 5, 9, 10, 1, 45, 21, 11, 3, 22, 21, 16, 6, 38, 1, 16, 42, 1, 10, 43, 8, 2, 18, 4, 30, 8, 1, 50, 65, 11, 7, 13, 2, 20, 27, 110, 1, 15, 1, 3, 3, 5, 25, 1, 67, 8, 21, 3, 2, 1, 12, 18, 21, 1, 6, 10, 10, 57, 12, 9, 14, 1, 9, 2, 11, 10, 11, 12, 5, 17, 7, 5, 8, 10, 26, 19, 4, 57, 39, 2, 47, 19, 12, 13, 13, 8, 1, 8, 2, 5, 9, 7, 3, 7, 7, 31, 4, 2, 11, 1, 9, 46, 1, 15, 49, 6, 110, 13, 10, 2, 1, 14, 15, 10, 6, 4, 5, 50, 35, 20, 8, 65, 46, 3, 3, 16, 9, 7, 67, 1, 19, 23, 25, 4, 26, 2, 1, 11, 12, 10, 6, 20, 4, 29, 5, 25, 34, 13, 54, 34, 6, 3, 2, 25, 1, 6, 12, 9, 4, 28, 1, 2, 23, 18, 21, 3, 37, 19, 18, 69, 8, 48, 16, 3, 19, 133, 16, 1, 23, 9, 69, 5, 40, 1, 7, 30, 22, 11, 267, 11, 30, 63, 9, 18, 24, 38, 29, 2, 24, 8, 7, 9, 1, 17, 13, 4, 13, 4, 47, 30, 3, 14, 9, 43, 18, 14, 50, 1, 5, 19, 18, 25, 22, 20, 21, 39, 24, 5, 1, 18, 6, 23, 22, 21, 1, 28, 27, 21, 3, 2, 16, 13, 27, 12, 169, 10, 27, 15, 16, 12, 2, 24, 33, 5, 48, 25, 155, 47, 35, 10, 25, 10, 19, 5, 14, 32, 8, 95, 7, 29, 44, 21, 38, 14, 14, 27, 3, 1, 42, 3, 34, 20, 23, 7, 73, 7, 43, 11, 33, 1, 21, 8, 51, 61, 1, 50, 3, 31, 6, 14, 8, 126, 1, 1, 3, 5, 65, 48, 25, 37, 10, 9, 59, 37, 1, 12, 19, 23, 17, 2, 74, 43, 3, 8, 26, 2, 28, 10, 65, 21, 7, 135, 6, 28, 17, 21, 1, 73, 13, 1, 38, 4, 29, 22, 30, 80, 3, 17, 21, 1, 2, 16, 45, 73, 12, 7, 83, 14, 5, 1, 27, 7, 21, 53, 23, 7, 1, 2, 30, 26, 31, 116, 1, 40, 38, 7, 4, 2, 1, 1, 36, 3, 43, 23, 6, 1, 45, 10, 26, 8, 7, 32, 10, 9, 32, 44, 8, 2, 12, 3, 7, 53, 43, 6, 3, 9, 30, 2, 1, 7, 7, 11, 15, 59, 46, 22, 18, 4, 26, 4, 8, 1, 2, 7, 11, 2, 6, 4, 19, 53, 47, 17, 14, 13, 70, 3, 13, 21, 6, 2, 5, 17, 41, 9, 1, 10, 32, 22, 14, 81, 15, 1, 1, 73, 55, 4, 7, 27, 9, 12, 58]\n",
      "Segments after filtering: 216\n",
      "nb_future_steps  6\n",
      "Total segments found: 13\n",
      "Segment lengths: [2015, 72, 1, 11, 49, 45, 622, 1728, 2851, 2847, 78, 1473, 131]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [1345, 2518, 2856, 2854, 2423]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 19\n",
      "Segment lengths: [1016, 17, 53, 508, 96, 1291, 257, 690, 14, 1706, 1440, 17, 2291, 6, 45, 8, 447, 312, 1693]\n",
      "Segments after filtering: 14\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [67, 23, 106, 2606, 305, 2, 1363, 2856, 576, 1440, 749, 1735]\n",
      "Segments after filtering: 11\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [766, 1, 1900, 331, 491, 855, 523, 1348, 2856, 2552]\n",
      "Segments after filtering: 9\n",
      "nb_future_steps  6\n",
      "Total segments found: 9\n",
      "Segment lengths: [1, 727, 2803, 2592, 2712, 108, 2592, 11, 277]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [473, 305, 57, 86, 1639, 575, 184, 175, 560, 27, 79, 320, 1286, 59, 1278, 9, 56, 691, 31, 328, 2, 145, 578, 54, 1769, 126, 158, 560]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [2781, 640, 37, 740, 2852, 1918, 315, 80, 144, 119, 42, 2, 5, 20, 17, 37, 46, 10, 2, 4, 1, 1984]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 15\n",
      "Segment lengths: [1331, 37, 1584, 143, 2004, 733, 688, 157, 927, 1085, 1292, 591, 102, 73, 1039]\n",
      "Segments after filtering: 15\n",
      "nb_future_steps  6\n",
      "Total segments found: 38\n",
      "Segment lengths: [71, 470, 279, 145, 145, 306, 553, 849, 145, 290, 140, 338, 336, 873, 145, 145, 37, 139, 764, 145, 125, 510, 37, 826, 3, 40, 908, 283, 145, 278, 296, 86, 295, 284, 271, 145, 188, 668]\n",
      "Segments after filtering: 37\n",
      "nb_future_steps  6\n",
      "Total segments found: 31\n",
      "Segment lengths: [940, 279, 134, 1577, 2, 286, 65, 271, 7, 69, 138, 10, 1, 74, 270, 410, 551, 1438, 439, 77, 841, 1, 1, 2, 27, 5, 2, 1041, 502, 71, 2305]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 29\n",
      "Segment lengths: [40, 330, 58, 903, 41, 925, 533, 3, 129, 384, 656, 857, 812, 4, 8, 856, 480, 321, 789, 393, 249, 813, 19, 1508, 81, 28, 113, 33, 606]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 28\n",
      "Segment lengths: [568, 145, 703, 266, 30, 117, 1313, 4, 222, 6, 1440, 751, 398, 273, 565, 446, 332, 228, 401, 276, 285, 443, 145, 253, 518, 1186, 141, 432]\n",
      "Segments after filtering: 26\n",
      "nb_future_steps  6\n",
      "Total segments found: 39\n",
      "Segment lengths: [2836, 1464, 287, 823, 2631, 4, 2, 3, 2, 3, 3, 1, 118, 52, 1484, 3, 3, 7, 3, 8, 1, 21, 145, 91, 4, 17, 34, 6, 181, 8, 3, 107, 3, 217, 5, 15, 28, 299, 884]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 34\n",
      "Segment lengths: [2402, 4, 248, 2115, 441, 90, 3, 23, 5, 97, 1636, 232, 46, 285, 271, 3, 4, 14, 1610, 5, 19, 81, 4, 204, 1, 1, 4, 2, 4, 2, 4, 127, 1, 448]\n",
      "Segments after filtering: 18\n",
      "nb_future_steps  6\n",
      "Total segments found: 35\n",
      "Segment lengths: [132, 726, 418, 145, 150, 1655, 292, 55, 108, 823, 145, 448, 95, 31, 322, 59, 448, 259, 37, 508, 400, 88, 102, 1813, 33, 76, 278, 275, 327, 190, 196, 64, 223, 592, 187]\n",
      "Segments after filtering: 35\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [2196, 1, 7, 90, 77, 166, 578, 2854, 1315, 115, 9, 385, 321, 13, 1, 33, 146, 4, 120, 376, 1, 2397, 275, 240]\n",
      "Segments after filtering: 17\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [1150, 1991, 224, 1896, 2011, 1782, 232, 1206, 190, 1143]\n",
      "Segments after filtering: 10\n",
      "nb_future_steps  6\n",
      "Total segments found: 5\n",
      "Segment lengths: [2831, 2858, 2856, 2856, 410]\n",
      "Segments after filtering: 5\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [178, 32, 15, 6, 2857, 2856, 2856, 2030]\n",
      "Segments after filtering: 6\n",
      "nb_future_steps  6\n",
      "Total segments found: 395\n",
      "Segment lengths: [6, 18, 4, 50, 81, 8, 127, 1, 2, 2, 8, 3, 12, 5, 11, 3, 14, 30, 29, 34, 13, 61, 18, 3, 7, 1, 17, 7, 4, 9, 4, 1, 1, 16, 2, 3, 2, 1, 47, 119, 5, 2, 14, 11, 9, 3, 5, 2, 2, 5, 6, 6, 8, 2, 2, 1, 40, 54, 32, 39, 2, 24, 5, 10, 6, 19, 11, 4, 6, 1, 3, 6, 48, 75, 5, 5, 36, 1, 12, 3, 7, 2, 3, 2, 1, 1, 1, 8, 12, 14, 9, 106, 9, 5, 7, 5, 2, 2, 1, 2, 1, 1, 2, 1, 12, 1, 6, 4, 2, 47, 11, 3, 1, 154, 4, 14, 22, 2, 1, 1, 6, 5, 1, 7, 4, 1, 1, 10, 77, 13, 3, 1, 1, 3, 8, 33, 13, 1, 3, 1, 142, 1, 1, 1, 9, 1, 8, 42, 10, 7, 6, 3, 1, 5, 196, 80, 144, 2, 8, 12, 2, 1, 5, 18, 1, 1, 1, 1, 5, 12, 86, 1, 1, 18, 15, 20, 12, 1, 7, 7, 1, 3, 2, 1, 2, 3, 6, 10, 46, 108, 15, 3, 2, 5, 1, 1, 2, 2, 1, 2, 13, 1, 1, 2, 1, 2, 3, 2, 19, 15, 1, 5, 1, 14, 133, 39, 1, 8, 2, 2, 3, 9, 1, 1, 19, 141, 46, 8, 5, 5, 27, 16, 158, 18, 7, 24, 62, 6, 21, 137, 2, 15, 18, 32, 2, 11, 2, 9, 5, 2, 16, 2, 13, 3, 12, 142, 5, 6, 8, 19, 10, 46, 7, 148, 2, 81, 159, 22, 8, 69, 3, 4, 39, 2, 14, 201, 4, 11, 17, 6, 2, 40, 93, 3, 2, 17, 18, 14, 5, 62, 5, 3, 2, 9, 166, 6, 7, 29, 6, 8, 15, 11, 137, 12, 7, 147, 103, 12, 10, 5, 37, 30, 4, 2, 16, 14, 183, 75, 45, 116, 10, 286, 58, 28, 32, 29, 3, 3, 134, 34, 16, 69, 4, 2, 18, 147, 8, 11, 21, 15, 21, 65, 86, 5, 9, 21, 2, 32, 45, 8, 90, 9, 21, 5, 12, 14, 4, 14, 24, 9, 9, 4, 14, 73, 17, 55, 5, 11, 143, 40, 7, 12, 4, 9, 3, 26, 43, 7, 3, 6, 4, 14, 99, 48, 1, 1, 6, 41, 17, 2, 54, 27, 1, 18, 11]\n",
      "Segments after filtering: 109\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [269, 231, 264, 78, 379, 36, 644, 774, 509, 699, 556, 538, 153, 278, 268, 602, 831, 165, 283, 509, 576, 545]\n",
      "Segments after filtering: 22\n",
      "nb_future_steps  6\n",
      "Total segments found: 8\n",
      "Segment lengths: [1773, 51, 109, 2569, 2039, 2304, 8, 121]\n",
      "Segments after filtering: 7\n",
      "nb_future_steps  6\n",
      "Total segments found: 30\n",
      "Segment lengths: [416, 77, 340, 263, 145, 428, 145, 400, 134, 37, 376, 281, 271, 274, 282, 408, 425, 280, 145, 426, 406, 145, 93, 421, 1140, 145, 145, 81, 117, 177]\n",
      "Segments after filtering: 30\n",
      "x_train.shape:  (366985, 12, 1)\n",
      "y_train.shape:  (366985, 1)\n",
      "x_valid.shape:  (91727, 12, 1)\n",
      "y_valid.shape:  (91727, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 15:28:51,830 WARNING Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "x.shape =  (?, 12, 32)\n",
      "x.shape =  (?, 12, 32)\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-19 15:28:51,890 WARNING Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-19 15:28:51,966 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 366985 samples, validate on 91727 samples\n",
      "Epoch 1/10000\n",
      "366592/366985 [============================>.] - ETA: 0s - loss: 1.0878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366985/366985 [==============================] - 9s 24us/sample - loss: 1.0869 - val_loss: 0.1070\n",
      "Epoch 2/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.1997 - val_loss: 0.0868\n",
      "Epoch 3/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.1542 - val_loss: 0.0744\n",
      "Epoch 4/10000\n",
      "366985/366985 [==============================] - 8s 20us/sample - loss: 0.1354 - val_loss: 0.0728\n",
      "Epoch 5/10000\n",
      "366985/366985 [==============================] - 9s 23us/sample - loss: 0.1211 - val_loss: 0.0665\n",
      "Epoch 6/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.1115 - val_loss: 0.0680\n",
      "Epoch 7/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.1040 - val_loss: 0.0644\n",
      "Epoch 8/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0990 - val_loss: 0.0642\n",
      "Epoch 9/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0937 - val_loss: 0.0636\n",
      "Epoch 10/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0895 - val_loss: 0.0648\n",
      "Epoch 11/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0878 - val_loss: 0.0659\n",
      "Epoch 12/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0867 - val_loss: 0.0650\n",
      "Epoch 13/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0861 - val_loss: 0.0665\n",
      "Epoch 14/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0859 - val_loss: 0.0662\n",
      "Epoch 15/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0860 - val_loss: 0.0669\n",
      "Epoch 16/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0861 - val_loss: 0.0653\n",
      "Epoch 17/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0859 - val_loss: 0.0662\n",
      "Epoch 18/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0854 - val_loss: 0.0676\n",
      "Epoch 19/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0851 - val_loss: 0.0651\n",
      "Epoch 20/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0843 - val_loss: 0.0670\n",
      "Epoch 21/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0833 - val_loss: 0.0676\n",
      "Epoch 22/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0831 - val_loss: 0.0654\n",
      "Epoch 23/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0821 - val_loss: 0.0646\n",
      "Epoch 24/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0820 - val_loss: 0.0643\n",
      "Epoch 25/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0819 - val_loss: 0.0649\n",
      "Epoch 26/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0818 - val_loss: 0.0649\n",
      "Epoch 27/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0813 - val_loss: 0.0661\n",
      "Epoch 28/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0817 - val_loss: 0.0644\n",
      "Epoch 29/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0819 - val_loss: 0.0644\n",
      "Epoch 30/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0817 - val_loss: 0.0640\n",
      "Epoch 31/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0813 - val_loss: 0.0650\n",
      "Epoch 32/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0812 - val_loss: 0.0640\n",
      "Epoch 33/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0814 - val_loss: 0.0645\n",
      "Epoch 34/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0814 - val_loss: 0.0636\n",
      "Epoch 35/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0816 - val_loss: 0.0637\n",
      "Epoch 36/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0813 - val_loss: 0.0639\n",
      "Epoch 37/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0811 - val_loss: 0.0637\n",
      "Epoch 38/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0810 - val_loss: 0.0639\n",
      "Epoch 39/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0811 - val_loss: 0.0655\n",
      "Epoch 40/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0810 - val_loss: 0.0651\n",
      "Epoch 41/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0811 - val_loss: 0.0650\n",
      "Epoch 42/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0807 - val_loss: 0.0635\n",
      "Epoch 43/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0808 - val_loss: 0.0639\n",
      "Epoch 44/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0808 - val_loss: 0.0640\n",
      "Epoch 45/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0810 - val_loss: 0.0635\n",
      "Epoch 46/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0806 - val_loss: 0.0634\n",
      "Epoch 47/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0808 - val_loss: 0.0635\n",
      "Epoch 48/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0807 - val_loss: 0.0634\n",
      "Epoch 49/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0768 - val_loss: 0.0612\n",
      "Epoch 50/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0747 - val_loss: 0.0625\n",
      "Epoch 51/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0738 - val_loss: 0.0613\n",
      "Epoch 52/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0735 - val_loss: 0.0626\n",
      "Epoch 53/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0732 - val_loss: 0.0619\n",
      "Epoch 54/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0732 - val_loss: 0.0621\n",
      "Epoch 55/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0733 - val_loss: 0.0613\n",
      "Epoch 56/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0730 - val_loss: 0.0605\n",
      "Epoch 57/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0732 - val_loss: 0.0617\n",
      "Epoch 58/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0733 - val_loss: 0.0626\n",
      "Epoch 59/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0727 - val_loss: 0.0625\n",
      "Epoch 60/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0728 - val_loss: 0.0610\n",
      "Epoch 61/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0729 - val_loss: 0.0607\n",
      "Epoch 62/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0729 - val_loss: 0.0622\n",
      "Epoch 63/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0729 - val_loss: 0.0613\n",
      "Epoch 64/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0730 - val_loss: 0.0622\n",
      "Epoch 65/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0725 - val_loss: 0.0614\n",
      "Epoch 66/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0729 - val_loss: 0.0610\n",
      "Epoch 67/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0731 - val_loss: 0.0616\n",
      "Epoch 68/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0727 - val_loss: 0.0611\n",
      "Epoch 69/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0730 - val_loss: 0.0604\n",
      "Epoch 70/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0730 - val_loss: 0.0607\n",
      "Epoch 71/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0726 - val_loss: 0.0607\n",
      "Epoch 72/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0726 - val_loss: 0.0611\n",
      "Epoch 73/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0726 - val_loss: 0.0610\n",
      "Epoch 74/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0726 - val_loss: 0.0610\n",
      "Epoch 75/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0727 - val_loss: 0.0613\n",
      "Epoch 76/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0727 - val_loss: 0.0607\n",
      "Epoch 77/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0726 - val_loss: 0.0605\n",
      "Epoch 78/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0724 - val_loss: 0.0610\n",
      "Epoch 79/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0728 - val_loss: 0.0618\n",
      "Epoch 80/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0728 - val_loss: 0.0610\n",
      "Epoch 81/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0727 - val_loss: 0.0613\n",
      "Epoch 82/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0726 - val_loss: 0.0613\n",
      "Epoch 83/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0608\n",
      "Epoch 84/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0725 - val_loss: 0.0606\n",
      "Epoch 85/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0605\n",
      "Epoch 86/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0725 - val_loss: 0.0612\n",
      "Epoch 87/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0725 - val_loss: 0.0612\n",
      "Epoch 88/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0728 - val_loss: 0.0607\n",
      "Epoch 89/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0726 - val_loss: 0.0608\n",
      "Epoch 90/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0723 - val_loss: 0.0607\n",
      "Epoch 91/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0621\n",
      "Epoch 92/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0727 - val_loss: 0.0603\n",
      "Epoch 93/10000\n",
      "366985/366985 [==============================] - 6s 18us/sample - loss: 0.0726 - val_loss: 0.0606\n",
      "Epoch 94/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0613\n",
      "Epoch 95/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0727 - val_loss: 0.0606\n",
      "Epoch 96/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0723 - val_loss: 0.0611\n",
      "Epoch 97/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0615\n",
      "Epoch 98/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0605\n",
      "Epoch 99/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0605\n",
      "Epoch 100/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0724 - val_loss: 0.0610\n",
      "Epoch 101/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0726 - val_loss: 0.0608\n",
      "Epoch 102/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0725 - val_loss: 0.0606\n",
      "Epoch 103/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0725 - val_loss: 0.0606\n",
      "Epoch 104/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0722 - val_loss: 0.0609\n",
      "Epoch 105/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0613\n",
      "Epoch 106/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0725 - val_loss: 0.0608\n",
      "Epoch 107/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0609\n",
      "Epoch 108/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0604\n",
      "Epoch 109/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0609\n",
      "Epoch 110/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0725 - val_loss: 0.0603\n",
      "Epoch 111/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0723 - val_loss: 0.0606\n",
      "Epoch 112/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0724 - val_loss: 0.0614\n",
      "Epoch 113/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0725 - val_loss: 0.0606\n",
      "Epoch 114/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0720 - val_loss: 0.0608\n",
      "Epoch 115/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0722 - val_loss: 0.0604\n",
      "Epoch 116/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0609\n",
      "Epoch 117/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0722 - val_loss: 0.0611\n",
      "Epoch 118/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0723 - val_loss: 0.0603\n",
      "Epoch 119/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0602\n",
      "Epoch 120/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0604\n",
      "Epoch 121/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0606\n",
      "Epoch 122/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0606\n",
      "Epoch 123/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0604\n",
      "Epoch 124/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0724 - val_loss: 0.0607\n",
      "Epoch 125/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0605\n",
      "Epoch 126/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0611\n",
      "Epoch 127/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0602\n",
      "Epoch 128/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0609\n",
      "Epoch 129/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0605\n",
      "Epoch 130/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0608\n",
      "Epoch 131/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0610\n",
      "Epoch 132/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0724 - val_loss: 0.0603\n",
      "Epoch 133/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0721 - val_loss: 0.0605\n",
      "Epoch 134/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0725 - val_loss: 0.0612\n",
      "Epoch 135/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0605\n",
      "Epoch 136/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0722 - val_loss: 0.0612\n",
      "Epoch 137/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0724 - val_loss: 0.0604\n",
      "Epoch 138/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0721 - val_loss: 0.0605\n",
      "Epoch 139/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0720 - val_loss: 0.0614\n",
      "Epoch 140/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0608\n",
      "Epoch 141/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0612\n",
      "Epoch 142/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0611\n",
      "Epoch 143/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0610\n",
      "Epoch 144/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0608\n",
      "Epoch 145/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0614\n",
      "Epoch 146/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0721 - val_loss: 0.0613\n",
      "Epoch 147/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0721 - val_loss: 0.0608\n",
      "Epoch 148/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0723 - val_loss: 0.0618\n",
      "Epoch 149/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0723 - val_loss: 0.0611\n",
      "Epoch 150/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0617\n",
      "Epoch 151/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0609\n",
      "Epoch 152/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0605\n",
      "Epoch 153/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0722 - val_loss: 0.0610\n",
      "Epoch 154/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0609\n",
      "Epoch 155/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0720 - val_loss: 0.0605\n",
      "Epoch 156/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0608\n",
      "Epoch 157/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0605\n",
      "Epoch 158/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0722 - val_loss: 0.0611\n",
      "Epoch 159/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0603\n",
      "Epoch 160/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0611\n",
      "Epoch 161/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0718 - val_loss: 0.0609\n",
      "Epoch 162/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0608\n",
      "Epoch 163/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0722 - val_loss: 0.0606\n",
      "Epoch 164/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0607\n",
      "Epoch 165/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0604\n",
      "Epoch 166/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0605\n",
      "Epoch 167/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0613\n",
      "Epoch 168/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0609\n",
      "Epoch 169/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0609\n",
      "Epoch 170/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0608\n",
      "Epoch 171/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0723 - val_loss: 0.0610\n",
      "Epoch 172/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0718 - val_loss: 0.0610\n",
      "Epoch 173/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0721 - val_loss: 0.0609\n",
      "Epoch 174/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0719 - val_loss: 0.0608\n",
      "Epoch 175/10000\n",
      "366985/366985 [==============================] - 6s 18us/sample - loss: 0.0723 - val_loss: 0.0608\n",
      "Epoch 176/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0719 - val_loss: 0.0607\n",
      "Epoch 177/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0722 - val_loss: 0.0607\n",
      "Epoch 178/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0718 - val_loss: 0.0604\n",
      "Epoch 179/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0721 - val_loss: 0.0608\n",
      "Epoch 180/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0720 - val_loss: 0.0606\n",
      "Epoch 181/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0720 - val_loss: 0.0605\n",
      "Epoch 182/10000\n",
      "366985/366985 [==============================] - 6s 17us/sample - loss: 0.0720 - val_loss: 0.0606\n",
      "Epoch 183/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0720 - val_loss: 0.0605\n",
      "Epoch 184/10000\n",
      "366985/366985 [==============================] - 7s 20us/sample - loss: 0.0719 - val_loss: 0.0610\n",
      "Epoch 185/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0717 - val_loss: 0.0625\n",
      "Epoch 186/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0717 - val_loss: 0.0605\n",
      "Epoch 187/10000\n",
      "366985/366985 [==============================] - 7s 19us/sample - loss: 0.0720 - val_loss: 0.0604\n",
      "Epoch 188/10000\n",
      "366985/366985 [==============================] - 7s 18us/sample - loss: 0.0719 - val_loss: 0.0607\n",
      "Epoch 189/10000\n",
      "163840/366985 [============>.................] - ETA: 3s - loss: 0.0719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 70\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#print(model.summary())\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# training mode\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m#print(\"training model ...\")\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_nll\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     72\u001b[0m     plot_nll(model, x_test, y_test, cfg)\n",
      "Cell \u001b[1;32mIn[6], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n\u001b[1;32m---> 66\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshuffle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifacts_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martifacts_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\train\\train_keras.py:6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, x_train, y_train, x_valid, y_valid, batch_size, epochs, patience, shuffle, artifacts_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, x_train, y_train, x_valid, y_valid, batch_size, epochs,\n\u001b[0;32m      5\u001b[0m         patience, shuffle, artifacts_path):\n\u001b[1;32m----> 6\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts_path\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifacts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:855\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    854\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    731\u001b[0m         )\n\u001b[0;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_arrays_v1.py:419\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[0;32m    415\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    421\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4569\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4573\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[0;32m   4574\u001b[0m ):\n\u001b[0;32m   4575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4577\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[0;32m   4579\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4581\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4582\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4583\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1481\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1485\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject10_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject11_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject1_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject2_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject3_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject4_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject5_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject6_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject7_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject8_evaluation.yaml',\n",
       " './vandoorn_diatrend_experiments_30min/fold1_eval\\\\processed_cgm_data_Subject9_evaluation.yaml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yaml_filepath = f\"./diatrend_test_cfg_method1/subject30_evaluate.yaml\"\n",
    "# mode = \"evaluate\"\n",
    "# Get all yaml files in the directory\n",
    "yaml_files = glob.glob(\"./vandoorn_diatrend_experiments_60min/fold1_eval/*.yaml\")\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject10.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject10.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 10\n",
      "Segment lengths: [2408, 312, 1051, 1352, 1728, 2000, 46, 405, 2394, 243]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11829, 6, 1)\n",
      "y_test.shape:  (11829, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-18 15:47:23,624 DEBUG Creating converter from 3 to 5\n",
      "370/370 [==============================] - 1s 3ms/step\n",
      "370/370 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject10.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject11.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 22\n",
      "Segment lengths: [61, 1571, 6, 60, 5, 800, 1411, 81, 1812, 752, 469, 330, 1145, 2, 35, 132, 82, 24, 2752, 5, 57, 247]\n",
      "Segments after filtering: 18\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11623, 6, 1)\n",
      "y_test.shape:  (11623, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "364/364 [==============================] - 1s 3ms/step\n",
      "364/364 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject1.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 209\n",
      "Segment lengths: [10, 1, 83, 37, 260, 29, 171, 63, 109, 98, 109, 14, 159, 48, 22, 12, 12, 9, 6, 27, 6, 106, 8, 24, 20, 28, 79, 71, 1, 58, 46, 5, 109, 107, 67, 17, 69, 28, 46, 53, 7, 13, 6, 11, 33, 138, 1, 6, 64, 71, 7, 2, 91, 3, 12, 2, 141, 2, 12, 39, 32, 57, 69, 12, 1, 1, 220, 19, 37, 1, 25, 209, 6, 2, 1, 169, 4, 42, 169, 48, 42, 5, 1, 2, 122, 26, 35, 10, 25, 5, 117, 19, 21, 14, 20, 2, 13, 16, 108, 18, 1, 68, 7, 12, 4, 26, 8, 128, 23, 3, 2, 21, 25, 10, 192, 6, 58, 154, 74, 58, 139, 111, 95, 34, 7, 58, 207, 41, 37, 4, 2, 1, 20, 136, 62, 13, 7, 1, 121, 65, 58, 164, 46, 73, 185, 36, 105, 139, 224, 51, 74, 12, 34, 3, 90, 50, 4, 35, 1, 40, 15, 8, 88, 43, 4, 54, 147, 75, 1, 2, 14, 2, 53, 1, 89, 6, 127, 50, 47, 13, 19, 87, 50, 90, 46, 52, 76, 43, 23, 28, 4, 120, 56, 45, 30, 111, 49, 121, 14, 136, 1, 1, 48, 11, 24, 213, 25, 16, 32]\n",
      "Segments after filtering: 150\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8612, 6, 1)\n",
      "y_test.shape:  (8612, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "270/270 [==============================] - 1s 3ms/step\n",
      "270/270 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject2.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject2.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 418\n",
      "Segment lengths: [0, 3, 51, 4, 3, 1, 16, 23, 20, 1, 1, 38, 1, 98, 21, 12, 1, 1, 19, 10, 11, 26, 2, 19, 26, 27, 83, 12, 2, 23, 15, 1, 9, 30, 42, 83, 27, 9, 9, 12, 9, 9, 1, 2, 6, 5, 3, 5, 3, 3, 1, 23, 42, 10, 50, 20, 1, 1, 92, 1, 35, 17, 2, 23, 84, 17, 1, 5, 30, 1, 3, 9, 5, 33, 5, 4, 5, 19, 11, 92, 2, 20, 29, 109, 70, 72, 11, 128, 4, 7, 17, 2, 252, 11, 154, 40, 7, 5, 8, 61, 1, 3, 50, 63, 4, 11, 47, 29, 9, 79, 62, 3, 6, 53, 46, 22, 133, 26, 51, 5, 14, 27, 29, 87, 91, 26, 31, 1, 2, 2, 11, 41, 3, 46, 10, 1, 5, 8, 4, 9, 2, 10, 43, 18, 12, 71, 34, 2, 13, 12, 13, 2, 11, 2, 3, 7, 89, 6, 5, 11, 35, 25, 9, 9, 4, 3, 1, 7, 5, 38, 4, 11, 217, 232, 55, 8, 12, 23, 13, 5, 2, 5, 12, 3, 9, 1, 43, 1, 1, 2, 17, 1, 2, 1, 191, 20, 9, 21, 25, 231, 7, 127, 28, 6, 63, 141, 53, 11, 27, 5, 56, 18, 159, 29, 5, 79, 154, 61, 43, 1, 175, 3, 54, 49, 4, 8, 10, 42, 2, 7, 1, 32, 62, 29, 45, 4, 7, 14, 16, 10, 1, 5, 6, 1, 1, 8, 1, 11, 2, 5, 3, 27, 64, 27, 12, 4, 14, 2, 21, 1, 6, 4, 43, 17, 6, 9, 1, 2, 12, 84, 49, 6, 18, 55, 29, 8, 18, 14, 7, 54, 17, 3, 4, 37, 67, 1, 46, 5, 22, 4, 13, 29, 34, 10, 37, 36, 12, 9, 4, 5, 3, 9, 8, 1, 16, 35, 59, 24, 1, 13, 45, 38, 1, 73, 1, 1, 1, 9, 24, 15, 65, 45, 1, 1, 1, 37, 1, 5, 12, 6, 108, 10, 5, 1, 16, 51, 10, 6, 43, 17, 13, 1, 12, 92, 6, 35, 7, 9, 3, 13, 28, 27, 32, 83, 11, 3, 2, 2, 11, 1, 2, 33, 22, 1, 16, 75, 1, 16, 21, 24, 2, 3, 2, 1, 120, 14, 57, 24, 4, 59, 7, 13, 133, 13, 13, 26, 38, 51, 12, 12, 13, 37, 12, 12, 34, 17, 2, 10, 11, 53, 2, 36, 13, 12, 13, 8, 15, 10, 11, 5, 24, 12, 40, 12, 7, 209, 17, 61]\n",
      "Segments after filtering: 214\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7435, 6, 1)\n",
      "y_test.shape:  (7435, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "233/233 [==============================] - 1s 2ms/step\n",
      "233/233 [==============================] - 0s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject2.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject3.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject3.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 346\n",
      "Segment lengths: [81, 14, 7, 2, 2, 39, 3, 18, 38, 1, 1, 165, 6, 39, 14, 2, 23, 234, 30, 13, 8, 22, 119, 35, 1, 9, 24, 18, 1, 11, 9, 179, 20, 2, 5, 17, 22, 169, 61, 28, 25, 116, 66, 14, 43, 3, 7, 10, 16, 71, 34, 45, 23, 1, 1, 1, 1, 3, 3, 149, 27, 20, 23, 1, 29, 44, 9, 63, 11, 55, 23, 9, 19, 1, 2, 53, 14, 106, 99, 26, 1, 16, 101, 13, 40, 10, 1, 22, 1, 4, 1, 10, 4, 1, 106, 20, 2, 21, 1, 97, 5, 6, 16, 144, 1, 50, 14, 16, 27, 35, 143, 2, 3, 49, 39, 1, 1, 2, 2, 16, 8, 1, 119, 41, 33, 7, 4, 3, 133, 3, 87, 11, 18, 32, 26, 121, 10, 30, 39, 1, 1, 25, 145, 7, 1, 13, 30, 3, 10, 3, 17, 1, 2, 2, 1, 25, 4, 7, 3, 119, 1, 1, 47, 54, 15, 6, 4, 6, 2, 14, 146, 52, 24, 4, 3, 20, 3, 9, 97, 13, 128, 6, 1, 41, 128, 81, 8, 4, 1, 30, 6, 3, 11, 102, 1, 1, 85, 67, 5, 49, 97, 5, 4, 1, 11, 23, 7, 1, 21, 130, 71, 15, 10, 13, 202, 14, 12, 3, 2, 2, 7, 165, 53, 26, 35, 3, 1, 152, 2, 14, 1, 9, 2, 56, 4, 23, 3, 122, 15, 14, 22, 17, 19, 20, 8, 58, 92, 9, 19, 8, 15, 92, 5, 37, 55, 52, 22, 1, 30, 7, 2, 24, 1, 1, 3, 150, 1, 1, 2, 1, 45, 41, 18, 24, 21, 124, 21, 58, 6, 1, 11, 9, 9, 6, 6, 3, 11, 96, 87, 27, 32, 22, 17, 29, 82, 63, 48, 122, 141, 1, 8, 10, 1, 130, 47, 12, 2, 2, 1, 1, 11, 1, 57, 3, 90, 3, 3, 95, 19, 2, 3, 11, 18, 22, 110, 15, 49, 20, 7, 5, 2, 1, 3, 17, 2, 114, 34, 1, 16, 161, 10, 4, 22, 90, 14, 13]\n",
      "Segments after filtering: 185\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (8101, 6, 1)\n",
      "y_test.shape:  (8101, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "254/254 [==============================] - 1s 3ms/step\n",
      "254/254 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject3.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject4.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject4.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 20\n",
      "Segment lengths: [1919, 354, 470, 5, 18, 420, 1991, 751, 40, 3, 471, 77, 10, 1098, 118, 685, 925, 677, 1307, 315]\n",
      "Segments after filtering: 17\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11449, 6, 1)\n",
      "y_test.shape:  (11449, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "358/358 [==============================] - 1s 2ms/step\n",
      "358/358 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject4.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject5.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject5.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 44\n",
      "Segment lengths: [398, 268, 104, 152, 269, 300, 375, 657, 93, 189, 129, 282, 202, 37, 144, 248, 102, 449, 239, 419, 404, 268, 900, 72, 470, 82, 159, 165, 206, 106, 318, 144, 144, 225, 135, 141, 138, 649, 325, 201, 232, 174, 395, 128]\n",
      "Segments after filtering: 44\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (10753, 6, 1)\n",
      "y_test.shape:  (10753, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "337/337 [==============================] - 1s 2ms/step\n",
      "337/337 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject5.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject6.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject6.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 24\n",
      "Segment lengths: [1039, 787, 1555, 87, 107, 16, 1, 50, 320, 1362, 148, 262, 1, 2, 284, 32, 71, 1953, 611, 212, 800, 395, 528, 1153]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11541, 6, 1)\n",
      "y_test.shape:  (11541, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "361/361 [==============================] - 1s 2ms/step\n",
      "361/361 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject6.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject7.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject7.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 12\n",
      "Segment lengths: [152, 108, 288, 1078, 288, 556, 1931, 1078, 499, 1714, 1885, 1944]\n",
      "Segments after filtering: 12\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11389, 6, 1)\n",
      "y_test.shape:  (11389, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject7.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject8.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject8.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 21\n",
      "Segment lengths: [512, 1411, 27, 201, 847, 46, 1431, 476, 213, 237, 122, 140, 254, 272, 367, 1068, 563, 1372, 896, 15, 1101]\n",
      "Segments after filtering: 21\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11340, 6, 1)\n",
      "y_test.shape:  (11340, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "355/355 [==============================] - 1s 2ms/step\n",
      "355/355 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject8.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_diatrend_subset\\\\processed_cgm_data_Subject9.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\diatrend.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_diatrend_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_diatrend_subset\\processed_cgm_data_Subject9.csv\n",
      "nb_future_steps  6\n",
      "Total segments found: 11\n",
      "Segment lengths: [0, 2658, 74, 2589, 165, 2199, 623, 150, 2687, 187, 523]\n",
      "Segments after filtering: 10\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (11745, 6, 1)\n",
      "y_test.shape:  (11745, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_diatrend_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "368/368 [==============================] - 1s 2ms/step\n",
      "368/368 [==============================] - 1s 2ms/step\n",
      "patient id:  processed_cgm_data_Subject9.csv\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yaml_filepath in yaml_files:\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify T1DEXI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '85.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '911.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 10:22:54,698 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2025-01-22 10:22:54,713 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-22 10:22:54,715 DEBUG interactive is False\n",
      "2025-01-22 10:22:54,715 DEBUG platform is win32\n",
      "2025-01-22 10:22:54,748 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-22 10:22:54,748 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # # Calculate MSE\n",
    "    # mse = np.mean((y_test - y_pred) ** 2)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    # print(\"patient id: \", patient_id)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # print(\"RMSE: \", rmse)\n",
    "    # print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entile train_eval tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 21:24:31,710 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (308851, 12, 1)\n",
      "y_train.shape:  (308851, 1)\n",
      "x_valid.shape:  (77188, 12, 1)\n",
      "y_valid.shape:  (77188, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-22 21:25:14,236 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "302/302 [==============================] - 4s 9ms/step - loss: 0.6330 - val_loss: 0.1290\n",
      "Epoch 2/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.1877 - val_loss: 0.0678\n",
      "Epoch 3/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.1483 - val_loss: 0.0626\n",
      "Epoch 4/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.1320 - val_loss: 0.0540\n",
      "Epoch 5/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.1166 - val_loss: 0.0522\n",
      "Epoch 6/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.1067 - val_loss: 0.0537\n",
      "Epoch 7/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0956 - val_loss: 0.0494\n",
      "Epoch 8/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0859 - val_loss: 0.0481\n",
      "Epoch 9/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0779 - val_loss: 0.0441\n",
      "Epoch 10/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0719 - val_loss: 0.0450\n",
      "Epoch 11/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0671 - val_loss: 0.0444\n",
      "Epoch 12/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0636 - val_loss: 0.0462\n",
      "Epoch 13/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0620 - val_loss: 0.0439\n",
      "Epoch 14/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0600 - val_loss: 0.0440\n",
      "Epoch 15/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0581 - val_loss: 0.0436\n",
      "Epoch 16/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0571 - val_loss: 0.0440\n",
      "Epoch 17/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0567 - val_loss: 0.0432\n",
      "Epoch 18/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0559 - val_loss: 0.0428\n",
      "Epoch 19/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0563 - val_loss: 0.0441\n",
      "Epoch 20/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0558 - val_loss: 0.0434\n",
      "Epoch 21/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0558 - val_loss: 0.0437\n",
      "Epoch 22/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0554 - val_loss: 0.0434\n",
      "Epoch 23/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0543 - val_loss: 0.0426\n",
      "Epoch 24/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0539 - val_loss: 0.0426\n",
      "Epoch 25/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0538 - val_loss: 0.0428\n",
      "Epoch 26/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0539 - val_loss: 0.0424\n",
      "Epoch 27/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0536 - val_loss: 0.0428\n",
      "Epoch 28/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0537 - val_loss: 0.0425\n",
      "Epoch 29/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0533 - val_loss: 0.0424\n",
      "Epoch 30/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0531 - val_loss: 0.0423\n",
      "Epoch 31/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0536 - val_loss: 0.0423\n",
      "Epoch 32/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0535 - val_loss: 0.0426\n",
      "Epoch 33/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0532 - val_loss: 0.0426\n",
      "Epoch 34/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0535 - val_loss: 0.0429\n",
      "Epoch 35/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0533 - val_loss: 0.0423\n",
      "Epoch 36/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0533 - val_loss: 0.0428\n",
      "Epoch 37/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0532 - val_loss: 0.0422\n",
      "Epoch 38/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0532 - val_loss: 0.0422\n",
      "Epoch 39/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0529 - val_loss: 0.0428\n",
      "Epoch 40/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0531 - val_loss: 0.0422\n",
      "Epoch 41/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0529 - val_loss: 0.0423\n",
      "Epoch 42/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0531 - val_loss: 0.0435\n",
      "Epoch 43/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0528 - val_loss: 0.0422\n",
      "Epoch 44/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0530 - val_loss: 0.0419\n",
      "Epoch 45/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0526 - val_loss: 0.0418\n",
      "Epoch 46/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0531 - val_loss: 0.0420\n",
      "Epoch 47/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0530 - val_loss: 0.0422\n",
      "Epoch 48/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0529 - val_loss: 0.0421\n",
      "Epoch 49/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0529 - val_loss: 0.0422\n",
      "Epoch 50/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0527 - val_loss: 0.0419\n",
      "Epoch 51/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0526 - val_loss: 0.0426\n",
      "Epoch 52/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0528 - val_loss: 0.0429\n",
      "Epoch 53/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0531 - val_loss: 0.0418\n",
      "Epoch 54/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0530 - val_loss: 0.0416\n",
      "Epoch 55/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0523 - val_loss: 0.0419\n",
      "Epoch 56/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0524 - val_loss: 0.0417\n",
      "Epoch 57/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0526 - val_loss: 0.0422\n",
      "Epoch 58/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0528 - val_loss: 0.0424\n",
      "Epoch 59/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0526 - val_loss: 0.0421\n",
      "Epoch 60/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0524 - val_loss: 0.0416\n",
      "Epoch 61/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0509 - val_loss: 0.0416\n",
      "Epoch 62/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0416\n",
      "Epoch 63/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0497 - val_loss: 0.0414\n",
      "Epoch 64/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0495 - val_loss: 0.0416\n",
      "Epoch 65/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0414\n",
      "Epoch 66/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0415\n",
      "Epoch 67/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0417\n",
      "Epoch 68/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0414\n",
      "Epoch 69/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0493 - val_loss: 0.0415\n",
      "Epoch 70/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0415\n",
      "Epoch 71/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0496 - val_loss: 0.0413\n",
      "Epoch 72/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0416\n",
      "Epoch 73/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0493 - val_loss: 0.0415\n",
      "Epoch 74/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0413\n",
      "Epoch 75/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0416\n",
      "Epoch 76/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0414\n",
      "Epoch 77/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0493 - val_loss: 0.0413\n",
      "Epoch 78/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0415\n",
      "Epoch 79/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0414\n",
      "Epoch 80/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0495 - val_loss: 0.0417\n",
      "Epoch 81/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0418\n",
      "Epoch 82/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0415\n",
      "Epoch 83/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0411\n",
      "Epoch 84/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0414\n",
      "Epoch 85/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0411\n",
      "Epoch 86/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0423\n",
      "Epoch 87/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0416\n",
      "Epoch 88/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0494 - val_loss: 0.0419\n",
      "Epoch 89/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0420\n",
      "Epoch 90/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0417\n",
      "Epoch 91/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0414\n",
      "Epoch 92/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0411\n",
      "Epoch 93/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0413\n",
      "Epoch 94/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0410\n",
      "Epoch 95/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0412\n",
      "Epoch 96/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0412\n",
      "Epoch 97/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0412\n",
      "Epoch 98/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0414\n",
      "Epoch 99/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0493 - val_loss: 0.0413\n",
      "Epoch 100/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0411\n",
      "Epoch 101/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0414\n",
      "Epoch 102/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0413\n",
      "Epoch 103/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0411\n",
      "Epoch 104/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0412\n",
      "Epoch 105/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0492 - val_loss: 0.0411\n",
      "Epoch 106/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0411\n",
      "Epoch 107/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0414\n",
      "Epoch 108/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0411\n",
      "Epoch 109/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0410\n",
      "Epoch 110/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0415\n",
      "Epoch 111/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0413\n",
      "Epoch 112/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0409\n",
      "Epoch 113/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0411\n",
      "Epoch 114/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0411\n",
      "Epoch 115/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0418\n",
      "Epoch 116/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0409\n",
      "Epoch 117/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0489 - val_loss: 0.0413\n",
      "Epoch 118/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0489 - val_loss: 0.0416\n",
      "Epoch 119/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0413\n",
      "Epoch 120/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0411\n",
      "Epoch 121/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0411\n",
      "Epoch 122/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0417\n",
      "Epoch 123/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0411\n",
      "Epoch 124/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0412\n",
      "Epoch 125/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0416\n",
      "Epoch 126/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0410\n",
      "Epoch 127/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0422\n",
      "Epoch 128/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0408\n",
      "Epoch 129/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0413\n",
      "Epoch 130/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0410\n",
      "Epoch 131/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0490 - val_loss: 0.0412\n",
      "Epoch 132/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0416\n",
      "Epoch 133/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0413\n",
      "Epoch 134/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0409\n",
      "Epoch 135/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0412\n",
      "Epoch 136/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0491 - val_loss: 0.0417\n",
      "Epoch 137/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0412\n",
      "Epoch 138/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0410\n",
      "Epoch 139/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0412\n",
      "Epoch 140/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0418\n",
      "Epoch 141/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0411\n",
      "Epoch 142/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0412\n",
      "Epoch 143/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 144/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0410\n",
      "Epoch 145/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0411\n",
      "Epoch 146/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0409\n",
      "Epoch 147/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 148/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0408\n",
      "Epoch 149/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0410\n",
      "Epoch 150/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0409\n",
      "Epoch 151/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0414\n",
      "Epoch 152/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0411\n",
      "Epoch 153/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0417\n",
      "Epoch 154/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 155/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0409\n",
      "Epoch 156/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0413\n",
      "Epoch 157/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 158/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0415\n",
      "Epoch 159/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0414\n",
      "Epoch 160/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0486 - val_loss: 0.0410\n",
      "Epoch 161/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0411\n",
      "Epoch 162/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0410\n",
      "Epoch 163/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0410\n",
      "Epoch 164/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0409\n",
      "Epoch 165/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0410\n",
      "Epoch 166/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 167/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0408\n",
      "Epoch 168/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0410\n",
      "Epoch 169/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0409\n",
      "Epoch 170/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 171/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0417\n",
      "Epoch 172/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0489 - val_loss: 0.0414\n",
      "Epoch 173/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0409\n",
      "Epoch 174/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0488 - val_loss: 0.0410\n",
      "Epoch 175/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0412\n",
      "Epoch 176/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0409\n",
      "Epoch 177/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0409\n",
      "Epoch 178/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0413\n",
      "Epoch 179/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0487 - val_loss: 0.0412\n",
      "Epoch 180/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0413\n",
      "Epoch 181/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0410\n",
      "Epoch 182/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 183/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0407\n",
      "Epoch 184/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 185/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0411\n",
      "Epoch 186/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0410\n",
      "Epoch 187/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0410\n",
      "Epoch 188/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0411\n",
      "Epoch 189/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0408\n",
      "Epoch 190/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 191/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0414\n",
      "Epoch 192/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0410\n",
      "Epoch 193/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0413\n",
      "Epoch 194/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0413\n",
      "Epoch 195/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 196/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0409\n",
      "Epoch 197/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 198/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 199/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0409\n",
      "Epoch 200/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0408\n",
      "Epoch 201/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0407\n",
      "Epoch 202/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0412\n",
      "Epoch 203/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0413\n",
      "Epoch 204/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0411\n",
      "Epoch 205/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0406\n",
      "Epoch 206/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0408\n",
      "Epoch 207/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 208/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0408\n",
      "Epoch 209/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 210/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0411\n",
      "Epoch 211/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0409\n",
      "Epoch 212/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 213/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0409\n",
      "Epoch 214/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0408\n",
      "Epoch 215/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0406\n",
      "Epoch 216/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0407\n",
      "Epoch 217/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0412\n",
      "Epoch 218/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0413\n",
      "Epoch 219/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0408\n",
      "Epoch 220/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 221/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0411\n",
      "Epoch 222/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 223/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0487 - val_loss: 0.0410\n",
      "Epoch 224/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 225/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 226/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0410\n",
      "Epoch 227/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0408\n",
      "Epoch 228/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0409\n",
      "Epoch 229/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 230/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0408\n",
      "Epoch 231/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0408\n",
      "Epoch 232/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0486 - val_loss: 0.0410\n",
      "Epoch 233/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0406\n",
      "Epoch 234/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0412\n",
      "Epoch 235/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 236/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 237/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 238/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 239/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0410\n",
      "Epoch 240/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 241/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 242/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0407\n",
      "Epoch 243/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0411\n",
      "Epoch 244/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 245/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 246/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 247/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0411\n",
      "Epoch 248/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 249/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0407\n",
      "Epoch 250/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0407\n",
      "Epoch 251/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0408\n",
      "Epoch 252/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 253/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0413\n",
      "Epoch 254/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 255/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0485 - val_loss: 0.0410\n",
      "Epoch 256/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 257/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0407\n",
      "Epoch 258/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0411\n",
      "Epoch 259/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 260/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0408\n",
      "Epoch 261/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0410\n",
      "Epoch 262/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0408\n",
      "Epoch 263/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 264/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0412\n",
      "Epoch 265/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0411\n",
      "Epoch 266/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 267/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0407\n",
      "Epoch 268/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 269/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0407\n",
      "Epoch 270/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0411\n",
      "Epoch 271/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0406\n",
      "Epoch 272/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0406\n",
      "Epoch 273/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0408\n",
      "Epoch 274/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0406\n",
      "Epoch 275/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0411\n",
      "Epoch 276/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 277/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 278/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0408\n",
      "Epoch 279/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0410\n",
      "Epoch 280/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 281/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 282/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0410\n",
      "Epoch 283/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0484 - val_loss: 0.0411\n",
      "Epoch 284/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0410\n",
      "Epoch 285/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0407\n",
      "Epoch 286/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0412\n",
      "Epoch 287/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 288/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 289/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0409\n",
      "Epoch 290/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 291/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0408\n",
      "Epoch 292/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0409\n",
      "Epoch 293/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 294/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0406\n",
      "Epoch 295/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0413\n",
      "Epoch 296/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0407\n",
      "Epoch 297/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0408\n",
      "Epoch 298/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0409\n",
      "Epoch 299/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0411\n",
      "Epoch 300/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0483 - val_loss: 0.0406\n",
      "Epoch 301/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0411\n",
      "Epoch 302/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0406\n",
      "Epoch 303/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 304/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 305/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0413\n",
      "Epoch 306/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 307/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0408\n",
      "Epoch 308/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0408\n",
      "Epoch 309/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0407\n",
      "Epoch 310/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0408\n",
      "Epoch 311/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0410\n",
      "Epoch 312/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 313/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0407\n",
      "Epoch 314/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 315/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0412\n",
      "Epoch 316/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0408\n",
      "Epoch 317/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0407\n",
      "Epoch 318/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0408\n",
      "Epoch 319/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0412\n",
      "Epoch 320/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0408\n",
      "Epoch 321/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0408\n",
      "Epoch 322/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0408\n",
      "Epoch 323/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 324/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0408\n",
      "Epoch 325/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0407\n",
      "Epoch 326/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 327/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 328/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0409\n",
      "Epoch 329/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0406\n",
      "Epoch 330/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0407\n",
      "Epoch 331/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0409\n",
      "Epoch 332/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0407\n",
      "Epoch 333/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0410\n",
      "Epoch 334/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0410\n",
      "Epoch 335/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 336/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0406\n",
      "Epoch 337/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0407\n",
      "Epoch 338/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0411\n",
      "Epoch 339/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0407\n",
      "Epoch 340/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0409\n",
      "Epoch 341/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0406\n",
      "Epoch 342/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 343/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0409\n",
      "Epoch 344/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0477 - val_loss: 0.0408\n",
      "Epoch 345/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0482 - val_loss: 0.0408\n",
      "Epoch 346/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0407\n",
      "Epoch 347/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0409\n",
      "Epoch 348/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0407\n",
      "Epoch 349/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0410\n",
      "Epoch 350/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0411\n",
      "Epoch 351/10000\n",
      "302/302 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0411\n",
      "Epoch 352/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0407\n",
      "Epoch 353/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0408\n",
      "Epoch 354/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0477 - val_loss: 0.0408\n",
      "Epoch 355/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0477 - val_loss: 0.0407\n",
      "Epoch 356/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0481 - val_loss: 0.0408\n",
      "Epoch 357/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0409\n",
      "Epoch 358/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0410\n",
      "Epoch 359/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0411\n",
      "Epoch 360/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0409\n",
      "Epoch 361/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0477 - val_loss: 0.0409\n",
      "Epoch 362/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0407\n",
      "Epoch 363/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0409\n",
      "Epoch 364/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0409\n",
      "Epoch 365/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0475 - val_loss: 0.0408\n",
      "Epoch 366/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0407\n",
      "Epoch 367/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0476 - val_loss: 0.0406\n",
      "Epoch 368/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0480 - val_loss: 0.0409\n",
      "Epoch 369/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0476 - val_loss: 0.0409\n",
      "Epoch 370/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0410\n",
      "Epoch 371/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0411\n",
      "Epoch 372/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0478 - val_loss: 0.0408\n",
      "Epoch 373/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0407\n",
      "Epoch 374/10000\n",
      "302/302 [==============================] - 2s 6ms/step - loss: 0.0479 - val_loss: 0.0410\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5854, 12, 1)\n",
      "y_test.shape:  (5854, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "183/183 [==============================] - 1s 2ms/step\n",
      "183/183 [==============================] - 0s 2ms/step\n",
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7813, 12, 1)\n",
      "y_test.shape:  (7813, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "245/245 [==============================] - 1s 2ms/step\n",
      "245/245 [==============================] - 1s 2ms/step\n",
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7953, 12, 1)\n",
      "y_test.shape:  (7953, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "249/249 [==============================] - 1s 3ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6694, 12, 1)\n",
      "y_test.shape:  (6694, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "210/210 [==============================] - 1s 3ms/step\n",
      "210/210 [==============================] - 0s 2ms/step\n",
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7349, 12, 1)\n",
      "y_test.shape:  (7349, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "230/230 [==============================] - 1s 3ms/step\n",
      "230/230 [==============================] - 1s 2ms/step\n",
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7644, 12, 1)\n",
      "y_test.shape:  (7644, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "239/239 [==============================] - 1s 3ms/step\n",
      "239/239 [==============================] - 1s 2ms/step\n",
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7649, 12, 1)\n",
      "y_test.shape:  (7649, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "240/240 [==============================] - 1s 2ms/step\n",
      "240/240 [==============================] - 1s 2ms/step\n",
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7844, 12, 1)\n",
      "y_test.shape:  (7844, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "246/246 [==============================] - 1s 3ms/step\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7563, 12, 1)\n",
      "y_test.shape:  (7563, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "237/237 [==============================] - 1s 3ms/step\n",
      "237/237 [==============================] - 1s 2ms/step\n",
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7897, 12, 1)\n",
      "y_test.shape:  (7897, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "247/247 [==============================] - 1s 3ms/step\n",
      "247/247 [==============================] - 1s 2ms/step\n",
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7856, 12, 1)\n",
      "y_test.shape:  (7856, 1)\n",
      "x.shape =  (None, 12, 32)\n",
      "x.shape =  (None, 12, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_12sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "patient id:  1726.csv\n",
      "2025-01-22 21:37:06,583 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (301747, 24, 1)\n",
      "y_train.shape:  (301747, 1)\n",
      "x_valid.shape:  (75415, 24, 1)\n",
      "y_valid.shape:  (75415, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-22 21:37:49,306 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "295/295 [==============================] - 4s 9ms/step - loss: 1.4255 - val_loss: 0.3731\n",
      "Epoch 2/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4689 - val_loss: 0.1191\n",
      "Epoch 3/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.1259 - val_loss: 0.0564\n",
      "Epoch 4/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0994 - val_loss: 0.0501\n",
      "Epoch 5/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0879 - val_loss: 0.0499\n",
      "Epoch 6/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0815 - val_loss: 0.0478\n",
      "Epoch 7/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0768 - val_loss: 0.0456\n",
      "Epoch 8/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0724 - val_loss: 0.0458\n",
      "Epoch 9/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0681 - val_loss: 0.0459\n",
      "Epoch 10/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0653 - val_loss: 0.0440\n",
      "Epoch 11/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0624 - val_loss: 0.0450\n",
      "Epoch 12/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0611 - val_loss: 0.0456\n",
      "Epoch 13/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0600 - val_loss: 0.0446\n",
      "Epoch 14/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0595 - val_loss: 0.0452\n",
      "Epoch 15/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0592 - val_loss: 0.0460\n",
      "Epoch 16/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0585 - val_loss: 0.0449\n",
      "Epoch 17/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0583 - val_loss: 0.0450\n",
      "Epoch 18/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0582 - val_loss: 0.0449\n",
      "Epoch 19/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0578 - val_loss: 0.0450\n",
      "Epoch 20/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0577 - val_loss: 0.0450\n",
      "Epoch 21/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0575 - val_loss: 0.0441\n",
      "Epoch 22/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0575 - val_loss: 0.0441\n",
      "Epoch 23/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0570 - val_loss: 0.0444\n",
      "Epoch 24/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0568 - val_loss: 0.0450\n",
      "Epoch 25/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0565 - val_loss: 0.0441\n",
      "Epoch 26/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0564 - val_loss: 0.0439\n",
      "Epoch 27/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0566 - val_loss: 0.0436\n",
      "Epoch 28/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0564 - val_loss: 0.0435\n",
      "Epoch 29/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0565 - val_loss: 0.0443\n",
      "Epoch 30/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0564 - val_loss: 0.0440\n",
      "Epoch 31/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0564 - val_loss: 0.0435\n",
      "Epoch 32/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0562 - val_loss: 0.0452\n",
      "Epoch 33/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0563 - val_loss: 0.0439\n",
      "Epoch 34/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0560 - val_loss: 0.0438\n",
      "Epoch 35/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0565 - val_loss: 0.0439\n",
      "Epoch 36/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0561 - val_loss: 0.0442\n",
      "Epoch 37/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0562 - val_loss: 0.0434\n",
      "Epoch 38/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0561 - val_loss: 0.0434\n",
      "Epoch 39/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0561 - val_loss: 0.0442\n",
      "Epoch 40/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0559 - val_loss: 0.0436\n",
      "Epoch 41/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0561 - val_loss: 0.0434\n",
      "Epoch 42/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0558 - val_loss: 0.0445\n",
      "Epoch 43/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0559 - val_loss: 0.0439\n",
      "Epoch 44/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0562 - val_loss: 0.0437\n",
      "Epoch 45/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0559 - val_loss: 0.0437\n",
      "Epoch 46/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0560 - val_loss: 0.0438\n",
      "Epoch 47/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0560 - val_loss: 0.0432\n",
      "Epoch 48/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0558 - val_loss: 0.0433\n",
      "Epoch 49/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0559 - val_loss: 0.0433\n",
      "Epoch 50/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0561 - val_loss: 0.0434\n",
      "Epoch 51/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0559 - val_loss: 0.0431\n",
      "Epoch 52/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0561 - val_loss: 0.0436\n",
      "Epoch 53/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0558 - val_loss: 0.0430\n",
      "Epoch 54/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0560 - val_loss: 0.0435\n",
      "Epoch 55/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0559 - val_loss: 0.0443\n",
      "Epoch 56/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0559 - val_loss: 0.0432\n",
      "Epoch 57/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0556 - val_loss: 0.0432\n",
      "Epoch 58/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0558 - val_loss: 0.0431\n",
      "Epoch 59/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0555 - val_loss: 0.0429\n",
      "Epoch 60/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0560 - val_loss: 0.0433\n",
      "Epoch 61/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0556 - val_loss: 0.0431\n",
      "Epoch 62/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0555 - val_loss: 0.0431\n",
      "Epoch 63/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0557 - val_loss: 0.0434\n",
      "Epoch 64/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0556 - val_loss: 0.0432\n",
      "Epoch 65/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0557 - val_loss: 0.0435\n",
      "Epoch 66/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0557 - val_loss: 0.0435\n",
      "Epoch 67/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0555 - val_loss: 0.0443\n",
      "Epoch 68/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0555 - val_loss: 0.0443\n",
      "Epoch 69/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0557 - val_loss: 0.0429\n",
      "Epoch 70/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0558 - val_loss: 0.0432\n",
      "Epoch 71/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0434\n",
      "Epoch 72/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0556 - val_loss: 0.0428\n",
      "Epoch 73/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0555 - val_loss: 0.0434\n",
      "Epoch 74/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0555 - val_loss: 0.0428\n",
      "Epoch 75/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0556 - val_loss: 0.0436\n",
      "Epoch 76/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0554 - val_loss: 0.0436\n",
      "Epoch 77/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0556 - val_loss: 0.0434\n",
      "Epoch 78/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0558 - val_loss: 0.0435\n",
      "Epoch 79/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0556 - val_loss: 0.0430\n",
      "Epoch 80/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0554 - val_loss: 0.0429\n",
      "Epoch 81/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0455\n",
      "Epoch 82/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0552 - val_loss: 0.0430\n",
      "Epoch 83/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0431\n",
      "Epoch 84/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0551 - val_loss: 0.0438\n",
      "Epoch 85/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0431\n",
      "Epoch 86/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0437\n",
      "Epoch 87/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0427\n",
      "Epoch 88/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0555 - val_loss: 0.0431\n",
      "Epoch 89/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0551 - val_loss: 0.0442\n",
      "Epoch 90/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0552 - val_loss: 0.0429\n",
      "Epoch 91/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0551 - val_loss: 0.0438\n",
      "Epoch 92/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0429\n",
      "Epoch 93/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0427\n",
      "Epoch 94/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0552 - val_loss: 0.0432\n",
      "Epoch 95/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0555 - val_loss: 0.0432\n",
      "Epoch 96/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0552 - val_loss: 0.0434\n",
      "Epoch 97/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0431\n",
      "Epoch 98/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0430\n",
      "Epoch 99/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0553 - val_loss: 0.0428\n",
      "Epoch 100/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0431\n",
      "Epoch 101/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0438\n",
      "Epoch 102/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0429\n",
      "Epoch 103/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0549 - val_loss: 0.0440\n",
      "Epoch 104/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0431\n",
      "Epoch 105/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0554 - val_loss: 0.0433\n",
      "Epoch 106/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0428\n",
      "Epoch 107/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0427\n",
      "Epoch 108/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0552 - val_loss: 0.0427\n",
      "Epoch 109/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0427\n",
      "Epoch 110/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0552 - val_loss: 0.0428\n",
      "Epoch 111/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0429\n",
      "Epoch 112/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0428\n",
      "Epoch 113/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0428\n",
      "Epoch 114/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0438\n",
      "Epoch 115/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0550 - val_loss: 0.0430\n",
      "Epoch 116/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0551 - val_loss: 0.0434\n",
      "Epoch 117/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0550 - val_loss: 0.0428\n",
      "Epoch 118/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0427\n",
      "Epoch 119/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0428\n",
      "Epoch 120/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0432\n",
      "Epoch 121/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0548 - val_loss: 0.0429\n",
      "Epoch 122/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0548 - val_loss: 0.0429\n",
      "Epoch 123/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0428\n",
      "Epoch 124/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0429\n",
      "Epoch 125/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0548 - val_loss: 0.0433\n",
      "Epoch 126/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0429\n",
      "Epoch 127/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0430\n",
      "Epoch 128/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0426\n",
      "Epoch 129/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0550 - val_loss: 0.0426\n",
      "Epoch 130/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0548 - val_loss: 0.0426\n",
      "Epoch 131/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0549 - val_loss: 0.0429\n",
      "Epoch 132/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0546 - val_loss: 0.0431\n",
      "Epoch 133/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0430\n",
      "Epoch 134/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0428\n",
      "Epoch 135/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0546 - val_loss: 0.0426\n",
      "Epoch 136/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0544 - val_loss: 0.0438\n",
      "Epoch 137/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0434\n",
      "Epoch 138/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0426\n",
      "Epoch 139/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0426\n",
      "Epoch 140/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0548 - val_loss: 0.0430\n",
      "Epoch 141/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0427\n",
      "Epoch 142/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0435\n",
      "Epoch 143/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0432\n",
      "Epoch 144/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0439\n",
      "Epoch 145/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0548 - val_loss: 0.0430\n",
      "Epoch 146/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0549 - val_loss: 0.0430\n",
      "Epoch 147/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0433\n",
      "Epoch 148/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0433\n",
      "Epoch 149/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0426\n",
      "Epoch 150/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0548 - val_loss: 0.0432\n",
      "Epoch 151/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0545 - val_loss: 0.0429\n",
      "Epoch 152/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0429\n",
      "Epoch 153/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0545 - val_loss: 0.0430\n",
      "Epoch 154/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0429\n",
      "Epoch 155/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0426\n",
      "Epoch 156/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0544 - val_loss: 0.0429\n",
      "Epoch 157/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0543 - val_loss: 0.0427\n",
      "Epoch 158/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0546 - val_loss: 0.0430\n",
      "Epoch 159/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0428\n",
      "Epoch 160/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0546 - val_loss: 0.0428\n",
      "Epoch 161/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0545 - val_loss: 0.0425\n",
      "Epoch 162/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0544 - val_loss: 0.0434\n",
      "Epoch 163/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0549 - val_loss: 0.0434\n",
      "Epoch 164/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0546 - val_loss: 0.0425\n",
      "Epoch 165/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0547 - val_loss: 0.0439\n",
      "Epoch 166/10000\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.0545 - val_loss: 0.0426\n",
      "Epoch 167/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0545 - val_loss: 0.0430\n",
      "Epoch 168/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0426\n",
      "Epoch 169/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0433\n",
      "Epoch 170/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0426\n",
      "Epoch 171/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0444\n",
      "Epoch 172/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0547 - val_loss: 0.0428\n",
      "Epoch 173/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0429\n",
      "Epoch 174/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0435\n",
      "Epoch 175/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0432\n",
      "Epoch 176/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0430\n",
      "Epoch 177/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0426\n",
      "Epoch 178/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0429\n",
      "Epoch 179/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0540 - val_loss: 0.0435\n",
      "Epoch 180/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0435\n",
      "Epoch 181/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0431\n",
      "Epoch 182/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0543 - val_loss: 0.0431\n",
      "Epoch 183/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0544 - val_loss: 0.0425\n",
      "Epoch 184/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0431\n",
      "Epoch 185/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0430\n",
      "Epoch 186/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0545 - val_loss: 0.0426\n",
      "Epoch 187/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0439\n",
      "Epoch 188/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0427\n",
      "Epoch 189/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0432\n",
      "Epoch 190/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0432\n",
      "Epoch 191/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0540 - val_loss: 0.0426\n",
      "Epoch 192/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0428\n",
      "Epoch 193/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0428\n",
      "Epoch 194/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0544 - val_loss: 0.0435\n",
      "Epoch 195/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0544 - val_loss: 0.0424\n",
      "Epoch 196/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0543 - val_loss: 0.0430\n",
      "Epoch 197/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0543 - val_loss: 0.0432\n",
      "Epoch 198/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0435\n",
      "Epoch 199/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0543 - val_loss: 0.0432\n",
      "Epoch 200/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0436\n",
      "Epoch 201/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0437\n",
      "Epoch 202/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0540 - val_loss: 0.0433\n",
      "Epoch 203/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0540 - val_loss: 0.0431\n",
      "Epoch 204/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0441\n",
      "Epoch 205/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0429\n",
      "Epoch 206/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0427\n",
      "Epoch 207/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0541 - val_loss: 0.0436\n",
      "Epoch 208/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0540 - val_loss: 0.0431\n",
      "Epoch 209/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0539 - val_loss: 0.0430\n",
      "Epoch 210/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0539 - val_loss: 0.0428\n",
      "Epoch 211/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0539 - val_loss: 0.0429\n",
      "Epoch 212/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0539 - val_loss: 0.0428\n",
      "Epoch 213/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0542 - val_loss: 0.0428\n",
      "Epoch 214/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0539 - val_loss: 0.0429\n",
      "Epoch 215/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0517 - val_loss: 0.0421\n",
      "Epoch 216/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0494 - val_loss: 0.0416\n",
      "Epoch 217/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0487 - val_loss: 0.0414\n",
      "Epoch 218/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0488 - val_loss: 0.0415\n",
      "Epoch 219/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0420\n",
      "Epoch 220/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0415\n",
      "Epoch 221/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0414\n",
      "Epoch 222/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0486 - val_loss: 0.0411\n",
      "Epoch 223/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0411\n",
      "Epoch 224/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0412\n",
      "Epoch 225/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0483 - val_loss: 0.0414\n",
      "Epoch 226/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0416\n",
      "Epoch 227/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0413\n",
      "Epoch 228/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0486 - val_loss: 0.0415\n",
      "Epoch 229/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0414\n",
      "Epoch 230/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0484 - val_loss: 0.0413\n",
      "Epoch 231/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0413\n",
      "Epoch 232/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0415\n",
      "Epoch 233/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0484 - val_loss: 0.0414\n",
      "Epoch 234/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0485 - val_loss: 0.0414\n",
      "Epoch 235/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0413\n",
      "Epoch 236/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0483 - val_loss: 0.0415\n",
      "Epoch 237/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0482 - val_loss: 0.0418\n",
      "Epoch 238/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0418\n",
      "Epoch 239/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0417\n",
      "Epoch 240/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0416\n",
      "Epoch 241/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0418\n",
      "Epoch 242/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0483 - val_loss: 0.0419\n",
      "Epoch 243/10000\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.0484 - val_loss: 0.0419\n",
      "Epoch 244/10000\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.0484 - val_loss: 0.0416\n",
      "Epoch 245/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0483 - val_loss: 0.0412\n",
      "Epoch 246/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0415\n",
      "Epoch 247/10000\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.0480 - val_loss: 0.0418\n",
      "Epoch 248/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0478 - val_loss: 0.0417\n",
      "Epoch 249/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0484 - val_loss: 0.0417\n",
      "Epoch 250/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0413\n",
      "Epoch 251/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0415\n",
      "Epoch 252/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0414\n",
      "Epoch 253/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0416\n",
      "Epoch 254/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0482 - val_loss: 0.0412\n",
      "Epoch 255/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0418\n",
      "Epoch 256/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0418\n",
      "Epoch 257/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0482 - val_loss: 0.0419\n",
      "Epoch 258/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0412\n",
      "Epoch 259/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0422\n",
      "Epoch 260/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0482 - val_loss: 0.0413\n",
      "Epoch 261/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0479 - val_loss: 0.0421\n",
      "Epoch 262/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0420\n",
      "Epoch 263/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0478 - val_loss: 0.0419\n",
      "Epoch 264/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0418\n",
      "Epoch 265/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0418\n",
      "Epoch 266/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0420\n",
      "Epoch 267/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0419\n",
      "Epoch 268/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0484 - val_loss: 0.0418\n",
      "Epoch 269/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0483 - val_loss: 0.0415\n",
      "Epoch 270/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0416\n",
      "Epoch 271/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0481 - val_loss: 0.0417\n",
      "Epoch 272/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0481 - val_loss: 0.0418\n",
      "Epoch 273/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0415\n",
      "Epoch 274/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0418\n",
      "Epoch 275/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0418\n",
      "Epoch 276/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0478 - val_loss: 0.0420\n",
      "Epoch 277/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0417\n",
      "Epoch 278/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0480 - val_loss: 0.0419\n",
      "Epoch 279/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0416\n",
      "Epoch 280/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0480 - val_loss: 0.0419\n",
      "Epoch 281/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0419\n",
      "Epoch 282/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0420\n",
      "Epoch 283/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0478 - val_loss: 0.0419\n",
      "Epoch 284/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0422\n",
      "Epoch 285/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 286/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0416\n",
      "Epoch 287/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0416\n",
      "Epoch 288/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0420\n",
      "Epoch 289/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0422\n",
      "Epoch 290/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0415\n",
      "Epoch 291/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0417\n",
      "Epoch 292/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0422\n",
      "Epoch 293/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 294/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0417\n",
      "Epoch 295/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0479 - val_loss: 0.0416\n",
      "Epoch 296/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0423\n",
      "Epoch 297/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 298/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0417\n",
      "Epoch 299/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0479 - val_loss: 0.0416\n",
      "Epoch 300/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0421\n",
      "Epoch 301/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0419\n",
      "Epoch 302/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0418\n",
      "Epoch 303/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0479 - val_loss: 0.0423\n",
      "Epoch 304/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0418\n",
      "Epoch 305/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0420\n",
      "Epoch 306/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0420\n",
      "Epoch 307/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0418\n",
      "Epoch 308/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0425\n",
      "Epoch 309/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0476 - val_loss: 0.0420\n",
      "Epoch 310/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0476 - val_loss: 0.0419\n",
      "Epoch 311/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 312/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 313/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0476 - val_loss: 0.0417\n",
      "Epoch 314/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0475 - val_loss: 0.0421\n",
      "Epoch 315/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0474 - val_loss: 0.0421\n",
      "Epoch 316/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0477 - val_loss: 0.0421\n",
      "Epoch 317/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0476 - val_loss: 0.0420\n",
      "Epoch 318/10000\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.0477 - val_loss: 0.0422\n",
      "Epoch 319/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0476 - val_loss: 0.0422\n",
      "Epoch 320/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0478 - val_loss: 0.0420\n",
      "Epoch 321/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0475 - val_loss: 0.0421\n",
      "Epoch 322/10000\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.0476 - val_loss: 0.0420\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5179, 24, 1)\n",
      "y_test.shape:  (5179, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7735, 24, 1)\n",
      "y_test.shape:  (7735, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "242/242 [==============================] - 1s 2ms/step\n",
      "242/242 [==============================] - 1s 2ms/step\n",
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7917, 24, 1)\n",
      "y_test.shape:  (7917, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "248/248 [==============================] - 1s 3ms/step\n",
      "248/248 [==============================] - 1s 3ms/step\n",
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6336, 24, 1)\n",
      "y_test.shape:  (6336, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "198/198 [==============================] - 1s 2ms/step\n",
      "198/198 [==============================] - 0s 2ms/step\n",
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7289, 24, 1)\n",
      "y_test.shape:  (7289, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "228/228 [==============================] - 1s 3ms/step\n",
      "228/228 [==============================] - 1s 3ms/step\n",
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7585, 24, 1)\n",
      "y_test.shape:  (7585, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "238/238 [==============================] - 1s 3ms/step\n",
      "238/238 [==============================] - 1s 3ms/step\n",
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7545, 24, 1)\n",
      "y_test.shape:  (7545, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "236/236 [==============================] - 1s 3ms/step\n",
      "236/236 [==============================] - 1s 2ms/step\n",
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7760, 24, 1)\n",
      "y_test.shape:  (7760, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "243/243 [==============================] - 1s 3ms/step\n",
      "243/243 [==============================] - 1s 3ms/step\n",
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7527, 24, 1)\n",
      "y_test.shape:  (7527, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "236/236 [==============================] - 1s 3ms/step\n",
      "236/236 [==============================] - 1s 3ms/step\n",
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7837, 24, 1)\n",
      "y_test.shape:  (7837, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "245/245 [==============================] - 1s 3ms/step\n",
      "245/245 [==============================] - 1s 3ms/step\n",
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7796, 24, 1)\n",
      "y_test.shape:  (7796, 1)\n",
      "x.shape =  (None, 24, 32)\n",
      "x.shape =  (None, 24, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_24sh\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "244/244 [==============================] - 1s 3ms/step\n",
      "244/244 [==============================] - 1s 3ms/step\n",
      "patient id:  1726.csv\n"
     ]
    }
   ],
   "source": [
    "for turning_sh in [60, 120]:\n",
    "    fold_number = 5\n",
    "    # for fold_number in range(1, 6):\n",
    "    yaml_filepath = f\"./vandoorn_t1dexi_experiments_{turning_sh}min/all_final_experiment_fold{fold_number}.yaml\"\n",
    "    mode = \"train\"\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        \n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)\n",
    "    yaml_files = glob.glob(f\"./vandoorn_t1dexi_experiments_{turning_sh}min/fold{fold_number}_eval/*.yaml\")\n",
    "    mode = \"evaluate\"\n",
    "    for yl_filepath in yaml_files:\n",
    "        cfgs = load_cfgs(yl_filepath)\n",
    "        print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "        for cfg in cfgs:\n",
    "            seed = int(cfg['train']['seed'])\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Print the configuration - just to make sure that you loaded what you\n",
    "            # wanted to load\n",
    "\n",
    "            module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "            module_model         = load_module(cfg['model']['script_path'])\n",
    "            module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "            module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "            module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "            pp = pprint.PrettyPrinter(indent=4)\n",
    "            pp.pprint(cfg)\n",
    "\n",
    "            #print(\"loading dataset ...\")\n",
    "            #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "            #nb_past_steps_tmp = 36\n",
    "            #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "            x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "            #x_train = x_train[:,-nb_past_steps:,:]\n",
    "            #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "            #x_test = x_test[:,-nb_past_steps:,:]\n",
    "            print(\"x_train.shape: \", x_train.shape)\n",
    "            print(\"y_train.shape: \", y_train.shape)\n",
    "            print(\"x_valid.shape: \", x_valid.shape)\n",
    "            print(\"y_valid.shape: \", y_valid.shape)\n",
    "            print(\"x_test.shape: \", x_test.shape)\n",
    "            print(\"y_test.shape: \", y_test.shape)\n",
    "            #print(\"loading optimizer ...\")\n",
    "            optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "            #print(\"loading loss function ...\")\n",
    "            loss_function = module_loss_function.load()\n",
    "            #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "            #print(\"loading model ...\")\n",
    "            if 'tf_nll' in loss_function.__name__:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1]*2,\n",
    "                    cfg['model']\n",
    "                )\n",
    "            else:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1],\n",
    "                    cfg['model']\n",
    "                )\n",
    "\n",
    "            if 'initial_weights_path' in cfg['train']:\n",
    "                #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function\n",
    "            )\n",
    "\n",
    "            #print(model.summary())\n",
    "\n",
    "            # training mode\n",
    "            if mode == 'train':\n",
    "                #print(\"training model ...\")\n",
    "                train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "            if mode == 'plot_nll':\n",
    "                plot_nll(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_noise_experiment':\n",
    "                plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_seg':\n",
    "                plot_seg(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_dist':\n",
    "                plot_target_distribution(y_test, cfg)\n",
    "\n",
    "            # evaluation mode\n",
    "            if mode == 'evaluate':\n",
    "                evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./vandoorn_t1dexi_experiments_30min/all_final_experiment_fold1.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 11:04:36,822 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "2025-01-22 11:04:36,823 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (301784, 6, 1)\n",
      "y_train.shape:  (301784, 1)\n",
      "x_valid.shape:  (75422, 6, 1)\n",
      "y_valid.shape:  (75422, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-22 11:05:15,577 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "295/295 [==============================] - 10s 7ms/step - loss: 0.5347 - val_loss: 0.0821\n",
      "Epoch 2/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1635 - val_loss: 0.0644\n",
      "Epoch 3/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1394 - val_loss: 0.0603\n",
      "Epoch 4/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1243 - val_loss: 0.0565\n",
      "Epoch 5/10000\n",
      "285/295 [===========================>..] - ETA: 0s - loss: 0.11172025-01-22 11:05:31,952 DEBUG Creating converter from 5 to 3\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1115 - val_loss: 0.0538\n",
      "Epoch 6/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1011 - val_loss: 0.0606\n",
      "Epoch 7/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0919 - val_loss: 0.0480\n",
      "Epoch 8/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0846 - val_loss: 0.0461\n",
      "Epoch 9/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0775 - val_loss: 0.0473\n",
      "Epoch 10/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0721 - val_loss: 0.0444\n",
      "Epoch 11/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0675 - val_loss: 0.0453\n",
      "Epoch 12/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0643 - val_loss: 0.0447\n",
      "Epoch 13/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0633 - val_loss: 0.0460\n",
      "Epoch 14/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0629 - val_loss: 0.0478\n",
      "Epoch 15/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0629 - val_loss: 0.0462\n",
      "Epoch 16/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0628 - val_loss: 0.0463\n",
      "Epoch 17/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0629 - val_loss: 0.0459\n",
      "Epoch 18/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0622 - val_loss: 0.0461\n",
      "Epoch 19/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0626 - val_loss: 0.0464\n",
      "Epoch 20/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0623 - val_loss: 0.0460\n",
      "Epoch 21/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0629 - val_loss: 0.0461\n",
      "Epoch 22/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0622 - val_loss: 0.0454\n",
      "Epoch 23/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0624 - val_loss: 0.0478\n",
      "Epoch 24/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0625 - val_loss: 0.0463\n",
      "Epoch 25/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0624 - val_loss: 0.0482\n",
      "Epoch 26/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0622 - val_loss: 0.0454\n",
      "Epoch 27/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0455\n",
      "Epoch 28/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0626 - val_loss: 0.0488\n",
      "Epoch 29/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0622 - val_loss: 0.0452\n",
      "Epoch 30/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0464\n",
      "Epoch 31/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0457\n",
      "Epoch 32/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0459\n",
      "Epoch 33/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0622 - val_loss: 0.0458\n",
      "Epoch 34/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0624 - val_loss: 0.0471\n",
      "Epoch 35/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0452\n",
      "Epoch 36/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0459\n",
      "Epoch 37/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0620 - val_loss: 0.0478\n",
      "Epoch 38/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0623 - val_loss: 0.0471\n",
      "Epoch 39/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0453\n",
      "Epoch 40/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0462\n",
      "Epoch 41/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0622 - val_loss: 0.0453\n",
      "Epoch 42/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0462\n",
      "Epoch 43/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0617 - val_loss: 0.0456\n",
      "Epoch 44/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0620 - val_loss: 0.0464\n",
      "Epoch 45/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0618 - val_loss: 0.0486\n",
      "Epoch 46/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0620 - val_loss: 0.0460\n",
      "Epoch 47/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0459\n",
      "Epoch 48/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0618 - val_loss: 0.0462\n",
      "Epoch 49/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0626 - val_loss: 0.0463\n",
      "Epoch 50/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0617 - val_loss: 0.0452\n",
      "Epoch 51/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0469\n",
      "Epoch 52/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0619 - val_loss: 0.0450\n",
      "Epoch 53/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0620 - val_loss: 0.0451\n",
      "Epoch 54/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0620 - val_loss: 0.0468\n",
      "Epoch 55/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0621 - val_loss: 0.0453\n",
      "Epoch 56/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0620 - val_loss: 0.0455\n",
      "Epoch 57/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0451\n",
      "Epoch 58/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0620 - val_loss: 0.0464\n",
      "Epoch 59/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0619 - val_loss: 0.0449\n",
      "Epoch 60/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0619 - val_loss: 0.0451\n",
      "Epoch 61/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0621 - val_loss: 0.0453\n",
      "Epoch 62/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0461\n",
      "Epoch 63/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0620 - val_loss: 0.0453\n",
      "Epoch 64/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0617 - val_loss: 0.0454\n",
      "Epoch 65/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0460\n",
      "Epoch 66/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0620 - val_loss: 0.0447\n",
      "Epoch 67/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0621 - val_loss: 0.0451\n",
      "Epoch 68/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0616 - val_loss: 0.0447\n",
      "Epoch 69/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0618 - val_loss: 0.0450\n",
      "Epoch 70/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0618 - val_loss: 0.0462\n",
      "Epoch 71/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0479\n",
      "Epoch 72/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0457\n",
      "Epoch 73/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0453\n",
      "Epoch 74/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0452\n",
      "Epoch 75/10000\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.0614 - val_loss: 0.0469\n",
      "Epoch 76/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0459\n",
      "Epoch 77/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0619 - val_loss: 0.0448\n",
      "Epoch 78/10000\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.0615 - val_loss: 0.0449\n",
      "Epoch 79/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0451\n",
      "Epoch 80/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0617 - val_loss: 0.0447\n",
      "Epoch 81/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0612 - val_loss: 0.0447\n",
      "Epoch 82/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0463\n",
      "Epoch 83/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0455\n",
      "Epoch 84/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0458\n",
      "Epoch 85/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0456\n",
      "Epoch 86/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0618 - val_loss: 0.0461\n",
      "Epoch 87/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0613 - val_loss: 0.0446\n",
      "Epoch 88/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0621 - val_loss: 0.0450\n",
      "Epoch 89/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0612 - val_loss: 0.0453\n",
      "Epoch 90/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0616 - val_loss: 0.0447\n",
      "Epoch 91/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0613 - val_loss: 0.0466\n",
      "Epoch 92/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0618 - val_loss: 0.0458\n",
      "Epoch 93/10000\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.0614 - val_loss: 0.0465\n",
      "Epoch 94/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0452\n",
      "Epoch 95/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0619 - val_loss: 0.0453\n",
      "Epoch 96/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0617 - val_loss: 0.0453\n",
      "Epoch 97/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0451\n",
      "Epoch 98/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0618 - val_loss: 0.0448\n",
      "Epoch 99/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0446\n",
      "Epoch 100/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0613 - val_loss: 0.0446\n",
      "Epoch 101/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0616 - val_loss: 0.0463\n",
      "Epoch 102/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0613 - val_loss: 0.0448\n",
      "Epoch 103/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0451\n",
      "Epoch 104/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0613 - val_loss: 0.0446\n",
      "Epoch 105/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0613 - val_loss: 0.0450\n",
      "Epoch 106/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0449\n",
      "Epoch 107/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0455\n",
      "Epoch 108/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0614 - val_loss: 0.0446\n",
      "Epoch 109/10000\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.0616 - val_loss: 0.0478\n",
      "Epoch 110/10000\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.0615 - val_loss: 0.0448\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vandoorn_t1dexi_experiments_30min/fold1_eval\\\\103_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\114_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\115_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\11_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\144_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\152_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\173_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\187_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\18_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\1_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\248_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\24_evaluation.yaml',\n",
       " './vandoorn_t1dexi_experiments_30min/fold1_eval\\\\25_evaluation.yaml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_files = glob.glob(\"./vandoorn_t1dexi_experiments_30min/fold1_eval/*.yaml\")\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5701, 6, 1)\n",
      "y_test.shape:  (5701, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-22 11:11:40,692 DEBUG Creating converter from 3 to 5\n",
      "179/179 [==============================] - 1s 3ms/step\n",
      "179/179 [==============================] - 0s 2ms/step\n",
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7700, 6, 1)\n",
      "y_test.shape:  (7700, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "241/241 [==============================] - 1s 2ms/step\n",
      "241/241 [==============================] - 0s 2ms/step\n",
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7931, 6, 1)\n",
      "y_test.shape:  (7931, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "248/248 [==============================] - 1s 2ms/step\n",
      "248/248 [==============================] - 0s 2ms/step\n",
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7970, 6, 1)\n",
      "y_test.shape:  (7970, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "250/250 [==============================] - 1s 2ms/step\n",
      "250/250 [==============================] - 1s 2ms/step\n",
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7456, 6, 1)\n",
      "y_test.shape:  (7456, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "233/233 [==============================] - 1s 2ms/step\n",
      "233/233 [==============================] - 0s 2ms/step\n",
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7869, 6, 1)\n",
      "y_test.shape:  (7869, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "246/246 [==============================] - 0s 2ms/step\n",
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7806, 6, 1)\n",
      "y_test.shape:  (7806, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "244/244 [==============================] - 1s 2ms/step\n",
      "244/244 [==============================] - 0s 2ms/step\n",
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7871, 6, 1)\n",
      "y_test.shape:  (7871, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "246/246 [==============================] - 1s 2ms/step\n",
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7944, 6, 1)\n",
      "y_test.shape:  (7944, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "249/249 [==============================] - 1s 2ms/step\n",
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7799, 6, 1)\n",
      "y_test.shape:  (7799, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "244/244 [==============================] - 1s 2ms/step\n",
      "244/244 [==============================] - 1s 2ms/step\n",
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5698, 6, 1)\n",
      "y_test.shape:  (5698, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "179/179 [==============================] - 1s 2ms/step\n",
      "179/179 [==============================] - 0s 2ms/step\n",
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7684, 6, 1)\n",
      "y_test.shape:  (7684, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "241/241 [==============================] - 1s 2ms/step\n",
      "241/241 [==============================] - 0s 2ms/step\n",
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': 0.0001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7443, 6, 1)\n",
      "y_test.shape:  (7443, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "233/233 [==============================] - 1s 2ms/step\n",
      "233/233 [==============================] - 0s 2ms/step\n",
      "patient id:  25.csv\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yl_filepath in yaml_files:\n",
    "    cfgs = load_cfgs(yl_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./t1dexi_test_cfg_method1/subject1617_evaluate.yaml\"\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
