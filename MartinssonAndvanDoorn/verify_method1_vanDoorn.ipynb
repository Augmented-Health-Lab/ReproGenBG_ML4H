{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 22:19:42,658 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2024-11-30 22:19:42,666 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-11-30 22:19:42,667 DEBUG interactive is False\n",
      "2024-11-30 22:19:42,668 DEBUG platform is win32\n",
      "2024-11-30 22:19:42,707 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-11-30 22:19:42,712 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_vandoorn_experiments/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "        # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 21:50:21,810 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\OhioT1DM\\2018\\train\\all does not exist.\n",
      "2024-11-30 21:50:21,811 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment does not exist.\n",
      "Running 3 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (53671, 6, 1)\n",
      "y_train.shape:  (53671, 1)\n",
      "x_valid.shape:  (13415, 6, 1)\n",
      "y_valid.shape:  (13415, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-30 21:50:28,485 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "53/53 [==============================] - 3s 16ms/step - loss: 2.8380 - val_loss: 2.8515\n",
      "Epoch 2/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 2.4623 - val_loss: 2.3512\n",
      "Epoch 3/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 2.0041 - val_loss: 1.8735\n",
      "Epoch 4/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.5327 - val_loss: 1.3347\n",
      "Epoch 5/10000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 1.03522024-11-30 21:50:33,385 DEBUG Creating converter from 5 to 3\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 1.0341 - val_loss: 0.8044\n",
      "Epoch 6/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.6608 - val_loss: 0.5054\n",
      "Epoch 7/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.5118 - val_loss: 0.3951\n",
      "Epoch 8/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.4583 - val_loss: 0.3379\n",
      "Epoch 9/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.4213 - val_loss: 0.2999\n",
      "Epoch 10/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.4059 - val_loss: 0.2705\n",
      "Epoch 11/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3804 - val_loss: 0.2481\n",
      "Epoch 12/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3698 - val_loss: 0.2284\n",
      "Epoch 13/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3510 - val_loss: 0.2124\n",
      "Epoch 14/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3429 - val_loss: 0.1999\n",
      "Epoch 15/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3360 - val_loss: 0.1895\n",
      "Epoch 16/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3265 - val_loss: 0.1800\n",
      "Epoch 17/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.3220 - val_loss: 0.1708\n",
      "Epoch 18/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3111 - val_loss: 0.1635\n",
      "Epoch 19/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3051 - val_loss: 0.1609\n",
      "Epoch 20/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.3054 - val_loss: 0.1555\n",
      "Epoch 21/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2962 - val_loss: 0.1511\n",
      "Epoch 22/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3025 - val_loss: 0.1471\n",
      "Epoch 23/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2950 - val_loss: 0.1449\n",
      "Epoch 24/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2899 - val_loss: 0.1406\n",
      "Epoch 25/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2867 - val_loss: 0.1382\n",
      "Epoch 26/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2849 - val_loss: 0.1376\n",
      "Epoch 27/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2729 - val_loss: 0.1335\n",
      "Epoch 28/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2750 - val_loss: 0.1317\n",
      "Epoch 29/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2666 - val_loss: 0.1292\n",
      "Epoch 30/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2646 - val_loss: 0.1277\n",
      "Epoch 31/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2704 - val_loss: 0.1259\n",
      "Epoch 32/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2621 - val_loss: 0.1212\n",
      "Epoch 33/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2570 - val_loss: 0.1195\n",
      "Epoch 34/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2612 - val_loss: 0.1192\n",
      "Epoch 35/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2537 - val_loss: 0.1184\n",
      "Epoch 36/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2507 - val_loss: 0.1139\n",
      "Epoch 37/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2454 - val_loss: 0.1122\n",
      "Epoch 38/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2408 - val_loss: 0.1127\n",
      "Epoch 39/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2367 - val_loss: 0.1076\n",
      "Epoch 40/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2362 - val_loss: 0.1077\n",
      "Epoch 41/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2290 - val_loss: 0.1057\n",
      "Epoch 42/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2297 - val_loss: 0.1021\n",
      "Epoch 43/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2251 - val_loss: 0.1027\n",
      "Epoch 44/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2237 - val_loss: 0.0959\n",
      "Epoch 45/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2195 - val_loss: 0.0983\n",
      "Epoch 46/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2167 - val_loss: 0.0923\n",
      "Epoch 47/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2119 - val_loss: 0.0938\n",
      "Epoch 48/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2123 - val_loss: 0.0908\n",
      "Epoch 49/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2034 - val_loss: 0.0880\n",
      "Epoch 50/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2009 - val_loss: 0.0885\n",
      "Epoch 51/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2008 - val_loss: 0.0880\n",
      "Epoch 52/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1954 - val_loss: 0.0834\n",
      "Epoch 53/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1962 - val_loss: 0.0817\n",
      "Epoch 54/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1904 - val_loss: 0.0822\n",
      "Epoch 55/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1876 - val_loss: 0.0806\n",
      "Epoch 56/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1884 - val_loss: 0.0798\n",
      "Epoch 57/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1857 - val_loss: 0.0783\n",
      "Epoch 58/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1848 - val_loss: 0.0799\n",
      "Epoch 59/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1781 - val_loss: 0.0782\n",
      "Epoch 60/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1796 - val_loss: 0.0722\n",
      "Epoch 61/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1787 - val_loss: 0.0738\n",
      "Epoch 62/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1743 - val_loss: 0.0781\n",
      "Epoch 63/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1758 - val_loss: 0.0722\n",
      "Epoch 64/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1714 - val_loss: 0.0718\n",
      "Epoch 65/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1687 - val_loss: 0.0723\n",
      "Epoch 66/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1685 - val_loss: 0.0741\n",
      "Epoch 67/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1672 - val_loss: 0.0732\n",
      "Epoch 68/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1654 - val_loss: 0.0732\n",
      "Epoch 69/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1651 - val_loss: 0.0703\n",
      "Epoch 70/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1609 - val_loss: 0.0671\n",
      "Epoch 71/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1625 - val_loss: 0.0704\n",
      "Epoch 72/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1626 - val_loss: 0.0678\n",
      "Epoch 73/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1576 - val_loss: 0.0707\n",
      "Epoch 74/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1589 - val_loss: 0.0687\n",
      "Epoch 75/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1542 - val_loss: 0.0685\n",
      "Epoch 76/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1573 - val_loss: 0.0687\n",
      "Epoch 77/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1525 - val_loss: 0.0691\n",
      "Epoch 78/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1518 - val_loss: 0.0682\n",
      "Epoch 79/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1540 - val_loss: 0.0675\n",
      "Epoch 80/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1537 - val_loss: 0.0668\n",
      "Epoch 81/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1521 - val_loss: 0.0689\n",
      "Epoch 82/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1492 - val_loss: 0.0674\n",
      "Epoch 83/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1487 - val_loss: 0.0664\n",
      "Epoch 84/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1470 - val_loss: 0.0660\n",
      "Epoch 85/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1459 - val_loss: 0.0659\n",
      "Epoch 86/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1467 - val_loss: 0.0665\n",
      "Epoch 87/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1458 - val_loss: 0.0655\n",
      "Epoch 88/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1431 - val_loss: 0.0666\n",
      "Epoch 89/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1469 - val_loss: 0.0636\n",
      "Epoch 90/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1464 - val_loss: 0.0652\n",
      "Epoch 91/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1425 - val_loss: 0.0640\n",
      "Epoch 92/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1414 - val_loss: 0.0673\n",
      "Epoch 93/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1432 - val_loss: 0.0652\n",
      "Epoch 94/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1417 - val_loss: 0.0646\n",
      "Epoch 95/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1418 - val_loss: 0.0627\n",
      "Epoch 96/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1409 - val_loss: 0.0638\n",
      "Epoch 97/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1387 - val_loss: 0.0636\n",
      "Epoch 98/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1376 - val_loss: 0.0619\n",
      "Epoch 99/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1354 - val_loss: 0.0625\n",
      "Epoch 100/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1366 - val_loss: 0.0633\n",
      "Epoch 101/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1339 - val_loss: 0.0621\n",
      "Epoch 102/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1343 - val_loss: 0.0641\n",
      "Epoch 103/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1338 - val_loss: 0.0628\n",
      "Epoch 104/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1323 - val_loss: 0.0619\n",
      "Epoch 105/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1326 - val_loss: 0.0632\n",
      "Epoch 106/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1316 - val_loss: 0.0620\n",
      "Epoch 107/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1304 - val_loss: 0.0615\n",
      "Epoch 108/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1314 - val_loss: 0.0632\n",
      "Epoch 109/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1297 - val_loss: 0.0633\n",
      "Epoch 110/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1276 - val_loss: 0.0616\n",
      "Epoch 111/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1289 - val_loss: 0.0613\n",
      "Epoch 112/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1282 - val_loss: 0.0614\n",
      "Epoch 113/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1262 - val_loss: 0.0597\n",
      "Epoch 114/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1271 - val_loss: 0.0598\n",
      "Epoch 115/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1247 - val_loss: 0.0613\n",
      "Epoch 116/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1257 - val_loss: 0.0602\n",
      "Epoch 117/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1255 - val_loss: 0.0593\n",
      "Epoch 118/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1232 - val_loss: 0.0615\n",
      "Epoch 119/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1231 - val_loss: 0.0623\n",
      "Epoch 120/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1240 - val_loss: 0.0624\n",
      "Epoch 121/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1230 - val_loss: 0.0608\n",
      "Epoch 122/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1242 - val_loss: 0.0609\n",
      "Epoch 123/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1196 - val_loss: 0.0615\n",
      "Epoch 124/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1199 - val_loss: 0.0595\n",
      "Epoch 125/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1207 - val_loss: 0.0606\n",
      "Epoch 126/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1182 - val_loss: 0.0597\n",
      "Epoch 127/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1188 - val_loss: 0.0609\n",
      "Epoch 128/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1190 - val_loss: 0.0592\n",
      "Epoch 129/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1165 - val_loss: 0.0583\n",
      "Epoch 130/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1161 - val_loss: 0.0616\n",
      "Epoch 131/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1179 - val_loss: 0.0586\n",
      "Epoch 132/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1146 - val_loss: 0.0598\n",
      "Epoch 133/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1144 - val_loss: 0.0585\n",
      "Epoch 134/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1156 - val_loss: 0.0598\n",
      "Epoch 135/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1150 - val_loss: 0.0591\n",
      "Epoch 136/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1151 - val_loss: 0.0599\n",
      "Epoch 137/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1146 - val_loss: 0.0588\n",
      "Epoch 138/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1145 - val_loss: 0.0582\n",
      "Epoch 139/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1130 - val_loss: 0.0598\n",
      "Epoch 140/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1125 - val_loss: 0.0601\n",
      "Epoch 141/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1114 - val_loss: 0.0598\n",
      "Epoch 142/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1098 - val_loss: 0.0581\n",
      "Epoch 143/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1122 - val_loss: 0.0575\n",
      "Epoch 144/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1107 - val_loss: 0.0572\n",
      "Epoch 145/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1111 - val_loss: 0.0577\n",
      "Epoch 146/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1101 - val_loss: 0.0576\n",
      "Epoch 147/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1076 - val_loss: 0.0565\n",
      "Epoch 148/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1082 - val_loss: 0.0569\n",
      "Epoch 149/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1079 - val_loss: 0.0586\n",
      "Epoch 150/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1068 - val_loss: 0.0567\n",
      "Epoch 151/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1075 - val_loss: 0.0569\n",
      "Epoch 152/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1058 - val_loss: 0.0590\n",
      "Epoch 153/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1076 - val_loss: 0.0586\n",
      "Epoch 154/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.0580\n",
      "Epoch 155/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1059 - val_loss: 0.0579\n",
      "Epoch 156/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1037 - val_loss: 0.0567\n",
      "Epoch 157/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1050 - val_loss: 0.0571\n",
      "Epoch 158/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1032 - val_loss: 0.0567\n",
      "Epoch 159/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1043 - val_loss: 0.0564\n",
      "Epoch 160/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1029 - val_loss: 0.0579\n",
      "Epoch 161/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1016 - val_loss: 0.0565\n",
      "Epoch 162/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1022 - val_loss: 0.0555\n",
      "Epoch 163/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0998 - val_loss: 0.0556\n",
      "Epoch 164/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1014 - val_loss: 0.0555\n",
      "Epoch 165/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1013 - val_loss: 0.0574\n",
      "Epoch 166/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0990 - val_loss: 0.0556\n",
      "Epoch 167/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0993 - val_loss: 0.0560\n",
      "Epoch 168/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0986 - val_loss: 0.0549\n",
      "Epoch 169/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1003 - val_loss: 0.0561\n",
      "Epoch 170/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0977 - val_loss: 0.0539\n",
      "Epoch 171/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0986 - val_loss: 0.0551\n",
      "Epoch 172/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0973 - val_loss: 0.0558\n",
      "Epoch 173/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0972 - val_loss: 0.0570\n",
      "Epoch 174/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0957 - val_loss: 0.0556\n",
      "Epoch 175/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0956 - val_loss: 0.0556\n",
      "Epoch 176/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0959 - val_loss: 0.0542\n",
      "Epoch 177/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0962 - val_loss: 0.0554\n",
      "Epoch 178/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0969 - val_loss: 0.0556\n",
      "Epoch 179/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0945 - val_loss: 0.0547\n",
      "Epoch 180/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0941 - val_loss: 0.0557\n",
      "Epoch 181/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0923 - val_loss: 0.0538\n",
      "Epoch 182/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0938 - val_loss: 0.0550\n",
      "Epoch 183/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0935 - val_loss: 0.0537\n",
      "Epoch 184/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0910 - val_loss: 0.0540\n",
      "Epoch 185/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0931 - val_loss: 0.0540\n",
      "Epoch 186/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0920 - val_loss: 0.0533\n",
      "Epoch 187/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0922 - val_loss: 0.0538\n",
      "Epoch 188/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0914 - val_loss: 0.0550\n",
      "Epoch 189/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0910 - val_loss: 0.0539\n",
      "Epoch 190/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0904 - val_loss: 0.0531\n",
      "Epoch 191/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0528\n",
      "Epoch 192/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0896 - val_loss: 0.0546\n",
      "Epoch 193/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0893 - val_loss: 0.0534\n",
      "Epoch 194/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0883 - val_loss: 0.0548\n",
      "Epoch 195/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0877 - val_loss: 0.0531\n",
      "Epoch 196/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0883 - val_loss: 0.0536\n",
      "Epoch 197/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0879 - val_loss: 0.0541\n",
      "Epoch 198/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0873 - val_loss: 0.0541\n",
      "Epoch 199/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0866 - val_loss: 0.0526\n",
      "Epoch 200/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0859 - val_loss: 0.0528\n",
      "Epoch 201/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.0526\n",
      "Epoch 202/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0860 - val_loss: 0.0552\n",
      "Epoch 203/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.0527\n",
      "Epoch 204/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0854 - val_loss: 0.0524\n",
      "Epoch 205/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0848 - val_loss: 0.0524\n",
      "Epoch 206/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0843 - val_loss: 0.0529\n",
      "Epoch 207/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0839 - val_loss: 0.0529\n",
      "Epoch 208/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0837 - val_loss: 0.0512\n",
      "Epoch 209/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0833 - val_loss: 0.0521\n",
      "Epoch 210/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0826 - val_loss: 0.0528\n",
      "Epoch 211/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0840 - val_loss: 0.0520\n",
      "Epoch 212/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0816 - val_loss: 0.0526\n",
      "Epoch 213/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0819 - val_loss: 0.0510\n",
      "Epoch 214/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0818 - val_loss: 0.0513\n",
      "Epoch 215/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0813 - val_loss: 0.0512\n",
      "Epoch 216/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0818 - val_loss: 0.0517\n",
      "Epoch 217/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0805 - val_loss: 0.0513\n",
      "Epoch 218/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.0507\n",
      "Epoch 219/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0796 - val_loss: 0.0528\n",
      "Epoch 220/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0806 - val_loss: 0.0504\n",
      "Epoch 221/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0799 - val_loss: 0.0504\n",
      "Epoch 222/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0802 - val_loss: 0.0514\n",
      "Epoch 223/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.0521\n",
      "Epoch 224/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.0517\n",
      "Epoch 225/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0786 - val_loss: 0.0504\n",
      "Epoch 226/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0779 - val_loss: 0.0508\n",
      "Epoch 227/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0789 - val_loss: 0.0509\n",
      "Epoch 228/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0784 - val_loss: 0.0503\n",
      "Epoch 229/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0773 - val_loss: 0.0512\n",
      "Epoch 230/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0768 - val_loss: 0.0503\n",
      "Epoch 231/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0767 - val_loss: 0.0497\n",
      "Epoch 232/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0771 - val_loss: 0.0515\n",
      "Epoch 233/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0771 - val_loss: 0.0517\n",
      "Epoch 234/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0761 - val_loss: 0.0512\n",
      "Epoch 235/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0765 - val_loss: 0.0498\n",
      "Epoch 236/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0764 - val_loss: 0.0502\n",
      "Epoch 237/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0755 - val_loss: 0.0502\n",
      "Epoch 238/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0750 - val_loss: 0.0505\n",
      "Epoch 239/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0750 - val_loss: 0.0502\n",
      "Epoch 240/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0752 - val_loss: 0.0508\n",
      "Epoch 241/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0738 - val_loss: 0.0507\n",
      "Epoch 242/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0749 - val_loss: 0.0500\n",
      "Epoch 243/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0742 - val_loss: 0.0501\n",
      "Epoch 244/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0741 - val_loss: 0.0506\n",
      "Epoch 245/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0740 - val_loss: 0.0498\n",
      "Epoch 246/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0739 - val_loss: 0.0501\n",
      "Epoch 247/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0733 - val_loss: 0.0508\n",
      "Epoch 248/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0513\n",
      "Epoch 249/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0737 - val_loss: 0.0510\n",
      "Epoch 250/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0505\n",
      "Epoch 251/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0734 - val_loss: 0.0499\n",
      "Epoch 252/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0517\n",
      "Epoch 253/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0499\n",
      "Epoch 254/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0493\n",
      "Epoch 255/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0503\n",
      "Epoch 256/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0491\n",
      "Epoch 257/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0498\n",
      "Epoch 258/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0497\n",
      "Epoch 259/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0712 - val_loss: 0.0497\n",
      "Epoch 260/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0495\n",
      "Epoch 261/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0498\n",
      "Epoch 262/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0509\n",
      "Epoch 263/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0713 - val_loss: 0.0498\n",
      "Epoch 264/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0504\n",
      "Epoch 265/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0510\n",
      "Epoch 266/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0493\n",
      "Epoch 267/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0705 - val_loss: 0.0502\n",
      "Epoch 268/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0495\n",
      "Epoch 269/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0497\n",
      "Epoch 270/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0500\n",
      "Epoch 271/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0507\n",
      "Epoch 272/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0498\n",
      "Epoch 273/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0494\n",
      "Epoch 274/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0495\n",
      "Epoch 275/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0498\n",
      "Epoch 276/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0499\n",
      "Epoch 277/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0679 - val_loss: 0.0503\n",
      "Epoch 278/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0495\n",
      "Epoch 279/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0493\n",
      "Epoch 280/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0495\n",
      "Epoch 281/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0495\n",
      "Epoch 282/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0496\n",
      "Epoch 283/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0500\n",
      "Epoch 284/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0499\n",
      "Epoch 285/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0502\n",
      "Epoch 286/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0503\n",
      "Epoch 287/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0500\n",
      "Epoch 288/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0503\n",
      "Epoch 289/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0500\n",
      "Epoch 290/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0506\n",
      "Epoch 291/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0511\n",
      "Epoch 292/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0494\n",
      "Epoch 293/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0508\n",
      "Epoch 294/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0668 - val_loss: 0.0502\n",
      "Epoch 295/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0512\n",
      "Epoch 296/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0496\n",
      "Epoch 297/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0498\n",
      "Epoch 298/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0501\n",
      "Epoch 299/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0507\n",
      "Epoch 300/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0502\n",
      "Epoch 301/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0498\n",
      "Epoch 302/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0497\n",
      "Epoch 303/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0500\n",
      "Epoch 304/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0677 - val_loss: 0.0506\n",
      "Epoch 305/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0677 - val_loss: 0.0497\n",
      "Epoch 306/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.0506\n",
      "Epoch 307/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.0509\n",
      "Epoch 308/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0502\n",
      "Epoch 309/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0497\n",
      "Epoch 310/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.0503\n",
      "Epoch 311/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0505\n",
      "Epoch 312/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0520\n",
      "Epoch 313/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0671 - val_loss: 0.0499\n",
      "Epoch 314/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0674 - val_loss: 0.0497\n",
      "Epoch 315/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0501\n",
      "Epoch 316/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0502\n",
      "Epoch 317/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0500\n",
      "Epoch 318/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0672 - val_loss: 0.0499\n",
      "Epoch 319/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0667 - val_loss: 0.0498\n",
      "Epoch 320/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.0504\n",
      "Epoch 321/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0674 - val_loss: 0.0503\n",
      "Epoch 322/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0523\n",
      "Epoch 323/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0507\n",
      "Epoch 324/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0503\n",
      "Epoch 325/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0668 - val_loss: 0.0512\n",
      "Epoch 326/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0669 - val_loss: 0.0497\n",
      "Epoch 327/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.0499\n",
      "Epoch 328/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0673 - val_loss: 0.0511\n",
      "Epoch 329/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0507\n",
      "Epoch 330/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0665 - val_loss: 0.0507\n",
      "Epoch 331/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0671 - val_loss: 0.0503\n",
      "Epoch 332/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0669 - val_loss: 0.0507\n",
      "Epoch 333/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0675 - val_loss: 0.0502\n",
      "Epoch 334/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0668 - val_loss: 0.0506\n",
      "Epoch 335/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0499\n",
      "Epoch 336/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0673 - val_loss: 0.0505\n",
      "Epoch 337/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0667 - val_loss: 0.0503\n",
      "Epoch 338/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0677 - val_loss: 0.0514\n",
      "Epoch 339/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0673 - val_loss: 0.0506\n",
      "Epoch 340/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0666 - val_loss: 0.0498\n",
      "Epoch 341/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0670 - val_loss: 0.0497\n",
      "Epoch 342/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0669 - val_loss: 0.0498\n",
      "Epoch 343/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0666 - val_loss: 0.0502\n",
      "Epoch 344/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0671 - val_loss: 0.0497\n",
      "Epoch 345/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0666 - val_loss: 0.0503\n",
      "Epoch 346/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0669 - val_loss: 0.0501\n",
      "Epoch 347/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0498\n",
      "Epoch 348/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0676 - val_loss: 0.0508\n",
      "Epoch 349/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0671 - val_loss: 0.0496\n",
      "Epoch 350/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0668 - val_loss: 0.0501\n",
      "Epoch 351/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0659 - val_loss: 0.0501\n",
      "Epoch 352/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0670 - val_loss: 0.0500\n",
      "Epoch 353/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0665 - val_loss: 0.0508\n",
      "Epoch 354/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0667 - val_loss: 0.0497\n",
      "Epoch 355/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.0501\n",
      "Epoch 356/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0495\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_25_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 25,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (53671, 6, 1)\n",
      "y_train.shape:  (53671, 1)\n",
      "x_valid.shape:  (13415, 6, 1)\n",
      "y_valid.shape:  (13415, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-30 21:52:33,473 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "53/53 [==============================] - 2s 18ms/step - loss: 2.7198 - val_loss: 2.8309\n",
      "Epoch 2/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 2.6140 - val_loss: 2.7051\n",
      "Epoch 3/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 2.4756 - val_loss: 2.5316\n",
      "Epoch 4/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 2.2872 - val_loss: 2.2990\n",
      "Epoch 5/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 2.0635 - val_loss: 2.0498\n",
      "Epoch 6/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.8708 - val_loss: 1.8642\n",
      "Epoch 7/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.7565 - val_loss: 1.7636\n",
      "Epoch 8/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.7095 - val_loss: 1.7170\n",
      "Epoch 9/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.6927 - val_loss: 1.6947\n",
      "Epoch 10/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.6809 - val_loss: 1.6827\n",
      "Epoch 11/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.6784 - val_loss: 1.6750\n",
      "Epoch 12/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 1.6715 - val_loss: 1.6691\n",
      "Epoch 13/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.6660 - val_loss: 1.6642\n",
      "Epoch 14/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 1.6593 - val_loss: 1.6594\n",
      "Epoch 15/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 1.6555 - val_loss: 1.6547\n",
      "Epoch 16/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.6509 - val_loss: 1.6502\n",
      "Epoch 17/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.6447 - val_loss: 1.6457\n",
      "Epoch 18/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.6402 - val_loss: 1.6417\n",
      "Epoch 19/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.6359 - val_loss: 1.6380\n",
      "Epoch 20/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.6358 - val_loss: 1.6342\n",
      "Epoch 21/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.6284 - val_loss: 1.6285\n",
      "Epoch 22/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.6120 - val_loss: 1.5691\n",
      "Epoch 23/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.5157 - val_loss: 1.4477\n",
      "Epoch 24/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.4271 - val_loss: 1.3636\n",
      "Epoch 25/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.3581 - val_loss: 1.2976\n",
      "Epoch 26/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.3072 - val_loss: 1.2414\n",
      "Epoch 27/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.2561 - val_loss: 1.1911\n",
      "Epoch 28/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.2175 - val_loss: 1.1456\n",
      "Epoch 29/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.1790 - val_loss: 1.1036\n",
      "Epoch 30/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.1389 - val_loss: 1.0643\n",
      "Epoch 31/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.1063 - val_loss: 1.0261\n",
      "Epoch 32/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.0759 - val_loss: 0.9907\n",
      "Epoch 33/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.0404 - val_loss: 0.9560\n",
      "Epoch 34/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.0124 - val_loss: 0.9230\n",
      "Epoch 35/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.9841 - val_loss: 0.8906\n",
      "Epoch 36/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.9559 - val_loss: 0.8592\n",
      "Epoch 37/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.9287 - val_loss: 0.8285\n",
      "Epoch 38/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8993 - val_loss: 0.7979\n",
      "Epoch 39/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8755 - val_loss: 0.7684\n",
      "Epoch 40/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8482 - val_loss: 0.7396\n",
      "Epoch 41/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8214 - val_loss: 0.7103\n",
      "Epoch 42/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.7959 - val_loss: 0.6824\n",
      "Epoch 43/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7740 - val_loss: 0.6550\n",
      "Epoch 44/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7500 - val_loss: 0.6280\n",
      "Epoch 45/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6651 - val_loss: 0.3917\n",
      "Epoch 46/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5280 - val_loss: 0.3281\n",
      "Epoch 47/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.5038 - val_loss: 0.3000\n",
      "Epoch 48/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4777 - val_loss: 0.2754\n",
      "Epoch 49/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4624 - val_loss: 0.2553\n",
      "Epoch 50/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4409 - val_loss: 0.2393\n",
      "Epoch 51/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4279 - val_loss: 0.2234\n",
      "Epoch 52/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4269 - val_loss: 0.2128\n",
      "Epoch 53/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4099 - val_loss: 0.1982\n",
      "Epoch 54/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3946 - val_loss: 0.1904\n",
      "Epoch 55/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3839 - val_loss: 0.1831\n",
      "Epoch 56/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3824 - val_loss: 0.1740\n",
      "Epoch 57/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3764 - val_loss: 0.1672\n",
      "Epoch 58/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3725 - val_loss: 0.1637\n",
      "Epoch 59/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3587 - val_loss: 0.1532\n",
      "Epoch 60/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3576 - val_loss: 0.1504\n",
      "Epoch 61/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3474 - val_loss: 0.1476\n",
      "Epoch 62/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3380 - val_loss: 0.1391\n",
      "Epoch 63/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3464 - val_loss: 0.1350\n",
      "Epoch 64/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3262 - val_loss: 0.1344\n",
      "Epoch 65/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3277 - val_loss: 0.1247\n",
      "Epoch 66/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3230 - val_loss: 0.1269\n",
      "Epoch 67/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3176 - val_loss: 0.1225\n",
      "Epoch 68/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3075 - val_loss: 0.1214\n",
      "Epoch 69/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3059 - val_loss: 0.1171\n",
      "Epoch 70/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3024 - val_loss: 0.1154\n",
      "Epoch 71/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.3010 - val_loss: 0.1129\n",
      "Epoch 72/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3004 - val_loss: 0.1073\n",
      "Epoch 73/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2933 - val_loss: 0.1094\n",
      "Epoch 74/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2879 - val_loss: 0.1077\n",
      "Epoch 75/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2869 - val_loss: 0.1052\n",
      "Epoch 76/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2908 - val_loss: 0.1018\n",
      "Epoch 77/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2817 - val_loss: 0.0991\n",
      "Epoch 78/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2832 - val_loss: 0.1010\n",
      "Epoch 79/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2755 - val_loss: 0.0978\n",
      "Epoch 80/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2667 - val_loss: 0.1116\n",
      "Epoch 81/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1991 - val_loss: 0.0883\n",
      "Epoch 82/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1860 - val_loss: 0.0812\n",
      "Epoch 83/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1768 - val_loss: 0.0786\n",
      "Epoch 84/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1757 - val_loss: 0.0761\n",
      "Epoch 85/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1743 - val_loss: 0.0762\n",
      "Epoch 86/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1712 - val_loss: 0.0737\n",
      "Epoch 87/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1685 - val_loss: 0.0701\n",
      "Epoch 88/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1650 - val_loss: 0.0707\n",
      "Epoch 89/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1664 - val_loss: 0.0699\n",
      "Epoch 90/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1639 - val_loss: 0.0728\n",
      "Epoch 91/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1640 - val_loss: 0.0722\n",
      "Epoch 92/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1629 - val_loss: 0.0670\n",
      "Epoch 93/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1623 - val_loss: 0.0697\n",
      "Epoch 94/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1613 - val_loss: 0.0672\n",
      "Epoch 95/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1586 - val_loss: 0.0666\n",
      "Epoch 96/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1594 - val_loss: 0.0675\n",
      "Epoch 97/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1615 - val_loss: 0.0640\n",
      "Epoch 98/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1586 - val_loss: 0.0637\n",
      "Epoch 99/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1586 - val_loss: 0.0635\n",
      "Epoch 100/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1537 - val_loss: 0.0658\n",
      "Epoch 101/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1432 - val_loss: 0.0692\n",
      "Epoch 102/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1352 - val_loss: 0.0681\n",
      "Epoch 103/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1327 - val_loss: 0.0658\n",
      "Epoch 104/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1313 - val_loss: 0.0658\n",
      "Epoch 105/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1281 - val_loss: 0.0640\n",
      "Epoch 106/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1277 - val_loss: 0.0637\n",
      "Epoch 107/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1270 - val_loss: 0.0645\n",
      "Epoch 108/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1246 - val_loss: 0.0643\n",
      "Epoch 109/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1249 - val_loss: 0.0642\n",
      "Epoch 110/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1258 - val_loss: 0.0641\n",
      "Epoch 111/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1255 - val_loss: 0.0624\n",
      "Epoch 112/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1224 - val_loss: 0.0642\n",
      "Epoch 113/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1230 - val_loss: 0.0623\n",
      "Epoch 114/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1218 - val_loss: 0.0614\n",
      "Epoch 115/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1214 - val_loss: 0.0616\n",
      "Epoch 116/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1219 - val_loss: 0.0613\n",
      "Epoch 117/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1205 - val_loss: 0.0619\n",
      "Epoch 118/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1197 - val_loss: 0.0626\n",
      "Epoch 119/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1199 - val_loss: 0.0613\n",
      "Epoch 120/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1187 - val_loss: 0.0607\n",
      "Epoch 121/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1200 - val_loss: 0.0615\n",
      "Epoch 122/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1175 - val_loss: 0.0599\n",
      "Epoch 123/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1184 - val_loss: 0.0616\n",
      "Epoch 124/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1180 - val_loss: 0.0612\n",
      "Epoch 125/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1174 - val_loss: 0.0611\n",
      "Epoch 126/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1173 - val_loss: 0.0609\n",
      "Epoch 127/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1163 - val_loss: 0.0614\n",
      "Epoch 128/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1143 - val_loss: 0.0614\n",
      "Epoch 129/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1163 - val_loss: 0.0610\n",
      "Epoch 130/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1152 - val_loss: 0.0618\n",
      "Epoch 131/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1136 - val_loss: 0.0613\n",
      "Epoch 132/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1147 - val_loss: 0.0596\n",
      "Epoch 133/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1155 - val_loss: 0.0589\n",
      "Epoch 134/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1138 - val_loss: 0.0586\n",
      "Epoch 135/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1124 - val_loss: 0.0584\n",
      "Epoch 136/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1140 - val_loss: 0.0592\n",
      "Epoch 137/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1119 - val_loss: 0.0582\n",
      "Epoch 138/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1119 - val_loss: 0.0599\n",
      "Epoch 139/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1109 - val_loss: 0.0591\n",
      "Epoch 140/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1107 - val_loss: 0.0586\n",
      "Epoch 141/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1108 - val_loss: 0.0595\n",
      "Epoch 142/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1115 - val_loss: 0.0572\n",
      "Epoch 143/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1109 - val_loss: 0.0592\n",
      "Epoch 144/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1105 - val_loss: 0.0579\n",
      "Epoch 145/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1077 - val_loss: 0.0591\n",
      "Epoch 146/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1087 - val_loss: 0.0566\n",
      "Epoch 147/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1095 - val_loss: 0.0574\n",
      "Epoch 148/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1072 - val_loss: 0.0567\n",
      "Epoch 149/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1062 - val_loss: 0.0588\n",
      "Epoch 150/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1083 - val_loss: 0.0586\n",
      "Epoch 151/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1074 - val_loss: 0.0583\n",
      "Epoch 152/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1057 - val_loss: 0.0564\n",
      "Epoch 153/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1054 - val_loss: 0.0573\n",
      "Epoch 154/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1068 - val_loss: 0.0570\n",
      "Epoch 155/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1067 - val_loss: 0.0580\n",
      "Epoch 156/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1056 - val_loss: 0.0581\n",
      "Epoch 157/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1045 - val_loss: 0.0576\n",
      "Epoch 158/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1051 - val_loss: 0.0561\n",
      "Epoch 159/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1048 - val_loss: 0.0594\n",
      "Epoch 160/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1027 - val_loss: 0.0566\n",
      "Epoch 161/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1041 - val_loss: 0.0580\n",
      "Epoch 162/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1020 - val_loss: 0.0570\n",
      "Epoch 163/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1016 - val_loss: 0.0570\n",
      "Epoch 164/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1023 - val_loss: 0.0578\n",
      "Epoch 165/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1016 - val_loss: 0.0553\n",
      "Epoch 166/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1001 - val_loss: 0.0559\n",
      "Epoch 167/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1010 - val_loss: 0.0562\n",
      "Epoch 168/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1005 - val_loss: 0.0574\n",
      "Epoch 169/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0990 - val_loss: 0.0563\n",
      "Epoch 170/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0984 - val_loss: 0.0569\n",
      "Epoch 171/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0986 - val_loss: 0.0570\n",
      "Epoch 172/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0993 - val_loss: 0.0562\n",
      "Epoch 173/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0986 - val_loss: 0.0548\n",
      "Epoch 174/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0985 - val_loss: 0.0556\n",
      "Epoch 175/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0985 - val_loss: 0.0550\n",
      "Epoch 176/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0971 - val_loss: 0.0548\n",
      "Epoch 177/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0964 - val_loss: 0.0562\n",
      "Epoch 178/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0969 - val_loss: 0.0550\n",
      "Epoch 179/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0955 - val_loss: 0.0562\n",
      "Epoch 180/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0957 - val_loss: 0.0544\n",
      "Epoch 181/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0944 - val_loss: 0.0545\n",
      "Epoch 182/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0941 - val_loss: 0.0562\n",
      "Epoch 183/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0961 - val_loss: 0.0549\n",
      "Epoch 184/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0931 - val_loss: 0.0558\n",
      "Epoch 185/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0940 - val_loss: 0.0547\n",
      "Epoch 186/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0932 - val_loss: 0.0551\n",
      "Epoch 187/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0935 - val_loss: 0.0530\n",
      "Epoch 188/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0933 - val_loss: 0.0558\n",
      "Epoch 189/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0920 - val_loss: 0.0537\n",
      "Epoch 190/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0910 - val_loss: 0.0540\n",
      "Epoch 191/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0917 - val_loss: 0.0538\n",
      "Epoch 192/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0917 - val_loss: 0.0541\n",
      "Epoch 193/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0914 - val_loss: 0.0549\n",
      "Epoch 194/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0897 - val_loss: 0.0547\n",
      "Epoch 195/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0902 - val_loss: 0.0536\n",
      "Epoch 196/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0897 - val_loss: 0.0555\n",
      "Epoch 197/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0887 - val_loss: 0.0546\n",
      "Epoch 198/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0884 - val_loss: 0.0534\n",
      "Epoch 199/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0882 - val_loss: 0.0538\n",
      "Epoch 200/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0892 - val_loss: 0.0542\n",
      "Epoch 201/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0860 - val_loss: 0.0540\n",
      "Epoch 202/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0874 - val_loss: 0.0532\n",
      "Epoch 203/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0867 - val_loss: 0.0537\n",
      "Epoch 204/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0882 - val_loss: 0.0534\n",
      "Epoch 205/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0863 - val_loss: 0.0532\n",
      "Epoch 206/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0861 - val_loss: 0.0544\n",
      "Epoch 207/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0855 - val_loss: 0.0533\n",
      "Epoch 208/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0860 - val_loss: 0.0531\n",
      "Epoch 209/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0861 - val_loss: 0.0529\n",
      "Epoch 210/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0847 - val_loss: 0.0545\n",
      "Epoch 211/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0846 - val_loss: 0.0531\n",
      "Epoch 212/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0844 - val_loss: 0.0545\n",
      "Epoch 213/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0845 - val_loss: 0.0527\n",
      "Epoch 214/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0835 - val_loss: 0.0524\n",
      "Epoch 215/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0825 - val_loss: 0.0527\n",
      "Epoch 216/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0838 - val_loss: 0.0519\n",
      "Epoch 217/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0826 - val_loss: 0.0533\n",
      "Epoch 218/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0824 - val_loss: 0.0535\n",
      "Epoch 219/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0821 - val_loss: 0.0523\n",
      "Epoch 220/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0817 - val_loss: 0.0534\n",
      "Epoch 221/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0818 - val_loss: 0.0530\n",
      "Epoch 222/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0812 - val_loss: 0.0522\n",
      "Epoch 223/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0797 - val_loss: 0.0524\n",
      "Epoch 224/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0817 - val_loss: 0.0519\n",
      "Epoch 225/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.0529\n",
      "Epoch 226/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0806 - val_loss: 0.0522\n",
      "Epoch 227/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0794 - val_loss: 0.0520\n",
      "Epoch 228/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0795 - val_loss: 0.0522\n",
      "Epoch 229/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0785 - val_loss: 0.0529\n",
      "Epoch 230/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0789 - val_loss: 0.0512\n",
      "Epoch 231/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0790 - val_loss: 0.0514\n",
      "Epoch 232/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0789 - val_loss: 0.0530\n",
      "Epoch 233/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0779 - val_loss: 0.0542\n",
      "Epoch 234/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0512\n",
      "Epoch 235/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0772 - val_loss: 0.0534\n",
      "Epoch 236/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0775 - val_loss: 0.0513\n",
      "Epoch 237/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0780 - val_loss: 0.0515\n",
      "Epoch 238/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0762 - val_loss: 0.0534\n",
      "Epoch 239/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0773 - val_loss: 0.0522\n",
      "Epoch 240/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0763 - val_loss: 0.0509\n",
      "Epoch 241/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0761 - val_loss: 0.0516\n",
      "Epoch 242/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0758 - val_loss: 0.0516\n",
      "Epoch 243/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0758 - val_loss: 0.0509\n",
      "Epoch 244/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0755 - val_loss: 0.0512\n",
      "Epoch 245/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0757 - val_loss: 0.0509\n",
      "Epoch 246/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0749 - val_loss: 0.0519\n",
      "Epoch 247/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0746 - val_loss: 0.0511\n",
      "Epoch 248/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0738 - val_loss: 0.0509\n",
      "Epoch 249/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0754 - val_loss: 0.0516\n",
      "Epoch 250/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0739 - val_loss: 0.0521\n",
      "Epoch 251/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0736 - val_loss: 0.0508\n",
      "Epoch 252/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0507\n",
      "Epoch 253/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0731 - val_loss: 0.0512\n",
      "Epoch 254/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0730 - val_loss: 0.0515\n",
      "Epoch 255/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0721 - val_loss: 0.0506\n",
      "Epoch 256/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0728 - val_loss: 0.0507\n",
      "Epoch 257/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0508\n",
      "Epoch 258/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0718 - val_loss: 0.0515\n",
      "Epoch 259/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0715 - val_loss: 0.0517\n",
      "Epoch 260/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0727 - val_loss: 0.0518\n",
      "Epoch 261/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0511\n",
      "Epoch 262/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0722 - val_loss: 0.0510\n",
      "Epoch 263/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0707 - val_loss: 0.0511\n",
      "Epoch 264/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0715 - val_loss: 0.0504\n",
      "Epoch 265/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0527\n",
      "Epoch 266/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0512\n",
      "Epoch 267/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0709 - val_loss: 0.0506\n",
      "Epoch 268/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0711 - val_loss: 0.0505\n",
      "Epoch 269/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0709 - val_loss: 0.0511\n",
      "Epoch 270/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0699 - val_loss: 0.0505\n",
      "Epoch 271/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0511\n",
      "Epoch 272/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0698 - val_loss: 0.0509\n",
      "Epoch 273/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0697 - val_loss: 0.0519\n",
      "Epoch 274/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0699 - val_loss: 0.0506\n",
      "Epoch 275/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0698 - val_loss: 0.0508\n",
      "Epoch 276/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0699 - val_loss: 0.0515\n",
      "Epoch 277/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0694 - val_loss: 0.0511\n",
      "Epoch 278/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0508\n",
      "Epoch 279/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0506\n",
      "Epoch 280/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0503\n",
      "Epoch 281/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0519\n",
      "Epoch 282/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0511\n",
      "Epoch 283/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0506\n",
      "Epoch 284/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0503\n",
      "Epoch 285/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0507\n",
      "Epoch 286/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0508\n",
      "Epoch 287/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0507\n",
      "Epoch 288/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0513\n",
      "Epoch 289/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0521\n",
      "Epoch 290/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0515\n",
      "Epoch 291/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0517\n",
      "Epoch 292/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0511\n",
      "Epoch 293/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0683 - val_loss: 0.0509\n",
      "Epoch 294/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0511\n",
      "Epoch 295/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0511\n",
      "Epoch 296/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0677 - val_loss: 0.0525\n",
      "Epoch 297/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0518\n",
      "Epoch 298/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0509\n",
      "Epoch 299/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0680 - val_loss: 0.0511\n",
      "Epoch 300/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0678 - val_loss: 0.0513\n",
      "Epoch 301/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0505\n",
      "Epoch 302/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0521\n",
      "Epoch 303/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0506\n",
      "Epoch 304/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0670 - val_loss: 0.0515\n",
      "Epoch 305/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0510\n",
      "Epoch 306/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0669 - val_loss: 0.0517\n",
      "Epoch 307/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0681 - val_loss: 0.0503\n",
      "Epoch 308/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0511\n",
      "Epoch 309/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.0504\n",
      "Epoch 310/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.0514\n",
      "Epoch 311/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0673 - val_loss: 0.0515\n",
      "Epoch 312/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0505\n",
      "Epoch 313/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0680 - val_loss: 0.0515\n",
      "Epoch 314/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0678 - val_loss: 0.0508\n",
      "Epoch 315/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0676 - val_loss: 0.0503\n",
      "Epoch 316/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0672 - val_loss: 0.0520\n",
      "Epoch 317/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0509\n",
      "Epoch 318/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0672 - val_loss: 0.0503\n",
      "Epoch 319/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0672 - val_loss: 0.0506\n",
      "Epoch 320/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.0502\n",
      "Epoch 321/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0675 - val_loss: 0.0503\n",
      "Epoch 322/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0669 - val_loss: 0.0506\n",
      "Epoch 323/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0673 - val_loss: 0.0505\n",
      "Epoch 324/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0673 - val_loss: 0.0512\n",
      "Epoch 325/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0674 - val_loss: 0.0513\n",
      "Epoch 326/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0668 - val_loss: 0.0509\n",
      "Epoch 327/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0662 - val_loss: 0.0518\n",
      "Epoch 328/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0672 - val_loss: 0.0502\n",
      "Epoch 329/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0670 - val_loss: 0.0517\n",
      "Epoch 330/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0670 - val_loss: 0.0503\n",
      "Epoch 331/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0671 - val_loss: 0.0509\n",
      "Epoch 332/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0668 - val_loss: 0.0515\n",
      "Epoch 333/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0666 - val_loss: 0.0502\n",
      "Epoch 334/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0673 - val_loss: 0.0503\n",
      "Epoch 335/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0673 - val_loss: 0.0510\n",
      "Epoch 336/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0671 - val_loss: 0.0510\n",
      "Epoch 337/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0670 - val_loss: 0.0503\n",
      "Epoch 338/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0666 - val_loss: 0.0500\n",
      "Epoch 339/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0672 - val_loss: 0.0501\n",
      "Epoch 340/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0676 - val_loss: 0.0510\n",
      "Epoch 341/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0666 - val_loss: 0.0509\n",
      "Epoch 342/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0669 - val_loss: 0.0506\n",
      "Epoch 343/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0666 - val_loss: 0.0501\n",
      "Epoch 344/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0667 - val_loss: 0.0512\n",
      "Epoch 345/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0671 - val_loss: 0.0506\n",
      "Epoch 346/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0658 - val_loss: 0.0530\n",
      "Epoch 347/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0667 - val_loss: 0.0502\n",
      "Epoch 348/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0662 - val_loss: 0.0500\n",
      "Epoch 349/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0658 - val_loss: 0.0506\n",
      "Epoch 350/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0648 - val_loss: 0.0503\n",
      "Epoch 351/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0657 - val_loss: 0.0509\n",
      "Epoch 352/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0659 - val_loss: 0.0500\n",
      "Epoch 353/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0653 - val_loss: 0.0500\n",
      "Epoch 354/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0659 - val_loss: 0.0500\n",
      "Epoch 355/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0654 - val_loss: 0.0500\n",
      "Epoch 356/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0651 - val_loss: 0.0498\n",
      "Epoch 357/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0642 - val_loss: 0.0503\n",
      "Epoch 358/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0639 - val_loss: 0.0506\n",
      "Epoch 359/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0652 - val_loss: 0.0501\n",
      "Epoch 360/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0645 - val_loss: 0.0504\n",
      "Epoch 361/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0650 - val_loss: 0.0506\n",
      "Epoch 362/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0646 - val_loss: 0.0507\n",
      "Epoch 363/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0637 - val_loss: 0.0507\n",
      "Epoch 364/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0638 - val_loss: 0.0500\n",
      "Epoch 365/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0640 - val_loss: 0.0501\n",
      "Epoch 366/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0640 - val_loss: 0.0505\n",
      "Epoch 367/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0639 - val_loss: 0.0497\n",
      "Epoch 368/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0640 - val_loss: 0.0497\n",
      "Epoch 369/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0639 - val_loss: 0.0499\n",
      "Epoch 370/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0640 - val_loss: 0.0495\n",
      "Epoch 371/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0635 - val_loss: 0.0495\n",
      "Epoch 372/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0637 - val_loss: 0.0505\n",
      "Epoch 373/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0633 - val_loss: 0.0503\n",
      "Epoch 374/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0635 - val_loss: 0.0501\n",
      "Epoch 375/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0639 - val_loss: 0.0494\n",
      "Epoch 376/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0633 - val_loss: 0.0502\n",
      "Epoch 377/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0632 - val_loss: 0.0499\n",
      "Epoch 378/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0636 - val_loss: 0.0499\n",
      "Epoch 379/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0632 - val_loss: 0.0494\n",
      "Epoch 380/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0631 - val_loss: 0.0498\n",
      "Epoch 381/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0634 - val_loss: 0.0494\n",
      "Epoch 382/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0636 - val_loss: 0.0494\n",
      "Epoch 383/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0496\n",
      "Epoch 384/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0637 - val_loss: 0.0496\n",
      "Epoch 385/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0630 - val_loss: 0.0495\n",
      "Epoch 386/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0634 - val_loss: 0.0495\n",
      "Epoch 387/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0497\n",
      "Epoch 388/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0632 - val_loss: 0.0496\n",
      "Epoch 389/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0629 - val_loss: 0.0495\n",
      "Epoch 390/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0630 - val_loss: 0.0494\n",
      "Epoch 391/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0631 - val_loss: 0.0505\n",
      "Epoch 392/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0491\n",
      "Epoch 393/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0501\n",
      "Epoch 394/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0630 - val_loss: 0.0495\n",
      "Epoch 395/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0628 - val_loss: 0.0501\n",
      "Epoch 396/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.0493\n",
      "Epoch 397/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0493\n",
      "Epoch 398/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0497\n",
      "Epoch 399/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.0502\n",
      "Epoch 400/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0496\n",
      "Epoch 401/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0490\n",
      "Epoch 402/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0492\n",
      "Epoch 403/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0631 - val_loss: 0.0494\n",
      "Epoch 404/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0493\n",
      "Epoch 405/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0493\n",
      "Epoch 406/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0629 - val_loss: 0.0496\n",
      "Epoch 407/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0503\n",
      "Epoch 408/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0491\n",
      "Epoch 409/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0631 - val_loss: 0.0497\n",
      "Epoch 410/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0622 - val_loss: 0.0497\n",
      "Epoch 411/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0491\n",
      "Epoch 412/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0489\n",
      "Epoch 413/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0495\n",
      "Epoch 414/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0491\n",
      "Epoch 415/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0500\n",
      "Epoch 416/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0490\n",
      "Epoch 417/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0498\n",
      "Epoch 418/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0491\n",
      "Epoch 419/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0493\n",
      "Epoch 420/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0489\n",
      "Epoch 421/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0499\n",
      "Epoch 422/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0491\n",
      "Epoch 423/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0496\n",
      "Epoch 424/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0490\n",
      "Epoch 425/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0497\n",
      "Epoch 426/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0497\n",
      "Epoch 427/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0500\n",
      "Epoch 428/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0502\n",
      "Epoch 429/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0490\n",
      "Epoch 430/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0620 - val_loss: 0.0496\n",
      "Epoch 431/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0629 - val_loss: 0.0499\n",
      "Epoch 432/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0489\n",
      "Epoch 433/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0630 - val_loss: 0.0491\n",
      "Epoch 434/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0629 - val_loss: 0.0489\n",
      "Epoch 435/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0489\n",
      "Epoch 436/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0490\n",
      "Epoch 437/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0494\n",
      "Epoch 438/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0617 - val_loss: 0.0492\n",
      "Epoch 439/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0489\n",
      "Epoch 440/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0490\n",
      "Epoch 441/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0493\n",
      "Epoch 442/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0491\n",
      "Epoch 443/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0490\n",
      "Epoch 444/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0501\n",
      "Epoch 445/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0489\n",
      "Epoch 446/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0626 - val_loss: 0.0490\n",
      "Epoch 447/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0501\n",
      "Epoch 448/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0495\n",
      "Epoch 449/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0491\n",
      "Epoch 450/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0492\n",
      "Epoch 451/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0493\n",
      "Epoch 452/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.0489\n",
      "Epoch 453/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0498\n",
      "Epoch 454/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.0489\n",
      "Epoch 455/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0494\n",
      "Epoch 456/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0618 - val_loss: 0.0490\n",
      "Epoch 457/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0488\n",
      "Epoch 458/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0493\n",
      "Epoch 459/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0489\n",
      "Epoch 460/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0493\n",
      "Epoch 461/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0493\n",
      "Epoch 462/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0496\n",
      "Epoch 463/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0496\n",
      "Epoch 464/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0499\n",
      "Epoch 465/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0486\n",
      "Epoch 466/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0492\n",
      "Epoch 467/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0498\n",
      "Epoch 468/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0488\n",
      "Epoch 469/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0486\n",
      "Epoch 470/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0493\n",
      "Epoch 471/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0487\n",
      "Epoch 472/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0492\n",
      "Epoch 473/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0493\n",
      "Epoch 474/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0626 - val_loss: 0.0494\n",
      "Epoch 475/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0488\n",
      "Epoch 476/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0489\n",
      "Epoch 477/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0618 - val_loss: 0.0489\n",
      "Epoch 478/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0488\n",
      "Epoch 479/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0487\n",
      "Epoch 480/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 481/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0488\n",
      "Epoch 482/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0491\n",
      "Epoch 483/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0486\n",
      "Epoch 484/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.0485\n",
      "Epoch 485/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0493\n",
      "Epoch 486/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0491\n",
      "Epoch 487/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0615 - val_loss: 0.0486\n",
      "Epoch 488/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0618 - val_loss: 0.0487\n",
      "Epoch 489/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0487\n",
      "Epoch 490/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0487\n",
      "Epoch 491/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0610 - val_loss: 0.0495\n",
      "Epoch 492/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0489\n",
      "Epoch 493/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0491\n",
      "Epoch 494/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0490\n",
      "Epoch 495/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0488\n",
      "Epoch 496/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0485\n",
      "Epoch 497/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0494\n",
      "Epoch 498/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0486\n",
      "Epoch 499/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0487\n",
      "Epoch 500/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0489\n",
      "Epoch 501/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.0492\n",
      "Epoch 502/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0628 - val_loss: 0.0508\n",
      "Epoch 503/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0618 - val_loss: 0.0494\n",
      "Epoch 504/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0625 - val_loss: 0.0492\n",
      "Epoch 505/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0487\n",
      "Epoch 506/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0488\n",
      "Epoch 507/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0489\n",
      "Epoch 508/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0624 - val_loss: 0.0485\n",
      "Epoch 509/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0489\n",
      "Epoch 510/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0485\n",
      "Epoch 511/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.0492\n",
      "Epoch 512/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0487\n",
      "Epoch 513/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0485\n",
      "Epoch 514/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0485\n",
      "Epoch 515/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0487\n",
      "Epoch 516/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0623 - val_loss: 0.0489\n",
      "Epoch 517/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.0488\n",
      "Epoch 518/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0491\n",
      "Epoch 519/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0493\n",
      "Epoch 520/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.0490\n",
      "Epoch 521/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0490\n",
      "Epoch 522/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0615 - val_loss: 0.0485\n",
      "Epoch 523/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0484\n",
      "Epoch 524/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0623 - val_loss: 0.0490\n",
      "Epoch 525/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0613 - val_loss: 0.0486\n",
      "Epoch 526/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0619 - val_loss: 0.0487\n",
      "Epoch 527/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0617 - val_loss: 0.0488\n",
      "Epoch 528/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0489\n",
      "Epoch 529/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0492\n",
      "Epoch 530/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0487\n",
      "Epoch 531/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0500\n",
      "Epoch 532/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0496\n",
      "Epoch 533/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0499\n",
      "Epoch 534/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0620 - val_loss: 0.0486\n",
      "Epoch 535/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 536/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0486\n",
      "Epoch 537/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0624 - val_loss: 0.0496\n",
      "Epoch 538/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0488\n",
      "Epoch 539/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0496\n",
      "Epoch 540/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 541/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0625 - val_loss: 0.0484\n",
      "Epoch 542/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0496\n",
      "Epoch 543/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0484\n",
      "Epoch 544/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0485\n",
      "Epoch 545/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0486\n",
      "Epoch 546/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0621 - val_loss: 0.0483\n",
      "Epoch 547/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0617 - val_loss: 0.0485\n",
      "Epoch 548/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0489\n",
      "Epoch 549/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0486\n",
      "Epoch 550/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 551/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0487\n",
      "Epoch 552/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0496\n",
      "Epoch 553/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0502\n",
      "Epoch 554/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0484\n",
      "Epoch 555/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0484\n",
      "Epoch 556/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0485\n",
      "Epoch 557/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0613 - val_loss: 0.0489\n",
      "Epoch 558/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0625 - val_loss: 0.0487\n",
      "Epoch 559/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0492\n",
      "Epoch 560/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0483\n",
      "Epoch 561/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0612 - val_loss: 0.0486\n",
      "Epoch 562/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0611 - val_loss: 0.0488\n",
      "Epoch 563/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0612 - val_loss: 0.0483\n",
      "Epoch 564/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0615 - val_loss: 0.0482\n",
      "Epoch 565/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0626 - val_loss: 0.0498\n",
      "Epoch 566/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0623 - val_loss: 0.0487\n",
      "Epoch 567/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 0.0486\n",
      "Epoch 568/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0616 - val_loss: 0.0483\n",
      "Epoch 569/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0615 - val_loss: 0.0484\n",
      "Epoch 570/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0488\n",
      "Epoch 571/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0612 - val_loss: 0.0501\n",
      "Epoch 572/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0497\n",
      "Epoch 573/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0485\n",
      "Epoch 574/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0483\n",
      "Epoch 575/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0614 - val_loss: 0.0483\n",
      "Epoch 576/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0623 - val_loss: 0.0483\n",
      "Epoch 577/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0488\n",
      "Epoch 578/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0612 - val_loss: 0.0482\n",
      "Epoch 579/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0483\n",
      "Epoch 580/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0610 - val_loss: 0.0483\n",
      "Epoch 581/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0612 - val_loss: 0.0483\n",
      "Epoch 582/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0619 - val_loss: 0.0486\n",
      "Epoch 583/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0482\n",
      "Epoch 584/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0484\n",
      "Epoch 585/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0492\n",
      "Epoch 586/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 587/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0621 - val_loss: 0.0483\n",
      "Epoch 588/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0615 - val_loss: 0.0488\n",
      "Epoch 589/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0612 - val_loss: 0.0488\n",
      "Epoch 590/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0482\n",
      "Epoch 591/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0485\n",
      "Epoch 592/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0486\n",
      "Epoch 593/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0485\n",
      "Epoch 594/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0488\n",
      "Epoch 595/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0615 - val_loss: 0.0488\n",
      "Epoch 596/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0485\n",
      "Epoch 597/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0613 - val_loss: 0.0484\n",
      "Epoch 598/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0615 - val_loss: 0.0491\n",
      "Epoch 599/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0498\n",
      "Epoch 600/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0482\n",
      "Epoch 601/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0615 - val_loss: 0.0485\n",
      "Epoch 602/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0489\n",
      "Epoch 603/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0485\n",
      "Epoch 604/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0610 - val_loss: 0.0484\n",
      "Epoch 605/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0485\n",
      "Epoch 606/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0622 - val_loss: 0.0485\n",
      "Epoch 607/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0618 - val_loss: 0.0489\n",
      "Epoch 608/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0485\n",
      "Epoch 609/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0612 - val_loss: 0.0489\n",
      "Epoch 610/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0617 - val_loss: 0.0496\n",
      "Epoch 611/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0621 - val_loss: 0.0485\n",
      "Epoch 612/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0627 - val_loss: 0.0484\n",
      "Epoch 613/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0615 - val_loss: 0.0495\n",
      "Epoch 614/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0481\n",
      "Epoch 615/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0481\n",
      "Epoch 616/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0482\n",
      "Epoch 617/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0620 - val_loss: 0.0492\n",
      "Epoch 618/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0620 - val_loss: 0.0483\n",
      "Epoch 619/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.0485\n",
      "Epoch 620/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0622 - val_loss: 0.0487\n",
      "Epoch 621/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0487\n",
      "Epoch 622/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0490\n",
      "Epoch 623/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0613 - val_loss: 0.0483\n",
      "Epoch 624/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0619 - val_loss: 0.0491\n",
      "Epoch 625/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0484\n",
      "Epoch 626/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0611 - val_loss: 0.0481\n",
      "Epoch 627/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0611 - val_loss: 0.0489\n",
      "Epoch 628/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0611 - val_loss: 0.0485\n",
      "Epoch 629/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0481\n",
      "Epoch 630/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - val_loss: 0.0487\n",
      "Epoch 631/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0493\n",
      "Epoch 632/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - val_loss: 0.0481\n",
      "Epoch 633/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0611 - val_loss: 0.0481\n",
      "Epoch 634/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0606 - val_loss: 0.0490\n",
      "Epoch 635/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0617 - val_loss: 0.0494\n",
      "Epoch 636/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0612 - val_loss: 0.0482\n",
      "Epoch 637/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 0.0488\n",
      "Epoch 638/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 0.0493\n",
      "Epoch 639/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0614 - val_loss: 0.0486\n",
      "Epoch 640/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0608 - val_loss: 0.0484\n",
      "Epoch 641/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0484\n",
      "Epoch 642/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0603 - val_loss: 0.0486\n",
      "Epoch 643/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0601 - val_loss: 0.0484\n",
      "Epoch 644/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0600 - val_loss: 0.0480\n",
      "Epoch 645/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0599 - val_loss: 0.0498\n",
      "Epoch 646/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0598 - val_loss: 0.0489\n",
      "Epoch 647/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0496\n",
      "Epoch 648/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0596 - val_loss: 0.0491\n",
      "Epoch 649/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0479\n",
      "Epoch 650/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0480\n",
      "Epoch 651/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0598 - val_loss: 0.0481\n",
      "Epoch 652/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0594 - val_loss: 0.0482\n",
      "Epoch 653/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 654/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0595 - val_loss: 0.0480\n",
      "Epoch 655/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0495\n",
      "Epoch 656/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0485\n",
      "Epoch 657/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0481\n",
      "Epoch 658/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0489\n",
      "Epoch 659/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0482\n",
      "Epoch 660/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 661/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0482\n",
      "Epoch 662/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0479\n",
      "Epoch 663/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0479\n",
      "Epoch 664/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0479\n",
      "Epoch 665/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0481\n",
      "Epoch 666/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0486\n",
      "Epoch 667/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0596 - val_loss: 0.0484\n",
      "Epoch 668/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0497\n",
      "Epoch 669/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0599 - val_loss: 0.0492\n",
      "Epoch 670/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0478\n",
      "Epoch 671/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0478\n",
      "Epoch 672/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0478\n",
      "Epoch 673/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0595 - val_loss: 0.0480\n",
      "Epoch 674/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0598 - val_loss: 0.0483\n",
      "Epoch 675/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0479\n",
      "Epoch 676/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 677/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0585 - val_loss: 0.0482\n",
      "Epoch 678/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0483\n",
      "Epoch 679/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 680/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0482\n",
      "Epoch 681/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0584 - val_loss: 0.0477\n",
      "Epoch 682/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0480\n",
      "Epoch 683/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0478\n",
      "Epoch 684/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0596 - val_loss: 0.0481\n",
      "Epoch 685/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0481\n",
      "Epoch 686/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 687/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 688/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0477\n",
      "Epoch 689/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 690/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0478\n",
      "Epoch 691/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0477\n",
      "Epoch 692/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0592 - val_loss: 0.0476\n",
      "Epoch 693/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0483\n",
      "Epoch 694/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0596 - val_loss: 0.0479\n",
      "Epoch 695/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0480\n",
      "Epoch 696/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0484\n",
      "Epoch 697/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0477\n",
      "Epoch 698/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0593 - val_loss: 0.0476\n",
      "Epoch 699/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0476\n",
      "Epoch 700/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0591 - val_loss: 0.0481\n",
      "Epoch 701/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0477\n",
      "Epoch 702/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0477\n",
      "Epoch 703/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0485\n",
      "Epoch 704/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0479\n",
      "Epoch 705/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0593 - val_loss: 0.0476\n",
      "Epoch 706/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0477\n",
      "Epoch 707/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0478\n",
      "Epoch 708/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0476\n",
      "Epoch 709/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0476\n",
      "Epoch 710/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0588 - val_loss: 0.0488\n",
      "Epoch 711/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0475\n",
      "Epoch 712/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0590 - val_loss: 0.0485\n",
      "Epoch 713/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0584 - val_loss: 0.0490\n",
      "Epoch 714/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 715/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0476\n",
      "Epoch 716/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 717/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0476\n",
      "Epoch 718/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0477\n",
      "Epoch 719/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0588 - val_loss: 0.0479\n",
      "Epoch 720/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 721/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0591 - val_loss: 0.0477\n",
      "Epoch 722/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0593 - val_loss: 0.0481\n",
      "Epoch 723/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 0.0475\n",
      "Epoch 724/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0477\n",
      "Epoch 725/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0484\n",
      "Epoch 726/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0476\n",
      "Epoch 727/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0480\n",
      "Epoch 728/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0476\n",
      "Epoch 729/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0593 - val_loss: 0.0476\n",
      "Epoch 730/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0477\n",
      "Epoch 731/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0481\n",
      "Epoch 732/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0476\n",
      "Epoch 733/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0480\n",
      "Epoch 734/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0477\n",
      "Epoch 735/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0484\n",
      "Epoch 736/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0477\n",
      "Epoch 737/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0480\n",
      "Epoch 738/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0476\n",
      "Epoch 739/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 0.0483\n",
      "Epoch 740/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 741/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0475\n",
      "Epoch 742/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 743/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0479\n",
      "Epoch 744/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0593 - val_loss: 0.0476\n",
      "Epoch 745/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0583 - val_loss: 0.0493\n",
      "Epoch 746/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0477\n",
      "Epoch 747/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0478\n",
      "Epoch 748/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0477\n",
      "Epoch 749/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0484\n",
      "Epoch 750/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0477\n",
      "Epoch 751/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 752/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0475\n",
      "Epoch 753/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0486\n",
      "Epoch 754/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0476\n",
      "Epoch 755/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0593 - val_loss: 0.0476\n",
      "Epoch 756/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0484\n",
      "Epoch 757/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0492\n",
      "Epoch 758/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0479\n",
      "Epoch 759/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0475\n",
      "Epoch 760/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 761/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0478\n",
      "Epoch 762/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0477\n",
      "Epoch 763/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0480\n",
      "Epoch 764/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 765/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0476\n",
      "Epoch 766/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 767/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0475\n",
      "Epoch 768/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0477\n",
      "Epoch 769/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0483\n",
      "Epoch 770/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0478\n",
      "Epoch 771/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0478\n",
      "Epoch 772/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 773/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 774/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0487\n",
      "Epoch 775/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0587 - val_loss: 0.0489\n",
      "Epoch 776/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0588 - val_loss: 0.0490\n",
      "Epoch 777/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0596 - val_loss: 0.0481\n",
      "Epoch 778/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0476\n",
      "Epoch 779/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0492\n",
      "Epoch 780/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0492\n",
      "Epoch 781/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 782/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0581 - val_loss: 0.0482\n",
      "Epoch 783/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0592 - val_loss: 0.0477\n",
      "Epoch 784/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0580 - val_loss: 0.0475\n",
      "Epoch 785/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 786/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0588 - val_loss: 0.0474\n",
      "Epoch 787/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 788/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0476\n",
      "Epoch 789/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 790/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0478\n",
      "Epoch 791/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0477\n",
      "Epoch 792/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0581 - val_loss: 0.0487\n",
      "Epoch 793/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0590 - val_loss: 0.0478\n",
      "Epoch 794/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0475\n",
      "Epoch 795/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0475\n",
      "Epoch 796/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 797/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 798/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 799/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 800/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0475\n",
      "Epoch 801/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0480\n",
      "Epoch 802/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0474\n",
      "Epoch 803/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 0.0479\n",
      "Epoch 804/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0485\n",
      "Epoch 805/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 806/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0476\n",
      "Epoch 807/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0476\n",
      "Epoch 808/10000\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0584 - val_loss: 0.0480\n",
      "Epoch 809/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 810/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.0476\n",
      "Epoch 811/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 812/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0475\n",
      "Epoch 813/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0580 - val_loss: 0.0476\n",
      "Epoch 814/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 815/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0481\n",
      "Epoch 816/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0477\n",
      "Epoch 817/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0590 - val_loss: 0.0474\n",
      "Epoch 818/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0473\n",
      "Epoch 819/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0484\n",
      "Epoch 820/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0590 - val_loss: 0.0484\n",
      "Epoch 821/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 822/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0590 - val_loss: 0.0473\n",
      "Epoch 823/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 824/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0474\n",
      "Epoch 825/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0586 - val_loss: 0.0473\n",
      "Epoch 826/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0482\n",
      "Epoch 827/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0474\n",
      "Epoch 828/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 829/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0582 - val_loss: 0.0479\n",
      "Epoch 830/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0474\n",
      "Epoch 831/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 832/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0474\n",
      "Epoch 833/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0476\n",
      "Epoch 834/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0484\n",
      "Epoch 835/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 836/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0487\n",
      "Epoch 837/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0473\n",
      "Epoch 838/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0478\n",
      "Epoch 839/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 840/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 841/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0474\n",
      "Epoch 842/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0583 - val_loss: 0.0480\n",
      "Epoch 843/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0591 - val_loss: 0.0478\n",
      "Epoch 844/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0586 - val_loss: 0.0480\n",
      "Epoch 845/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 846/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 847/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0481\n",
      "Epoch 848/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.0484\n",
      "Epoch 849/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.0481\n",
      "Epoch 850/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0479\n",
      "Epoch 851/10000\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.0477\n",
      "Epoch 852/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0591 - val_loss: 0.0480\n",
      "Epoch 853/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 854/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0483\n",
      "Epoch 855/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0473\n",
      "Epoch 856/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0474\n",
      "Epoch 857/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 858/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0478\n",
      "Epoch 859/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 860/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 861/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0473\n",
      "Epoch 862/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0472\n",
      "Epoch 863/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0478\n",
      "Epoch 864/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 865/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0474\n",
      "Epoch 866/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0474\n",
      "Epoch 867/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0477\n",
      "Epoch 868/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 869/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0481\n",
      "Epoch 870/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0483\n",
      "Epoch 871/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0590 - val_loss: 0.0485\n",
      "Epoch 872/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0481\n",
      "Epoch 873/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 0.0475\n",
      "Epoch 874/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 875/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0593 - val_loss: 0.0475\n",
      "Epoch 876/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0586 - val_loss: 0.0473\n",
      "Epoch 877/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0473\n",
      "Epoch 878/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 879/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 880/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0475\n",
      "Epoch 881/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0481\n",
      "Epoch 882/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0484\n",
      "Epoch 883/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 884/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0480\n",
      "Epoch 885/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0480\n",
      "Epoch 886/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0486\n",
      "Epoch 887/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0589 - val_loss: 0.0477\n",
      "Epoch 888/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0477\n",
      "Epoch 889/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 890/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 891/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0477\n",
      "Epoch 892/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 893/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 894/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0586 - val_loss: 0.0478\n",
      "Epoch 895/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 896/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0582 - val_loss: 0.0473\n",
      "Epoch 897/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 898/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0473\n",
      "Epoch 899/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 900/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 901/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 902/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0582 - val_loss: 0.0479\n",
      "Epoch 903/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0480\n",
      "Epoch 904/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0479\n",
      "Epoch 905/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0582 - val_loss: 0.0476\n",
      "Epoch 906/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0479\n",
      "Epoch 907/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0478\n",
      "Epoch 908/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0473\n",
      "Epoch 909/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0477\n",
      "Epoch 910/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 911/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 912/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0477\n",
      "Epoch 913/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0475\n",
      "Epoch 914/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0476\n",
      "Epoch 915/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 916/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0475\n",
      "Epoch 917/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0484\n",
      "Epoch 918/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0478\n",
      "Epoch 919/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0473\n",
      "Epoch 920/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0476\n",
      "Epoch 921/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 922/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0582 - val_loss: 0.0486\n",
      "Epoch 923/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 924/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0477\n",
      "Epoch 925/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0474\n",
      "Epoch 926/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 927/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0477\n",
      "Epoch 928/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 929/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0586 - val_loss: 0.0472\n",
      "Epoch 930/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 931/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0474\n",
      "Epoch 932/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 933/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0471\n",
      "Epoch 934/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0474\n",
      "Epoch 935/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 936/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0490\n",
      "Epoch 937/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0474\n",
      "Epoch 938/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0479\n",
      "Epoch 939/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0475\n",
      "Epoch 940/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0472\n",
      "Epoch 941/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 942/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 943/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 944/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0589 - val_loss: 0.0472\n",
      "Epoch 945/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0474\n",
      "Epoch 946/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0475\n",
      "Epoch 947/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0472\n",
      "Epoch 948/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0481\n",
      "Epoch 949/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 950/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 951/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0473\n",
      "Epoch 952/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 953/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0479\n",
      "Epoch 954/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0474\n",
      "Epoch 955/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 956/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0474\n",
      "Epoch 957/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 958/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0485\n",
      "Epoch 959/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 960/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0473\n",
      "Epoch 961/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0480\n",
      "Epoch 962/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 963/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 964/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 965/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0475\n",
      "Epoch 966/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 967/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0479\n",
      "Epoch 968/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0474\n",
      "Epoch 969/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0476\n",
      "Epoch 970/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 971/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0476\n",
      "Epoch 972/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0482\n",
      "Epoch 973/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 974/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0471\n",
      "Epoch 975/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0471\n",
      "Epoch 976/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0474\n",
      "Epoch 977/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0479\n",
      "Epoch 978/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0471\n",
      "Epoch 979/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 980/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0477\n",
      "Epoch 981/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0472\n",
      "Epoch 982/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0473\n",
      "Epoch 983/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0471\n",
      "Epoch 984/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 985/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 986/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0472\n",
      "Epoch 987/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0473\n",
      "Epoch 988/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0474\n",
      "Epoch 989/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 990/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 991/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 992/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 993/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 994/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0472\n",
      "Epoch 995/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0473\n",
      "Epoch 996/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0480\n",
      "Epoch 997/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0474\n",
      "Epoch 998/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0472\n",
      "Epoch 999/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0474\n",
      "Epoch 1000/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0473\n",
      "Epoch 1001/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0471\n",
      "Epoch 1002/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0471\n",
      "Epoch 1003/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0480\n",
      "Epoch 1004/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 1005/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1006/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0471\n",
      "Epoch 1007/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 1008/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0475\n",
      "Epoch 1009/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0473\n",
      "Epoch 1010/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 1011/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0473\n",
      "Epoch 1012/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0471\n",
      "Epoch 1013/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0472\n",
      "Epoch 1014/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0473\n",
      "Epoch 1015/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0470\n",
      "Epoch 1016/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 1017/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 1018/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0483\n",
      "Epoch 1019/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0476\n",
      "Epoch 1020/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 1021/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0480\n",
      "Epoch 1022/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0474\n",
      "Epoch 1023/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 1024/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0475\n",
      "Epoch 1025/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1026/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0471\n",
      "Epoch 1027/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 1028/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0475\n",
      "Epoch 1029/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1030/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0473\n",
      "Epoch 1031/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0476\n",
      "Epoch 1032/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0470\n",
      "Epoch 1033/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 1034/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0472\n",
      "Epoch 1035/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0473\n",
      "Epoch 1036/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0470\n",
      "Epoch 1037/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0484\n",
      "Epoch 1038/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 1039/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0486\n",
      "Epoch 1040/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0476\n",
      "Epoch 1041/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0482\n",
      "Epoch 1042/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0471\n",
      "Epoch 1043/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0471\n",
      "Epoch 1044/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0487\n",
      "Epoch 1045/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0473\n",
      "Epoch 1046/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0471\n",
      "Epoch 1047/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0471\n",
      "Epoch 1048/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0474\n",
      "Epoch 1049/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0471\n",
      "Epoch 1050/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0470\n",
      "Epoch 1051/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0486\n",
      "Epoch 1052/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0474\n",
      "Epoch 1053/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0475\n",
      "Epoch 1054/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0481\n",
      "Epoch 1055/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0483\n",
      "Epoch 1056/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 1057/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0471\n",
      "Epoch 1058/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 1059/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 1060/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 1061/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0478\n",
      "Epoch 1062/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1063/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0470\n",
      "Epoch 1064/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0470\n",
      "Epoch 1065/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0479\n",
      "Epoch 1066/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0476\n",
      "Epoch 1067/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0472\n",
      "Epoch 1068/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 1069/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0471\n",
      "Epoch 1070/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 1071/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1072/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0477\n",
      "Epoch 1073/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1074/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0471\n",
      "Epoch 1075/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1076/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0470\n",
      "Epoch 1077/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1078/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0472\n",
      "Epoch 1079/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0472\n",
      "Epoch 1080/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 1081/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1082/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0479\n",
      "Epoch 1083/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0485\n",
      "Epoch 1084/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0479\n",
      "Epoch 1085/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0476\n",
      "Epoch 1086/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0483\n",
      "Epoch 1087/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0471\n",
      "Epoch 1088/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0482\n",
      "Epoch 1089/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0471\n",
      "Epoch 1090/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1091/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0475\n",
      "Epoch 1092/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0480\n",
      "Epoch 1093/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0470\n",
      "Epoch 1094/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 1095/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 1096/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0476\n",
      "Epoch 1097/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 1098/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0469\n",
      "Epoch 1099/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1100/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1101/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0473\n",
      "Epoch 1102/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0470\n",
      "Epoch 1103/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0470\n",
      "Epoch 1104/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1105/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1106/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0470\n",
      "Epoch 1107/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0473\n",
      "Epoch 1108/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0472\n",
      "Epoch 1109/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0470\n",
      "Epoch 1110/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0470\n",
      "Epoch 1111/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0480\n",
      "Epoch 1112/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0477\n",
      "Epoch 1113/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0479\n",
      "Epoch 1114/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0471\n",
      "Epoch 1115/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0475\n",
      "Epoch 1116/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 1117/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1118/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0480\n",
      "Epoch 1119/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0475\n",
      "Epoch 1120/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 1121/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1122/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0485\n",
      "Epoch 1123/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0472\n",
      "Epoch 1124/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0490\n",
      "Epoch 1125/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0483\n",
      "Epoch 1126/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0470\n",
      "Epoch 1127/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0470\n",
      "Epoch 1128/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0473\n",
      "Epoch 1129/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0477\n",
      "Epoch 1130/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0471\n",
      "Epoch 1131/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1132/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 1133/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0471\n",
      "Epoch 1134/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0474\n",
      "Epoch 1135/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1136/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0478\n",
      "Epoch 1137/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0485\n",
      "Epoch 1138/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0471\n",
      "Epoch 1139/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0471\n",
      "Epoch 1140/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0576 - val_loss: 0.0474\n",
      "Epoch 1141/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0477\n",
      "Epoch 1142/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0478\n",
      "Epoch 1143/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 1144/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 1145/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.0476\n",
      "Epoch 1146/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1147/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0472\n",
      "Epoch 1148/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 1149/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0588 - val_loss: 0.0483\n",
      "Epoch 1150/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0477\n",
      "Epoch 1151/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 1152/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0471\n",
      "Epoch 1153/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0471\n",
      "Epoch 1154/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0471\n",
      "Epoch 1155/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0470\n",
      "Epoch 1156/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1157/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 1158/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0475\n",
      "Epoch 1159/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0474\n",
      "Epoch 1160/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0470\n",
      "Epoch 1161/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0474\n",
      "Epoch 1162/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0471\n",
      "Epoch 1163/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0470\n",
      "Epoch 1164/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0472\n",
      "Epoch 1165/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1166/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0471\n",
      "Epoch 1167/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0476\n",
      "Epoch 1168/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0471\n",
      "Epoch 1169/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0474\n",
      "Epoch 1170/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0470\n",
      "Epoch 1171/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 1172/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0471\n",
      "Epoch 1173/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0475\n",
      "Epoch 1174/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0485\n",
      "Epoch 1175/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 1176/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 1177/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0576 - val_loss: 0.0470\n",
      "Epoch 1178/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0472\n",
      "Epoch 1179/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0479\n",
      "Epoch 1180/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0471\n",
      "Epoch 1181/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0586 - val_loss: 0.0469\n",
      "Epoch 1182/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0475\n",
      "Epoch 1183/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0470\n",
      "Epoch 1184/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 1185/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1186/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0477\n",
      "Epoch 1187/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0585 - val_loss: 0.0473\n",
      "Epoch 1188/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 1189/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0470\n",
      "Epoch 1190/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0472\n",
      "Epoch 1191/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0473\n",
      "Epoch 1192/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0474\n",
      "Epoch 1193/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0472\n",
      "Epoch 1194/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0576 - val_loss: 0.0477\n",
      "Epoch 1195/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 1196/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0471\n",
      "Epoch 1197/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 1198/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_50_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 50,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (53671, 6, 1)\n",
      "y_train.shape:  (53671, 1)\n",
      "x_valid.shape:  (13415, 6, 1)\n",
      "y_valid.shape:  (13415, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-30 21:59:42,391 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/10000\n",
      "53/53 [==============================] - 3s 18ms/step - loss: 2.8642 - val_loss: 2.9636\n",
      "Epoch 2/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 2.7150 - val_loss: 2.7861\n",
      "Epoch 3/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 2.5246 - val_loss: 2.5509\n",
      "Epoch 4/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 2.2505 - val_loss: 2.1965\n",
      "Epoch 5/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 1.8727 - val_loss: 1.7310\n",
      "Epoch 6/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.4118 - val_loss: 1.2170\n",
      "Epoch 7/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.0128 - val_loss: 0.8368\n",
      "Epoch 8/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.7594 - val_loss: 0.6038\n",
      "Epoch 9/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6173 - val_loss: 0.4609\n",
      "Epoch 10/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.5394 - val_loss: 0.3783\n",
      "Epoch 11/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4959 - val_loss: 0.3313\n",
      "Epoch 12/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4688 - val_loss: 0.3014\n",
      "Epoch 13/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4467 - val_loss: 0.2807\n",
      "Epoch 14/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4347 - val_loss: 0.2646\n",
      "Epoch 15/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.4202 - val_loss: 0.2513\n",
      "Epoch 16/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4107 - val_loss: 0.2390\n",
      "Epoch 17/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3958 - val_loss: 0.2266\n",
      "Epoch 18/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3806 - val_loss: 0.2156\n",
      "Epoch 19/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3678 - val_loss: 0.2051\n",
      "Epoch 20/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3617 - val_loss: 0.1961\n",
      "Epoch 21/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3534 - val_loss: 0.1870\n",
      "Epoch 22/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3422 - val_loss: 0.1789\n",
      "Epoch 23/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3317 - val_loss: 0.1719\n",
      "Epoch 24/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3317 - val_loss: 0.1657\n",
      "Epoch 25/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3239 - val_loss: 0.1606\n",
      "Epoch 26/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3209 - val_loss: 0.1557\n",
      "Epoch 27/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3122 - val_loss: 0.1509\n",
      "Epoch 28/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3041 - val_loss: 0.1469\n",
      "Epoch 29/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3029 - val_loss: 0.1442\n",
      "Epoch 30/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2993 - val_loss: 0.1424\n",
      "Epoch 31/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2898 - val_loss: 0.1380\n",
      "Epoch 32/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2943 - val_loss: 0.1349\n",
      "Epoch 33/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2859 - val_loss: 0.1307\n",
      "Epoch 34/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2823 - val_loss: 0.1254\n",
      "Epoch 35/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2796 - val_loss: 0.1264\n",
      "Epoch 36/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2778 - val_loss: 0.1231\n",
      "Epoch 37/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2726 - val_loss: 0.1201\n",
      "Epoch 38/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2731 - val_loss: 0.1178\n",
      "Epoch 39/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2703 - val_loss: 0.1148\n",
      "Epoch 40/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2657 - val_loss: 0.1117\n",
      "Epoch 41/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2628 - val_loss: 0.1108\n",
      "Epoch 42/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2572 - val_loss: 0.1116\n",
      "Epoch 43/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2581 - val_loss: 0.1093\n",
      "Epoch 44/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2516 - val_loss: 0.1041\n",
      "Epoch 45/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2495 - val_loss: 0.1028\n",
      "Epoch 46/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2450 - val_loss: 0.1019\n",
      "Epoch 47/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2415 - val_loss: 0.0980\n",
      "Epoch 48/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2427 - val_loss: 0.0976\n",
      "Epoch 49/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2365 - val_loss: 0.0961\n",
      "Epoch 50/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2348 - val_loss: 0.0933\n",
      "Epoch 51/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2338 - val_loss: 0.0917\n",
      "Epoch 52/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2307 - val_loss: 0.0932\n",
      "Epoch 53/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2273 - val_loss: 0.0906\n",
      "Epoch 54/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2269 - val_loss: 0.0872\n",
      "Epoch 55/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2228 - val_loss: 0.0893\n",
      "Epoch 56/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2204 - val_loss: 0.0869\n",
      "Epoch 57/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2184 - val_loss: 0.0852\n",
      "Epoch 58/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2151 - val_loss: 0.0817\n",
      "Epoch 59/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2132 - val_loss: 0.0825\n",
      "Epoch 60/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2110 - val_loss: 0.0807\n",
      "Epoch 61/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2078 - val_loss: 0.0817\n",
      "Epoch 62/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2089 - val_loss: 0.0788\n",
      "Epoch 63/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2025 - val_loss: 0.0764\n",
      "Epoch 64/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2051 - val_loss: 0.0785\n",
      "Epoch 65/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2018 - val_loss: 0.0780\n",
      "Epoch 66/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2002 - val_loss: 0.0741\n",
      "Epoch 67/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1983 - val_loss: 0.0786\n",
      "Epoch 68/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1971 - val_loss: 0.0741\n",
      "Epoch 69/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1925 - val_loss: 0.0734\n",
      "Epoch 70/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1912 - val_loss: 0.0717\n",
      "Epoch 71/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1924 - val_loss: 0.0716\n",
      "Epoch 72/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1922 - val_loss: 0.0705\n",
      "Epoch 73/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1842 - val_loss: 0.0739\n",
      "Epoch 74/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1880 - val_loss: 0.0717\n",
      "Epoch 75/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1878 - val_loss: 0.0675\n",
      "Epoch 76/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1831 - val_loss: 0.0682\n",
      "Epoch 77/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1838 - val_loss: 0.0685\n",
      "Epoch 78/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1815 - val_loss: 0.0701\n",
      "Epoch 79/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1807 - val_loss: 0.0677\n",
      "Epoch 80/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1794 - val_loss: 0.0707\n",
      "Epoch 81/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1782 - val_loss: 0.0711\n",
      "Epoch 82/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1770 - val_loss: 0.0679\n",
      "Epoch 83/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1746 - val_loss: 0.0713\n",
      "Epoch 84/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1741 - val_loss: 0.0662\n",
      "Epoch 85/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1748 - val_loss: 0.0675\n",
      "Epoch 86/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1707 - val_loss: 0.0690\n",
      "Epoch 87/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1709 - val_loss: 0.0641\n",
      "Epoch 88/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1706 - val_loss: 0.0694\n",
      "Epoch 89/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1691 - val_loss: 0.0648\n",
      "Epoch 90/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1686 - val_loss: 0.0681\n",
      "Epoch 91/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1653 - val_loss: 0.0683\n",
      "Epoch 92/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1661 - val_loss: 0.0666\n",
      "Epoch 93/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1633 - val_loss: 0.0648\n",
      "Epoch 94/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1626 - val_loss: 0.0660\n",
      "Epoch 95/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1618 - val_loss: 0.0627\n",
      "Epoch 96/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1608 - val_loss: 0.0679\n",
      "Epoch 97/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1599 - val_loss: 0.0659\n",
      "Epoch 98/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1577 - val_loss: 0.0626\n",
      "Epoch 99/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1569 - val_loss: 0.0621\n",
      "Epoch 100/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1557 - val_loss: 0.0654\n",
      "Epoch 101/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1573 - val_loss: 0.0622\n",
      "Epoch 102/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1524 - val_loss: 0.0638\n",
      "Epoch 103/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1542 - val_loss: 0.0649\n",
      "Epoch 104/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1543 - val_loss: 0.0625\n",
      "Epoch 105/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1547 - val_loss: 0.0624\n",
      "Epoch 106/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1545 - val_loss: 0.0635\n",
      "Epoch 107/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1486 - val_loss: 0.0621\n",
      "Epoch 108/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1497 - val_loss: 0.0620\n",
      "Epoch 109/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1493 - val_loss: 0.0650\n",
      "Epoch 110/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1480 - val_loss: 0.0628\n",
      "Epoch 111/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1457 - val_loss: 0.0623\n",
      "Epoch 112/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1450 - val_loss: 0.0639\n",
      "Epoch 113/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1453 - val_loss: 0.0623\n",
      "Epoch 114/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1442 - val_loss: 0.0619\n",
      "Epoch 115/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1427 - val_loss: 0.0632\n",
      "Epoch 116/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1402 - val_loss: 0.0606\n",
      "Epoch 117/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1392 - val_loss: 0.0638\n",
      "Epoch 118/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1408 - val_loss: 0.0637\n",
      "Epoch 119/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1403 - val_loss: 0.0631\n",
      "Epoch 120/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1403 - val_loss: 0.0602\n",
      "Epoch 121/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1380 - val_loss: 0.0628\n",
      "Epoch 122/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1389 - val_loss: 0.0610\n",
      "Epoch 123/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1360 - val_loss: 0.0620\n",
      "Epoch 124/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1360 - val_loss: 0.0609\n",
      "Epoch 125/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1358 - val_loss: 0.0650\n",
      "Epoch 126/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1348 - val_loss: 0.0613\n",
      "Epoch 127/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1354 - val_loss: 0.0631\n",
      "Epoch 128/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1330 - val_loss: 0.0610\n",
      "Epoch 129/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1330 - val_loss: 0.0592\n",
      "Epoch 130/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1305 - val_loss: 0.0611\n",
      "Epoch 131/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1324 - val_loss: 0.0588\n",
      "Epoch 132/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1284 - val_loss: 0.0591\n",
      "Epoch 133/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1284 - val_loss: 0.0594\n",
      "Epoch 134/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1282 - val_loss: 0.0610\n",
      "Epoch 135/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1292 - val_loss: 0.0586\n",
      "Epoch 136/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1290 - val_loss: 0.0595\n",
      "Epoch 137/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1260 - val_loss: 0.0613\n",
      "Epoch 138/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1262 - val_loss: 0.0590\n",
      "Epoch 139/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1265 - val_loss: 0.0602\n",
      "Epoch 140/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1254 - val_loss: 0.0596\n",
      "Epoch 141/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1237 - val_loss: 0.0583\n",
      "Epoch 142/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1219 - val_loss: 0.0627\n",
      "Epoch 143/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1225 - val_loss: 0.0576\n",
      "Epoch 144/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1200 - val_loss: 0.0586\n",
      "Epoch 145/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1188 - val_loss: 0.0585\n",
      "Epoch 146/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1197 - val_loss: 0.0614\n",
      "Epoch 147/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1208 - val_loss: 0.0627\n",
      "Epoch 148/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1201 - val_loss: 0.0585\n",
      "Epoch 149/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1170 - val_loss: 0.0585\n",
      "Epoch 150/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1183 - val_loss: 0.0568\n",
      "Epoch 151/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1157 - val_loss: 0.0599\n",
      "Epoch 152/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1157 - val_loss: 0.0579\n",
      "Epoch 153/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1168 - val_loss: 0.0589\n",
      "Epoch 154/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1160 - val_loss: 0.0599\n",
      "Epoch 155/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1159 - val_loss: 0.0578\n",
      "Epoch 156/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1140 - val_loss: 0.0563\n",
      "Epoch 157/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1134 - val_loss: 0.0566\n",
      "Epoch 158/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1136 - val_loss: 0.0602\n",
      "Epoch 159/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1124 - val_loss: 0.0568\n",
      "Epoch 160/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1107 - val_loss: 0.0581\n",
      "Epoch 161/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1123 - val_loss: 0.0574\n",
      "Epoch 162/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1105 - val_loss: 0.0560\n",
      "Epoch 163/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1111 - val_loss: 0.0570\n",
      "Epoch 164/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1093 - val_loss: 0.0561\n",
      "Epoch 165/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1095 - val_loss: 0.0567\n",
      "Epoch 166/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1088 - val_loss: 0.0566\n",
      "Epoch 167/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1078 - val_loss: 0.0574\n",
      "Epoch 168/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1079 - val_loss: 0.0562\n",
      "Epoch 169/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1068 - val_loss: 0.0574\n",
      "Epoch 170/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1070 - val_loss: 0.0565\n",
      "Epoch 171/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1056 - val_loss: 0.0589\n",
      "Epoch 172/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1058 - val_loss: 0.0554\n",
      "Epoch 173/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1051 - val_loss: 0.0569\n",
      "Epoch 174/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1056 - val_loss: 0.0572\n",
      "Epoch 175/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1034 - val_loss: 0.0576\n",
      "Epoch 176/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1028 - val_loss: 0.0562\n",
      "Epoch 177/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1031 - val_loss: 0.0545\n",
      "Epoch 178/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1032 - val_loss: 0.0589\n",
      "Epoch 179/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1011 - val_loss: 0.0564\n",
      "Epoch 180/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1023 - val_loss: 0.0555\n",
      "Epoch 181/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1021 - val_loss: 0.0574\n",
      "Epoch 182/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1018 - val_loss: 0.0559\n",
      "Epoch 183/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1000 - val_loss: 0.0558\n",
      "Epoch 184/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0990 - val_loss: 0.0550\n",
      "Epoch 185/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0991 - val_loss: 0.0558\n",
      "Epoch 186/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0991 - val_loss: 0.0551\n",
      "Epoch 187/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0980 - val_loss: 0.0549\n",
      "Epoch 188/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0977 - val_loss: 0.0560\n",
      "Epoch 189/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0978 - val_loss: 0.0561\n",
      "Epoch 190/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0972 - val_loss: 0.0547\n",
      "Epoch 191/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0967 - val_loss: 0.0564\n",
      "Epoch 192/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0966 - val_loss: 0.0556\n",
      "Epoch 193/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0965 - val_loss: 0.0556\n",
      "Epoch 194/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0958 - val_loss: 0.0548\n",
      "Epoch 195/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0959 - val_loss: 0.0555\n",
      "Epoch 196/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0975 - val_loss: 0.0563\n",
      "Epoch 197/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0948 - val_loss: 0.0543\n",
      "Epoch 198/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0945 - val_loss: 0.0558\n",
      "Epoch 199/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0932 - val_loss: 0.0555\n",
      "Epoch 200/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0915 - val_loss: 0.0537\n",
      "Epoch 201/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0946 - val_loss: 0.0536\n",
      "Epoch 202/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0936 - val_loss: 0.0546\n",
      "Epoch 203/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0911 - val_loss: 0.0562\n",
      "Epoch 204/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0912 - val_loss: 0.0557\n",
      "Epoch 205/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0925 - val_loss: 0.0560\n",
      "Epoch 206/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0899 - val_loss: 0.0541\n",
      "Epoch 207/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0903 - val_loss: 0.0546\n",
      "Epoch 208/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0915 - val_loss: 0.0530\n",
      "Epoch 209/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0907 - val_loss: 0.0558\n",
      "Epoch 210/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0892 - val_loss: 0.0559\n",
      "Epoch 211/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0905 - val_loss: 0.0559\n",
      "Epoch 212/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0898 - val_loss: 0.0549\n",
      "Epoch 213/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0894 - val_loss: 0.0533\n",
      "Epoch 214/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0883 - val_loss: 0.0530\n",
      "Epoch 215/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0888 - val_loss: 0.0541\n",
      "Epoch 216/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0888 - val_loss: 0.0530\n",
      "Epoch 217/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0880 - val_loss: 0.0547\n",
      "Epoch 218/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0863 - val_loss: 0.0523\n",
      "Epoch 219/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0867 - val_loss: 0.0536\n",
      "Epoch 220/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0866 - val_loss: 0.0550\n",
      "Epoch 221/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0860 - val_loss: 0.0537\n",
      "Epoch 222/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0861 - val_loss: 0.0543\n",
      "Epoch 223/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0855 - val_loss: 0.0531\n",
      "Epoch 224/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0861 - val_loss: 0.0523\n",
      "Epoch 225/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0845 - val_loss: 0.0524\n",
      "Epoch 226/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0853 - val_loss: 0.0544\n",
      "Epoch 227/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0848 - val_loss: 0.0527\n",
      "Epoch 228/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0835 - val_loss: 0.0531\n",
      "Epoch 229/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0844 - val_loss: 0.0520\n",
      "Epoch 230/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0849 - val_loss: 0.0523\n",
      "Epoch 231/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0836 - val_loss: 0.0535\n",
      "Epoch 232/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0828 - val_loss: 0.0528\n",
      "Epoch 233/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.0531\n",
      "Epoch 234/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0822 - val_loss: 0.0549\n",
      "Epoch 235/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0822 - val_loss: 0.0537\n",
      "Epoch 236/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0822 - val_loss: 0.0540\n",
      "Epoch 237/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0822 - val_loss: 0.0523\n",
      "Epoch 238/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0809 - val_loss: 0.0524\n",
      "Epoch 239/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0822 - val_loss: 0.0521\n",
      "Epoch 240/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0813 - val_loss: 0.0536\n",
      "Epoch 241/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0804 - val_loss: 0.0520\n",
      "Epoch 242/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0795 - val_loss: 0.0539\n",
      "Epoch 243/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0813 - val_loss: 0.0525\n",
      "Epoch 244/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0795 - val_loss: 0.0522\n",
      "Epoch 245/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0803 - val_loss: 0.0524\n",
      "Epoch 246/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0803 - val_loss: 0.0526\n",
      "Epoch 247/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0804 - val_loss: 0.0532\n",
      "Epoch 248/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0786 - val_loss: 0.0521\n",
      "Epoch 249/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0797 - val_loss: 0.0515\n",
      "Epoch 250/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0786 - val_loss: 0.0522\n",
      "Epoch 251/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0788 - val_loss: 0.0512\n",
      "Epoch 252/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0791 - val_loss: 0.0516\n",
      "Epoch 253/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0789 - val_loss: 0.0528\n",
      "Epoch 254/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0779 - val_loss: 0.0529\n",
      "Epoch 255/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0805 - val_loss: 0.0513\n",
      "Epoch 256/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0786 - val_loss: 0.0526\n",
      "Epoch 257/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0769 - val_loss: 0.0525\n",
      "Epoch 258/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0778 - val_loss: 0.0537\n",
      "Epoch 259/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0766 - val_loss: 0.0528\n",
      "Epoch 260/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0758 - val_loss: 0.0521\n",
      "Epoch 261/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0761 - val_loss: 0.0521\n",
      "Epoch 262/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0774 - val_loss: 0.0518\n",
      "Epoch 263/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0763 - val_loss: 0.0527\n",
      "Epoch 264/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0771 - val_loss: 0.0526\n",
      "Epoch 265/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0771 - val_loss: 0.0520\n",
      "Epoch 266/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0765 - val_loss: 0.0515\n",
      "Epoch 267/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0755 - val_loss: 0.0522\n",
      "Epoch 268/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0757 - val_loss: 0.0523\n",
      "Epoch 269/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0759 - val_loss: 0.0517\n",
      "Epoch 270/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0753 - val_loss: 0.0512\n",
      "Epoch 271/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0745 - val_loss: 0.0523\n",
      "Epoch 272/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0750 - val_loss: 0.0540\n",
      "Epoch 273/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0742 - val_loss: 0.0521\n",
      "Epoch 274/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0741 - val_loss: 0.0517\n",
      "Epoch 275/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0737 - val_loss: 0.0514\n",
      "Epoch 276/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0516\n",
      "Epoch 277/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0748 - val_loss: 0.0516\n",
      "Epoch 278/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0742 - val_loss: 0.0505\n",
      "Epoch 279/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0740 - val_loss: 0.0522\n",
      "Epoch 280/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0731 - val_loss: 0.0531\n",
      "Epoch 281/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0722 - val_loss: 0.0512\n",
      "Epoch 282/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0726 - val_loss: 0.0513\n",
      "Epoch 283/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0728 - val_loss: 0.0513\n",
      "Epoch 284/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0722 - val_loss: 0.0530\n",
      "Epoch 285/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0728 - val_loss: 0.0503\n",
      "Epoch 286/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0723 - val_loss: 0.0510\n",
      "Epoch 287/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0719 - val_loss: 0.0511\n",
      "Epoch 288/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0724 - val_loss: 0.0515\n",
      "Epoch 289/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0504\n",
      "Epoch 290/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0514\n",
      "Epoch 291/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0705 - val_loss: 0.0503\n",
      "Epoch 292/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0705 - val_loss: 0.0514\n",
      "Epoch 293/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0708 - val_loss: 0.0505\n",
      "Epoch 294/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0718 - val_loss: 0.0525\n",
      "Epoch 295/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0696 - val_loss: 0.0513\n",
      "Epoch 296/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0699 - val_loss: 0.0519\n",
      "Epoch 297/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0705 - val_loss: 0.0521\n",
      "Epoch 298/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0691 - val_loss: 0.0502\n",
      "Epoch 299/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0696 - val_loss: 0.0505\n",
      "Epoch 300/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0693 - val_loss: 0.0499\n",
      "Epoch 301/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0693 - val_loss: 0.0507\n",
      "Epoch 302/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0696 - val_loss: 0.0507\n",
      "Epoch 303/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0513\n",
      "Epoch 304/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0501\n",
      "Epoch 305/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0509\n",
      "Epoch 306/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0509\n",
      "Epoch 307/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0692 - val_loss: 0.0514\n",
      "Epoch 308/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0678 - val_loss: 0.0508\n",
      "Epoch 309/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0681 - val_loss: 0.0520\n",
      "Epoch 310/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0672 - val_loss: 0.0501\n",
      "Epoch 311/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0680 - val_loss: 0.0511\n",
      "Epoch 312/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0671 - val_loss: 0.0500\n",
      "Epoch 313/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0683 - val_loss: 0.0504\n",
      "Epoch 314/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0677 - val_loss: 0.0497\n",
      "Epoch 315/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0677 - val_loss: 0.0507\n",
      "Epoch 316/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0674 - val_loss: 0.0503\n",
      "Epoch 317/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0674 - val_loss: 0.0502\n",
      "Epoch 318/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0674 - val_loss: 0.0512\n",
      "Epoch 319/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0663 - val_loss: 0.0506\n",
      "Epoch 320/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0664 - val_loss: 0.0502\n",
      "Epoch 321/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0668 - val_loss: 0.0494\n",
      "Epoch 322/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0670 - val_loss: 0.0509\n",
      "Epoch 323/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0667 - val_loss: 0.0500\n",
      "Epoch 324/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0660 - val_loss: 0.0504\n",
      "Epoch 325/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0671 - val_loss: 0.0493\n",
      "Epoch 326/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0661 - val_loss: 0.0501\n",
      "Epoch 327/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0654 - val_loss: 0.0502\n",
      "Epoch 328/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0655 - val_loss: 0.0505\n",
      "Epoch 329/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0654 - val_loss: 0.0493\n",
      "Epoch 330/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0657 - val_loss: 0.0518\n",
      "Epoch 331/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0657 - val_loss: 0.0494\n",
      "Epoch 332/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0656 - val_loss: 0.0496\n",
      "Epoch 333/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0657 - val_loss: 0.0491\n",
      "Epoch 334/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0654 - val_loss: 0.0502\n",
      "Epoch 335/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0653 - val_loss: 0.0496\n",
      "Epoch 336/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0657 - val_loss: 0.0502\n",
      "Epoch 337/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0648 - val_loss: 0.0490\n",
      "Epoch 338/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0658 - val_loss: 0.0494\n",
      "Epoch 339/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0638 - val_loss: 0.0495\n",
      "Epoch 340/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0653 - val_loss: 0.0490\n",
      "Epoch 341/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0648 - val_loss: 0.0491\n",
      "Epoch 342/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0654 - val_loss: 0.0489\n",
      "Epoch 343/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0648 - val_loss: 0.0490\n",
      "Epoch 344/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0644 - val_loss: 0.0489\n",
      "Epoch 345/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0656 - val_loss: 0.0493\n",
      "Epoch 346/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0643 - val_loss: 0.0504\n",
      "Epoch 347/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0642 - val_loss: 0.0500\n",
      "Epoch 348/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0646 - val_loss: 0.0488\n",
      "Epoch 349/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0640 - val_loss: 0.0495\n",
      "Epoch 350/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0641 - val_loss: 0.0499\n",
      "Epoch 351/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0644 - val_loss: 0.0490\n",
      "Epoch 352/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0639 - val_loss: 0.0490\n",
      "Epoch 353/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0639 - val_loss: 0.0486\n",
      "Epoch 354/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0633 - val_loss: 0.0490\n",
      "Epoch 355/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0631 - val_loss: 0.0488\n",
      "Epoch 356/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0634 - val_loss: 0.0491\n",
      "Epoch 357/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0636 - val_loss: 0.0496\n",
      "Epoch 358/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0637 - val_loss: 0.0487\n",
      "Epoch 359/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0634 - val_loss: 0.0502\n",
      "Epoch 360/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0640 - val_loss: 0.0494\n",
      "Epoch 361/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0630 - val_loss: 0.0487\n",
      "Epoch 362/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0638 - val_loss: 0.0488\n",
      "Epoch 363/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0637 - val_loss: 0.0489\n",
      "Epoch 364/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0635 - val_loss: 0.0485\n",
      "Epoch 365/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0631 - val_loss: 0.0488\n",
      "Epoch 366/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0626 - val_loss: 0.0488\n",
      "Epoch 367/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0618 - val_loss: 0.0500\n",
      "Epoch 368/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0622 - val_loss: 0.0485\n",
      "Epoch 369/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0621 - val_loss: 0.0497\n",
      "Epoch 370/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0625 - val_loss: 0.0488\n",
      "Epoch 371/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0620 - val_loss: 0.0492\n",
      "Epoch 372/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0618 - val_loss: 0.0490\n",
      "Epoch 373/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0612 - val_loss: 0.0490\n",
      "Epoch 374/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0618 - val_loss: 0.0498\n",
      "Epoch 375/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0617 - val_loss: 0.0483\n",
      "Epoch 376/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0613 - val_loss: 0.0495\n",
      "Epoch 377/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0484\n",
      "Epoch 378/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0616 - val_loss: 0.0487\n",
      "Epoch 379/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0619 - val_loss: 0.0486\n",
      "Epoch 380/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0615 - val_loss: 0.0491\n",
      "Epoch 381/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0617 - val_loss: 0.0486\n",
      "Epoch 382/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0484\n",
      "Epoch 383/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0617 - val_loss: 0.0486\n",
      "Epoch 384/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0606 - val_loss: 0.0482\n",
      "Epoch 385/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0611 - val_loss: 0.0483\n",
      "Epoch 386/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0612 - val_loss: 0.0495\n",
      "Epoch 387/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0611 - val_loss: 0.0482\n",
      "Epoch 388/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 0.0484\n",
      "Epoch 389/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0606 - val_loss: 0.0480\n",
      "Epoch 390/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0604 - val_loss: 0.0481\n",
      "Epoch 391/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0597 - val_loss: 0.0490\n",
      "Epoch 392/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0604 - val_loss: 0.0479\n",
      "Epoch 393/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0607 - val_loss: 0.0481\n",
      "Epoch 394/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0607 - val_loss: 0.0479\n",
      "Epoch 395/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0603 - val_loss: 0.0490\n",
      "Epoch 396/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0606 - val_loss: 0.0488\n",
      "Epoch 397/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0599 - val_loss: 0.0491\n",
      "Epoch 398/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0599 - val_loss: 0.0490\n",
      "Epoch 399/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0601 - val_loss: 0.0484\n",
      "Epoch 400/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0601 - val_loss: 0.0482\n",
      "Epoch 401/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0599 - val_loss: 0.0478\n",
      "Epoch 402/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 0.0494\n",
      "Epoch 403/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0600 - val_loss: 0.0479\n",
      "Epoch 404/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0594 - val_loss: 0.0479\n",
      "Epoch 405/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 0.0485\n",
      "Epoch 406/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0606 - val_loss: 0.0477\n",
      "Epoch 407/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0596 - val_loss: 0.0478\n",
      "Epoch 408/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 0.0479\n",
      "Epoch 409/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0594 - val_loss: 0.0486\n",
      "Epoch 410/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0592 - val_loss: 0.0479\n",
      "Epoch 411/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0599 - val_loss: 0.0477\n",
      "Epoch 412/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0597 - val_loss: 0.0493\n",
      "Epoch 413/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 0.0475\n",
      "Epoch 414/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0476\n",
      "Epoch 415/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0592 - val_loss: 0.0476\n",
      "Epoch 416/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0592 - val_loss: 0.0476\n",
      "Epoch 417/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 0.0477\n",
      "Epoch 418/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 0.0476\n",
      "Epoch 419/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0592 - val_loss: 0.0484\n",
      "Epoch 420/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0480\n",
      "Epoch 421/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0474\n",
      "Epoch 422/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0479\n",
      "Epoch 423/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 0.0474\n",
      "Epoch 424/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0592 - val_loss: 0.0476\n",
      "Epoch 425/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0476\n",
      "Epoch 426/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0474\n",
      "Epoch 427/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 0.0473\n",
      "Epoch 428/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0472\n",
      "Epoch 429/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0592 - val_loss: 0.0479\n",
      "Epoch 430/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0589 - val_loss: 0.0473\n",
      "Epoch 431/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.0475\n",
      "Epoch 432/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0590 - val_loss: 0.0472\n",
      "Epoch 433/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0476\n",
      "Epoch 434/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0477\n",
      "Epoch 435/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0478\n",
      "Epoch 436/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0475\n",
      "Epoch 437/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0479\n",
      "Epoch 438/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0475\n",
      "Epoch 439/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0477\n",
      "Epoch 440/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0587 - val_loss: 0.0479\n",
      "Epoch 441/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 442/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 443/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0485\n",
      "Epoch 444/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0474\n",
      "Epoch 445/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0475\n",
      "Epoch 446/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0474\n",
      "Epoch 447/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0483\n",
      "Epoch 448/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0584 - val_loss: 0.0479\n",
      "Epoch 449/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 450/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 451/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0478\n",
      "Epoch 452/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0474\n",
      "Epoch 453/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0471\n",
      "Epoch 454/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0481\n",
      "Epoch 455/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 0.0480\n",
      "Epoch 456/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0471\n",
      "Epoch 457/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0583 - val_loss: 0.0470\n",
      "Epoch 458/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0472\n",
      "Epoch 459/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0469\n",
      "Epoch 460/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 0.0473\n",
      "Epoch 461/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0470\n",
      "Epoch 462/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 0.0469\n",
      "Epoch 463/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 0.0470\n",
      "Epoch 464/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0470\n",
      "Epoch 465/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0478\n",
      "Epoch 466/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0473\n",
      "Epoch 467/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0476\n",
      "Epoch 468/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0472\n",
      "Epoch 469/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0482\n",
      "Epoch 470/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0479\n",
      "Epoch 471/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0469\n",
      "Epoch 472/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0468\n",
      "Epoch 473/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0582 - val_loss: 0.0476\n",
      "Epoch 474/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0469\n",
      "Epoch 475/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0470\n",
      "Epoch 476/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0476\n",
      "Epoch 477/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0472\n",
      "Epoch 478/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0467\n",
      "Epoch 479/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0471\n",
      "Epoch 480/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0473\n",
      "Epoch 481/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0472\n",
      "Epoch 482/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0471\n",
      "Epoch 483/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0472\n",
      "Epoch 484/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0471\n",
      "Epoch 485/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 0.0474\n",
      "Epoch 486/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0467\n",
      "Epoch 487/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0473\n",
      "Epoch 488/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0467\n",
      "Epoch 489/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0467\n",
      "Epoch 490/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0467\n",
      "Epoch 491/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 0.0467\n",
      "Epoch 492/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0576 - val_loss: 0.0471\n",
      "Epoch 493/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0474\n",
      "Epoch 494/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0574 - val_loss: 0.0473\n",
      "Epoch 495/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0473\n",
      "Epoch 496/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0581 - val_loss: 0.0468\n",
      "Epoch 497/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0468\n",
      "Epoch 498/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0472\n",
      "Epoch 499/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0471\n",
      "Epoch 500/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0579 - val_loss: 0.0466\n",
      "Epoch 501/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0468\n",
      "Epoch 502/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0482\n",
      "Epoch 503/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0469\n",
      "Epoch 504/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 505/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0471\n",
      "Epoch 506/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0474\n",
      "Epoch 507/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0469\n",
      "Epoch 508/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 509/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0476\n",
      "Epoch 510/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0484\n",
      "Epoch 511/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0576 - val_loss: 0.0466\n",
      "Epoch 512/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0464\n",
      "Epoch 513/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0464\n",
      "Epoch 514/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0464\n",
      "Epoch 515/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0466\n",
      "Epoch 516/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 517/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0470\n",
      "Epoch 518/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0466\n",
      "Epoch 519/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0466\n",
      "Epoch 520/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0574 - val_loss: 0.0465\n",
      "Epoch 521/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0467\n",
      "Epoch 522/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0464\n",
      "Epoch 523/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0466\n",
      "Epoch 524/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0467\n",
      "Epoch 525/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0464\n",
      "Epoch 526/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0466\n",
      "Epoch 527/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0467\n",
      "Epoch 528/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0464\n",
      "Epoch 529/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0472\n",
      "Epoch 530/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0465\n",
      "Epoch 531/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 0.0465\n",
      "Epoch 532/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0464\n",
      "Epoch 533/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0470\n",
      "Epoch 534/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0477\n",
      "Epoch 535/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 0.0464\n",
      "Epoch 536/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 0.0471\n",
      "Epoch 537/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0465\n",
      "Epoch 538/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0468\n",
      "Epoch 539/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0464\n",
      "Epoch 540/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0463\n",
      "Epoch 541/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0465\n",
      "Epoch 542/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0468\n",
      "Epoch 543/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0465\n",
      "Epoch 544/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0563 - val_loss: 0.0463\n",
      "Epoch 545/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0476\n",
      "Epoch 546/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 547/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0464\n",
      "Epoch 548/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0463\n",
      "Epoch 549/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0467\n",
      "Epoch 550/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 0.0464\n",
      "Epoch 551/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0463\n",
      "Epoch 552/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 553/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 554/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0465\n",
      "Epoch 555/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 556/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0465\n",
      "Epoch 557/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0463\n",
      "Epoch 558/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0464\n",
      "Epoch 559/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0466\n",
      "Epoch 560/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 561/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0471\n",
      "Epoch 562/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0464\n",
      "Epoch 563/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0461\n",
      "Epoch 564/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0465\n",
      "Epoch 565/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 0.0466\n",
      "Epoch 566/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 567/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 0.0465\n",
      "Epoch 568/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0463\n",
      "Epoch 569/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0465\n",
      "Epoch 570/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 0.0469\n",
      "Epoch 571/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0570 - val_loss: 0.0463\n",
      "Epoch 572/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0568 - val_loss: 0.0468\n",
      "Epoch 573/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 0.0462\n",
      "Epoch 574/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 575/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0470\n",
      "Epoch 576/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0465\n",
      "Epoch 577/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 578/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 579/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0462\n",
      "Epoch 580/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0465\n",
      "Epoch 581/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0464\n",
      "Epoch 582/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0462\n",
      "Epoch 583/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0462\n",
      "Epoch 584/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0462\n",
      "Epoch 585/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0572 - val_loss: 0.0462\n",
      "Epoch 586/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0469\n",
      "Epoch 587/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0473\n",
      "Epoch 588/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0470\n",
      "Epoch 589/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0566 - val_loss: 0.0465\n",
      "Epoch 590/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0569 - val_loss: 0.0470\n",
      "Epoch 591/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0463\n",
      "Epoch 592/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0464\n",
      "Epoch 593/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0469\n",
      "Epoch 594/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0565 - val_loss: 0.0467\n",
      "Epoch 595/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 596/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 597/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0467\n",
      "Epoch 598/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0471\n",
      "Epoch 599/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0473\n",
      "Epoch 600/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0465\n",
      "Epoch 601/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 0.0465\n",
      "Epoch 602/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0466\n",
      "Epoch 603/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0561 - val_loss: 0.0460\n",
      "Epoch 604/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 605/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0473\n",
      "Epoch 606/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0466\n",
      "Epoch 607/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0461\n",
      "Epoch 608/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0577 - val_loss: 0.0468\n",
      "Epoch 609/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0562 - val_loss: 0.0461\n",
      "Epoch 610/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0465\n",
      "Epoch 611/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0464\n",
      "Epoch 612/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0575 - val_loss: 0.0463\n",
      "Epoch 613/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 614/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0565 - val_loss: 0.0464\n",
      "Epoch 615/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0463\n",
      "Epoch 616/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 617/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0464\n",
      "Epoch 618/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0568 - val_loss: 0.0462\n",
      "Epoch 619/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0566 - val_loss: 0.0468\n",
      "Epoch 620/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0464\n",
      "Epoch 621/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0568 - val_loss: 0.0471\n",
      "Epoch 622/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0574 - val_loss: 0.0465\n",
      "Epoch 623/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0463\n",
      "Epoch 624/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0467\n",
      "Epoch 625/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 626/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0463\n",
      "Epoch 627/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 628/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0465\n",
      "Epoch 629/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0461\n",
      "Epoch 630/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 631/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0465\n",
      "Epoch 632/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0461\n",
      "Epoch 633/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 0.0464\n",
      "Epoch 634/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0467\n",
      "Epoch 635/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0462\n",
      "Epoch 636/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0461\n",
      "Epoch 637/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0462\n",
      "Epoch 638/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0566 - val_loss: 0.0461\n",
      "Epoch 639/10000\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0562 - val_loss: 0.0464\n",
      "Epoch 640/10000\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0570 - val_loss: 0.0464\n",
      "Epoch 641/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0573 - val_loss: 0.0464\n",
      "Epoch 642/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0462\n",
      "Epoch 643/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 644/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 645/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0566 - val_loss: 0.0462\n",
      "Epoch 646/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0463\n",
      "Epoch 647/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0565 - val_loss: 0.0466\n",
      "Epoch 648/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0473\n",
      "Epoch 649/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0561 - val_loss: 0.0462\n",
      "Epoch 650/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0463\n",
      "Epoch 651/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0462\n",
      "Epoch 652/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0466\n",
      "Epoch 653/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0462\n",
      "Epoch 654/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 0.0474\n",
      "Epoch 655/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 656/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0565 - val_loss: 0.0462\n",
      "Epoch 657/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0465\n",
      "Epoch 658/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0464\n",
      "Epoch 659/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0464\n",
      "Epoch 660/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0462\n",
      "Epoch 661/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 662/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0464\n",
      "Epoch 663/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0468\n",
      "Epoch 664/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0461\n",
      "Epoch 665/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 666/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 667/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0464\n",
      "Epoch 668/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0461\n",
      "Epoch 669/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 670/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0462\n",
      "Epoch 671/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0463\n",
      "Epoch 672/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 0.0461\n",
      "Epoch 673/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 674/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 0.0464\n",
      "Epoch 675/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0462\n",
      "Epoch 676/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 677/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0570 - val_loss: 0.0465\n",
      "Epoch 678/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0563 - val_loss: 0.0465\n",
      "Epoch 679/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0561 - val_loss: 0.0464\n",
      "Epoch 680/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0461\n",
      "Epoch 681/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 682/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 0.0461\n",
      "Epoch 683/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0460\n",
      "Epoch 684/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0563 - val_loss: 0.0462\n",
      "Epoch 685/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0462\n",
      "Epoch 686/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0461\n",
      "Epoch 687/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0562 - val_loss: 0.0462\n",
      "Epoch 688/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 689/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 690/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0568 - val_loss: 0.0473\n",
      "Epoch 691/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0566 - val_loss: 0.0461\n",
      "Epoch 692/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 693/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 0.0461\n",
      "Epoch 694/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0566 - val_loss: 0.0464\n",
      "Epoch 695/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0461\n",
      "Epoch 696/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0563 - val_loss: 0.0462\n",
      "Epoch 697/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0567 - val_loss: 0.0466\n",
      "Epoch 698/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0562 - val_loss: 0.0462\n",
      "Epoch 699/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0556 - val_loss: 0.0466\n",
      "Epoch 700/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 0.0464\n",
      "Epoch 701/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0461\n",
      "Epoch 702/10000\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0564 - val_loss: 0.0465\n",
      "Epoch 703/10000\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0565 - val_loss: 0.0466\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ohio Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_filepath = f\"./experiments/559_all_final_evaluation.yaml\"\n",
    "yaml_filepath = f\"./original_vandoorn_experiments/591_all_final_evaluation.yaml\"\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2705, 6, 1)\n",
      "y_test.shape:  (2705, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "85/85 [==============================] - 1s 3ms/step\n",
      "85/85 [==============================] - 0s 3ms/step\n",
      "patient id:  591\n",
      "RMSE:  21.251781406479367\n",
      "t0 RMSE:  24.428877490643426\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_25_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 25,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2705, 6, 1)\n",
      "y_test.shape:  (2705, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment\\nb_future_steps_6_seed_25_\\model.hdf5\n",
      "85/85 [==============================] - 1s 3ms/step\n",
      "85/85 [==============================] - 0s 3ms/step\n",
      "patient id:  591\n",
      "RMSE:  21.316816503696845\n",
      "t0 RMSE:  24.428877490643426\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\mse_keras.py'},\n",
      "    'model': {   'activation_function': 'relu',\n",
      "                 'nb_lstm_states': 32,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras_vanDoorn.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-4',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\vandoorn_original_experiment\\\\nb_future_steps_6_seed_50_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 100,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 50,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2705, 6, 1)\n",
      "y_test.shape:  (2705, 1)\n",
      "x.shape =  (None, 6, 32)\n",
      "x.shape =  (None, 6, 32)\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\vandoorn_original_experiment\\nb_future_steps_6_seed_50_\\model.hdf5\n",
      "85/85 [==============================] - 1s 3ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "patient id:  591\n",
      "RMSE:  20.80025512434348\n",
      "t0 RMSE:  24.428877490643426\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify DiaTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"t0 RMSE: \", t0_rmse)\n",
    "    print(\"SEG: \", seg)\n",
    "    print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./diatrend_cfg/all_final_experiment_method1.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./diatrend_test_cfg_method1/subject30_evaluate.yaml\"\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify T1DEXI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '85.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '911.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-26 23:17:17,734 DEBUG matplotlib data path: c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2024-08-26 23:17:17,734 DEBUG CONFIGDIR=C:\\Users\\username\\.matplotlib\n",
      "2024-08-26 23:17:17,734 DEBUG interactive is False\n",
      "2024-08-26 23:17:17,734 DEBUG platform is win32\n",
      "2024-08-26 23:17:17,784 DEBUG CACHEDIR=C:\\Users\\username\\.matplotlib\n",
      "2024-08-26 23:17:17,784 DEBUG Using fontManager instance from C:\\Users\\username\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"t0 RMSE: \", t0_rmse)\n",
    "    print(\"SEG: \", seg)\n",
    "    print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./t1dexi_cfg/all_final_experiment_method1.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./t1dexi_test_cfg_method1/subject1617_evaluate.yaml\"\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
