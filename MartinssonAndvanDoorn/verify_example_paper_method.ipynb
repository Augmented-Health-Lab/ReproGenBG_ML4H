{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:10:18,150 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2024-11-27 20:10:18,158 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-11-27 20:10:18,158 DEBUG interactive is False\n",
      "2024-11-27 20:10:18,158 DEBUG platform is win32\n",
      "2024-11-27 20:10:18,219 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-11-27 20:10:18,223 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # Calculate SEG\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # Calculate baseline (t0) metrics\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    t0_mse = np.mean((y_test - t0) ** 2)\n",
    "    t0_mape = np.mean(np.abs((y_test - t0) / y_test)) * 100\n",
    "    \n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_mse))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_mape))\n",
    "\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # Print all metrics\n",
    "    print(\"Model Performance Metrics:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MSE:  {mse:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    # print(f\"SEG:  {seg:.2f}\")\n",
    "    print(\"\\nBaseline (t0) Performance:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"t0 RMSE: {t0_rmse:.2f}\")\n",
    "    print(f\"t0 MSE:  {t0_mse:.2f}\")\n",
    "    print(f\"t0 MAPE: {t0_mape:.2f}%\")\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:10:32,466 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\OhioT1DM\\2018\\train\\all does not exist.\n",
      "Running 3 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2024-11-27 20:10:32,475 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (52735, 12, 1)\n",
      "y_train.shape:  (52735, 1)\n",
      "x_valid.shape:  (13181, 12, 1)\n",
      "y_valid.shape:  (13181, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 20:10:38,263 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-11-27 20:10:38,521 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-11-27 20:10:38,530 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-27 20:10:38,540 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 52735 samples, validate on 13181 samples\n",
      "Epoch 1/10000\n",
      "52735/52735 [==============================] - 2s 29us/sample - loss: 0.6427 - val_loss: 0.1441\n",
      "Epoch 2/10000\n",
      " 1024/52735 [..............................] - ETA: 0s - loss: 0.2362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52735/52735 [==============================] - 1s 15us/sample - loss: 0.1823 - val_loss: 0.0044\n",
      "Epoch 3/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: 0.0997 - val_loss: -0.0265\n",
      "Epoch 4/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: 0.0721 - val_loss: -0.0025\n",
      "Epoch 5/10000\n",
      "49152/52735 [==========================>...] - ETA: 0s - loss: 0.04472024-11-27 20:10:43,279 DEBUG Creating converter from 5 to 3\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: 0.0436 - val_loss: -0.0422\n",
      "Epoch 6/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: 0.0190 - val_loss: -0.0324\n",
      "Epoch 7/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0047 - val_loss: -0.0827\n",
      "Epoch 8/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.0208 - val_loss: -0.0930\n",
      "Epoch 9/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0138 - val_loss: -0.1272\n",
      "Epoch 10/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0301 - val_loss: -0.1497\n",
      "Epoch 11/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0573 - val_loss: -0.1704\n",
      "Epoch 12/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0689 - val_loss: -0.1000\n",
      "Epoch 13/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0704 - val_loss: -0.1646\n",
      "Epoch 14/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0775 - val_loss: -0.1236\n",
      "Epoch 15/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0868 - val_loss: -0.1708\n",
      "Epoch 16/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0964 - val_loss: -0.1863\n",
      "Epoch 17/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1037 - val_loss: -0.1799\n",
      "Epoch 18/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1002 - val_loss: -0.1707\n",
      "Epoch 19/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1142 - val_loss: -0.1907\n",
      "Epoch 20/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1161 - val_loss: -0.1282\n",
      "Epoch 21/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1261 - val_loss: -0.1916\n",
      "Epoch 22/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1321 - val_loss: -0.2036\n",
      "Epoch 23/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1327 - val_loss: -0.1721\n",
      "Epoch 24/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1285 - val_loss: -0.1787\n",
      "Epoch 25/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1408 - val_loss: -0.1855\n",
      "Epoch 26/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1382 - val_loss: -0.1570\n",
      "Epoch 27/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1422 - val_loss: -0.2077\n",
      "Epoch 28/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1471 - val_loss: -0.2077\n",
      "Epoch 29/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1438 - val_loss: -0.1755\n",
      "Epoch 30/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1466 - val_loss: -0.2014\n",
      "Epoch 31/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1604 - val_loss: -0.2098\n",
      "Epoch 32/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1576 - val_loss: -0.1915\n",
      "Epoch 33/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1551 - val_loss: -0.1750\n",
      "Epoch 34/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1657 - val_loss: -0.1816\n",
      "Epoch 35/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1548 - val_loss: -0.2039\n",
      "Epoch 36/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1638 - val_loss: -0.1925\n",
      "Epoch 37/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1642 - val_loss: -0.2085\n",
      "Epoch 38/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1659 - val_loss: -0.2003\n",
      "Epoch 39/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1659 - val_loss: -0.2240\n",
      "Epoch 40/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1722 - val_loss: -0.1939\n",
      "Epoch 41/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1820 - val_loss: -0.2016\n",
      "Epoch 42/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1800 - val_loss: -0.2148\n",
      "Epoch 43/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1845 - val_loss: -0.1830\n",
      "Epoch 44/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1794 - val_loss: -0.2038\n",
      "Epoch 45/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1876 - val_loss: -0.2248\n",
      "Epoch 46/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1898 - val_loss: -0.2334\n",
      "Epoch 47/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1910 - val_loss: -0.2379\n",
      "Epoch 48/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1888 - val_loss: -0.1781\n",
      "Epoch 49/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1927 - val_loss: -0.2310\n",
      "Epoch 50/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1985 - val_loss: -0.2265\n",
      "Epoch 51/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.1955 - val_loss: -0.2214\n",
      "Epoch 52/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1985 - val_loss: -0.2315\n",
      "Epoch 53/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2006 - val_loss: -0.2251\n",
      "Epoch 54/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.1998 - val_loss: -0.1661\n",
      "Epoch 55/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2032 - val_loss: -0.1947\n",
      "Epoch 56/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2036 - val_loss: -0.2454\n",
      "Epoch 57/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2069 - val_loss: -0.2472\n",
      "Epoch 58/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2042 - val_loss: -0.2257\n",
      "Epoch 59/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2101 - val_loss: -0.2251\n",
      "Epoch 60/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2107 - val_loss: -0.2447\n",
      "Epoch 61/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2119 - val_loss: -0.2483\n",
      "Epoch 62/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2132 - val_loss: -0.2109\n",
      "Epoch 63/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2172 - val_loss: -0.2435\n",
      "Epoch 64/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2142 - val_loss: -0.2479\n",
      "Epoch 65/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2198 - val_loss: -0.2399\n",
      "Epoch 66/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2183 - val_loss: -0.2486\n",
      "Epoch 67/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2256 - val_loss: -0.2338\n",
      "Epoch 68/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2236 - val_loss: -0.2365\n",
      "Epoch 69/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2224 - val_loss: -0.2440\n",
      "Epoch 70/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2264 - val_loss: -0.2439\n",
      "Epoch 71/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2284 - val_loss: -0.2520\n",
      "Epoch 72/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2290 - val_loss: -0.2309\n",
      "Epoch 73/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2276 - val_loss: -0.2393\n",
      "Epoch 74/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2296 - val_loss: -0.2460\n",
      "Epoch 75/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2302 - val_loss: -0.2193\n",
      "Epoch 76/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2332 - val_loss: -0.2211\n",
      "Epoch 77/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2366 - val_loss: -0.2459\n",
      "Epoch 78/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2308 - val_loss: -0.2538\n",
      "Epoch 79/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2371 - val_loss: -0.2565\n",
      "Epoch 80/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2403 - val_loss: -0.2497\n",
      "Epoch 81/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2403 - val_loss: -0.2518\n",
      "Epoch 82/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2421 - val_loss: -0.2394\n",
      "Epoch 83/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2423 - val_loss: -0.2586\n",
      "Epoch 84/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2452 - val_loss: -0.2386\n",
      "Epoch 85/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2455 - val_loss: -0.2487\n",
      "Epoch 86/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2464 - val_loss: -0.2618\n",
      "Epoch 87/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2480 - val_loss: -0.1992\n",
      "Epoch 88/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2446 - val_loss: -0.2531\n",
      "Epoch 89/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2527 - val_loss: -0.2562\n",
      "Epoch 90/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2509 - val_loss: -0.2534\n",
      "Epoch 91/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2493 - val_loss: -0.2428\n",
      "Epoch 92/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2528 - val_loss: -0.2664\n",
      "Epoch 93/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2521 - val_loss: -0.2596\n",
      "Epoch 94/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2535 - val_loss: -0.2117\n",
      "Epoch 95/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2518 - val_loss: -0.2600\n",
      "Epoch 96/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2488 - val_loss: -0.2322\n",
      "Epoch 97/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2569 - val_loss: -0.2403\n",
      "Epoch 98/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2545 - val_loss: -0.2543\n",
      "Epoch 99/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2575 - val_loss: -0.2543\n",
      "Epoch 100/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2564 - val_loss: -0.2534\n",
      "Epoch 101/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2587 - val_loss: -0.2593\n",
      "Epoch 102/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2605 - val_loss: -0.2562\n",
      "Epoch 103/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2565 - val_loss: -0.2585\n",
      "Epoch 104/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2648 - val_loss: -0.2538\n",
      "Epoch 105/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2661 - val_loss: -0.2577\n",
      "Epoch 106/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2650 - val_loss: -0.2562\n",
      "Epoch 107/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2645 - val_loss: -0.2665\n",
      "Epoch 108/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2685 - val_loss: -0.2474\n",
      "Epoch 109/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2693 - val_loss: -0.2483\n",
      "Epoch 110/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2705 - val_loss: -0.2613\n",
      "Epoch 111/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2697 - val_loss: -0.2594\n",
      "Epoch 112/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2736 - val_loss: -0.2652\n",
      "Epoch 113/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2692 - val_loss: -0.2400\n",
      "Epoch 114/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2705 - val_loss: -0.2517\n",
      "Epoch 115/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2753 - val_loss: -0.2527\n",
      "Epoch 116/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2720 - val_loss: -0.2518\n",
      "Epoch 117/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2741 - val_loss: -0.2534\n",
      "Epoch 118/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2768 - val_loss: -0.2565\n",
      "Epoch 119/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2775 - val_loss: -0.2541\n",
      "Epoch 120/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2711 - val_loss: -0.2509\n",
      "Epoch 121/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2747 - val_loss: -0.2412\n",
      "Epoch 122/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2772 - val_loss: -0.2577\n",
      "Epoch 123/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2777 - val_loss: -0.2673\n",
      "Epoch 124/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2771 - val_loss: -0.2601\n",
      "Epoch 125/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2765 - val_loss: -0.2568\n",
      "Epoch 126/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2754 - val_loss: -0.2472\n",
      "Epoch 127/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2782 - val_loss: -0.2596\n",
      "Epoch 128/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2830 - val_loss: -0.2640\n",
      "Epoch 129/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2763 - val_loss: -0.2467\n",
      "Epoch 130/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2867 - val_loss: -0.2632\n",
      "Epoch 131/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2824 - val_loss: -0.2548\n",
      "Epoch 132/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2857 - val_loss: -0.2644\n",
      "Epoch 133/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2843 - val_loss: -0.2602\n",
      "Epoch 134/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2832 - val_loss: -0.2613\n",
      "Epoch 135/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2802 - val_loss: -0.2589\n",
      "Epoch 136/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2857 - val_loss: -0.2608\n",
      "Epoch 137/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2836 - val_loss: -0.2568\n",
      "Epoch 138/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2852 - val_loss: -0.2373\n",
      "Epoch 139/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2868 - val_loss: -0.2642\n",
      "Epoch 140/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2870 - val_loss: -0.2644\n",
      "Epoch 141/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2879 - val_loss: -0.2656\n",
      "Epoch 142/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2894 - val_loss: -0.2502\n",
      "Epoch 143/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2874 - val_loss: -0.2666\n",
      "Epoch 144/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2919 - val_loss: -0.2617\n",
      "Epoch 145/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2881 - val_loss: -0.2559\n",
      "Epoch 146/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2930 - val_loss: -0.2580\n",
      "Epoch 147/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2900 - val_loss: -0.2639\n",
      "Epoch 148/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2916 - val_loss: -0.2468\n",
      "Epoch 149/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2870 - val_loss: -0.2395\n",
      "Epoch 150/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2892 - val_loss: -0.2511\n",
      "Epoch 151/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2873 - val_loss: -0.2574\n",
      "Epoch 152/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2978 - val_loss: -0.2524\n",
      "Epoch 153/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2941 - val_loss: -0.2543\n",
      "Epoch 154/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2974 - val_loss: -0.2637\n",
      "Epoch 155/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2993 - val_loss: -0.2565\n",
      "Epoch 156/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2947 - val_loss: -0.2577\n",
      "Epoch 157/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2957 - val_loss: -0.2582\n",
      "Epoch 158/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3006 - val_loss: -0.2555\n",
      "Epoch 159/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2979 - val_loss: -0.2591\n",
      "Epoch 160/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2980 - val_loss: -0.2639\n",
      "Epoch 161/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2994 - val_loss: -0.2532\n",
      "Epoch 162/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2999 - val_loss: -0.2576\n",
      "Epoch 163/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2971 - val_loss: -0.2633\n",
      "Epoch 164/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3020 - val_loss: -0.2630\n",
      "Epoch 165/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2952 - val_loss: -0.2534\n",
      "Epoch 166/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2996 - val_loss: -0.2554\n",
      "Epoch 167/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3026 - val_loss: -0.2553\n",
      "Epoch 168/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3026 - val_loss: -0.2593\n",
      "Epoch 169/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2997 - val_loss: -0.2573\n",
      "Epoch 170/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3037 - val_loss: -0.2580\n",
      "Epoch 171/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3008 - val_loss: -0.2552\n",
      "Epoch 172/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3052 - val_loss: -0.2568\n",
      "Epoch 173/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3022 - val_loss: -0.2647\n",
      "Epoch 174/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3026 - val_loss: -0.2594\n",
      "Epoch 175/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3056 - val_loss: -0.2572\n",
      "Epoch 176/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3044 - val_loss: -0.2544\n",
      "Epoch 177/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3017 - val_loss: -0.2540\n",
      "Epoch 178/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3034 - val_loss: -0.2628\n",
      "Epoch 179/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3060 - val_loss: -0.2632\n",
      "Epoch 180/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3068 - val_loss: -0.2449\n",
      "Epoch 181/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3083 - val_loss: -0.2501\n",
      "Epoch 182/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3060 - val_loss: -0.2361\n",
      "Epoch 183/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3008 - val_loss: -0.2333\n",
      "Epoch 184/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3058 - val_loss: -0.2610\n",
      "Epoch 185/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3089 - val_loss: -0.2593\n",
      "Epoch 186/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3088 - val_loss: -0.2596\n",
      "Epoch 187/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3080 - val_loss: -0.2502\n",
      "Epoch 188/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3082 - val_loss: -0.2518\n",
      "Epoch 189/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3076 - val_loss: -0.2553\n",
      "Epoch 190/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3099 - val_loss: -0.2587\n",
      "Epoch 191/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3091 - val_loss: -0.2583\n",
      "Epoch 192/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3106 - val_loss: -0.2415\n",
      "Epoch 193/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3105 - val_loss: -0.2471\n",
      "Epoch 194/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3097 - val_loss: -0.2636\n",
      "Epoch 195/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3119 - val_loss: -0.2544\n",
      "Epoch 196/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3107 - val_loss: -0.2515\n",
      "Epoch 197/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3121 - val_loss: -0.2529\n",
      "Epoch 198/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3098 - val_loss: -0.2327\n",
      "Epoch 199/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3112 - val_loss: -0.2549\n",
      "Epoch 200/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3141 - val_loss: -0.2560\n",
      "Epoch 201/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3160 - val_loss: -0.2473\n",
      "Epoch 202/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3124 - val_loss: -0.2577\n",
      "Epoch 203/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3126 - val_loss: -0.2484\n",
      "Epoch 204/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3137 - val_loss: -0.2340\n",
      "Epoch 205/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3138 - val_loss: -0.2535\n",
      "Epoch 206/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3174 - val_loss: -0.2418\n",
      "Epoch 207/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3159 - val_loss: -0.2471\n",
      "Epoch 208/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3191 - val_loss: -0.2485\n",
      "Epoch 209/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3172 - val_loss: -0.2413\n",
      "Epoch 210/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3167 - val_loss: -0.2455\n",
      "Epoch 211/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3172 - val_loss: -0.2473\n",
      "Epoch 212/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3180 - val_loss: -0.2554\n",
      "Epoch 213/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3206 - val_loss: -0.2494\n",
      "Epoch 214/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3165 - val_loss: -0.2402\n",
      "Epoch 215/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3223 - val_loss: -0.2536\n",
      "Epoch 216/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3240 - val_loss: -0.2510\n",
      "Epoch 217/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3227 - val_loss: -0.2481\n",
      "Epoch 218/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3174 - val_loss: -0.2419\n",
      "Epoch 219/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3169 - val_loss: -0.2553\n",
      "Epoch 220/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3250 - val_loss: -0.2421\n",
      "Epoch 221/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3247 - val_loss: -0.2330\n",
      "Epoch 222/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3252 - val_loss: -0.2478\n",
      "Epoch 223/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3242 - val_loss: -0.2441\n",
      "Epoch 224/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3225 - val_loss: -0.2470\n",
      "Epoch 225/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3253 - val_loss: -0.2434\n",
      "Epoch 226/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3257 - val_loss: -0.2352\n",
      "Epoch 227/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3273 - val_loss: -0.2468\n",
      "Epoch 228/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3231 - val_loss: -0.2432\n",
      "Epoch 229/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3244 - val_loss: -0.2526\n",
      "Epoch 230/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3298 - val_loss: -0.2315\n",
      "Epoch 231/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3288 - val_loss: -0.2424\n",
      "Epoch 232/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3302 - val_loss: -0.2365\n",
      "Epoch 233/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3286 - val_loss: -0.2503\n",
      "Epoch 234/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3295 - val_loss: -0.2456\n",
      "Epoch 235/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3218 - val_loss: -0.2415\n",
      "Epoch 236/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3311 - val_loss: -0.2460\n",
      "Epoch 237/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3276 - val_loss: -0.2275\n",
      "Epoch 238/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3298 - val_loss: -0.2340\n",
      "Epoch 239/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3317 - val_loss: -0.2463\n",
      "Epoch 240/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3323 - val_loss: -0.2387\n",
      "Epoch 241/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3324 - val_loss: -0.2389\n",
      "Epoch 242/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3312 - val_loss: -0.2365\n",
      "Epoch 243/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3331 - val_loss: -0.2440\n",
      "Epoch 244/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3336 - val_loss: -0.2413\n",
      "Epoch 245/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3286 - val_loss: -0.2391\n",
      "Epoch 246/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3309 - val_loss: -0.2226\n",
      "Epoch 247/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3373 - val_loss: -0.2327\n",
      "Epoch 248/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3373 - val_loss: -0.2402\n",
      "Epoch 249/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3368 - val_loss: -0.2282\n",
      "Epoch 250/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3363 - val_loss: -0.2488\n",
      "Epoch 251/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3338 - val_loss: -0.2414\n",
      "Epoch 252/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3403 - val_loss: -0.2274\n",
      "Epoch 253/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3378 - val_loss: -0.2317\n",
      "Epoch 254/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3414 - val_loss: -0.2265\n",
      "Epoch 255/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3380 - val_loss: -0.2356\n",
      "Epoch 256/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3351 - val_loss: -0.2388\n",
      "Epoch 257/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3426 - val_loss: -0.2305\n",
      "Epoch 258/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3435 - val_loss: -0.2405\n",
      "Epoch 259/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3426 - val_loss: -0.2377\n",
      "Epoch 260/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3437 - val_loss: -0.2373\n",
      "Epoch 261/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3399 - val_loss: -0.2337\n",
      "Epoch 262/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3404 - val_loss: -0.2368\n",
      "Epoch 263/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3404 - val_loss: -0.2363\n",
      "Epoch 264/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3423 - val_loss: -0.2412\n",
      "Epoch 265/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3480 - val_loss: -0.2232\n",
      "Epoch 266/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3410 - val_loss: -0.2378\n",
      "Epoch 267/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3458 - val_loss: -0.2186\n",
      "Epoch 268/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3452 - val_loss: -0.2301\n",
      "Epoch 269/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3439 - val_loss: -0.2429\n",
      "Epoch 270/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3481 - val_loss: -0.2256\n",
      "Epoch 271/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3472 - val_loss: -0.2151\n",
      "Epoch 272/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3462 - val_loss: -0.2238\n",
      "Epoch 273/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3441 - val_loss: -0.2276\n",
      "Epoch 274/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3478 - val_loss: -0.2382\n",
      "Epoch 275/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3503 - val_loss: -0.2228\n",
      "Epoch 276/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3525 - val_loss: -0.2326\n",
      "Epoch 277/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3468 - val_loss: -0.2135\n",
      "Epoch 278/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3514 - val_loss: -0.2255\n",
      "Epoch 279/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3486 - val_loss: -0.2185\n",
      "Epoch 280/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3515 - val_loss: -0.2270\n",
      "Epoch 281/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3528 - val_loss: -0.2251\n",
      "Epoch 282/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3519 - val_loss: -0.2292\n",
      "Epoch 283/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3505 - val_loss: -0.2240\n",
      "Epoch 284/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3525 - val_loss: -0.2202\n",
      "Epoch 285/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3506 - val_loss: -0.2183\n",
      "Epoch 286/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3553 - val_loss: -0.2183\n",
      "Epoch 287/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3519 - val_loss: -0.2055\n",
      "Epoch 288/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3555 - val_loss: -0.2216\n",
      "Epoch 289/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3558 - val_loss: -0.2273\n",
      "Epoch 290/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3556 - val_loss: -0.2220\n",
      "Epoch 291/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3567 - val_loss: -0.2180\n",
      "Epoch 292/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3552 - val_loss: -0.2196\n",
      "Epoch 293/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3570 - val_loss: -0.2141\n",
      "Epoch 294/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3560 - val_loss: -0.2252\n",
      "Epoch 295/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3569 - val_loss: -0.1989\n",
      "Epoch 296/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3566 - val_loss: -0.2132\n",
      "Epoch 297/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3570 - val_loss: -0.2125\n",
      "Epoch 298/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3598 - val_loss: -0.2207\n",
      "Epoch 299/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3605 - val_loss: -0.2073\n",
      "Epoch 300/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3562 - val_loss: -0.2233\n",
      "Epoch 301/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3583 - val_loss: -0.2129\n",
      "Epoch 302/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3636 - val_loss: -0.2187\n",
      "Epoch 303/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3633 - val_loss: -0.2233\n",
      "Epoch 304/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3608 - val_loss: -0.2048\n",
      "Epoch 305/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3616 - val_loss: -0.2176\n",
      "Epoch 306/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3631 - val_loss: -0.2113\n",
      "Epoch 307/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3662 - val_loss: -0.2104\n",
      "Epoch 308/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3645 - val_loss: -0.2126\n",
      "Epoch 309/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3674 - val_loss: -0.2004\n",
      "Epoch 310/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3648 - val_loss: -0.1927\n",
      "Epoch 311/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3642 - val_loss: -0.2101\n",
      "Epoch 312/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3681 - val_loss: -0.2102\n",
      "Epoch 313/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3657 - val_loss: -0.2066\n",
      "Epoch 314/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3709 - val_loss: -0.2106\n",
      "Epoch 315/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3713 - val_loss: -0.1859\n",
      "Epoch 316/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3723 - val_loss: -0.2059\n",
      "Epoch 317/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3689 - val_loss: -0.2025\n",
      "Epoch 318/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3714 - val_loss: -0.1905\n",
      "Epoch 319/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3735 - val_loss: -0.2066\n",
      "Epoch 320/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3664 - val_loss: -0.1874\n",
      "Epoch 321/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3696 - val_loss: -0.1961\n",
      "Epoch 322/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3748 - val_loss: -0.1965\n",
      "Epoch 323/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3743 - val_loss: -0.2022\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_25_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 25,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (52735, 12, 1)\n",
      "y_train.shape:  (52735, 1)\n",
      "x_valid.shape:  (13181, 12, 1)\n",
      "y_valid.shape:  (13181, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 20:14:17,330 WARNING Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-27 20:14:17,419 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 52735 samples, validate on 13181 samples\n",
      "Epoch 1/10000\n",
      "52735/52735 [==============================] - 1s 24us/sample - loss: 0.6403 - val_loss: 0.1849\n",
      "Epoch 2/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: 0.1700 - val_loss: 0.0395\n",
      "Epoch 3/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: 0.1002 - val_loss: -0.0136\n",
      "Epoch 4/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: 0.0762 - val_loss: -0.0416\n",
      "Epoch 5/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: 0.0485 - val_loss: -0.0195\n",
      "Epoch 6/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: 0.0271 - val_loss: -0.0647\n",
      "Epoch 7/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.0061 - val_loss: -0.0882\n",
      "Epoch 8/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0160 - val_loss: -0.1027\n",
      "Epoch 9/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0334 - val_loss: -0.1144\n",
      "Epoch 10/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0522 - val_loss: -0.1312\n",
      "Epoch 11/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0624 - val_loss: -0.1175\n",
      "Epoch 12/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0738 - val_loss: -0.1627\n",
      "Epoch 13/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0857 - val_loss: -0.1433\n",
      "Epoch 14/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0760 - val_loss: -0.1196\n",
      "Epoch 15/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0877 - val_loss: -0.1445\n",
      "Epoch 16/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0954 - val_loss: -0.1620\n",
      "Epoch 17/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1032 - val_loss: -0.1362\n",
      "Epoch 18/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1141 - val_loss: -0.1651\n",
      "Epoch 19/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1242 - val_loss: -0.1750\n",
      "Epoch 20/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1133 - val_loss: -0.1880\n",
      "Epoch 21/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1254 - val_loss: -0.1911\n",
      "Epoch 22/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1264 - val_loss: -0.1891\n",
      "Epoch 23/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1370 - val_loss: -0.1806\n",
      "Epoch 24/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1394 - val_loss: -0.1943\n",
      "Epoch 25/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1417 - val_loss: -0.1507\n",
      "Epoch 26/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1516 - val_loss: -0.1812\n",
      "Epoch 27/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1520 - val_loss: -0.1626\n",
      "Epoch 28/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1498 - val_loss: -0.2055\n",
      "Epoch 29/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1599 - val_loss: -0.2063\n",
      "Epoch 30/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1594 - val_loss: -0.1781\n",
      "Epoch 31/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1506 - val_loss: -0.2045\n",
      "Epoch 32/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1676 - val_loss: -0.2225\n",
      "Epoch 33/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1684 - val_loss: -0.2185\n",
      "Epoch 34/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1688 - val_loss: -0.2046\n",
      "Epoch 35/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1703 - val_loss: -0.2174\n",
      "Epoch 36/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1730 - val_loss: -0.2108\n",
      "Epoch 37/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1717 - val_loss: -0.2131\n",
      "Epoch 38/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1755 - val_loss: -0.2145\n",
      "Epoch 39/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1830 - val_loss: -0.2212\n",
      "Epoch 40/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1819 - val_loss: -0.2208\n",
      "Epoch 41/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1794 - val_loss: -0.2179\n",
      "Epoch 42/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1808 - val_loss: -0.2212\n",
      "Epoch 43/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1879 - val_loss: -0.1774\n",
      "Epoch 44/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1883 - val_loss: -0.2267\n",
      "Epoch 45/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.1874 - val_loss: -0.2318\n",
      "Epoch 46/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1939 - val_loss: -0.2077\n",
      "Epoch 47/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1953 - val_loss: -0.2272\n",
      "Epoch 48/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1949 - val_loss: -0.2247\n",
      "Epoch 49/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1985 - val_loss: -0.2384\n",
      "Epoch 50/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1907 - val_loss: -0.2344\n",
      "Epoch 51/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1963 - val_loss: -0.2328\n",
      "Epoch 52/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2066 - val_loss: -0.2354\n",
      "Epoch 53/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2034 - val_loss: -0.2289\n",
      "Epoch 54/10000\n",
      "52735/52735 [==============================] - 1s 19us/sample - loss: -0.2019 - val_loss: -0.2244\n",
      "Epoch 55/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.2078 - val_loss: -0.2221\n",
      "Epoch 56/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.2148 - val_loss: -0.2191\n",
      "Epoch 57/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2148 - val_loss: -0.2240\n",
      "Epoch 58/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2140 - val_loss: -0.2374\n",
      "Epoch 59/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2141 - val_loss: -0.2345\n",
      "Epoch 60/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2156 - val_loss: -0.2512\n",
      "Epoch 61/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2137 - val_loss: -0.2433\n",
      "Epoch 62/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2197 - val_loss: -0.2385\n",
      "Epoch 63/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2183 - val_loss: -0.2433\n",
      "Epoch 64/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2210 - val_loss: -0.2453\n",
      "Epoch 65/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2209 - val_loss: -0.2398\n",
      "Epoch 66/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2262 - val_loss: -0.2408\n",
      "Epoch 67/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2202 - val_loss: -0.2367\n",
      "Epoch 68/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2304 - val_loss: -0.2404\n",
      "Epoch 69/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2282 - val_loss: -0.2267\n",
      "Epoch 70/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2240 - val_loss: -0.2299\n",
      "Epoch 71/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2207 - val_loss: -0.2486\n",
      "Epoch 72/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2252 - val_loss: -0.2247\n",
      "Epoch 73/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2346 - val_loss: -0.2246\n",
      "Epoch 74/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2345 - val_loss: -0.2600\n",
      "Epoch 75/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2377 - val_loss: -0.2528\n",
      "Epoch 76/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2343 - val_loss: -0.2531\n",
      "Epoch 77/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2403 - val_loss: -0.2564\n",
      "Epoch 78/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2404 - val_loss: -0.2121\n",
      "Epoch 79/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2357 - val_loss: -0.2475\n",
      "Epoch 80/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2453 - val_loss: -0.2477\n",
      "Epoch 81/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2466 - val_loss: -0.2384\n",
      "Epoch 82/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2414 - val_loss: -0.2453\n",
      "Epoch 83/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2458 - val_loss: -0.2584\n",
      "Epoch 84/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2431 - val_loss: -0.2524\n",
      "Epoch 85/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2483 - val_loss: -0.2555\n",
      "Epoch 86/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2463 - val_loss: -0.2555\n",
      "Epoch 87/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2528 - val_loss: -0.2515\n",
      "Epoch 88/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2513 - val_loss: -0.2470\n",
      "Epoch 89/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2550 - val_loss: -0.2477\n",
      "Epoch 90/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2513 - val_loss: -0.2548\n",
      "Epoch 91/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2511 - val_loss: -0.2370\n",
      "Epoch 92/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2508 - val_loss: -0.2457\n",
      "Epoch 93/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2563 - val_loss: -0.2543\n",
      "Epoch 94/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2582 - val_loss: -0.2593\n",
      "Epoch 95/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2578 - val_loss: -0.2574\n",
      "Epoch 96/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2587 - val_loss: -0.2529\n",
      "Epoch 97/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2598 - val_loss: -0.2529\n",
      "Epoch 98/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2600 - val_loss: -0.2648\n",
      "Epoch 99/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2622 - val_loss: -0.2438\n",
      "Epoch 100/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2617 - val_loss: -0.2554\n",
      "Epoch 101/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2631 - val_loss: -0.2530\n",
      "Epoch 102/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2656 - val_loss: -0.2630\n",
      "Epoch 103/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2630 - val_loss: -0.2474\n",
      "Epoch 104/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2645 - val_loss: -0.2529\n",
      "Epoch 105/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2657 - val_loss: -0.2531\n",
      "Epoch 106/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2663 - val_loss: -0.2433\n",
      "Epoch 107/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2690 - val_loss: -0.2476\n",
      "Epoch 108/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2679 - val_loss: -0.2445\n",
      "Epoch 109/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2691 - val_loss: -0.2544\n",
      "Epoch 110/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2706 - val_loss: -0.2632\n",
      "Epoch 111/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2738 - val_loss: -0.2604\n",
      "Epoch 112/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2689 - val_loss: -0.2534\n",
      "Epoch 113/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2711 - val_loss: -0.2537\n",
      "Epoch 114/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2644 - val_loss: -0.2465\n",
      "Epoch 115/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2703 - val_loss: -0.2659\n",
      "Epoch 116/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2741 - val_loss: -0.2517\n",
      "Epoch 117/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2768 - val_loss: -0.2536\n",
      "Epoch 118/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2755 - val_loss: -0.2551\n",
      "Epoch 119/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2735 - val_loss: -0.2568\n",
      "Epoch 120/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2812 - val_loss: -0.2571\n",
      "Epoch 121/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2758 - val_loss: -0.2533\n",
      "Epoch 122/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2738 - val_loss: -0.2516\n",
      "Epoch 123/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2794 - val_loss: -0.2581\n",
      "Epoch 124/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2783 - val_loss: -0.2487\n",
      "Epoch 125/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2805 - val_loss: -0.2471\n",
      "Epoch 126/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2790 - val_loss: -0.2672\n",
      "Epoch 127/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2764 - val_loss: -0.2509\n",
      "Epoch 128/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2794 - val_loss: -0.2528\n",
      "Epoch 129/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2769 - val_loss: -0.2556\n",
      "Epoch 130/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2821 - val_loss: -0.2607\n",
      "Epoch 131/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2829 - val_loss: -0.2623\n",
      "Epoch 132/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2840 - val_loss: -0.2672\n",
      "Epoch 133/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2843 - val_loss: -0.2581\n",
      "Epoch 134/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2876 - val_loss: -0.2597\n",
      "Epoch 135/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2846 - val_loss: -0.2541\n",
      "Epoch 136/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2828 - val_loss: -0.2646\n",
      "Epoch 137/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2878 - val_loss: -0.2576\n",
      "Epoch 138/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2854 - val_loss: -0.2538\n",
      "Epoch 139/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2885 - val_loss: -0.2585\n",
      "Epoch 140/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2861 - val_loss: -0.2618\n",
      "Epoch 141/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2893 - val_loss: -0.2595\n",
      "Epoch 142/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2801 - val_loss: -0.2424\n",
      "Epoch 143/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2902 - val_loss: -0.2682\n",
      "Epoch 144/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2869 - val_loss: -0.2586\n",
      "Epoch 145/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2959 - val_loss: -0.2590\n",
      "Epoch 146/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2884 - val_loss: -0.2648\n",
      "Epoch 147/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2962 - val_loss: -0.2651\n",
      "Epoch 148/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2939 - val_loss: -0.2519\n",
      "Epoch 149/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2856 - val_loss: -0.2549\n",
      "Epoch 150/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2942 - val_loss: -0.2624\n",
      "Epoch 151/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2944 - val_loss: -0.2594\n",
      "Epoch 152/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2940 - val_loss: -0.2630\n",
      "Epoch 153/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2947 - val_loss: -0.2373\n",
      "Epoch 154/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2933 - val_loss: -0.2553\n",
      "Epoch 155/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2961 - val_loss: -0.2395\n",
      "Epoch 156/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2960 - val_loss: -0.2585\n",
      "Epoch 157/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2954 - val_loss: -0.2560\n",
      "Epoch 158/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2948 - val_loss: -0.2551\n",
      "Epoch 159/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2974 - val_loss: -0.2681\n",
      "Epoch 160/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2960 - val_loss: -0.2582\n",
      "Epoch 161/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3007 - val_loss: -0.2577\n",
      "Epoch 162/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2939 - val_loss: -0.2509\n",
      "Epoch 163/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2960 - val_loss: -0.2613\n",
      "Epoch 164/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3026 - val_loss: -0.2625\n",
      "Epoch 165/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2996 - val_loss: -0.2575\n",
      "Epoch 166/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2879 - val_loss: -0.2547\n",
      "Epoch 167/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.2947 - val_loss: -0.2586\n",
      "Epoch 168/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2997 - val_loss: -0.2591\n",
      "Epoch 169/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3017 - val_loss: -0.2507\n",
      "Epoch 170/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3021 - val_loss: -0.2528\n",
      "Epoch 171/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3040 - val_loss: -0.2547\n",
      "Epoch 172/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3012 - val_loss: -0.2639\n",
      "Epoch 173/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3035 - val_loss: -0.2568\n",
      "Epoch 174/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3018 - val_loss: -0.2600\n",
      "Epoch 175/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3028 - val_loss: -0.2517\n",
      "Epoch 176/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3047 - val_loss: -0.2485\n",
      "Epoch 177/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3057 - val_loss: -0.2551\n",
      "Epoch 178/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3056 - val_loss: -0.2593\n",
      "Epoch 179/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3032 - val_loss: -0.2508\n",
      "Epoch 180/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3069 - val_loss: -0.2482\n",
      "Epoch 181/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3056 - val_loss: -0.2611\n",
      "Epoch 182/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3043 - val_loss: -0.2541\n",
      "Epoch 183/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3107 - val_loss: -0.2500\n",
      "Epoch 184/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3094 - val_loss: -0.2542\n",
      "Epoch 185/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3080 - val_loss: -0.2593\n",
      "Epoch 186/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3074 - val_loss: -0.2496\n",
      "Epoch 187/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3075 - val_loss: -0.2514\n",
      "Epoch 188/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3135 - val_loss: -0.2588\n",
      "Epoch 189/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3105 - val_loss: -0.2620\n",
      "Epoch 190/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3034 - val_loss: -0.2497\n",
      "Epoch 191/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3137 - val_loss: -0.2569\n",
      "Epoch 192/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3116 - val_loss: -0.2628\n",
      "Epoch 193/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3082 - val_loss: -0.2519\n",
      "Epoch 194/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3136 - val_loss: -0.2564\n",
      "Epoch 195/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3151 - val_loss: -0.2439\n",
      "Epoch 196/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3068 - val_loss: -0.2504\n",
      "Epoch 197/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3124 - val_loss: -0.2530\n",
      "Epoch 198/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3146 - val_loss: -0.2486\n",
      "Epoch 199/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3160 - val_loss: -0.2351\n",
      "Epoch 200/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3152 - val_loss: -0.2532\n",
      "Epoch 201/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3165 - val_loss: -0.2433\n",
      "Epoch 202/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3144 - val_loss: -0.2603\n",
      "Epoch 203/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3148 - val_loss: -0.2463\n",
      "Epoch 204/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3195 - val_loss: -0.2517\n",
      "Epoch 205/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3177 - val_loss: -0.2483\n",
      "Epoch 206/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3168 - val_loss: -0.2479\n",
      "Epoch 207/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3226 - val_loss: -0.2357\n",
      "Epoch 208/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3182 - val_loss: -0.2486\n",
      "Epoch 209/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3231 - val_loss: -0.2494\n",
      "Epoch 210/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3147 - val_loss: -0.2375\n",
      "Epoch 211/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3187 - val_loss: -0.2322\n",
      "Epoch 212/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3179 - val_loss: -0.2397\n",
      "Epoch 213/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3206 - val_loss: -0.2446\n",
      "Epoch 214/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3188 - val_loss: -0.2515\n",
      "Epoch 215/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3207 - val_loss: -0.2313\n",
      "Epoch 216/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3188 - val_loss: -0.2396\n",
      "Epoch 217/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3193 - val_loss: -0.2356\n",
      "Epoch 218/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3231 - val_loss: -0.2443\n",
      "Epoch 219/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3230 - val_loss: -0.2385\n",
      "Epoch 220/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3245 - val_loss: -0.2519\n",
      "Epoch 221/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3267 - val_loss: -0.2391\n",
      "Epoch 222/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3238 - val_loss: -0.2437\n",
      "Epoch 223/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3249 - val_loss: -0.2456\n",
      "Epoch 224/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3265 - val_loss: -0.2292\n",
      "Epoch 225/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3202 - val_loss: -0.2330\n",
      "Epoch 226/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3270 - val_loss: -0.2424\n",
      "Epoch 227/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3230 - val_loss: -0.2290\n",
      "Epoch 228/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3272 - val_loss: -0.2403\n",
      "Epoch 229/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3262 - val_loss: -0.2259\n",
      "Epoch 230/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3305 - val_loss: -0.2437\n",
      "Epoch 231/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3273 - val_loss: -0.2407\n",
      "Epoch 232/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3290 - val_loss: -0.2406\n",
      "Epoch 233/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3292 - val_loss: -0.2355\n",
      "Epoch 234/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3278 - val_loss: -0.2406\n",
      "Epoch 235/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3278 - val_loss: -0.2315\n",
      "Epoch 236/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3296 - val_loss: -0.2385\n",
      "Epoch 237/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3270 - val_loss: -0.2244\n",
      "Epoch 238/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3306 - val_loss: -0.2383\n",
      "Epoch 239/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3314 - val_loss: -0.2385\n",
      "Epoch 240/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3325 - val_loss: -0.2444\n",
      "Epoch 241/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3328 - val_loss: -0.2237\n",
      "Epoch 242/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3309 - val_loss: -0.2343\n",
      "Epoch 243/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3335 - val_loss: -0.2308\n",
      "Epoch 244/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3334 - val_loss: -0.2368\n",
      "Epoch 245/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3350 - val_loss: -0.2296\n",
      "Epoch 246/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3331 - val_loss: -0.2181\n",
      "Epoch 247/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3337 - val_loss: -0.2325\n",
      "Epoch 248/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3330 - val_loss: -0.2274\n",
      "Epoch 249/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3364 - val_loss: -0.2104\n",
      "Epoch 250/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3350 - val_loss: -0.2261\n",
      "Epoch 251/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3357 - val_loss: -0.2366\n",
      "Epoch 252/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3379 - val_loss: -0.2407\n",
      "Epoch 253/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3352 - val_loss: -0.2241\n",
      "Epoch 254/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3345 - val_loss: -0.2349\n",
      "Epoch 255/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3389 - val_loss: -0.2249\n",
      "Epoch 256/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3385 - val_loss: -0.2235\n",
      "Epoch 257/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3397 - val_loss: -0.2325\n",
      "Epoch 258/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3390 - val_loss: -0.2304\n",
      "Epoch 259/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3390 - val_loss: -0.2205\n",
      "Epoch 260/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3431 - val_loss: -0.2324\n",
      "Epoch 261/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3438 - val_loss: -0.2355\n",
      "Epoch 262/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3434 - val_loss: -0.2298\n",
      "Epoch 263/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3417 - val_loss: -0.2307\n",
      "Epoch 264/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3434 - val_loss: -0.2154\n",
      "Epoch 265/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3408 - val_loss: -0.2278\n",
      "Epoch 266/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3449 - val_loss: -0.2392\n",
      "Epoch 267/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3468 - val_loss: -0.2330\n",
      "Epoch 268/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3443 - val_loss: -0.2278\n",
      "Epoch 269/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3491 - val_loss: -0.2255\n",
      "Epoch 270/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3504 - val_loss: -0.2286\n",
      "Epoch 271/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3447 - val_loss: -0.2175\n",
      "Epoch 272/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3449 - val_loss: -0.2223\n",
      "Epoch 273/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3463 - val_loss: -0.2330\n",
      "Epoch 274/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3495 - val_loss: -0.2135\n",
      "Epoch 275/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3470 - val_loss: -0.2349\n",
      "Epoch 276/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3527 - val_loss: -0.2197\n",
      "Epoch 277/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3470 - val_loss: -0.2296\n",
      "Epoch 278/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3488 - val_loss: -0.2239\n",
      "Epoch 279/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3481 - val_loss: -0.2126\n",
      "Epoch 280/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3484 - val_loss: -0.2167\n",
      "Epoch 281/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3496 - val_loss: -0.2065\n",
      "Epoch 282/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3526 - val_loss: -0.2302\n",
      "Epoch 283/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3525 - val_loss: -0.2135\n",
      "Epoch 284/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3521 - val_loss: -0.2215\n",
      "Epoch 285/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3466 - val_loss: -0.2201\n",
      "Epoch 286/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3486 - val_loss: -0.2197\n",
      "Epoch 287/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3520 - val_loss: -0.2261\n",
      "Epoch 288/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3567 - val_loss: -0.2223\n",
      "Epoch 289/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3554 - val_loss: -0.2255\n",
      "Epoch 290/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3570 - val_loss: -0.2045\n",
      "Epoch 291/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3496 - val_loss: -0.2256\n",
      "Epoch 292/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3602 - val_loss: -0.2123\n",
      "Epoch 293/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3571 - val_loss: -0.2228\n",
      "Epoch 294/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3575 - val_loss: -0.2152\n",
      "Epoch 295/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3587 - val_loss: -0.2140\n",
      "Epoch 296/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3569 - val_loss: -0.2016\n",
      "Epoch 297/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3591 - val_loss: -0.2125\n",
      "Epoch 298/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3599 - val_loss: -0.2084\n",
      "Epoch 299/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3602 - val_loss: -0.1965\n",
      "Epoch 300/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3603 - val_loss: -0.2123\n",
      "Epoch 301/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3624 - val_loss: -0.2151\n",
      "Epoch 302/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3567 - val_loss: -0.2120\n",
      "Epoch 303/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3615 - val_loss: -0.2114\n",
      "Epoch 304/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3596 - val_loss: -0.2113\n",
      "Epoch 305/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3557 - val_loss: -0.2076\n",
      "Epoch 306/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3643 - val_loss: -0.2105\n",
      "Epoch 307/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3648 - val_loss: -0.2097\n",
      "Epoch 308/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3660 - val_loss: -0.2030\n",
      "Epoch 309/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3623 - val_loss: -0.1928\n",
      "Epoch 310/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3635 - val_loss: -0.2071\n",
      "Epoch 311/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3642 - val_loss: -0.1997\n",
      "Epoch 312/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3654 - val_loss: -0.2207\n",
      "Epoch 313/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3657 - val_loss: -0.1820\n",
      "Epoch 314/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3634 - val_loss: -0.1953\n",
      "Epoch 315/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.3680 - val_loss: -0.2059\n",
      "Epoch 316/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3712 - val_loss: -0.1965\n",
      "Epoch 317/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3696 - val_loss: -0.1953\n",
      "Epoch 318/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3676 - val_loss: -0.2040\n",
      "Epoch 319/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3703 - val_loss: -0.2043\n",
      "Epoch 320/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3680 - val_loss: -0.1955\n",
      "Epoch 321/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3695 - val_loss: -0.1897\n",
      "Epoch 322/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3709 - val_loss: -0.1807\n",
      "Epoch 323/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3730 - val_loss: -0.1857\n",
      "Epoch 324/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3735 - val_loss: -0.1917\n",
      "Epoch 325/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3700 - val_loss: -0.2049\n",
      "Epoch 326/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3744 - val_loss: -0.1902\n",
      "Epoch 327/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3747 - val_loss: -0.1787\n",
      "Epoch 328/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3716 - val_loss: -0.1930\n",
      "Epoch 329/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3830 - val_loss: -0.1779\n",
      "Epoch 330/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3767 - val_loss: -0.1850\n",
      "Epoch 331/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3727 - val_loss: -0.1889\n",
      "Epoch 332/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3780 - val_loss: -0.1885\n",
      "Epoch 333/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3764 - val_loss: -0.1885\n",
      "Epoch 334/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3798 - val_loss: -0.1928\n",
      "Epoch 335/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3799 - val_loss: -0.1796\n",
      "Epoch 336/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3793 - val_loss: -0.1891\n",
      "Epoch 337/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3792 - val_loss: -0.1803\n",
      "Epoch 338/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3816 - val_loss: -0.1923\n",
      "Epoch 339/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3846 - val_loss: -0.1815\n",
      "Epoch 340/10000\n",
      "52735/52735 [==============================] - 1s 20us/sample - loss: -0.3849 - val_loss: -0.1732\n",
      "Epoch 341/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3793 - val_loss: -0.1829\n",
      "Epoch 342/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3843 - val_loss: -0.1868\n",
      "Epoch 343/10000\n",
      "52735/52735 [==============================] - 1s 19us/sample - loss: -0.3843 - val_loss: -0.1660\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\train\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_50_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 50,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (52735, 12, 1)\n",
      "y_train.shape:  (52735, 1)\n",
      "x_valid.shape:  (13181, 12, 1)\n",
      "y_valid.shape:  (13181, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 20:18:13,651 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-11-27 20:18:13,823 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 52735 samples, validate on 13181 samples\n",
      "Epoch 1/10000\n",
      "52735/52735 [==============================] - 1s 27us/sample - loss: 0.6647 - val_loss: 0.2847\n",
      "Epoch 2/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: 0.1747 - val_loss: 0.0445\n",
      "Epoch 3/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: 0.0865 - val_loss: -0.0157\n",
      "Epoch 4/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: 0.0684 - val_loss: -0.0410\n",
      "Epoch 5/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: 0.0381 - val_loss: -0.0759\n",
      "Epoch 6/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: 0.0157 - val_loss: -0.1067\n",
      "Epoch 7/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.0019 - val_loss: -0.0804\n",
      "Epoch 8/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0253 - val_loss: -0.1264\n",
      "Epoch 9/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0460 - val_loss: -0.1453\n",
      "Epoch 10/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0573 - val_loss: -0.1319\n",
      "Epoch 11/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0681 - val_loss: -0.1373\n",
      "Epoch 12/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0706 - val_loss: -0.1688\n",
      "Epoch 13/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0796 - val_loss: -0.1717\n",
      "Epoch 14/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.0893 - val_loss: -0.1538\n",
      "Epoch 15/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.0972 - val_loss: -0.1429\n",
      "Epoch 16/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1034 - val_loss: -0.1720\n",
      "Epoch 17/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1004 - val_loss: -0.1953\n",
      "Epoch 18/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1156 - val_loss: -0.1749\n",
      "Epoch 19/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1176 - val_loss: -0.1338\n",
      "Epoch 20/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1232 - val_loss: -0.1801\n",
      "Epoch 21/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1284 - val_loss: -0.1859\n",
      "Epoch 22/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1283 - val_loss: -0.1752\n",
      "Epoch 23/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1352 - val_loss: -0.1929\n",
      "Epoch 24/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1409 - val_loss: -0.1994\n",
      "Epoch 25/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1372 - val_loss: -0.1997\n",
      "Epoch 26/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1500 - val_loss: -0.2154\n",
      "Epoch 27/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1435 - val_loss: -0.2112\n",
      "Epoch 28/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1484 - val_loss: -0.1859\n",
      "Epoch 29/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1547 - val_loss: -0.1871\n",
      "Epoch 30/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1625 - val_loss: -0.1973\n",
      "Epoch 31/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1591 - val_loss: -0.1975\n",
      "Epoch 32/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1567 - val_loss: -0.2097\n",
      "Epoch 33/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1622 - val_loss: -0.2039\n",
      "Epoch 34/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1618 - val_loss: -0.2087\n",
      "Epoch 35/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1668 - val_loss: -0.2143\n",
      "Epoch 36/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1708 - val_loss: -0.2021\n",
      "Epoch 37/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1766 - val_loss: -0.2124\n",
      "Epoch 38/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1749 - val_loss: -0.1851\n",
      "Epoch 39/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1744 - val_loss: -0.2010\n",
      "Epoch 40/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.1787 - val_loss: -0.2172\n",
      "Epoch 41/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1700 - val_loss: -0.2228\n",
      "Epoch 42/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1843 - val_loss: -0.2149\n",
      "Epoch 43/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1871 - val_loss: -0.2124\n",
      "Epoch 44/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.1849 - val_loss: -0.2134\n",
      "Epoch 45/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1912 - val_loss: -0.2268\n",
      "Epoch 46/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1891 - val_loss: -0.2192\n",
      "Epoch 47/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1892 - val_loss: -0.1904\n",
      "Epoch 48/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.1906 - val_loss: -0.2279\n",
      "Epoch 49/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.1940 - val_loss: -0.2274\n",
      "Epoch 50/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.1952 - val_loss: -0.2417\n",
      "Epoch 51/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2015 - val_loss: -0.2333\n",
      "Epoch 52/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2033 - val_loss: -0.2338\n",
      "Epoch 53/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2001 - val_loss: -0.2391\n",
      "Epoch 54/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.1981 - val_loss: -0.1849\n",
      "Epoch 55/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.1953 - val_loss: -0.2268\n",
      "Epoch 56/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2077 - val_loss: -0.2384\n",
      "Epoch 57/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2079 - val_loss: -0.2071\n",
      "Epoch 58/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2077 - val_loss: -0.2424\n",
      "Epoch 59/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2067 - val_loss: -0.2361\n",
      "Epoch 60/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2097 - val_loss: -0.2347\n",
      "Epoch 61/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2155 - val_loss: -0.2358\n",
      "Epoch 62/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2182 - val_loss: -0.2316\n",
      "Epoch 63/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2140 - val_loss: -0.2362\n",
      "Epoch 64/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2234 - val_loss: -0.2302\n",
      "Epoch 65/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2142 - val_loss: -0.2485\n",
      "Epoch 66/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2213 - val_loss: -0.2499\n",
      "Epoch 67/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2279 - val_loss: -0.2475\n",
      "Epoch 68/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2244 - val_loss: -0.2431\n",
      "Epoch 69/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2226 - val_loss: -0.2388\n",
      "Epoch 70/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2277 - val_loss: -0.2459\n",
      "Epoch 71/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2297 - val_loss: -0.2478\n",
      "Epoch 72/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2278 - val_loss: -0.2475\n",
      "Epoch 73/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2304 - val_loss: -0.2331\n",
      "Epoch 74/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2325 - val_loss: -0.2358\n",
      "Epoch 75/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2333 - val_loss: -0.2468\n",
      "Epoch 76/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2273 - val_loss: -0.2536\n",
      "Epoch 77/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2321 - val_loss: -0.2382\n",
      "Epoch 78/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2404 - val_loss: -0.2483\n",
      "Epoch 79/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2359 - val_loss: -0.2372\n",
      "Epoch 80/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2313 - val_loss: -0.2426\n",
      "Epoch 81/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2381 - val_loss: -0.2391\n",
      "Epoch 82/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2402 - val_loss: -0.2476\n",
      "Epoch 83/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2397 - val_loss: -0.2498\n",
      "Epoch 84/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2405 - val_loss: -0.2294\n",
      "Epoch 85/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2453 - val_loss: -0.2576\n",
      "Epoch 86/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2487 - val_loss: -0.2529\n",
      "Epoch 87/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2503 - val_loss: -0.2388\n",
      "Epoch 88/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2446 - val_loss: -0.2359\n",
      "Epoch 89/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2497 - val_loss: -0.2495\n",
      "Epoch 90/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2517 - val_loss: -0.2344\n",
      "Epoch 91/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2536 - val_loss: -0.2583\n",
      "Epoch 92/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2524 - val_loss: -0.2618\n",
      "Epoch 93/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2553 - val_loss: -0.2538\n",
      "Epoch 94/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2507 - val_loss: -0.2546\n",
      "Epoch 95/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2547 - val_loss: -0.2327\n",
      "Epoch 96/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2561 - val_loss: -0.2520\n",
      "Epoch 97/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2566 - val_loss: -0.2572\n",
      "Epoch 98/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2554 - val_loss: -0.2317\n",
      "Epoch 99/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2606 - val_loss: -0.2401\n",
      "Epoch 100/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2559 - val_loss: -0.2643\n",
      "Epoch 101/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2589 - val_loss: -0.2509\n",
      "Epoch 102/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2630 - val_loss: -0.2510\n",
      "Epoch 103/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2656 - val_loss: -0.2490\n",
      "Epoch 104/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2649 - val_loss: -0.2530\n",
      "Epoch 105/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2639 - val_loss: -0.2401\n",
      "Epoch 106/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2654 - val_loss: -0.2579\n",
      "Epoch 107/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2705 - val_loss: -0.2522\n",
      "Epoch 108/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2647 - val_loss: -0.2503\n",
      "Epoch 109/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2713 - val_loss: -0.2597\n",
      "Epoch 110/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2673 - val_loss: -0.2422\n",
      "Epoch 111/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2643 - val_loss: -0.2461\n",
      "Epoch 112/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2680 - val_loss: -0.2600\n",
      "Epoch 113/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2730 - val_loss: -0.2552\n",
      "Epoch 114/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2683 - val_loss: -0.2423\n",
      "Epoch 115/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2731 - val_loss: -0.2533\n",
      "Epoch 116/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2684 - val_loss: -0.2612\n",
      "Epoch 117/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2668 - val_loss: -0.2496\n",
      "Epoch 118/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2708 - val_loss: -0.2538\n",
      "Epoch 119/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2728 - val_loss: -0.2457\n",
      "Epoch 120/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2719 - val_loss: -0.2604\n",
      "Epoch 121/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2776 - val_loss: -0.2599\n",
      "Epoch 122/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2657 - val_loss: -0.2596\n",
      "Epoch 123/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2777 - val_loss: -0.2609\n",
      "Epoch 124/10000\n",
      "52735/52735 [==============================] - 1s 21us/sample - loss: -0.2782 - val_loss: -0.2154\n",
      "Epoch 125/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.2739 - val_loss: -0.2581\n",
      "Epoch 126/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2750 - val_loss: -0.2577\n",
      "Epoch 127/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2811 - val_loss: -0.2551\n",
      "Epoch 128/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2734 - val_loss: -0.2597\n",
      "Epoch 129/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2762 - val_loss: -0.2490\n",
      "Epoch 130/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2843 - val_loss: -0.2544\n",
      "Epoch 131/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2812 - val_loss: -0.2515\n",
      "Epoch 132/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2745 - val_loss: -0.2570\n",
      "Epoch 133/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2761 - val_loss: -0.2504\n",
      "Epoch 134/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2800 - val_loss: -0.2447\n",
      "Epoch 135/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2834 - val_loss: -0.2503\n",
      "Epoch 136/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2771 - val_loss: -0.2602\n",
      "Epoch 137/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2849 - val_loss: -0.2543\n",
      "Epoch 138/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2877 - val_loss: -0.2560\n",
      "Epoch 139/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2863 - val_loss: -0.2543\n",
      "Epoch 140/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2867 - val_loss: -0.2630\n",
      "Epoch 141/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2848 - val_loss: -0.2558\n",
      "Epoch 142/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2856 - val_loss: -0.2435\n",
      "Epoch 143/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2850 - val_loss: -0.2551\n",
      "Epoch 144/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2857 - val_loss: -0.2588\n",
      "Epoch 145/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2847 - val_loss: -0.2524\n",
      "Epoch 146/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2889 - val_loss: -0.2516\n",
      "Epoch 147/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2875 - val_loss: -0.2635\n",
      "Epoch 148/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2904 - val_loss: -0.2571\n",
      "Epoch 149/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2877 - val_loss: -0.2491\n",
      "Epoch 150/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2900 - val_loss: -0.2481\n",
      "Epoch 151/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2894 - val_loss: -0.2478\n",
      "Epoch 152/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2933 - val_loss: -0.2572\n",
      "Epoch 153/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2921 - val_loss: -0.2610\n",
      "Epoch 154/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2920 - val_loss: -0.2656\n",
      "Epoch 155/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2934 - val_loss: -0.2538\n",
      "Epoch 156/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2952 - val_loss: -0.2492\n",
      "Epoch 157/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2934 - val_loss: -0.2536\n",
      "Epoch 158/10000\n",
      "52735/52735 [==============================] - 1s 16us/sample - loss: -0.2921 - val_loss: -0.2568\n",
      "Epoch 159/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2923 - val_loss: -0.2578\n",
      "Epoch 160/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2963 - val_loss: -0.2496\n",
      "Epoch 161/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2954 - val_loss: -0.2585\n",
      "Epoch 162/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2912 - val_loss: -0.2460\n",
      "Epoch 163/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.2964 - val_loss: -0.2520\n",
      "Epoch 164/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3003 - val_loss: -0.2565\n",
      "Epoch 165/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2983 - val_loss: -0.2567\n",
      "Epoch 166/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2973 - val_loss: -0.2487\n",
      "Epoch 167/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2989 - val_loss: -0.2687\n",
      "Epoch 168/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3008 - val_loss: -0.2441\n",
      "Epoch 169/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3009 - val_loss: -0.2596\n",
      "Epoch 170/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.2919 - val_loss: -0.2337\n",
      "Epoch 171/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2956 - val_loss: -0.2540\n",
      "Epoch 172/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3001 - val_loss: -0.2570\n",
      "Epoch 173/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3029 - val_loss: -0.2548\n",
      "Epoch 174/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.2976 - val_loss: -0.2522\n",
      "Epoch 175/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3012 - val_loss: -0.2582\n",
      "Epoch 176/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.2998 - val_loss: -0.2664\n",
      "Epoch 177/10000\n",
      "52735/52735 [==============================] - 1s 20us/sample - loss: -0.3024 - val_loss: -0.2553\n",
      "Epoch 178/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3020 - val_loss: -0.2584\n",
      "Epoch 179/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3027 - val_loss: -0.2518\n",
      "Epoch 180/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3009 - val_loss: -0.2383\n",
      "Epoch 181/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3031 - val_loss: -0.2510\n",
      "Epoch 182/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3060 - val_loss: -0.2569\n",
      "Epoch 183/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3053 - val_loss: -0.2530\n",
      "Epoch 184/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3055 - val_loss: -0.2598\n",
      "Epoch 185/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3067 - val_loss: -0.2543\n",
      "Epoch 186/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3079 - val_loss: -0.2463\n",
      "Epoch 187/10000\n",
      "52735/52735 [==============================] - 1s 11us/sample - loss: -0.3057 - val_loss: -0.2629\n",
      "Epoch 188/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3101 - val_loss: -0.2498\n",
      "Epoch 189/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3073 - val_loss: -0.2585\n",
      "Epoch 190/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3048 - val_loss: -0.2541\n",
      "Epoch 191/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3063 - val_loss: -0.2545\n",
      "Epoch 192/10000\n",
      "52735/52735 [==============================] - 1s 24us/sample - loss: -0.3061 - val_loss: -0.2098\n",
      "Epoch 193/10000\n",
      "52735/52735 [==============================] - 2s 31us/sample - loss: -0.3019 - val_loss: -0.2217\n",
      "Epoch 194/10000\n",
      "52735/52735 [==============================] - 1s 21us/sample - loss: -0.3110 - val_loss: -0.2570\n",
      "Epoch 195/10000\n",
      "52735/52735 [==============================] - 1s 18us/sample - loss: -0.3097 - val_loss: -0.2514\n",
      "Epoch 196/10000\n",
      "52735/52735 [==============================] - 1s 19us/sample - loss: -0.3092 - val_loss: -0.2547\n",
      "Epoch 197/10000\n",
      "52735/52735 [==============================] - 1s 17us/sample - loss: -0.3126 - val_loss: -0.2511\n",
      "Epoch 198/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3111 - val_loss: -0.2519\n",
      "Epoch 199/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3112 - val_loss: -0.2417\n",
      "Epoch 200/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3128 - val_loss: -0.2554\n",
      "Epoch 201/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3113 - val_loss: -0.2483\n",
      "Epoch 202/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3126 - val_loss: -0.2618\n",
      "Epoch 203/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3127 - val_loss: -0.2434\n",
      "Epoch 204/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3146 - val_loss: -0.2403\n",
      "Epoch 205/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3148 - val_loss: -0.2434\n",
      "Epoch 206/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3135 - val_loss: -0.2449\n",
      "Epoch 207/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3125 - val_loss: -0.2568\n",
      "Epoch 208/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3135 - val_loss: -0.2376\n",
      "Epoch 209/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3127 - val_loss: -0.2451\n",
      "Epoch 210/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3164 - val_loss: -0.2517\n",
      "Epoch 211/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3186 - val_loss: -0.2512\n",
      "Epoch 212/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3161 - val_loss: -0.2487\n",
      "Epoch 213/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3159 - val_loss: -0.2517\n",
      "Epoch 214/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3180 - val_loss: -0.2418\n",
      "Epoch 215/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3223 - val_loss: -0.2494\n",
      "Epoch 216/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3227 - val_loss: -0.2486\n",
      "Epoch 217/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3194 - val_loss: -0.2419\n",
      "Epoch 218/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3203 - val_loss: -0.2491\n",
      "Epoch 219/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3219 - val_loss: -0.2461\n",
      "Epoch 220/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3228 - val_loss: -0.2476\n",
      "Epoch 221/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3218 - val_loss: -0.2452\n",
      "Epoch 222/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3222 - val_loss: -0.2484\n",
      "Epoch 223/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3218 - val_loss: -0.2376\n",
      "Epoch 224/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3186 - val_loss: -0.2475\n",
      "Epoch 225/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3209 - val_loss: -0.2484\n",
      "Epoch 226/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3246 - val_loss: -0.2400\n",
      "Epoch 227/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3199 - val_loss: -0.2525\n",
      "Epoch 228/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3232 - val_loss: -0.2458\n",
      "Epoch 229/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3212 - val_loss: -0.2487\n",
      "Epoch 230/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3252 - val_loss: -0.2471\n",
      "Epoch 231/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3250 - val_loss: -0.2389\n",
      "Epoch 232/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3242 - val_loss: -0.2473\n",
      "Epoch 233/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3224 - val_loss: -0.2202\n",
      "Epoch 234/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3242 - val_loss: -0.2303\n",
      "Epoch 235/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3255 - val_loss: -0.2430\n",
      "Epoch 236/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3259 - val_loss: -0.2315\n",
      "Epoch 237/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3266 - val_loss: -0.2475\n",
      "Epoch 238/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3265 - val_loss: -0.2422\n",
      "Epoch 239/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3270 - val_loss: -0.2374\n",
      "Epoch 240/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3339 - val_loss: -0.2414\n",
      "Epoch 241/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3288 - val_loss: -0.2390\n",
      "Epoch 242/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3317 - val_loss: -0.2345\n",
      "Epoch 243/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3288 - val_loss: -0.2291\n",
      "Epoch 244/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3290 - val_loss: -0.2444\n",
      "Epoch 245/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3288 - val_loss: -0.2428\n",
      "Epoch 246/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3235 - val_loss: -0.2390\n",
      "Epoch 247/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3298 - val_loss: -0.2394\n",
      "Epoch 248/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3331 - val_loss: -0.2416\n",
      "Epoch 249/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3299 - val_loss: -0.2405\n",
      "Epoch 250/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3339 - val_loss: -0.2391\n",
      "Epoch 251/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3328 - val_loss: -0.2369\n",
      "Epoch 252/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3312 - val_loss: -0.2413\n",
      "Epoch 253/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3346 - val_loss: -0.2287\n",
      "Epoch 254/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3363 - val_loss: -0.2307\n",
      "Epoch 255/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3288 - val_loss: -0.2380\n",
      "Epoch 256/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3358 - val_loss: -0.2261\n",
      "Epoch 257/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3327 - val_loss: -0.2368\n",
      "Epoch 258/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3368 - val_loss: -0.2360\n",
      "Epoch 259/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3336 - val_loss: -0.2371\n",
      "Epoch 260/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3361 - val_loss: -0.2294\n",
      "Epoch 261/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3332 - val_loss: -0.2198\n",
      "Epoch 262/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3375 - val_loss: -0.2323\n",
      "Epoch 263/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3381 - val_loss: -0.2331\n",
      "Epoch 264/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3370 - val_loss: -0.2264\n",
      "Epoch 265/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3368 - val_loss: -0.2293\n",
      "Epoch 266/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3364 - val_loss: -0.2250\n",
      "Epoch 267/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3416 - val_loss: -0.2351\n",
      "Epoch 268/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3419 - val_loss: -0.2260\n",
      "Epoch 269/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3354 - val_loss: -0.2275\n",
      "Epoch 270/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3377 - val_loss: -0.2361\n",
      "Epoch 271/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3423 - val_loss: -0.2224\n",
      "Epoch 272/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3437 - val_loss: -0.2309\n",
      "Epoch 273/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3397 - val_loss: -0.2309\n",
      "Epoch 274/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3397 - val_loss: -0.2400\n",
      "Epoch 275/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3414 - val_loss: -0.2321\n",
      "Epoch 276/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3411 - val_loss: -0.2119\n",
      "Epoch 277/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3440 - val_loss: -0.2395\n",
      "Epoch 278/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3446 - val_loss: -0.2313\n",
      "Epoch 279/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3463 - val_loss: -0.2267\n",
      "Epoch 280/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3457 - val_loss: -0.2320\n",
      "Epoch 281/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3422 - val_loss: -0.2314\n",
      "Epoch 282/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3422 - val_loss: -0.2200\n",
      "Epoch 283/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3462 - val_loss: -0.2216\n",
      "Epoch 284/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3428 - val_loss: -0.2183\n",
      "Epoch 285/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3457 - val_loss: -0.2183\n",
      "Epoch 286/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3498 - val_loss: -0.2156\n",
      "Epoch 287/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3458 - val_loss: -0.2242\n",
      "Epoch 288/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3489 - val_loss: -0.2220\n",
      "Epoch 289/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3517 - val_loss: -0.2215\n",
      "Epoch 290/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3555 - val_loss: -0.2212\n",
      "Epoch 291/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3471 - val_loss: -0.2263\n",
      "Epoch 292/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3504 - val_loss: -0.2290\n",
      "Epoch 293/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3484 - val_loss: -0.2209\n",
      "Epoch 294/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3481 - val_loss: -0.2281\n",
      "Epoch 295/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3520 - val_loss: -0.2200\n",
      "Epoch 296/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3555 - val_loss: -0.2168\n",
      "Epoch 297/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3500 - val_loss: -0.2164\n",
      "Epoch 298/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3556 - val_loss: -0.2218\n",
      "Epoch 299/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3516 - val_loss: -0.2145\n",
      "Epoch 300/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3543 - val_loss: -0.2226\n",
      "Epoch 301/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3547 - val_loss: -0.2224\n",
      "Epoch 302/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3539 - val_loss: -0.2181\n",
      "Epoch 303/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3563 - val_loss: -0.2161\n",
      "Epoch 304/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3552 - val_loss: -0.2175\n",
      "Epoch 305/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3551 - val_loss: -0.1989\n",
      "Epoch 306/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3592 - val_loss: -0.2197\n",
      "Epoch 307/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3567 - val_loss: -0.2135\n",
      "Epoch 308/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3584 - val_loss: -0.2185\n",
      "Epoch 309/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3560 - val_loss: -0.2124\n",
      "Epoch 310/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3584 - val_loss: -0.2047\n",
      "Epoch 311/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3626 - val_loss: -0.2045\n",
      "Epoch 312/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3600 - val_loss: -0.2166\n",
      "Epoch 313/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3584 - val_loss: -0.2078\n",
      "Epoch 314/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3597 - val_loss: -0.2163\n",
      "Epoch 315/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3612 - val_loss: -0.2136\n",
      "Epoch 316/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3646 - val_loss: -0.2017\n",
      "Epoch 317/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3620 - val_loss: -0.2073\n",
      "Epoch 318/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3630 - val_loss: -0.2159\n",
      "Epoch 319/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3623 - val_loss: -0.2054\n",
      "Epoch 320/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3629 - val_loss: -0.2041\n",
      "Epoch 321/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3651 - val_loss: -0.1983\n",
      "Epoch 322/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3623 - val_loss: -0.1984\n",
      "Epoch 323/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3652 - val_loss: -0.2123\n",
      "Epoch 324/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3665 - val_loss: -0.2081\n",
      "Epoch 325/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3669 - val_loss: -0.2038\n",
      "Epoch 326/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3702 - val_loss: -0.1937\n",
      "Epoch 327/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3636 - val_loss: -0.2069\n",
      "Epoch 328/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3697 - val_loss: -0.1884\n",
      "Epoch 329/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3671 - val_loss: -0.2105\n",
      "Epoch 330/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3662 - val_loss: -0.1899\n",
      "Epoch 331/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3714 - val_loss: -0.1925\n",
      "Epoch 332/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3728 - val_loss: -0.1890\n",
      "Epoch 333/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3726 - val_loss: -0.2006\n",
      "Epoch 334/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3749 - val_loss: -0.1894\n",
      "Epoch 335/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3712 - val_loss: -0.1917\n",
      "Epoch 336/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3754 - val_loss: -0.1970\n",
      "Epoch 337/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3735 - val_loss: -0.1888\n",
      "Epoch 338/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3739 - val_loss: -0.2012\n",
      "Epoch 339/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3733 - val_loss: -0.1740\n",
      "Epoch 340/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3750 - val_loss: -0.1839\n",
      "Epoch 341/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3750 - val_loss: -0.1979\n",
      "Epoch 342/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3761 - val_loss: -0.1969\n",
      "Epoch 343/10000\n",
      "52735/52735 [==============================] - 1s 15us/sample - loss: -0.3747 - val_loss: -0.1926\n",
      "Epoch 344/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3799 - val_loss: -0.1926\n",
      "Epoch 345/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3783 - val_loss: -0.1995\n",
      "Epoch 346/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3782 - val_loss: -0.1939\n",
      "Epoch 347/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3792 - val_loss: -0.1895\n",
      "Epoch 348/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3774 - val_loss: -0.1978\n",
      "Epoch 349/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3821 - val_loss: -0.1877\n",
      "Epoch 350/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3833 - val_loss: -0.1751\n",
      "Epoch 351/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3818 - val_loss: -0.1878\n",
      "Epoch 352/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3845 - val_loss: -0.1851\n",
      "Epoch 353/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3824 - val_loss: -0.1785\n",
      "Epoch 354/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3815 - val_loss: -0.1770\n",
      "Epoch 355/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3862 - val_loss: -0.1754\n",
      "Epoch 356/10000\n",
      "52735/52735 [==============================] - 1s 14us/sample - loss: -0.3836 - val_loss: -0.1717\n",
      "Epoch 357/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3861 - val_loss: -0.1809\n",
      "Epoch 358/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3879 - val_loss: -0.1745\n",
      "Epoch 359/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3836 - val_loss: -0.1795\n",
      "Epoch 360/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3840 - val_loss: -0.1732\n",
      "Epoch 361/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3858 - val_loss: -0.1742\n",
      "Epoch 362/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3843 - val_loss: -0.1813\n",
      "Epoch 363/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3908 - val_loss: -0.1806\n",
      "Epoch 364/10000\n",
      "52735/52735 [==============================] - 1s 12us/sample - loss: -0.3857 - val_loss: -0.1682\n",
      "Epoch 365/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3845 - val_loss: -0.1757\n",
      "Epoch 366/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3880 - val_loss: -0.1745\n",
      "Epoch 367/10000\n",
      "52735/52735 [==============================] - 1s 13us/sample - loss: -0.3851 - val_loss: -0.1746\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments/591_all_final_evaluation.yaml\" # Replace the yaml\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_10_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 10,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2675, 12, 1)\n",
      "y_test.shape:  (2675, 1)\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 21:03:50,114 WARNING Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\all_final_experiment\\nb_future_steps_6_seed_10_\\model.hdf5\n",
      "patient id:  591\n",
      "Model Performance Metrics:\n",
      "-------------------------\n",
      "RMSE: 20.20\n",
      "MSE:  408.18\n",
      "MAPE: 11.60%\n",
      "\n",
      "Baseline (t0) Performance:\n",
      "-------------------------\n",
      "t0 RMSE: 24.25\n",
      "t0 MSE:  588.03\n",
      "t0 MAPE: 13.61%\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_25_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 25,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2675, 12, 1)\n",
      "y_test.shape:  (2675, 1)\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 21:03:52,271 WARNING Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\all_final_experiment\\nb_future_steps_6_seed_25_\\model.hdf5\n",
      "patient id:  591\n",
      "Model Performance Metrics:\n",
      "-------------------------\n",
      "RMSE: 20.31\n",
      "MSE:  412.68\n",
      "MAPE: 11.69%\n",
      "\n",
      "Baseline (t0) Performance:\n",
      "-------------------------\n",
      "t0 RMSE: 24.25\n",
      "t0 MSE:  588.03\n",
      "t0 MAPE: 13.61%\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_50_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [10, 25, 50],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 50,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2675, 12, 1)\n",
      "y_test.shape:  (2675, 1)\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-11-27 21:03:54,219 WARNING Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\all_final_experiment\\nb_future_steps_6_seed_50_\\model.hdf5\n",
      "patient id:  591\n",
      "Model Performance Metrics:\n",
      "-------------------------\n",
      "RMSE: 20.19\n",
      "MSE:  407.78\n",
      "MAPE: 11.36%\n",
      "\n",
      "Baseline (t0) Performance:\n",
      "-------------------------\n",
      "t0 RMSE: 24.25\n",
      "t0 MSE:  588.03\n",
      "t0 MAPE: 13.61%\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
