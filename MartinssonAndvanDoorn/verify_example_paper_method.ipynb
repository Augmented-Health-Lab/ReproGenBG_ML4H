{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 00:05:58,206 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2024-12-10 00:05:58,210 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-12-10 00:05:58,213 DEBUG interactive is False\n",
      "2024-12-10 00:05:58,214 DEBUG platform is win32\n",
      "2024-12-10 00:05:58,254 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2024-12-10 00:05:58,257 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments_90min/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # Calculate SEG\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # Calculate baseline (t0) metrics\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    t0_mse = np.mean((y_test - t0) ** 2)\n",
    "    t0_mape = np.mean(np.abs((y_test - t0) / y_test)) * 100\n",
    "    t0_mae = np.mean(np.abs(y_test - t0))\n",
    "    \n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mse))\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mape))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # # Calculate MAE\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_mae))\n",
    "\n",
    "    # Print all metrics\n",
    "    # print(\"Model Performance Metrics:\")\n",
    "    # print(\"-\" * 25)\n",
    "    # print(f\"RMSE: {rmse:.2f}\")\n",
    "    # print(f\"MSE:  {mse:.2f}\")\n",
    "    # print(f\"MAPE: {mape:.2f}%\")\n",
    "    # # print(f\"SEG:  {seg:.2f}\")\n",
    "    # print(\"\\nBaseline (t0) Performance:\")\n",
    "    # print(\"-\" * 25)\n",
    "    # print(f\"t0 RMSE: {t0_rmse:.2f}\")\n",
    "    # print(f\"t0 MSE:  {t0_mse:.2f}\")\n",
    "    # print(f\"t0 MAPE: {t0_mape:.2f}%\")\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'xml_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['xml_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 00:10:09,899 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\OhioT1DM 2020\\both\\all does not exist.\n",
      "2024-12-10 00:10:09,900 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh does not exist.\n",
      "Running 1 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2024-12-10 00:10:09,904 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\both\\\\all'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (99721, 18, 1)\n",
      "y_train.shape:  (99721, 1)\n",
      "x_valid.shape:  (24923, 18, 1)\n",
      "y_valid.shape:  (24923, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:10:20,566 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-12-10 00:10:20,710 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-12-10 00:10:20,713 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-12-10 00:10:20,724 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 99721 samples, validate on 24923 samples\n",
      "Epoch 1/10000\n",
      "99328/99721 [============================>.] - ETA: 0s - loss: 0.4295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99721/99721 [==============================] - 2s 25us/sample - loss: 0.4284 - val_loss: 0.0727\n",
      "Epoch 2/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: 0.0865 - val_loss: -0.0256\n",
      "Epoch 3/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: 0.0315 - val_loss: 0.0308\n",
      "Epoch 4/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.0026 - val_loss: -0.1060\n",
      "Epoch 5/10000\n",
      "99328/99721 [============================>.] - ETA: 0s - loss: -0.03112024-12-10 00:10:30,369 DEBUG Creating converter from 5 to 3\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.0311 - val_loss: -0.1487\n",
      "Epoch 6/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.0598 - val_loss: -0.1259\n",
      "Epoch 7/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.0751 - val_loss: -0.1337\n",
      "Epoch 8/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.0839 - val_loss: -0.1694\n",
      "Epoch 9/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.0960 - val_loss: -0.1814\n",
      "Epoch 10/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1126 - val_loss: -0.1615\n",
      "Epoch 11/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1159 - val_loss: -0.1559\n",
      "Epoch 12/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1246 - val_loss: -0.1803\n",
      "Epoch 13/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1325 - val_loss: -0.1990\n",
      "Epoch 14/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1368 - val_loss: -0.1670\n",
      "Epoch 15/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1443 - val_loss: -0.1852\n",
      "Epoch 16/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1500 - val_loss: -0.1920\n",
      "Epoch 17/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1551 - val_loss: -0.1959\n",
      "Epoch 18/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1583 - val_loss: -0.2156\n",
      "Epoch 19/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1631 - val_loss: -0.1874\n",
      "Epoch 20/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1652 - val_loss: -0.2210\n",
      "Epoch 21/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1729 - val_loss: -0.1773\n",
      "Epoch 22/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1742 - val_loss: -0.2194\n",
      "Epoch 23/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1812 - val_loss: -0.2342\n",
      "Epoch 24/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1837 - val_loss: -0.2066\n",
      "Epoch 25/10000\n",
      "99721/99721 [==============================] - 1s 15us/sample - loss: -0.1873 - val_loss: -0.2170\n",
      "Epoch 26/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.1907 - val_loss: -0.2186\n",
      "Epoch 27/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1938 - val_loss: -0.2162\n",
      "Epoch 28/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.1948 - val_loss: -0.2246\n",
      "Epoch 29/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.1998 - val_loss: -0.2317\n",
      "Epoch 30/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2032 - val_loss: -0.2242\n",
      "Epoch 31/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2046 - val_loss: -0.2186\n",
      "Epoch 32/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2059 - val_loss: -0.2463\n",
      "Epoch 33/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2081 - val_loss: -0.1756\n",
      "Epoch 34/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2124 - val_loss: -0.2405\n",
      "Epoch 35/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2146 - val_loss: -0.2343\n",
      "Epoch 36/10000\n",
      "99721/99721 [==============================] - 2s 15us/sample - loss: -0.2187 - val_loss: -0.2406\n",
      "Epoch 37/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2185 - val_loss: -0.2259\n",
      "Epoch 38/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2206 - val_loss: -0.2230\n",
      "Epoch 39/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2247 - val_loss: -0.2415\n",
      "Epoch 40/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2271 - val_loss: -0.2087\n",
      "Epoch 41/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2255 - val_loss: -0.2374\n",
      "Epoch 42/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2321 - val_loss: -0.2310\n",
      "Epoch 43/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.2344 - val_loss: -0.2560\n",
      "Epoch 44/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2364 - val_loss: -0.2382\n",
      "Epoch 45/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2364 - val_loss: -0.2554\n",
      "Epoch 46/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2425 - val_loss: -0.2438\n",
      "Epoch 47/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2401 - val_loss: -0.2258\n",
      "Epoch 48/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2430 - val_loss: -0.2323\n",
      "Epoch 49/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2448 - val_loss: -0.2546\n",
      "Epoch 50/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2492 - val_loss: -0.2464\n",
      "Epoch 51/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2465 - val_loss: -0.2420\n",
      "Epoch 52/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2500 - val_loss: -0.2513\n",
      "Epoch 53/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2546 - val_loss: -0.2497\n",
      "Epoch 54/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2539 - val_loss: -0.2470\n",
      "Epoch 55/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2538 - val_loss: -0.2555\n",
      "Epoch 56/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2546 - val_loss: -0.2499\n",
      "Epoch 57/10000\n",
      "99721/99721 [==============================] - 2s 15us/sample - loss: -0.2587 - val_loss: -0.2545\n",
      "Epoch 58/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2591 - val_loss: -0.2332\n",
      "Epoch 59/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2604 - val_loss: -0.2626\n",
      "Epoch 60/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2649 - val_loss: -0.2586\n",
      "Epoch 61/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2620 - val_loss: -0.2442\n",
      "Epoch 62/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2671 - val_loss: -0.2420\n",
      "Epoch 63/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2660 - val_loss: -0.2397\n",
      "Epoch 64/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2659 - val_loss: -0.2460\n",
      "Epoch 65/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2724 - val_loss: -0.2508\n",
      "Epoch 66/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2715 - val_loss: -0.2582\n",
      "Epoch 67/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2727 - val_loss: -0.2299\n",
      "Epoch 68/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2725 - val_loss: -0.2390\n",
      "Epoch 69/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2724 - val_loss: -0.2475\n",
      "Epoch 70/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2748 - val_loss: -0.2521\n",
      "Epoch 71/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2780 - val_loss: -0.2568\n",
      "Epoch 72/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2757 - val_loss: -0.2541\n",
      "Epoch 73/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2776 - val_loss: -0.2540\n",
      "Epoch 74/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2760 - val_loss: -0.2275\n",
      "Epoch 75/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2785 - val_loss: -0.2549\n",
      "Epoch 76/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.2782 - val_loss: -0.2555\n",
      "Epoch 77/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2834 - val_loss: -0.2422\n",
      "Epoch 78/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2842 - val_loss: -0.2619\n",
      "Epoch 79/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2834 - val_loss: -0.2468\n",
      "Epoch 80/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2869 - val_loss: -0.2527\n",
      "Epoch 81/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2839 - val_loss: -0.2526\n",
      "Epoch 82/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2874 - val_loss: -0.2499\n",
      "Epoch 83/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2877 - val_loss: -0.2533\n",
      "Epoch 84/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2906 - val_loss: -0.2339\n",
      "Epoch 85/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2901 - val_loss: -0.2365\n",
      "Epoch 86/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2889 - val_loss: -0.2514\n",
      "Epoch 87/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2921 - val_loss: -0.2517\n",
      "Epoch 88/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2909 - val_loss: -0.2414\n",
      "Epoch 89/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2901 - val_loss: -0.2496\n",
      "Epoch 90/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2923 - val_loss: -0.2492\n",
      "Epoch 91/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2962 - val_loss: -0.2508\n",
      "Epoch 92/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2935 - val_loss: -0.2282\n",
      "Epoch 93/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2941 - val_loss: -0.2444\n",
      "Epoch 94/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2953 - val_loss: -0.2483\n",
      "Epoch 95/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.2974 - val_loss: -0.2502\n",
      "Epoch 96/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2972 - val_loss: -0.2475\n",
      "Epoch 97/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3015 - val_loss: -0.2506\n",
      "Epoch 98/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3011 - val_loss: -0.2465\n",
      "Epoch 99/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.2997 - val_loss: -0.2516\n",
      "Epoch 100/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3015 - val_loss: -0.2424\n",
      "Epoch 101/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3018 - val_loss: -0.2326\n",
      "Epoch 102/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3045 - val_loss: -0.2422\n",
      "Epoch 103/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3031 - val_loss: -0.2466\n",
      "Epoch 104/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3054 - val_loss: -0.2417\n",
      "Epoch 105/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3046 - val_loss: -0.2470\n",
      "Epoch 106/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3072 - val_loss: -0.2379\n",
      "Epoch 107/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3045 - val_loss: -0.2371\n",
      "Epoch 108/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3030 - val_loss: -0.2454\n",
      "Epoch 109/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3077 - val_loss: -0.2408\n",
      "Epoch 110/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3087 - val_loss: -0.2461\n",
      "Epoch 111/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3085 - val_loss: -0.2446\n",
      "Epoch 112/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3077 - val_loss: -0.2307\n",
      "Epoch 113/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3111 - val_loss: -0.2412\n",
      "Epoch 114/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3121 - val_loss: -0.2419\n",
      "Epoch 115/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3118 - val_loss: -0.2370\n",
      "Epoch 116/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3096 - val_loss: -0.2342\n",
      "Epoch 117/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3167 - val_loss: -0.2369\n",
      "Epoch 118/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3130 - val_loss: -0.2405\n",
      "Epoch 119/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3138 - val_loss: -0.2278\n",
      "Epoch 120/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3128 - val_loss: -0.2219\n",
      "Epoch 121/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3160 - val_loss: -0.2458\n",
      "Epoch 122/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3196 - val_loss: -0.2344\n",
      "Epoch 123/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3164 - val_loss: -0.2363\n",
      "Epoch 124/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3206 - val_loss: -0.2294\n",
      "Epoch 125/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3177 - val_loss: -0.2215\n",
      "Epoch 126/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3230 - val_loss: -0.2250\n",
      "Epoch 127/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3248 - val_loss: -0.2244\n",
      "Epoch 128/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3224 - val_loss: -0.2320\n",
      "Epoch 129/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3235 - val_loss: -0.2331\n",
      "Epoch 130/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3267 - val_loss: -0.2203\n",
      "Epoch 131/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3239 - val_loss: -0.2261\n",
      "Epoch 132/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3279 - val_loss: -0.2248\n",
      "Epoch 133/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3272 - val_loss: -0.2296\n",
      "Epoch 134/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3293 - val_loss: -0.2265\n",
      "Epoch 135/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3294 - val_loss: -0.2318\n",
      "Epoch 136/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3264 - val_loss: -0.2182\n",
      "Epoch 137/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3314 - val_loss: -0.2170\n",
      "Epoch 138/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3345 - val_loss: -0.2215\n",
      "Epoch 139/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3332 - val_loss: -0.2209\n",
      "Epoch 140/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3333 - val_loss: -0.2259\n",
      "Epoch 141/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3347 - val_loss: -0.2237\n",
      "Epoch 142/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3373 - val_loss: -0.2235\n",
      "Epoch 143/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3401 - val_loss: -0.2263\n",
      "Epoch 144/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3371 - val_loss: -0.2249\n",
      "Epoch 145/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3420 - val_loss: -0.2141\n",
      "Epoch 146/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3438 - val_loss: -0.2083\n",
      "Epoch 147/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3423 - val_loss: -0.2151\n",
      "Epoch 148/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3433 - val_loss: -0.2063\n",
      "Epoch 149/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3451 - val_loss: -0.2079\n",
      "Epoch 150/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.3446 - val_loss: -0.2023\n",
      "Epoch 151/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3483 - val_loss: -0.2105\n",
      "Epoch 152/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3460 - val_loss: -0.2143\n",
      "Epoch 153/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3483 - val_loss: -0.2059\n",
      "Epoch 154/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3504 - val_loss: -0.2109\n",
      "Epoch 155/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3521 - val_loss: -0.2049\n",
      "Epoch 156/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3534 - val_loss: -0.2026\n",
      "Epoch 157/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3535 - val_loss: -0.1945\n",
      "Epoch 158/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3547 - val_loss: -0.2009\n",
      "Epoch 159/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3543 - val_loss: -0.1944\n",
      "Epoch 160/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3564 - val_loss: -0.1981\n",
      "Epoch 161/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3587 - val_loss: -0.1897\n",
      "Epoch 162/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3598 - val_loss: -0.1946\n",
      "Epoch 163/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3618 - val_loss: -0.1962\n",
      "Epoch 164/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3634 - val_loss: -0.1882\n",
      "Epoch 165/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3652 - val_loss: -0.1878\n",
      "Epoch 166/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3628 - val_loss: -0.1901\n",
      "Epoch 167/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3680 - val_loss: -0.1847\n",
      "Epoch 168/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3706 - val_loss: -0.1853\n",
      "Epoch 169/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3705 - val_loss: -0.1889\n",
      "Epoch 170/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.3695 - val_loss: -0.1740\n",
      "Epoch 171/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3719 - val_loss: -0.1759\n",
      "Epoch 172/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3725 - val_loss: -0.1787\n",
      "Epoch 173/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3723 - val_loss: -0.1803\n",
      "Epoch 174/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3799 - val_loss: -0.1769\n",
      "Epoch 175/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3778 - val_loss: -0.1519\n",
      "Epoch 176/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3791 - val_loss: -0.1628\n",
      "Epoch 177/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3778 - val_loss: -0.1717\n",
      "Epoch 178/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3863 - val_loss: -0.1618\n",
      "Epoch 179/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3876 - val_loss: -0.1543\n",
      "Epoch 180/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3864 - val_loss: -0.1499\n",
      "Epoch 181/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.3914 - val_loss: -0.1608\n",
      "Epoch 182/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3904 - val_loss: -0.1637\n",
      "Epoch 183/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.3925 - val_loss: -0.1389\n",
      "Epoch 184/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3919 - val_loss: -0.1521\n",
      "Epoch 185/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.3964 - val_loss: -0.1583\n",
      "Epoch 186/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.3944 - val_loss: -0.1457\n",
      "Epoch 187/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3984 - val_loss: -0.1396\n",
      "Epoch 188/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.3999 - val_loss: -0.1386\n",
      "Epoch 189/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4003 - val_loss: -0.1337\n",
      "Epoch 190/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4040 - val_loss: -0.1517\n",
      "Epoch 191/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4024 - val_loss: -0.1261\n",
      "Epoch 192/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.4034 - val_loss: -0.1422\n",
      "Epoch 193/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4077 - val_loss: -0.1164\n",
      "Epoch 194/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4082 - val_loss: -0.1247\n",
      "Epoch 195/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4120 - val_loss: -0.1244\n",
      "Epoch 196/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4130 - val_loss: -0.1184\n",
      "Epoch 197/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4144 - val_loss: -0.1259\n",
      "Epoch 198/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4161 - val_loss: -0.1104\n",
      "Epoch 199/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4199 - val_loss: -0.0953\n",
      "Epoch 200/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4240 - val_loss: -0.1131\n",
      "Epoch 201/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4279 - val_loss: -0.0871\n",
      "Epoch 202/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4242 - val_loss: -0.0928\n",
      "Epoch 203/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4281 - val_loss: -0.0796\n",
      "Epoch 204/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4292 - val_loss: -0.0834\n",
      "Epoch 205/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4307 - val_loss: -0.0881\n",
      "Epoch 206/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4328 - val_loss: -0.0812\n",
      "Epoch 207/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4340 - val_loss: -0.0839\n",
      "Epoch 208/10000\n",
      "99721/99721 [==============================] - 2s 15us/sample - loss: -0.4375 - val_loss: -0.0706\n",
      "Epoch 209/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4402 - val_loss: -0.0709\n",
      "Epoch 210/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4414 - val_loss: -0.0522\n",
      "Epoch 211/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4413 - val_loss: -0.0715\n",
      "Epoch 212/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4456 - val_loss: -0.0952\n",
      "Epoch 213/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4459 - val_loss: -0.0667\n",
      "Epoch 214/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4500 - val_loss: -0.0550\n",
      "Epoch 215/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4547 - val_loss: -0.0567\n",
      "Epoch 216/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4521 - val_loss: -0.0285\n",
      "Epoch 217/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4570 - val_loss: -0.0508\n",
      "Epoch 218/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4545 - val_loss: -0.0545\n",
      "Epoch 219/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4577 - val_loss: -0.0595\n",
      "Epoch 220/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4639 - val_loss: -0.0295\n",
      "Epoch 221/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4625 - val_loss: -0.0348\n",
      "Epoch 222/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4677 - val_loss: -0.0270\n",
      "Epoch 223/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4710 - val_loss: -0.0210\n",
      "Epoch 224/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4728 - val_loss: -0.0130\n",
      "Epoch 225/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4702 - val_loss: -0.0214\n",
      "Epoch 226/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4812 - val_loss: 0.0138\n",
      "Epoch 227/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4793 - val_loss: 0.0065\n",
      "Epoch 228/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4815 - val_loss: 0.0171\n",
      "Epoch 229/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4816 - val_loss: 0.0245\n",
      "Epoch 230/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4828 - val_loss: 0.0203\n",
      "Epoch 231/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4848 - val_loss: 0.0210\n",
      "Epoch 232/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4919 - val_loss: 0.0284\n",
      "Epoch 233/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4943 - val_loss: 0.0201\n",
      "Epoch 234/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4965 - val_loss: 0.0346\n",
      "Epoch 235/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4994 - val_loss: 0.0595\n",
      "Epoch 236/10000\n",
      "99721/99721 [==============================] - 2s 17us/sample - loss: -0.4994 - val_loss: 0.0153\n",
      "Epoch 237/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4984 - val_loss: 0.0585\n",
      "Epoch 238/10000\n",
      "99721/99721 [==============================] - 2s 16us/sample - loss: -0.4970 - val_loss: 0.0447\n",
      "Epoch 239/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5093 - val_loss: 0.0760\n",
      "Epoch 240/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5107 - val_loss: 0.0783\n",
      "Epoch 241/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5127 - val_loss: 0.0705\n",
      "Epoch 242/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5126 - val_loss: 0.0647\n",
      "Epoch 243/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5145 - val_loss: 0.0221\n",
      "Epoch 244/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5177 - val_loss: 0.0876\n",
      "Epoch 245/10000\n",
      "99721/99721 [==============================] - 2s 21us/sample - loss: -0.5206 - val_loss: 0.1170\n",
      "Epoch 246/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5224 - val_loss: 0.1066\n",
      "Epoch 247/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5268 - val_loss: 0.1164\n",
      "Epoch 248/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5271 - val_loss: 0.1001\n",
      "Epoch 249/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5304 - val_loss: 0.1154\n",
      "Epoch 250/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5332 - val_loss: 0.1380\n",
      "Epoch 251/10000\n",
      "99721/99721 [==============================] - 2s 20us/sample - loss: -0.5337 - val_loss: 0.1362\n",
      "Epoch 252/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5378 - val_loss: 0.1125\n",
      "Epoch 253/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5352 - val_loss: 0.1373\n",
      "Epoch 254/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5471 - val_loss: 0.1239\n",
      "Epoch 255/10000\n",
      "99721/99721 [==============================] - 2s 19us/sample - loss: -0.5452 - val_loss: 0.1420\n",
      "Epoch 256/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5447 - val_loss: 0.1989\n",
      "Epoch 257/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5494 - val_loss: 0.1566\n",
      "Epoch 258/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5547 - val_loss: 0.1784\n",
      "Epoch 259/10000\n",
      "99721/99721 [==============================] - 2s 18us/sample - loss: -0.5523 - val_loss: 0.1371\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./original_ohio_experiments_90min\\\\540_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\544_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\552_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\559_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\563_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\567_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\570_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\575_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\584_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\588_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\591_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\596_all_final_evaluation.yaml',\n",
       " './original_ohio_experiments_90min\\\\all_final_experiment.yaml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Get all yaml files in the directory\n",
    "yaml_files = glob.glob(\"./original_ohio_experiments_90min/*.yaml\")\n",
    "\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\540-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2666, 18, 1)\n",
      "y_test.shape:  (2666, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:44,329 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2024-12-10 00:27:44,454 DEBUG Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  540\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\544-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2532, 18, 1)\n",
      "y_test.shape:  (2532, 1)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:46,164 WARNING Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  544\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\552-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2111, 18, 1)\n",
      "y_test.shape:  (2111, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:47,366 WARNING Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  552\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\559-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2238, 18, 1)\n",
      "y_test.shape:  (2238, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:48,428 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  559\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\563-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2478, 18, 1)\n",
      "y_test.shape:  (2478, 1)\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:49,660 WARNING Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  563\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\567-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2090, 18, 1)\n",
      "y_test.shape:  (2090, 1)\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:50,920 WARNING Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  567\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\570-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2515, 18, 1)\n",
      "y_test.shape:  (2515, 1)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:52,576 WARNING Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  570\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\575-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2346, 18, 1)\n",
      "y_test.shape:  (2346, 1)\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:54,515 WARNING Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  575\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\584-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2297, 18, 1)\n",
      "y_test.shape:  (2297, 1)\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:56,408 WARNING Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  584\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\588-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2722, 18, 1)\n",
      "y_test.shape:  (2722, 1)\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:27:58,244 WARNING Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  588\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\591-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2645, 18, 1)\n",
      "y_test.shape:  (2645, 1)\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:28:00,182 WARNING Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  591\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\OhioT1DM '\n",
      "                               '2020\\\\2020\\\\test\\\\596-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_ohio_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2582, 18, 1)\n",
      "y_test.shape:  (2582, 1)\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-10 00:28:02,180 WARNING Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_ohio_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "patient id:  596\n"
     ]
    }
   ],
   "source": [
    "mode = \"evaluate\"\n",
    "for yaml_filepath in yaml_files[:-1]:\n",
    "    cfgs = load_cfgs(yaml_filepath)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_ohio_experiments/559_all_final_evaluation.yaml\" # Replace the yaml\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\ohio.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0,\n",
      "                   'xml_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\OhioT1DM\\\\2018\\\\test\\\\559-ws-testing.xml'},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\all_final_experiment\\\\nb_future_steps_6_seed_60_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [60],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 60,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (2310, 12, 1)\n",
      "y_test.shape:  (2310, 1)\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-12-09 23:42:15,299 WARNING Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\all_final_experiment\\nb_future_steps_6_seed_60_\\model.hdf5\n",
      "patient id:  559\n",
      "Model Performance Metrics:\n",
      "-------------------------\n",
      "RMSE: 18.62\n",
      "MSE:  346.63\n",
      "MAPE: 8.23%\n",
      "\n",
      "Baseline (t0) Performance:\n",
      "-------------------------\n",
      "t0 RMSE: 23.40\n",
      "t0 MSE:  547.60\n",
      "t0 MAPE: 10.57%\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
