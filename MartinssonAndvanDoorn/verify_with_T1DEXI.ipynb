{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 13:04:12,830 DEBUG matplotlib data path: c:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2025-01-21 13:04:12,830 DEBUG CONFIGDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-21 13:04:12,830 DEBUG interactive is False\n",
      "2025-01-21 13:04:12,830 DEBUG platform is win32\n",
      "2025-01-21 13:04:12,865 DEBUG CACHEDIR=C:\\Users\\baiyi\\.matplotlib\n",
      "2025-01-21 13:04:12,874 DEBUG Using fontManager instance from C:\\Users\\baiyi\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mae.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mae))\n",
    "\n",
    "    # Calculate MSE\n",
    "    # mse = np.mean((y_test - y_pred) ** 2)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(mse))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Multiply by 100 for percentage\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mape.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(mape))\n",
    "\n",
    "    # rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    # print(\"patient id: \", patient_id)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    # seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    # t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    # t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    # with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "    #     outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    # print(\"RMSE: \", rmse)\n",
    "    # print(\"t0 RMSE: \", t0_rmse)\n",
    "    # print(\"SEG: \", seg)\n",
    "    # print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling horizon turning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 13:04:23,210 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2025-01-21 13:04:23,213 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (301784, 6, 1)\n",
      "y_train.shape:  (301784, 1)\n",
      "x_valid.shape:  (75422, 6, 1)\n",
      "y_valid.shape:  (75422, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:05:02,565 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-21 13:05:02,773 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-21 13:05:02,775 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 13:05:02,784 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 301784 samples, validate on 75422 samples\n",
      "Epoch 1/10000\n",
      "299008/301784 [============================>.] - ETA: 0s - loss: 0.1559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301784/301784 [==============================] - 3s 9us/sample - loss: 0.1546 - val_loss: -0.1055\n",
      "Epoch 2/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.0555 - val_loss: -0.1502\n",
      "Epoch 3/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.0965 - val_loss: -0.1766\n",
      "Epoch 4/10000\n",
      "301784/301784 [==============================] - 2s 7us/sample - loss: -0.1196 - val_loss: -0.1815\n",
      "Epoch 5/10000\n",
      "301056/301784 [============================>.] - ETA: 0s - loss: -0.14172025-01-21 13:05:15,786 DEBUG Creating converter from 5 to 3\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.1418 - val_loss: -0.2048\n",
      "Epoch 6/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.1587 - val_loss: -0.1925\n",
      "Epoch 7/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.1711 - val_loss: -0.2346\n",
      "Epoch 8/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.1801 - val_loss: -0.2385\n",
      "Epoch 9/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.1906 - val_loss: -0.2362\n",
      "Epoch 10/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.1981 - val_loss: -0.2352\n",
      "Epoch 11/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2007 - val_loss: -0.2321\n",
      "Epoch 12/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2077 - val_loss: -0.2402\n",
      "Epoch 13/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2148 - val_loss: -0.2393\n",
      "Epoch 14/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2219 - val_loss: -0.2536\n",
      "Epoch 15/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2233 - val_loss: -0.2566\n",
      "Epoch 16/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2291 - val_loss: -0.2549\n",
      "Epoch 17/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2333 - val_loss: -0.2384\n",
      "Epoch 18/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2360 - val_loss: -0.2554\n",
      "Epoch 19/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2396 - val_loss: -0.2330\n",
      "Epoch 20/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2409 - val_loss: -0.2585\n",
      "Epoch 21/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2442 - val_loss: -0.2449\n",
      "Epoch 22/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2463 - val_loss: -0.2531\n",
      "Epoch 23/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2496 - val_loss: -0.2628\n",
      "Epoch 24/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2519 - val_loss: -0.2613\n",
      "Epoch 25/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2523 - val_loss: -0.2608\n",
      "Epoch 26/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2523 - val_loss: -0.2548\n",
      "Epoch 27/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2563 - val_loss: -0.2645\n",
      "Epoch 28/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2562 - val_loss: -0.2623\n",
      "Epoch 29/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2591 - val_loss: -0.2694\n",
      "Epoch 30/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2567 - val_loss: -0.2541\n",
      "Epoch 31/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2592 - val_loss: -0.2649\n",
      "Epoch 32/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2604 - val_loss: -0.2672\n",
      "Epoch 33/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2615 - val_loss: -0.2613\n",
      "Epoch 34/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2620 - val_loss: -0.2649\n",
      "Epoch 35/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2623 - val_loss: -0.2588\n",
      "Epoch 36/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2631 - val_loss: -0.2681\n",
      "Epoch 37/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2638 - val_loss: -0.2677\n",
      "Epoch 38/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2650 - val_loss: -0.2668\n",
      "Epoch 39/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2640 - val_loss: -0.2703\n",
      "Epoch 40/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2645 - val_loss: -0.2627\n",
      "Epoch 41/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2656 - val_loss: -0.2617\n",
      "Epoch 42/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2657 - val_loss: -0.2639\n",
      "Epoch 43/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2664 - val_loss: -0.2589\n",
      "Epoch 44/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2673 - val_loss: -0.2517\n",
      "Epoch 45/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2668 - val_loss: -0.2678\n",
      "Epoch 46/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2685 - val_loss: -0.2620\n",
      "Epoch 47/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2689 - val_loss: -0.2682\n",
      "Epoch 48/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2692 - val_loss: -0.2648\n",
      "Epoch 49/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2693 - val_loss: -0.2658\n",
      "Epoch 50/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2693 - val_loss: -0.2668\n",
      "Epoch 51/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2697 - val_loss: -0.2709\n",
      "Epoch 52/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2708 - val_loss: -0.2674\n",
      "Epoch 53/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2704 - val_loss: -0.2730\n",
      "Epoch 54/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2710 - val_loss: -0.2544\n",
      "Epoch 55/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2717 - val_loss: -0.2706\n",
      "Epoch 56/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2730 - val_loss: -0.2676\n",
      "Epoch 57/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2729 - val_loss: -0.2746\n",
      "Epoch 58/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2750 - val_loss: -0.2736\n",
      "Epoch 59/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2732 - val_loss: -0.2683\n",
      "Epoch 60/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2741 - val_loss: -0.2762\n",
      "Epoch 61/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2740 - val_loss: -0.2731\n",
      "Epoch 62/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2740 - val_loss: -0.2714\n",
      "Epoch 63/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2746 - val_loss: -0.2704\n",
      "Epoch 64/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2758 - val_loss: -0.2697\n",
      "Epoch 65/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2745 - val_loss: -0.2699\n",
      "Epoch 66/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2750 - val_loss: -0.2691\n",
      "Epoch 67/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2763 - val_loss: -0.2741\n",
      "Epoch 68/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2717\n",
      "Epoch 69/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2774 - val_loss: -0.2766\n",
      "Epoch 70/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2766 - val_loss: -0.2681\n",
      "Epoch 71/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2757 - val_loss: -0.2664\n",
      "Epoch 72/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2759 - val_loss: -0.2673\n",
      "Epoch 73/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2759 - val_loss: -0.2724\n",
      "Epoch 74/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2772 - val_loss: -0.2709\n",
      "Epoch 75/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2749 - val_loss: -0.2768\n",
      "Epoch 76/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2769 - val_loss: -0.2683\n",
      "Epoch 77/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2760 - val_loss: -0.2688\n",
      "Epoch 78/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2760 - val_loss: -0.2766\n",
      "Epoch 79/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2780 - val_loss: -0.2726\n",
      "Epoch 80/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2766 - val_loss: -0.2717\n",
      "Epoch 81/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2781 - val_loss: -0.2757\n",
      "Epoch 82/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2775 - val_loss: -0.2732\n",
      "Epoch 83/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2783 - val_loss: -0.2724\n",
      "Epoch 84/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2690\n",
      "Epoch 85/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2708\n",
      "Epoch 86/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2783 - val_loss: -0.2659\n",
      "Epoch 87/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2775 - val_loss: -0.2736\n",
      "Epoch 88/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2794 - val_loss: -0.2769\n",
      "Epoch 89/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2698\n",
      "Epoch 90/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2647\n",
      "Epoch 91/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2738\n",
      "Epoch 92/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2794 - val_loss: -0.2728\n",
      "Epoch 93/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2735\n",
      "Epoch 94/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2796 - val_loss: -0.2755\n",
      "Epoch 95/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2736\n",
      "Epoch 96/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2787 - val_loss: -0.2569\n",
      "Epoch 97/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2794 - val_loss: -0.2756\n",
      "Epoch 98/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2795 - val_loss: -0.2748\n",
      "Epoch 99/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2786 - val_loss: -0.2766\n",
      "Epoch 100/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2800 - val_loss: -0.2768\n",
      "Epoch 101/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2734\n",
      "Epoch 102/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2790 - val_loss: -0.2780\n",
      "Epoch 103/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2800 - val_loss: -0.2785\n",
      "Epoch 104/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2799 - val_loss: -0.2770\n",
      "Epoch 105/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2805 - val_loss: -0.2763\n",
      "Epoch 106/10000\n",
      "301784/301784 [==============================] - 3s 8us/sample - loss: -0.2781 - val_loss: -0.2723\n",
      "Epoch 107/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2801 - val_loss: -0.2750\n",
      "Epoch 108/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2801 - val_loss: -0.2761\n",
      "Epoch 109/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2812 - val_loss: -0.2702\n",
      "Epoch 110/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2816 - val_loss: -0.2778\n",
      "Epoch 111/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2799 - val_loss: -0.2735\n",
      "Epoch 112/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2807 - val_loss: -0.2776\n",
      "Epoch 113/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2799 - val_loss: -0.2771\n",
      "Epoch 114/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2812 - val_loss: -0.2772\n",
      "Epoch 115/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2807 - val_loss: -0.2778\n",
      "Epoch 116/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2806 - val_loss: -0.2753\n",
      "Epoch 117/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2805 - val_loss: -0.2775\n",
      "Epoch 118/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2817 - val_loss: -0.2763\n",
      "Epoch 119/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2807 - val_loss: -0.2648\n",
      "Epoch 120/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2810 - val_loss: -0.2751\n",
      "Epoch 121/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2797 - val_loss: -0.2790\n",
      "Epoch 122/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2811 - val_loss: -0.2772\n",
      "Epoch 123/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2806 - val_loss: -0.2701\n",
      "Epoch 124/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2823 - val_loss: -0.2737\n",
      "Epoch 125/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2815 - val_loss: -0.2766\n",
      "Epoch 126/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2815 - val_loss: -0.2758\n",
      "Epoch 127/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2818 - val_loss: -0.2710\n",
      "Epoch 128/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2815 - val_loss: -0.2716\n",
      "Epoch 129/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2803 - val_loss: -0.2737\n",
      "Epoch 130/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2825 - val_loss: -0.2750\n",
      "Epoch 131/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2812 - val_loss: -0.2737\n",
      "Epoch 132/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2819 - val_loss: -0.2728\n",
      "Epoch 133/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2759\n",
      "Epoch 134/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2814 - val_loss: -0.2771\n",
      "Epoch 135/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2820 - val_loss: -0.2725\n",
      "Epoch 136/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2805 - val_loss: -0.2735\n",
      "Epoch 137/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2832 - val_loss: -0.2755\n",
      "Epoch 138/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2747\n",
      "Epoch 139/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2820 - val_loss: -0.2781\n",
      "Epoch 140/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2815 - val_loss: -0.2761\n",
      "Epoch 141/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2827 - val_loss: -0.2707\n",
      "Epoch 142/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2833 - val_loss: -0.2758\n",
      "Epoch 143/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2836 - val_loss: -0.2758\n",
      "Epoch 144/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2828 - val_loss: -0.2784\n",
      "Epoch 145/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2819 - val_loss: -0.2756\n",
      "Epoch 146/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2833 - val_loss: -0.2673\n",
      "Epoch 147/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2829 - val_loss: -0.2775\n",
      "Epoch 148/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2823 - val_loss: -0.2746\n",
      "Epoch 149/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2829 - val_loss: -0.2759\n",
      "Epoch 150/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2833 - val_loss: -0.2780\n",
      "Epoch 151/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2827 - val_loss: -0.2785\n",
      "Epoch 152/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2840 - val_loss: -0.2742\n",
      "Epoch 153/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2821 - val_loss: -0.2732\n",
      "Epoch 154/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2830 - val_loss: -0.2759\n",
      "Epoch 155/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2839 - val_loss: -0.2757\n",
      "Epoch 156/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2825 - val_loss: -0.2765\n",
      "Epoch 157/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2828 - val_loss: -0.2763\n",
      "Epoch 158/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2841 - val_loss: -0.2800\n",
      "Epoch 159/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2834 - val_loss: -0.2787\n",
      "Epoch 160/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2836 - val_loss: -0.2764\n",
      "Epoch 161/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2828 - val_loss: -0.2796\n",
      "Epoch 162/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2841 - val_loss: -0.2764\n",
      "Epoch 163/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2830 - val_loss: -0.2690\n",
      "Epoch 164/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2841 - val_loss: -0.2766\n",
      "Epoch 165/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2845 - val_loss: -0.2756\n",
      "Epoch 166/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2836 - val_loss: -0.2745\n",
      "Epoch 167/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2846 - val_loss: -0.2769\n",
      "Epoch 168/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2828 - val_loss: -0.2749\n",
      "Epoch 169/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2838 - val_loss: -0.2778\n",
      "Epoch 170/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2844 - val_loss: -0.2788\n",
      "Epoch 171/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2820 - val_loss: -0.2777\n",
      "Epoch 172/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2847 - val_loss: -0.2726\n",
      "Epoch 173/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2837 - val_loss: -0.2751\n",
      "Epoch 174/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2840 - val_loss: -0.2761\n",
      "Epoch 175/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2839 - val_loss: -0.2773\n",
      "Epoch 176/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2843 - val_loss: -0.2758\n",
      "Epoch 177/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2841 - val_loss: -0.2768\n",
      "Epoch 178/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2848 - val_loss: -0.2760\n",
      "Epoch 179/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2845 - val_loss: -0.2778\n",
      "Epoch 180/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2848 - val_loss: -0.2759\n",
      "Epoch 181/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2846 - val_loss: -0.2762\n",
      "Epoch 182/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2842 - val_loss: -0.2792\n",
      "Epoch 183/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2859 - val_loss: -0.2685\n",
      "Epoch 184/10000\n",
      "301784/301784 [==============================] - 4s 15us/sample - loss: -0.2852 - val_loss: -0.2794\n",
      "Epoch 185/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2854 - val_loss: -0.2798\n",
      "Epoch 186/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2854 - val_loss: -0.2773\n",
      "Epoch 187/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2853 - val_loss: -0.2780\n",
      "Epoch 188/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2852 - val_loss: -0.2728\n",
      "Epoch 189/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2843 - val_loss: -0.2753\n",
      "Epoch 190/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2855 - val_loss: -0.2740\n",
      "Epoch 191/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2845 - val_loss: -0.2780\n",
      "Epoch 192/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2859 - val_loss: -0.2607\n",
      "Epoch 193/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2852 - val_loss: -0.2781\n",
      "Epoch 194/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2860 - val_loss: -0.2731\n",
      "Epoch 195/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2858 - val_loss: -0.2781\n",
      "Epoch 196/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2855 - val_loss: -0.2749\n",
      "Epoch 197/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2853 - val_loss: -0.2736\n",
      "Epoch 198/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2852 - val_loss: -0.2730\n",
      "Epoch 199/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2855 - val_loss: -0.2729\n",
      "Epoch 200/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2853 - val_loss: -0.2793\n",
      "Epoch 201/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2862 - val_loss: -0.2781\n",
      "Epoch 202/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2790\n",
      "Epoch 203/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2863 - val_loss: -0.2811\n",
      "Epoch 204/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2863 - val_loss: -0.2783\n",
      "Epoch 205/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2872 - val_loss: -0.2779\n",
      "Epoch 206/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2755\n",
      "Epoch 207/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2855 - val_loss: -0.2762\n",
      "Epoch 208/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2864 - val_loss: -0.2755\n",
      "Epoch 209/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2856 - val_loss: -0.2703\n",
      "Epoch 210/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2864 - val_loss: -0.2753\n",
      "Epoch 211/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2863 - val_loss: -0.2749\n",
      "Epoch 212/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2862 - val_loss: -0.2756\n",
      "Epoch 213/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2871 - val_loss: -0.2770\n",
      "Epoch 214/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2852 - val_loss: -0.2782\n",
      "Epoch 215/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2864 - val_loss: -0.2702\n",
      "Epoch 216/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2860 - val_loss: -0.2775\n",
      "Epoch 217/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2874 - val_loss: -0.2755\n",
      "Epoch 218/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2856 - val_loss: -0.2766\n",
      "Epoch 219/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2876 - val_loss: -0.2737\n",
      "Epoch 220/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2869 - val_loss: -0.2759\n",
      "Epoch 221/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2866 - val_loss: -0.2680\n",
      "Epoch 222/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2864 - val_loss: -0.2793\n",
      "Epoch 223/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2875 - val_loss: -0.2791\n",
      "Epoch 224/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2867 - val_loss: -0.2781\n",
      "Epoch 225/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2881 - val_loss: -0.2789\n",
      "Epoch 226/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2862 - val_loss: -0.2772\n",
      "Epoch 227/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2869 - val_loss: -0.2791\n",
      "Epoch 228/10000\n",
      "301784/301784 [==============================] - 5s 15us/sample - loss: -0.2879 - val_loss: -0.2777\n",
      "Epoch 229/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2877 - val_loss: -0.2768\n",
      "Epoch 230/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2861 - val_loss: -0.2774\n",
      "Epoch 231/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2870 - val_loss: -0.2747\n",
      "Epoch 232/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2860 - val_loss: -0.2637\n",
      "Epoch 233/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2875 - val_loss: -0.2783\n",
      "Epoch 234/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2879 - val_loss: -0.2754\n",
      "Epoch 235/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2871 - val_loss: -0.2784\n",
      "Epoch 236/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2879 - val_loss: -0.2788\n",
      "Epoch 237/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2882 - val_loss: -0.2791\n",
      "Epoch 238/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2869 - val_loss: -0.2787\n",
      "Epoch 239/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2884 - val_loss: -0.2758\n",
      "Epoch 240/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2882 - val_loss: -0.2742\n",
      "Epoch 241/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2878 - val_loss: -0.2719\n",
      "Epoch 242/10000\n",
      "301784/301784 [==============================] - 5s 17us/sample - loss: -0.2873 - val_loss: -0.2737\n",
      "Epoch 243/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2874 - val_loss: -0.2772\n",
      "Epoch 244/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2877 - val_loss: -0.2743\n",
      "Epoch 245/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2878 - val_loss: -0.2786\n",
      "Epoch 246/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2889 - val_loss: -0.2728\n",
      "Epoch 247/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2877 - val_loss: -0.2799\n",
      "Epoch 248/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2881 - val_loss: -0.2790\n",
      "Epoch 249/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2881 - val_loss: -0.2788\n",
      "Epoch 250/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2888 - val_loss: -0.2735\n",
      "Epoch 251/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2876 - val_loss: -0.2787\n",
      "Epoch 252/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2881 - val_loss: -0.2733\n",
      "Epoch 253/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2881 - val_loss: -0.2711\n",
      "Epoch 254/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2880 - val_loss: -0.2746\n",
      "Epoch 255/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2881 - val_loss: -0.2785\n",
      "Epoch 256/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2729\n",
      "Epoch 257/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2897 - val_loss: -0.2776\n",
      "Epoch 258/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2886 - val_loss: -0.2792\n",
      "Epoch 259/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2887 - val_loss: -0.2794\n",
      "Epoch 260/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2878 - val_loss: -0.2727\n",
      "Epoch 261/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2878 - val_loss: -0.2758\n",
      "Epoch 262/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2891 - val_loss: -0.2764\n",
      "Epoch 263/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2886 - val_loss: -0.2788\n",
      "Epoch 264/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2775\n",
      "Epoch 265/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2893 - val_loss: -0.2766\n",
      "Epoch 266/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2801\n",
      "Epoch 267/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2890 - val_loss: -0.2805\n",
      "Epoch 268/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2898 - val_loss: -0.2758\n",
      "Epoch 269/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2800\n",
      "Epoch 270/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2893 - val_loss: -0.2764\n",
      "Epoch 271/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2888 - val_loss: -0.2761\n",
      "Epoch 272/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2893 - val_loss: -0.2652\n",
      "Epoch 273/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2888 - val_loss: -0.2761\n",
      "Epoch 274/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2895 - val_loss: -0.2762\n",
      "Epoch 275/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2893 - val_loss: -0.2778\n",
      "Epoch 276/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2787\n",
      "Epoch 277/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2892 - val_loss: -0.2810\n",
      "Epoch 278/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2894 - val_loss: -0.2794\n",
      "Epoch 279/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2896 - val_loss: -0.2701\n",
      "Epoch 280/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2891 - val_loss: -0.2799\n",
      "Epoch 281/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2889 - val_loss: -0.2743\n",
      "Epoch 282/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2892 - val_loss: -0.2749\n",
      "Epoch 283/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2895 - val_loss: -0.2769\n",
      "Epoch 284/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2908 - val_loss: -0.2755\n",
      "Epoch 285/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2902 - val_loss: -0.2778\n",
      "Epoch 286/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2900 - val_loss: -0.2754\n",
      "Epoch 287/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2897 - val_loss: -0.2756\n",
      "Epoch 288/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2904 - val_loss: -0.2798\n",
      "Epoch 289/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2900 - val_loss: -0.2793\n",
      "Epoch 290/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2893 - val_loss: -0.2762\n",
      "Epoch 291/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2894 - val_loss: -0.2796\n",
      "Epoch 292/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2904 - val_loss: -0.2732\n",
      "Epoch 293/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2916 - val_loss: -0.2801\n",
      "Epoch 294/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2906 - val_loss: -0.2736\n",
      "Epoch 295/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2811\n",
      "Epoch 296/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2903 - val_loss: -0.2758\n",
      "Epoch 297/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2910 - val_loss: -0.2749\n",
      "Epoch 298/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2905 - val_loss: -0.2797\n",
      "Epoch 299/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2901 - val_loss: -0.2807\n",
      "Epoch 300/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2908 - val_loss: -0.2805\n",
      "Epoch 301/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2911 - val_loss: -0.2812\n",
      "Epoch 302/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2914 - val_loss: -0.2743\n",
      "Epoch 303/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2911 - val_loss: -0.2753\n",
      "Epoch 304/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2903 - val_loss: -0.2727\n",
      "Epoch 305/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2906 - val_loss: -0.2760\n",
      "Epoch 306/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2910 - val_loss: -0.2785\n",
      "Epoch 307/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2908 - val_loss: -0.2732\n",
      "Epoch 308/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2913 - val_loss: -0.2801\n",
      "Epoch 309/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2752\n",
      "Epoch 310/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2907 - val_loss: -0.2770\n",
      "Epoch 311/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2901 - val_loss: -0.2755\n",
      "Epoch 312/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2916 - val_loss: -0.2790\n",
      "Epoch 313/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2906 - val_loss: -0.2792\n",
      "Epoch 314/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2912 - val_loss: -0.2801\n",
      "Epoch 315/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2904 - val_loss: -0.2716\n",
      "Epoch 316/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2920 - val_loss: -0.2717\n",
      "Epoch 317/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2910 - val_loss: -0.2790\n",
      "Epoch 318/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2806\n",
      "Epoch 319/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2911 - val_loss: -0.2793\n",
      "Epoch 320/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2918 - val_loss: -0.2794\n",
      "Epoch 321/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2908 - val_loss: -0.2820\n",
      "Epoch 322/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2923 - val_loss: -0.2758\n",
      "Epoch 323/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2755\n",
      "Epoch 324/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2788\n",
      "Epoch 325/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2744\n",
      "Epoch 326/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2776\n",
      "Epoch 327/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2907 - val_loss: -0.2711\n",
      "Epoch 328/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2915 - val_loss: -0.2787\n",
      "Epoch 329/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2918 - val_loss: -0.2772\n",
      "Epoch 330/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2774\n",
      "Epoch 331/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2908 - val_loss: -0.2807\n",
      "Epoch 332/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2925 - val_loss: -0.2754\n",
      "Epoch 333/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2921 - val_loss: -0.2753\n",
      "Epoch 334/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2922 - val_loss: -0.2797\n",
      "Epoch 335/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2913 - val_loss: -0.2757\n",
      "Epoch 336/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2911 - val_loss: -0.2777\n",
      "Epoch 337/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2683\n",
      "Epoch 338/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2926 - val_loss: -0.2793\n",
      "Epoch 339/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2754\n",
      "Epoch 340/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2742\n",
      "Epoch 341/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2918 - val_loss: -0.2757\n",
      "Epoch 342/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2791\n",
      "Epoch 343/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2769\n",
      "Epoch 344/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2917 - val_loss: -0.2786\n",
      "Epoch 345/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2920 - val_loss: -0.2787\n",
      "Epoch 346/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2932 - val_loss: -0.2699\n",
      "Epoch 347/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2921 - val_loss: -0.2762\n",
      "Epoch 348/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2922 - val_loss: -0.2787\n",
      "Epoch 349/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2929 - val_loss: -0.2778\n",
      "Epoch 350/10000\n",
      "301784/301784 [==============================] - 2s 8us/sample - loss: -0.2925 - val_loss: -0.2766\n",
      "Epoch 351/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2779\n",
      "Epoch 352/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2715\n",
      "Epoch 353/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2787\n",
      "Epoch 354/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2792\n",
      "Epoch 355/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2756\n",
      "Epoch 356/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2936 - val_loss: -0.2760\n",
      "Epoch 357/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2770\n",
      "Epoch 358/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2777\n",
      "Epoch 359/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2804\n",
      "Epoch 360/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2775\n",
      "Epoch 361/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2783\n",
      "Epoch 362/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2942 - val_loss: -0.2762\n",
      "Epoch 363/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2936 - val_loss: -0.2802\n",
      "Epoch 364/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2783\n",
      "Epoch 365/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2942 - val_loss: -0.2753\n",
      "Epoch 366/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2763\n",
      "Epoch 367/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2926 - val_loss: -0.2792\n",
      "Epoch 368/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2918 - val_loss: -0.2756\n",
      "Epoch 369/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2941 - val_loss: -0.2735\n",
      "Epoch 370/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2948 - val_loss: -0.2757\n",
      "Epoch 371/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2936 - val_loss: -0.2783\n",
      "Epoch 372/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2932 - val_loss: -0.2791\n",
      "Epoch 373/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2930 - val_loss: -0.2810\n",
      "Epoch 374/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2936 - val_loss: -0.2767\n",
      "Epoch 375/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2762\n",
      "Epoch 376/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2930 - val_loss: -0.2791\n",
      "Epoch 377/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2743\n",
      "Epoch 378/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2946 - val_loss: -0.2784\n",
      "Epoch 379/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2934 - val_loss: -0.2808\n",
      "Epoch 380/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2791\n",
      "Epoch 381/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2942 - val_loss: -0.2769\n",
      "Epoch 382/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2949 - val_loss: -0.2773\n",
      "Epoch 383/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2782\n",
      "Epoch 384/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2953 - val_loss: -0.2787\n",
      "Epoch 385/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2943 - val_loss: -0.2760\n",
      "Epoch 386/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2942 - val_loss: -0.2761\n",
      "Epoch 387/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2941 - val_loss: -0.2769\n",
      "Epoch 388/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2948 - val_loss: -0.2804\n",
      "Epoch 389/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2940 - val_loss: -0.2789\n",
      "Epoch 390/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2941 - val_loss: -0.2787\n",
      "Epoch 391/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2948 - val_loss: -0.2751\n",
      "Epoch 392/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2962 - val_loss: -0.2796\n",
      "Epoch 393/10000\n",
      "301784/301784 [==============================] - 4s 14us/sample - loss: -0.2942 - val_loss: -0.2783\n",
      "Epoch 394/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2937 - val_loss: -0.2767\n",
      "Epoch 395/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2942 - val_loss: -0.2736\n",
      "Epoch 396/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2951 - val_loss: -0.2754\n",
      "Epoch 397/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2943 - val_loss: -0.2797\n",
      "Epoch 398/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2801\n",
      "Epoch 399/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2949 - val_loss: -0.2771\n",
      "Epoch 400/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2952 - val_loss: -0.2802\n",
      "Epoch 401/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2950 - val_loss: -0.2747\n",
      "Epoch 402/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2945 - val_loss: -0.2749\n",
      "Epoch 403/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2941 - val_loss: -0.2792\n",
      "Epoch 404/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2953 - val_loss: -0.2774\n",
      "Epoch 405/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2960 - val_loss: -0.2783\n",
      "Epoch 406/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2949 - val_loss: -0.2788\n",
      "Epoch 407/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2956 - val_loss: -0.2782\n",
      "Epoch 408/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2957 - val_loss: -0.2777\n",
      "Epoch 409/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2953 - val_loss: -0.2779\n",
      "Epoch 410/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2947 - val_loss: -0.2769\n",
      "Epoch 411/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2957 - val_loss: -0.2774\n",
      "Epoch 412/10000\n",
      "301784/301784 [==============================] - 3s 12us/sample - loss: -0.2955 - val_loss: -0.2758\n",
      "Epoch 413/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2959 - val_loss: -0.2805\n",
      "Epoch 414/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2958 - val_loss: -0.2747\n",
      "Epoch 415/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2957 - val_loss: -0.2805\n",
      "Epoch 416/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2963 - val_loss: -0.2792\n",
      "Epoch 417/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2955 - val_loss: -0.2758\n",
      "Epoch 418/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2958 - val_loss: -0.2784\n",
      "Epoch 419/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2951 - val_loss: -0.2778\n",
      "Epoch 420/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2955 - val_loss: -0.2768\n",
      "Epoch 421/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2759\n",
      "Epoch 422/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2968 - val_loss: -0.2760\n",
      "Epoch 423/10000\n",
      "301784/301784 [==============================] - 4s 13us/sample - loss: -0.2952 - val_loss: -0.2747\n",
      "Epoch 424/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2948 - val_loss: -0.2784\n",
      "Epoch 425/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2963 - val_loss: -0.2757\n",
      "Epoch 426/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2957 - val_loss: -0.2709\n",
      "Epoch 427/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2763\n",
      "Epoch 428/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2953 - val_loss: -0.2785\n",
      "Epoch 429/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2786\n",
      "Epoch 430/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2964 - val_loss: -0.2763\n",
      "Epoch 431/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2964 - val_loss: -0.2755\n",
      "Epoch 432/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2961 - val_loss: -0.2796\n",
      "Epoch 433/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2969 - val_loss: -0.2771\n",
      "Epoch 434/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2962 - val_loss: -0.2712\n",
      "Epoch 435/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2964 - val_loss: -0.2755\n",
      "Epoch 436/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2971 - val_loss: -0.2774\n",
      "Epoch 437/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2963 - val_loss: -0.2764\n",
      "Epoch 438/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2963 - val_loss: -0.2678\n",
      "Epoch 439/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2966 - val_loss: -0.2812\n",
      "Epoch 440/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2969 - val_loss: -0.2687\n",
      "Epoch 441/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2774\n",
      "Epoch 442/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2959 - val_loss: -0.2717\n",
      "Epoch 443/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2762\n",
      "Epoch 444/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2960 - val_loss: -0.2770\n",
      "Epoch 445/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2780\n",
      "Epoch 446/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2767\n",
      "Epoch 447/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2974 - val_loss: -0.2718\n",
      "Epoch 448/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2977 - val_loss: -0.2766\n",
      "Epoch 449/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2973 - val_loss: -0.2745\n",
      "Epoch 450/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2764\n",
      "Epoch 451/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2971 - val_loss: -0.2756\n",
      "Epoch 452/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2986 - val_loss: -0.2750\n",
      "Epoch 453/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2971 - val_loss: -0.2787\n",
      "Epoch 454/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2973 - val_loss: -0.2788\n",
      "Epoch 455/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2974 - val_loss: -0.2745\n",
      "Epoch 456/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2980 - val_loss: -0.2779\n",
      "Epoch 457/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2980 - val_loss: -0.2786\n",
      "Epoch 458/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.2985 - val_loss: -0.2785\n",
      "Epoch 459/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2979 - val_loss: -0.2750\n",
      "Epoch 460/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2763\n",
      "Epoch 461/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2972 - val_loss: -0.2777\n",
      "Epoch 462/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2977 - val_loss: -0.2779\n",
      "Epoch 463/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2982 - val_loss: -0.2795\n",
      "Epoch 464/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2979 - val_loss: -0.2789\n",
      "Epoch 465/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2977 - val_loss: -0.2771\n",
      "Epoch 466/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2974 - val_loss: -0.2799\n",
      "Epoch 467/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2992 - val_loss: -0.2782\n",
      "Epoch 468/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2983 - val_loss: -0.2791\n",
      "Epoch 469/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2985 - val_loss: -0.2772\n",
      "Epoch 470/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2982 - val_loss: -0.2758\n",
      "Epoch 471/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2981 - val_loss: -0.2744\n",
      "Epoch 472/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2978 - val_loss: -0.2735\n",
      "Epoch 473/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.3005 - val_loss: -0.2746\n",
      "Epoch 474/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2998 - val_loss: -0.2741\n",
      "Epoch 475/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2991 - val_loss: -0.2749\n",
      "Epoch 476/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2741\n",
      "Epoch 477/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2982 - val_loss: -0.2748\n",
      "Epoch 478/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2991 - val_loss: -0.2766\n",
      "Epoch 479/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2993 - val_loss: -0.2715\n",
      "Epoch 480/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2987 - val_loss: -0.2773\n",
      "Epoch 481/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3000 - val_loss: -0.2755\n",
      "Epoch 482/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2991 - val_loss: -0.2766\n",
      "Epoch 483/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2994 - val_loss: -0.2761\n",
      "Epoch 484/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2983 - val_loss: -0.2722\n",
      "Epoch 485/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2720\n",
      "Epoch 486/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.2985 - val_loss: -0.2756\n",
      "Epoch 487/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.3002 - val_loss: -0.2779\n",
      "Epoch 488/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2727\n",
      "Epoch 489/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2995 - val_loss: -0.2768\n",
      "Epoch 490/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3002 - val_loss: -0.2773\n",
      "Epoch 491/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3002 - val_loss: -0.2776\n",
      "Epoch 492/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3006 - val_loss: -0.2771\n",
      "Epoch 493/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2763\n",
      "Epoch 494/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3005 - val_loss: -0.2754\n",
      "Epoch 495/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3003 - val_loss: -0.2765\n",
      "Epoch 496/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2993 - val_loss: -0.2765\n",
      "Epoch 497/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2780\n",
      "Epoch 498/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2993 - val_loss: -0.2750\n",
      "Epoch 499/10000\n",
      "301784/301784 [==============================] - 4s 12us/sample - loss: -0.2989 - val_loss: -0.2759\n",
      "Epoch 500/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2997 - val_loss: -0.2765\n",
      "Epoch 501/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3004 - val_loss: -0.2757\n",
      "Epoch 502/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2996 - val_loss: -0.2762\n",
      "Epoch 503/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3012 - val_loss: -0.2781\n",
      "Epoch 504/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2732\n",
      "Epoch 505/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.3015 - val_loss: -0.2773\n",
      "Epoch 506/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3010 - val_loss: -0.2738\n",
      "Epoch 507/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3000 - val_loss: -0.2744\n",
      "Epoch 508/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3017 - val_loss: -0.2719\n",
      "Epoch 509/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3006 - val_loss: -0.2711\n",
      "Epoch 510/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3008 - val_loss: -0.2621\n",
      "Epoch 511/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.3000 - val_loss: -0.2774\n",
      "Epoch 512/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3000 - val_loss: -0.2755\n",
      "Epoch 513/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3007 - val_loss: -0.2746\n",
      "Epoch 514/10000\n",
      "301784/301784 [==============================] - 3s 10us/sample - loss: -0.3013 - val_loss: -0.2736\n",
      "Epoch 515/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.2996 - val_loss: -0.2768\n",
      "Epoch 516/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3002 - val_loss: -0.2700\n",
      "Epoch 517/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3005 - val_loss: -0.2707\n",
      "Epoch 518/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3011 - val_loss: -0.2742\n",
      "Epoch 519/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3009 - val_loss: -0.2750\n",
      "Epoch 520/10000\n",
      "301784/301784 [==============================] - 3s 11us/sample - loss: -0.3012 - val_loss: -0.2774\n",
      "Epoch 521/10000\n",
      "301784/301784 [==============================] - 3s 9us/sample - loss: -0.3019 - val_loss: -0.2759\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5701, 6, 1)\n",
      "y_test.shape:  (5701, 1)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:11,898 WARNING Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-21 13:31:12,029 DEBUG Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7700, 6, 1)\n",
      "y_test.shape:  (7700, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:14,162 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7931, 6, 1)\n",
      "y_test.shape:  (7931, 1)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:16,710 WARNING Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7970, 6, 1)\n",
      "y_test.shape:  (7970, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:19,472 WARNING Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7456, 6, 1)\n",
      "y_test.shape:  (7456, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:22,208 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7869, 6, 1)\n",
      "y_test.shape:  (7869, 1)\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:24,971 WARNING Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7806, 6, 1)\n",
      "y_test.shape:  (7806, 1)\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:27,522 WARNING Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7871, 6, 1)\n",
      "y_test.shape:  (7871, 1)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:30,277 WARNING Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7944, 6, 1)\n",
      "y_test.shape:  (7944, 1)\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:33,097 WARNING Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7799, 6, 1)\n",
      "y_test.shape:  (7799, 1)\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:35,901 WARNING Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5698, 6, 1)\n",
      "y_test.shape:  (5698, 1)\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:38,688 WARNING Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7684, 6, 1)\n",
      "y_test.shape:  (7684, 1)\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:41,253 WARNING Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7443, 6, 1)\n",
      "y_test.shape:  (7443, 1)\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:31:44,074 WARNING Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  25.csv\n",
      "2025-01-21 13:31:45,745 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (302161, 6, 1)\n",
      "y_train.shape:  (302161, 1)\n",
      "x_valid.shape:  (75516, 6, 1)\n",
      "y_valid.shape:  (75516, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:32:36,615 WARNING Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 13:32:36,773 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 302161 samples, validate on 75516 samples\n",
      "Epoch 1/10000\n",
      "301056/302161 [============================>.] - ETA: 0s - loss: 0.1615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302161/302161 [==============================] - 3s 11us/sample - loss: 0.1609 - val_loss: -0.1013\n",
      "Epoch 2/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.0376 - val_loss: -0.1329\n",
      "Epoch 3/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.0756 - val_loss: -0.1676\n",
      "Epoch 4/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1065 - val_loss: -0.1798\n",
      "Epoch 5/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.1250 - val_loss: -0.1771\n",
      "Epoch 6/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1401 - val_loss: -0.2154\n",
      "Epoch 7/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1527 - val_loss: -0.1874\n",
      "Epoch 8/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1612 - val_loss: -0.1969\n",
      "Epoch 9/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1694 - val_loss: -0.2066\n",
      "Epoch 10/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1743 - val_loss: -0.2345\n",
      "Epoch 11/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1848 - val_loss: -0.1886\n",
      "Epoch 12/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.1902 - val_loss: -0.2188\n",
      "Epoch 13/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.1982 - val_loss: -0.2336\n",
      "Epoch 14/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2040 - val_loss: -0.2389\n",
      "Epoch 15/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2077 - val_loss: -0.2416\n",
      "Epoch 16/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2147 - val_loss: -0.2416\n",
      "Epoch 17/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2164 - val_loss: -0.2225\n",
      "Epoch 18/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2194 - val_loss: -0.2312\n",
      "Epoch 19/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2263 - val_loss: -0.2352\n",
      "Epoch 20/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2283 - val_loss: -0.2529\n",
      "Epoch 21/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2302 - val_loss: -0.2430\n",
      "Epoch 22/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2318 - val_loss: -0.2460\n",
      "Epoch 23/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2350 - val_loss: -0.2541\n",
      "Epoch 24/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2373 - val_loss: -0.2461\n",
      "Epoch 25/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2383 - val_loss: -0.2467\n",
      "Epoch 26/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2393 - val_loss: -0.2519\n",
      "Epoch 27/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2395 - val_loss: -0.2467\n",
      "Epoch 28/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2416 - val_loss: -0.2388\n",
      "Epoch 29/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2433 - val_loss: -0.2480\n",
      "Epoch 30/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2452 - val_loss: -0.2560\n",
      "Epoch 31/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2449 - val_loss: -0.2500\n",
      "Epoch 32/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2470 - val_loss: -0.2562\n",
      "Epoch 33/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2474 - val_loss: -0.2552\n",
      "Epoch 34/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2473 - val_loss: -0.2487\n",
      "Epoch 35/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2497 - val_loss: -0.2321\n",
      "Epoch 36/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2462 - val_loss: -0.2533\n",
      "Epoch 37/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2492 - val_loss: -0.2584\n",
      "Epoch 38/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2485 - val_loss: -0.2394\n",
      "Epoch 39/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2509 - val_loss: -0.2608\n",
      "Epoch 40/10000\n",
      "302161/302161 [==============================] - 2s 8us/sample - loss: -0.2503 - val_loss: -0.2444\n",
      "Epoch 41/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2506 - val_loss: -0.2591\n",
      "Epoch 42/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2524 - val_loss: -0.2603\n",
      "Epoch 43/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2527 - val_loss: -0.2552\n",
      "Epoch 44/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2527 - val_loss: -0.2530\n",
      "Epoch 45/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2536 - val_loss: -0.2619\n",
      "Epoch 46/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2529 - val_loss: -0.2612\n",
      "Epoch 47/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2538 - val_loss: -0.2473\n",
      "Epoch 48/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2533 - val_loss: -0.2575\n",
      "Epoch 49/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2540 - val_loss: -0.2537\n",
      "Epoch 50/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2557 - val_loss: -0.2588\n",
      "Epoch 51/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2552 - val_loss: -0.2425\n",
      "Epoch 52/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2560 - val_loss: -0.2501\n",
      "Epoch 53/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2565 - val_loss: -0.2464\n",
      "Epoch 54/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2556 - val_loss: -0.2510\n",
      "Epoch 55/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2578 - val_loss: -0.2605\n",
      "Epoch 56/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2571 - val_loss: -0.2531\n",
      "Epoch 57/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2584 - val_loss: -0.2562\n",
      "Epoch 58/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2582 - val_loss: -0.2553\n",
      "Epoch 59/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2583 - val_loss: -0.2280\n",
      "Epoch 60/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2594 - val_loss: -0.2602\n",
      "Epoch 61/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2585 - val_loss: -0.2549\n",
      "Epoch 62/10000\n",
      "302161/302161 [==============================] - 3s 8us/sample - loss: -0.2587 - val_loss: -0.2530\n",
      "Epoch 63/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2603 - val_loss: -0.2658\n",
      "Epoch 64/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2588 - val_loss: -0.2614\n",
      "Epoch 65/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2591 - val_loss: -0.2599\n",
      "Epoch 66/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2602 - val_loss: -0.2616\n",
      "Epoch 67/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2606 - val_loss: -0.2601\n",
      "Epoch 68/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2602 - val_loss: -0.2633\n",
      "Epoch 69/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2609 - val_loss: -0.2580\n",
      "Epoch 70/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2607 - val_loss: -0.2558\n",
      "Epoch 71/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2604 - val_loss: -0.2646\n",
      "Epoch 72/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2606 - val_loss: -0.2615\n",
      "Epoch 73/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2616 - val_loss: -0.2348\n",
      "Epoch 74/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2613 - val_loss: -0.2593\n",
      "Epoch 75/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2627 - val_loss: -0.2640\n",
      "Epoch 76/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2620 - val_loss: -0.2646\n",
      "Epoch 77/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2619 - val_loss: -0.2631\n",
      "Epoch 78/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2621 - val_loss: -0.2621\n",
      "Epoch 79/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2623 - val_loss: -0.2523\n",
      "Epoch 80/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2620 - val_loss: -0.2375\n",
      "Epoch 81/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2633 - val_loss: -0.2655\n",
      "Epoch 82/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2631 - val_loss: -0.2609\n",
      "Epoch 83/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2630 - val_loss: -0.2628\n",
      "Epoch 84/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2635 - val_loss: -0.2596\n",
      "Epoch 85/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2621 - val_loss: -0.2614\n",
      "Epoch 86/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2628 - val_loss: -0.2618\n",
      "Epoch 87/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2637 - val_loss: -0.2301\n",
      "Epoch 88/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2615 - val_loss: -0.2609\n",
      "Epoch 89/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2646 - val_loss: -0.2583\n",
      "Epoch 90/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2641 - val_loss: -0.2623\n",
      "Epoch 91/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2634 - val_loss: -0.2287\n",
      "Epoch 92/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2622 - val_loss: -0.2608\n",
      "Epoch 93/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2646 - val_loss: -0.2659\n",
      "Epoch 94/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2646 - val_loss: -0.2617\n",
      "Epoch 95/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2659 - val_loss: -0.2632\n",
      "Epoch 96/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2641 - val_loss: -0.2471\n",
      "Epoch 97/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2657 - val_loss: -0.2600\n",
      "Epoch 98/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2650 - val_loss: -0.2628\n",
      "Epoch 99/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2655 - val_loss: -0.2605\n",
      "Epoch 100/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2654 - val_loss: -0.2561\n",
      "Epoch 101/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2633 - val_loss: -0.2665\n",
      "Epoch 102/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2657 - val_loss: -0.2572\n",
      "Epoch 103/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2641 - val_loss: -0.2669\n",
      "Epoch 104/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2661 - val_loss: -0.2610\n",
      "Epoch 105/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2647 - val_loss: -0.2374\n",
      "Epoch 106/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2650 - val_loss: -0.2636\n",
      "Epoch 107/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2653 - val_loss: -0.2628\n",
      "Epoch 108/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2640 - val_loss: -0.2592\n",
      "Epoch 109/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2662 - val_loss: -0.2639\n",
      "Epoch 110/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2648 - val_loss: -0.2591\n",
      "Epoch 111/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2655 - val_loss: -0.2603\n",
      "Epoch 112/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2655 - val_loss: -0.2618\n",
      "Epoch 113/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2652 - val_loss: -0.2607\n",
      "Epoch 114/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2660 - val_loss: -0.2668\n",
      "Epoch 115/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2656 - val_loss: -0.2601\n",
      "Epoch 116/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2665 - val_loss: -0.2616\n",
      "Epoch 117/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2648 - val_loss: -0.2643\n",
      "Epoch 118/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2661 - val_loss: -0.2662\n",
      "Epoch 119/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2667 - val_loss: -0.2643\n",
      "Epoch 120/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2661 - val_loss: -0.2676\n",
      "Epoch 121/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2665 - val_loss: -0.2671\n",
      "Epoch 122/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2666 - val_loss: -0.2604\n",
      "Epoch 123/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2668 - val_loss: -0.2665\n",
      "Epoch 124/10000\n",
      "302161/302161 [==============================] - 4s 13us/sample - loss: -0.2670 - val_loss: -0.2656\n",
      "Epoch 125/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2664 - val_loss: -0.2666\n",
      "Epoch 126/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2674 - val_loss: -0.2582\n",
      "Epoch 127/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2663 - val_loss: -0.2660\n",
      "Epoch 128/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2669 - val_loss: -0.2532\n",
      "Epoch 129/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2663 - val_loss: -0.2659\n",
      "Epoch 130/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2665 - val_loss: -0.2620\n",
      "Epoch 131/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2666 - val_loss: -0.2616\n",
      "Epoch 132/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2661 - val_loss: -0.2652\n",
      "Epoch 133/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2667 - val_loss: -0.2657\n",
      "Epoch 134/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2685 - val_loss: -0.2680\n",
      "Epoch 135/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2676 - val_loss: -0.2640\n",
      "Epoch 136/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2677 - val_loss: -0.2621\n",
      "Epoch 137/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2662 - val_loss: -0.2638\n",
      "Epoch 138/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2673 - val_loss: -0.2673\n",
      "Epoch 139/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2680 - val_loss: -0.2571\n",
      "Epoch 140/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2673 - val_loss: -0.2634\n",
      "Epoch 141/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2672 - val_loss: -0.2655\n",
      "Epoch 142/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2680 - val_loss: -0.2652\n",
      "Epoch 143/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2676 - val_loss: -0.2652\n",
      "Epoch 144/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2679 - val_loss: -0.2616\n",
      "Epoch 145/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2680 - val_loss: -0.2653\n",
      "Epoch 146/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2669 - val_loss: -0.2612\n",
      "Epoch 147/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2677 - val_loss: -0.2655\n",
      "Epoch 148/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2674 - val_loss: -0.2557\n",
      "Epoch 149/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2683 - val_loss: -0.2631\n",
      "Epoch 150/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2686 - val_loss: -0.2584\n",
      "Epoch 151/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2685 - val_loss: -0.2660\n",
      "Epoch 152/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2678 - val_loss: -0.2617\n",
      "Epoch 153/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2694 - val_loss: -0.2670\n",
      "Epoch 154/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2676 - val_loss: -0.2671\n",
      "Epoch 155/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2695 - val_loss: -0.2637\n",
      "Epoch 156/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2695 - val_loss: -0.2643\n",
      "Epoch 157/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2693 - val_loss: -0.2619\n",
      "Epoch 158/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2663 - val_loss: -0.2617\n",
      "Epoch 159/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2693 - val_loss: -0.2672\n",
      "Epoch 160/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2697 - val_loss: -0.2666\n",
      "Epoch 161/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2689 - val_loss: -0.2674\n",
      "Epoch 162/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2690 - val_loss: -0.2619\n",
      "Epoch 163/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2687 - val_loss: -0.2621\n",
      "Epoch 164/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2685 - val_loss: -0.2662\n",
      "Epoch 165/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2688 - val_loss: -0.2659\n",
      "Epoch 166/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2701 - val_loss: -0.2634\n",
      "Epoch 167/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2684 - val_loss: -0.2672\n",
      "Epoch 168/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2692 - val_loss: -0.2694\n",
      "Epoch 169/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2699 - val_loss: -0.2663\n",
      "Epoch 170/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2693 - val_loss: -0.2640\n",
      "Epoch 171/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2698 - val_loss: -0.2654\n",
      "Epoch 172/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2686 - val_loss: -0.2626\n",
      "Epoch 173/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2692 - val_loss: -0.2638\n",
      "Epoch 174/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2704 - val_loss: -0.2537\n",
      "Epoch 175/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2697 - val_loss: -0.2599\n",
      "Epoch 176/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2696 - val_loss: -0.2673\n",
      "Epoch 177/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2695 - val_loss: -0.2654\n",
      "Epoch 178/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2698 - val_loss: -0.2662\n",
      "Epoch 179/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2702 - val_loss: -0.2647\n",
      "Epoch 180/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2685 - val_loss: -0.2624\n",
      "Epoch 181/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2702 - val_loss: -0.2634\n",
      "Epoch 182/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2704 - val_loss: -0.2619\n",
      "Epoch 183/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2710 - val_loss: -0.2687\n",
      "Epoch 184/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2687 - val_loss: -0.2658\n",
      "Epoch 185/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2699 - val_loss: -0.2663\n",
      "Epoch 186/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2707 - val_loss: -0.2671\n",
      "Epoch 187/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2708 - val_loss: -0.2593\n",
      "Epoch 188/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2704 - val_loss: -0.2649\n",
      "Epoch 189/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2703 - val_loss: -0.2596\n",
      "Epoch 190/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2696 - val_loss: -0.2646\n",
      "Epoch 191/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2709 - val_loss: -0.2543\n",
      "Epoch 192/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2703 - val_loss: -0.2659\n",
      "Epoch 193/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2698 - val_loss: -0.2630\n",
      "Epoch 194/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2706 - val_loss: -0.2626\n",
      "Epoch 195/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2715 - val_loss: -0.2572\n",
      "Epoch 196/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2682 - val_loss: -0.2648\n",
      "Epoch 197/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2702 - val_loss: -0.2666\n",
      "Epoch 198/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2716 - val_loss: -0.2625\n",
      "Epoch 199/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2705 - val_loss: -0.2619\n",
      "Epoch 200/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2696 - val_loss: -0.2579\n",
      "Epoch 201/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2698 - val_loss: -0.2651\n",
      "Epoch 202/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2706 - val_loss: -0.2675\n",
      "Epoch 203/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2709 - val_loss: -0.2609\n",
      "Epoch 204/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2708 - val_loss: -0.2670\n",
      "Epoch 205/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2705 - val_loss: -0.2656\n",
      "Epoch 206/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2696 - val_loss: -0.2688\n",
      "Epoch 207/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2715 - val_loss: -0.2631\n",
      "Epoch 208/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2714 - val_loss: -0.2665\n",
      "Epoch 209/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2702 - val_loss: -0.2637\n",
      "Epoch 210/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2720 - val_loss: -0.2682\n",
      "Epoch 211/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2723 - val_loss: -0.2669\n",
      "Epoch 212/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2725 - val_loss: -0.2643\n",
      "Epoch 213/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2706 - val_loss: -0.2674\n",
      "Epoch 214/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2721 - val_loss: -0.2517\n",
      "Epoch 215/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2711 - val_loss: -0.2669\n",
      "Epoch 216/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2711 - val_loss: -0.2683\n",
      "Epoch 217/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2717 - val_loss: -0.2662\n",
      "Epoch 218/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2715 - val_loss: -0.2655\n",
      "Epoch 219/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2713 - val_loss: -0.2673\n",
      "Epoch 220/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2712 - val_loss: -0.2649\n",
      "Epoch 221/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2735 - val_loss: -0.2624\n",
      "Epoch 222/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2710 - val_loss: -0.2674\n",
      "Epoch 223/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2732 - val_loss: -0.2568\n",
      "Epoch 224/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2720 - val_loss: -0.2636\n",
      "Epoch 225/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2716 - val_loss: -0.2657\n",
      "Epoch 226/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2724 - val_loss: -0.2654\n",
      "Epoch 227/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2732 - val_loss: -0.2587\n",
      "Epoch 228/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2712 - val_loss: -0.2652\n",
      "Epoch 229/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2732 - val_loss: -0.2694\n",
      "Epoch 230/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2724 - val_loss: -0.2668\n",
      "Epoch 231/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2712 - val_loss: -0.2691\n",
      "Epoch 232/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2733 - val_loss: -0.2542\n",
      "Epoch 233/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2712 - val_loss: -0.2668\n",
      "Epoch 234/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2719 - val_loss: -0.2645\n",
      "Epoch 235/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2721 - val_loss: -0.2654\n",
      "Epoch 236/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2730 - val_loss: -0.2675\n",
      "Epoch 237/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2738 - val_loss: -0.2667\n",
      "Epoch 238/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2728 - val_loss: -0.2687\n",
      "Epoch 239/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2730 - val_loss: -0.2656\n",
      "Epoch 240/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2725 - val_loss: -0.2678\n",
      "Epoch 241/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2733 - val_loss: -0.2679\n",
      "Epoch 242/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2725 - val_loss: -0.2687\n",
      "Epoch 243/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2722 - val_loss: -0.2656\n",
      "Epoch 244/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2729 - val_loss: -0.2655\n",
      "Epoch 245/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2721 - val_loss: -0.2643\n",
      "Epoch 246/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2732 - val_loss: -0.2600\n",
      "Epoch 247/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2722 - val_loss: -0.2695\n",
      "Epoch 248/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2739 - val_loss: -0.2682\n",
      "Epoch 249/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2726 - val_loss: -0.2693\n",
      "Epoch 250/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2733 - val_loss: -0.2697\n",
      "Epoch 251/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2737 - val_loss: -0.2657\n",
      "Epoch 252/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2733 - val_loss: -0.2663\n",
      "Epoch 253/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2740 - val_loss: -0.2648\n",
      "Epoch 254/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2745 - val_loss: -0.2673\n",
      "Epoch 255/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2724 - val_loss: -0.2623\n",
      "Epoch 256/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2725 - val_loss: -0.2671\n",
      "Epoch 257/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2733 - val_loss: -0.2685\n",
      "Epoch 258/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2627\n",
      "Epoch 259/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2738 - val_loss: -0.2695\n",
      "Epoch 260/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2729 - val_loss: -0.2598\n",
      "Epoch 261/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2736 - val_loss: -0.2625\n",
      "Epoch 262/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2746 - val_loss: -0.2630\n",
      "Epoch 263/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2739 - val_loss: -0.2692\n",
      "Epoch 264/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2741 - val_loss: -0.2587\n",
      "Epoch 265/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2730 - val_loss: -0.2679\n",
      "Epoch 266/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2735 - val_loss: -0.2636\n",
      "Epoch 267/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2736 - val_loss: -0.2653\n",
      "Epoch 268/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2647\n",
      "Epoch 269/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2736 - val_loss: -0.2638\n",
      "Epoch 270/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2747 - val_loss: -0.2687\n",
      "Epoch 271/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2746 - val_loss: -0.2653\n",
      "Epoch 272/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2738 - val_loss: -0.2684\n",
      "Epoch 273/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2720 - val_loss: -0.2577\n",
      "Epoch 274/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2717 - val_loss: -0.2642\n",
      "Epoch 275/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2752 - val_loss: -0.2617\n",
      "Epoch 276/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2743 - val_loss: -0.2619\n",
      "Epoch 277/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2628\n",
      "Epoch 278/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2735 - val_loss: -0.2679\n",
      "Epoch 279/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2743 - val_loss: -0.2664\n",
      "Epoch 280/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2741 - val_loss: -0.2683\n",
      "Epoch 281/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2741 - val_loss: -0.2695\n",
      "Epoch 282/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2759 - val_loss: -0.2653\n",
      "Epoch 283/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2752 - val_loss: -0.2644\n",
      "Epoch 284/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2742 - val_loss: -0.2617\n",
      "Epoch 285/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2744 - val_loss: -0.2679\n",
      "Epoch 286/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2739 - val_loss: -0.2622\n",
      "Epoch 287/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2740 - val_loss: -0.2594\n",
      "Epoch 288/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2741 - val_loss: -0.2665\n",
      "Epoch 289/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2754 - val_loss: -0.2667\n",
      "Epoch 290/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2754 - val_loss: -0.2607\n",
      "Epoch 291/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2744 - val_loss: -0.2664\n",
      "Epoch 292/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2754 - val_loss: -0.2681\n",
      "Epoch 293/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2761 - val_loss: -0.2611\n",
      "Epoch 294/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2744 - val_loss: -0.2530\n",
      "Epoch 295/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2750 - val_loss: -0.2658\n",
      "Epoch 296/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2679\n",
      "Epoch 297/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2664\n",
      "Epoch 298/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2759 - val_loss: -0.2678\n",
      "Epoch 299/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2752 - val_loss: -0.2696\n",
      "Epoch 300/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2767 - val_loss: -0.2644\n",
      "Epoch 301/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2747 - val_loss: -0.2666\n",
      "Epoch 302/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2757 - val_loss: -0.2656\n",
      "Epoch 303/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2752 - val_loss: -0.2671\n",
      "Epoch 304/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2681\n",
      "Epoch 305/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2764 - val_loss: -0.2666\n",
      "Epoch 306/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2744 - val_loss: -0.2635\n",
      "Epoch 307/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2755 - val_loss: -0.2623\n",
      "Epoch 308/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2741 - val_loss: -0.2653\n",
      "Epoch 309/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2759 - val_loss: -0.2673\n",
      "Epoch 310/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2754 - val_loss: -0.2625\n",
      "Epoch 311/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2752 - val_loss: -0.2669\n",
      "Epoch 312/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2759 - val_loss: -0.2697\n",
      "Epoch 313/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2651\n",
      "Epoch 314/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2754 - val_loss: -0.2585\n",
      "Epoch 315/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2753 - val_loss: -0.2694\n",
      "Epoch 316/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2767 - val_loss: -0.2664\n",
      "Epoch 317/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2705\n",
      "Epoch 318/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2769 - val_loss: -0.2661\n",
      "Epoch 319/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2666\n",
      "Epoch 320/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2758 - val_loss: -0.2655\n",
      "Epoch 321/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2666\n",
      "Epoch 322/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2764 - val_loss: -0.2661\n",
      "Epoch 323/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2758 - val_loss: -0.2659\n",
      "Epoch 324/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2765 - val_loss: -0.2689\n",
      "Epoch 325/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2758 - val_loss: -0.2673\n",
      "Epoch 326/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2765 - val_loss: -0.2661\n",
      "Epoch 327/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2776 - val_loss: -0.2691\n",
      "Epoch 328/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2761 - val_loss: -0.2652\n",
      "Epoch 329/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2761 - val_loss: -0.2672\n",
      "Epoch 330/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2761 - val_loss: -0.2649\n",
      "Epoch 331/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2759 - val_loss: -0.2699\n",
      "Epoch 332/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2761 - val_loss: -0.2638\n",
      "Epoch 333/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2774 - val_loss: -0.2643\n",
      "Epoch 334/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2773 - val_loss: -0.2688\n",
      "Epoch 335/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2677\n",
      "Epoch 336/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2674\n",
      "Epoch 337/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2775 - val_loss: -0.2679\n",
      "Epoch 338/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2769 - val_loss: -0.2588\n",
      "Epoch 339/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2762 - val_loss: -0.2695\n",
      "Epoch 340/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2762 - val_loss: -0.2658\n",
      "Epoch 341/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2767 - val_loss: -0.2688\n",
      "Epoch 342/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2763 - val_loss: -0.2701\n",
      "Epoch 343/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2769 - val_loss: -0.2587\n",
      "Epoch 344/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2764 - val_loss: -0.2687\n",
      "Epoch 345/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2760 - val_loss: -0.2682\n",
      "Epoch 346/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2767 - val_loss: -0.2673\n",
      "Epoch 347/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2777 - val_loss: -0.2655\n",
      "Epoch 348/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2769 - val_loss: -0.2675\n",
      "Epoch 349/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2770 - val_loss: -0.2671\n",
      "Epoch 350/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2782 - val_loss: -0.2547\n",
      "Epoch 351/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2776 - val_loss: -0.2659\n",
      "Epoch 352/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2773 - val_loss: -0.2684\n",
      "Epoch 353/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2771 - val_loss: -0.2662\n",
      "Epoch 354/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2775 - val_loss: -0.2657\n",
      "Epoch 355/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2774 - val_loss: -0.2660\n",
      "Epoch 356/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2774 - val_loss: -0.2684\n",
      "Epoch 357/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2777 - val_loss: -0.2654\n",
      "Epoch 358/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2761 - val_loss: -0.2638\n",
      "Epoch 359/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2768 - val_loss: -0.2685\n",
      "Epoch 360/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2783 - val_loss: -0.2560\n",
      "Epoch 361/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2771 - val_loss: -0.2652\n",
      "Epoch 362/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2651\n",
      "Epoch 363/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2779 - val_loss: -0.2689\n",
      "Epoch 364/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2584\n",
      "Epoch 365/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2774 - val_loss: -0.2665\n",
      "Epoch 366/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2761 - val_loss: -0.2620\n",
      "Epoch 367/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2766 - val_loss: -0.2696\n",
      "Epoch 368/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2662\n",
      "Epoch 369/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2779 - val_loss: -0.2636\n",
      "Epoch 370/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2776 - val_loss: -0.2662\n",
      "Epoch 371/10000\n",
      "302161/302161 [==============================] - 3s 12us/sample - loss: -0.2782 - val_loss: -0.2652\n",
      "Epoch 372/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2786 - val_loss: -0.2691\n",
      "Epoch 373/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2788 - val_loss: -0.2626\n",
      "Epoch 374/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2790 - val_loss: -0.2639\n",
      "Epoch 375/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2756 - val_loss: -0.2677\n",
      "Epoch 376/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2675\n",
      "Epoch 377/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2789 - val_loss: -0.2625\n",
      "Epoch 378/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2793 - val_loss: -0.2642\n",
      "Epoch 379/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2773 - val_loss: -0.2633\n",
      "Epoch 380/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2777 - val_loss: -0.2691\n",
      "Epoch 381/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2680\n",
      "Epoch 382/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2673\n",
      "Epoch 383/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2786 - val_loss: -0.2668\n",
      "Epoch 384/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2786 - val_loss: -0.2679\n",
      "Epoch 385/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2785 - val_loss: -0.2673\n",
      "Epoch 386/10000\n",
      "302161/302161 [==============================] - 6s 19us/sample - loss: -0.2779 - val_loss: -0.2669\n",
      "Epoch 387/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2789 - val_loss: -0.2680\n",
      "Epoch 388/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2665\n",
      "Epoch 389/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2775 - val_loss: -0.2574\n",
      "Epoch 390/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2786 - val_loss: -0.2654\n",
      "Epoch 391/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2789 - val_loss: -0.2644\n",
      "Epoch 392/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2790 - val_loss: -0.2640\n",
      "Epoch 393/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2787 - val_loss: -0.2632\n",
      "Epoch 394/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2689\n",
      "Epoch 395/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2798 - val_loss: -0.2689\n",
      "Epoch 396/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2679\n",
      "Epoch 397/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2796 - val_loss: -0.2670\n",
      "Epoch 398/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2796 - val_loss: -0.2662\n",
      "Epoch 399/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2801 - val_loss: -0.2614\n",
      "Epoch 400/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2788 - val_loss: -0.2515\n",
      "Epoch 401/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2643\n",
      "Epoch 402/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2683\n",
      "Epoch 403/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2804 - val_loss: -0.2672\n",
      "Epoch 404/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2792 - val_loss: -0.2667\n",
      "Epoch 405/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2787 - val_loss: -0.2673\n",
      "Epoch 406/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2791 - val_loss: -0.2665\n",
      "Epoch 407/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2795 - val_loss: -0.2601\n",
      "Epoch 408/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2782 - val_loss: -0.2698\n",
      "Epoch 409/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2797 - val_loss: -0.2629\n",
      "Epoch 410/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2796 - val_loss: -0.2677\n",
      "Epoch 411/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2796 - val_loss: -0.2674\n",
      "Epoch 412/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2801 - val_loss: -0.2686\n",
      "Epoch 413/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2805 - val_loss: -0.2680\n",
      "Epoch 414/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2799 - val_loss: -0.2628\n",
      "Epoch 415/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2797 - val_loss: -0.2695\n",
      "Epoch 416/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2807 - val_loss: -0.2663\n",
      "Epoch 417/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2804 - val_loss: -0.2697\n",
      "Epoch 418/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2796 - val_loss: -0.2684\n",
      "Epoch 419/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2806 - val_loss: -0.2626\n",
      "Epoch 420/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2804 - val_loss: -0.2569\n",
      "Epoch 421/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2791 - val_loss: -0.2607\n",
      "Epoch 422/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2798 - val_loss: -0.2659\n",
      "Epoch 423/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2803 - val_loss: -0.2664\n",
      "Epoch 424/10000\n",
      "302161/302161 [==============================] - 3s 12us/sample - loss: -0.2810 - val_loss: -0.2494\n",
      "Epoch 425/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2812 - val_loss: -0.2605\n",
      "Epoch 426/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2809 - val_loss: -0.2675\n",
      "Epoch 427/10000\n",
      "302161/302161 [==============================] - 4s 13us/sample - loss: -0.2808 - val_loss: -0.2610\n",
      "Epoch 428/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2811 - val_loss: -0.2689\n",
      "Epoch 429/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2805 - val_loss: -0.2682\n",
      "Epoch 430/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2815 - val_loss: -0.2677\n",
      "Epoch 431/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2810 - val_loss: -0.2697\n",
      "Epoch 432/10000\n",
      "302161/302161 [==============================] - 4s 13us/sample - loss: -0.2810 - val_loss: -0.2651\n",
      "Epoch 433/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2803 - val_loss: -0.2658\n",
      "Epoch 434/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2805 - val_loss: -0.2692\n",
      "Epoch 435/10000\n",
      "302161/302161 [==============================] - 5s 15us/sample - loss: -0.2810 - val_loss: -0.2667\n",
      "Epoch 436/10000\n",
      "302161/302161 [==============================] - 4s 14us/sample - loss: -0.2814 - val_loss: -0.2663\n",
      "Epoch 437/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2809 - val_loss: -0.2549\n",
      "Epoch 438/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2821 - val_loss: -0.2636\n",
      "Epoch 439/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2812 - val_loss: -0.2697\n",
      "Epoch 440/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2804 - val_loss: -0.2665\n",
      "Epoch 441/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2809 - val_loss: -0.2622\n",
      "Epoch 442/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2821 - val_loss: -0.2672\n",
      "Epoch 443/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2815 - val_loss: -0.2676\n",
      "Epoch 444/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2817 - val_loss: -0.2688\n",
      "Epoch 445/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2820 - val_loss: -0.2606\n",
      "Epoch 446/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2810 - val_loss: -0.2684\n",
      "Epoch 447/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2807 - val_loss: -0.2677\n",
      "Epoch 448/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2816 - val_loss: -0.2675\n",
      "Epoch 449/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2821 - val_loss: -0.2668\n",
      "Epoch 450/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2826 - val_loss: -0.2643\n",
      "Epoch 451/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2823 - val_loss: -0.2638\n",
      "Epoch 452/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2807 - val_loss: -0.2650\n",
      "Epoch 453/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2803 - val_loss: -0.2640\n",
      "Epoch 454/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2816 - val_loss: -0.2607\n",
      "Epoch 455/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2810 - val_loss: -0.2691\n",
      "Epoch 456/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2828 - val_loss: -0.2658\n",
      "Epoch 457/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2815 - val_loss: -0.2669\n",
      "Epoch 458/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2828 - val_loss: -0.2595\n",
      "Epoch 459/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2822 - val_loss: -0.2671\n",
      "Epoch 460/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2820 - val_loss: -0.2673\n",
      "Epoch 461/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2829 - val_loss: -0.2677\n",
      "Epoch 462/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2821 - val_loss: -0.2691\n",
      "Epoch 463/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2825 - val_loss: -0.2671\n",
      "Epoch 464/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2825 - val_loss: -0.2667\n",
      "Epoch 465/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2809 - val_loss: -0.2680\n",
      "Epoch 466/10000\n",
      "302161/302161 [==============================] - 3s 12us/sample - loss: -0.2824 - val_loss: -0.2630\n",
      "Epoch 467/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2824 - val_loss: -0.2640\n",
      "Epoch 468/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2814 - val_loss: -0.2631\n",
      "Epoch 469/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2833 - val_loss: -0.2679\n",
      "Epoch 470/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2825 - val_loss: -0.2642\n",
      "Epoch 471/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2820 - val_loss: -0.2691\n",
      "Epoch 472/10000\n",
      "302161/302161 [==============================] - 3s 12us/sample - loss: -0.2830 - val_loss: -0.2645\n",
      "Epoch 473/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2831 - val_loss: -0.2667\n",
      "Epoch 474/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2838 - val_loss: -0.2663\n",
      "Epoch 475/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2833 - val_loss: -0.2642\n",
      "Epoch 476/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2829 - val_loss: -0.2652\n",
      "Epoch 477/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2827 - val_loss: -0.2692\n",
      "Epoch 478/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2831 - val_loss: -0.2635\n",
      "Epoch 479/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2832 - val_loss: -0.2648\n",
      "Epoch 480/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2832 - val_loss: -0.2643\n",
      "Epoch 481/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2825 - val_loss: -0.2675\n",
      "Epoch 482/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2830 - val_loss: -0.2658\n",
      "Epoch 483/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2816 - val_loss: -0.2645\n",
      "Epoch 484/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2842 - val_loss: -0.2658\n",
      "Epoch 485/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2830 - val_loss: -0.2649\n",
      "Epoch 486/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2827 - val_loss: -0.2657\n",
      "Epoch 487/10000\n",
      "302161/302161 [==============================] - 3s 11us/sample - loss: -0.2828 - val_loss: -0.2695\n",
      "Epoch 488/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2826 - val_loss: -0.2580\n",
      "Epoch 489/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2834 - val_loss: -0.2658\n",
      "Epoch 490/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2827 - val_loss: -0.2659\n",
      "Epoch 491/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2833 - val_loss: -0.2665\n",
      "Epoch 492/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2836 - val_loss: -0.2657\n",
      "Epoch 493/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2836 - val_loss: -0.2652\n",
      "Epoch 494/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2837 - val_loss: -0.2661\n",
      "Epoch 495/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2842 - val_loss: -0.2634\n",
      "Epoch 496/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2844 - val_loss: -0.2653\n",
      "Epoch 497/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2828 - val_loss: -0.2656\n",
      "Epoch 498/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2839 - val_loss: -0.2672\n",
      "Epoch 499/10000\n",
      "302161/302161 [==============================] - 4s 12us/sample - loss: -0.2834 - val_loss: -0.2616\n",
      "Epoch 500/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2827 - val_loss: -0.2665\n",
      "Epoch 501/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2839 - val_loss: -0.2527\n",
      "Epoch 502/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2837 - val_loss: -0.2663\n",
      "Epoch 503/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2847 - val_loss: -0.2677\n",
      "Epoch 504/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2598\n",
      "Epoch 505/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2832 - val_loss: -0.2569\n",
      "Epoch 506/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2834 - val_loss: -0.2669\n",
      "Epoch 507/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2838 - val_loss: -0.2664\n",
      "Epoch 508/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2667\n",
      "Epoch 509/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2849 - val_loss: -0.2630\n",
      "Epoch 510/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2842 - val_loss: -0.2668\n",
      "Epoch 511/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2837 - val_loss: -0.2636\n",
      "Epoch 512/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2845 - val_loss: -0.2676\n",
      "Epoch 513/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2851 - val_loss: -0.2618\n",
      "Epoch 514/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2844 - val_loss: -0.2591\n",
      "Epoch 515/10000\n",
      "302161/302161 [==============================] - 3s 10us/sample - loss: -0.2834 - val_loss: -0.2673\n",
      "Epoch 516/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2839 - val_loss: -0.2665\n",
      "Epoch 517/10000\n",
      "302161/302161 [==============================] - 3s 9us/sample - loss: -0.2860 - val_loss: -0.2684\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1010.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7552, 6, 1)\n",
      "y_test.shape:  (7552, 1)\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:43,171 WARNING Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1010.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1015.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6173, 6, 1)\n",
      "y_test.shape:  (6173, 1)\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:45,961 WARNING Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1015.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1043.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7807, 6, 1)\n",
      "y_test.shape:  (7807, 1)\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:48,940 WARNING Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1043.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1082.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7890, 6, 1)\n",
      "y_test.shape:  (7890, 1)\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:51,843 WARNING Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1082.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7827, 6, 1)\n",
      "y_test.shape:  (7827, 1)\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:54,715 WARNING Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1121.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7880, 6, 1)\n",
      "y_test.shape:  (7880, 1)\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:57:57,867 WARNING Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1121.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1127.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7091, 6, 1)\n",
      "y_test.shape:  (7091, 1)\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:00,759 WARNING Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1127.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1139.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5447, 6, 1)\n",
      "y_test.shape:  (5447, 1)\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:03,483 WARNING Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1139.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1143.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7667, 6, 1)\n",
      "y_test.shape:  (7667, 1)\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:06,296 WARNING Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1143.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1171.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7816, 6, 1)\n",
      "y_test.shape:  (7816, 1)\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:09,480 WARNING Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1171.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1194.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7959, 6, 1)\n",
      "y_test.shape:  (7959, 1)\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:12,713 WARNING Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1194.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1201.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7893, 6, 1)\n",
      "y_test.shape:  (7893, 1)\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:16,180 WARNING Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1201.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\252.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7399, 6, 1)\n",
      "y_test.shape:  (7399, 1)\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:58:19,362 WARNING Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  252.csv\n",
      "2025-01-21 13:58:21,472 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (298851, 6, 1)\n",
      "y_train.shape:  (298851, 1)\n",
      "x_valid.shape:  (74686, 6, 1)\n",
      "y_valid.shape:  (74686, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 13:59:11,957 WARNING Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 13:59:12,195 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 298851 samples, validate on 74686 samples\n",
      "Epoch 1/10000\n",
      "291840/298851 [============================>.] - ETA: 0s - loss: 0.1422"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298851/298851 [==============================] - 4s 12us/sample - loss: 0.1378 - val_loss: -0.1295\n",
      "Epoch 2/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.0594 - val_loss: -0.1466\n",
      "Epoch 3/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.0991 - val_loss: -0.2042\n",
      "Epoch 4/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1232 - val_loss: -0.2029\n",
      "Epoch 5/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.1430 - val_loss: -0.2262\n",
      "Epoch 6/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1587 - val_loss: -0.2363\n",
      "Epoch 7/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1692 - val_loss: -0.2336\n",
      "Epoch 8/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1808 - val_loss: -0.2457\n",
      "Epoch 9/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1867 - val_loss: -0.2301\n",
      "Epoch 10/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.1982 - val_loss: -0.2533\n",
      "Epoch 11/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2046 - val_loss: -0.2547\n",
      "Epoch 12/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2096 - val_loss: -0.2550\n",
      "Epoch 13/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2156 - val_loss: -0.2619\n",
      "Epoch 14/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2206 - val_loss: -0.2616\n",
      "Epoch 15/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2274 - val_loss: -0.2487\n",
      "Epoch 16/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2298 - val_loss: -0.2634\n",
      "Epoch 17/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2352 - val_loss: -0.2616\n",
      "Epoch 18/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2366 - val_loss: -0.2677\n",
      "Epoch 19/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2426 - val_loss: -0.2614\n",
      "Epoch 20/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2452 - val_loss: -0.2666\n",
      "Epoch 21/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2472 - val_loss: -0.2718\n",
      "Epoch 22/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2505 - val_loss: -0.2704\n",
      "Epoch 23/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2521 - val_loss: -0.2617\n",
      "Epoch 24/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2541 - val_loss: -0.2716\n",
      "Epoch 25/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2549 - val_loss: -0.2701\n",
      "Epoch 26/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2567 - val_loss: -0.2716\n",
      "Epoch 27/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2591 - val_loss: -0.2662\n",
      "Epoch 28/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2601 - val_loss: -0.2732\n",
      "Epoch 29/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2615 - val_loss: -0.2752\n",
      "Epoch 30/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2618 - val_loss: -0.2722\n",
      "Epoch 31/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2624 - val_loss: -0.2744\n",
      "Epoch 32/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2632 - val_loss: -0.2732\n",
      "Epoch 33/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2654 - val_loss: -0.2752\n",
      "Epoch 34/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2656 - val_loss: -0.2663\n",
      "Epoch 35/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2667 - val_loss: -0.2756\n",
      "Epoch 36/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2667 - val_loss: -0.2698\n",
      "Epoch 37/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2682 - val_loss: -0.2698\n",
      "Epoch 38/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2670 - val_loss: -0.2729\n",
      "Epoch 39/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2690 - val_loss: -0.2681\n",
      "Epoch 40/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2690 - val_loss: -0.2737\n",
      "Epoch 41/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.2693 - val_loss: -0.2762\n",
      "Epoch 42/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2694 - val_loss: -0.2764\n",
      "Epoch 43/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2716 - val_loss: -0.2732\n",
      "Epoch 44/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2725 - val_loss: -0.2724\n",
      "Epoch 45/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2704 - val_loss: -0.2747\n",
      "Epoch 46/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2718 - val_loss: -0.2768\n",
      "Epoch 47/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2729 - val_loss: -0.2772\n",
      "Epoch 48/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2726 - val_loss: -0.2790\n",
      "Epoch 49/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2728 - val_loss: -0.2787\n",
      "Epoch 50/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2738 - val_loss: -0.2798\n",
      "Epoch 51/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2736 - val_loss: -0.2778\n",
      "Epoch 52/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2742 - val_loss: -0.2745\n",
      "Epoch 53/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2804\n",
      "Epoch 54/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2740 - val_loss: -0.2693\n",
      "Epoch 55/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2745 - val_loss: -0.2803\n",
      "Epoch 56/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2775 - val_loss: -0.2813\n",
      "Epoch 57/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2750 - val_loss: -0.2771\n",
      "Epoch 58/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2775 - val_loss: -0.2815\n",
      "Epoch 59/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2763 - val_loss: -0.2781\n",
      "Epoch 60/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2755 - val_loss: -0.2803\n",
      "Epoch 61/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2798\n",
      "Epoch 62/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2777 - val_loss: -0.2743\n",
      "Epoch 63/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2768 - val_loss: -0.2757\n",
      "Epoch 64/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2783 - val_loss: -0.2822\n",
      "Epoch 65/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2790 - val_loss: -0.2750\n",
      "Epoch 66/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2780 - val_loss: -0.2805\n",
      "Epoch 67/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2780 - val_loss: -0.2808\n",
      "Epoch 68/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2779 - val_loss: -0.2820\n",
      "Epoch 69/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2841\n",
      "Epoch 70/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2787 - val_loss: -0.2808\n",
      "Epoch 71/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2785 - val_loss: -0.2797\n",
      "Epoch 72/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2795 - val_loss: -0.2801\n",
      "Epoch 73/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2781 - val_loss: -0.2758\n",
      "Epoch 74/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2792 - val_loss: -0.2743\n",
      "Epoch 75/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2795 - val_loss: -0.2771\n",
      "Epoch 76/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2808 - val_loss: -0.2805\n",
      "Epoch 77/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2802\n",
      "Epoch 78/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2797 - val_loss: -0.2801\n",
      "Epoch 79/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2797 - val_loss: -0.2826\n",
      "Epoch 80/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2800 - val_loss: -0.2836\n",
      "Epoch 81/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.2808 - val_loss: -0.2810\n",
      "Epoch 82/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2819 - val_loss: -0.2815\n",
      "Epoch 83/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2808 - val_loss: -0.2858\n",
      "Epoch 84/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2802 - val_loss: -0.2793\n",
      "Epoch 85/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2819 - val_loss: -0.2827\n",
      "Epoch 86/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2812 - val_loss: -0.2813\n",
      "Epoch 87/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2815 - val_loss: -0.2845\n",
      "Epoch 88/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2801 - val_loss: -0.2843\n",
      "Epoch 89/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2813 - val_loss: -0.2811\n",
      "Epoch 90/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2810 - val_loss: -0.2820\n",
      "Epoch 91/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2820 - val_loss: -0.2803\n",
      "Epoch 92/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2821 - val_loss: -0.2818\n",
      "Epoch 93/10000\n",
      "298851/298851 [==============================] - 3s 8us/sample - loss: -0.2818 - val_loss: -0.2774\n",
      "Epoch 94/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2815 - val_loss: -0.2805\n",
      "Epoch 95/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2813 - val_loss: -0.2781\n",
      "Epoch 96/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2829 - val_loss: -0.2786\n",
      "Epoch 97/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2821 - val_loss: -0.2764\n",
      "Epoch 98/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2826 - val_loss: -0.2843\n",
      "Epoch 99/10000\n",
      "298851/298851 [==============================] - 3s 8us/sample - loss: -0.2818 - val_loss: -0.2779\n",
      "Epoch 100/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2820 - val_loss: -0.2640\n",
      "Epoch 101/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2836 - val_loss: -0.2837\n",
      "Epoch 102/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2821 - val_loss: -0.2862\n",
      "Epoch 103/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2826 - val_loss: -0.2791\n",
      "Epoch 104/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2834 - val_loss: -0.2836\n",
      "Epoch 105/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2828 - val_loss: -0.2824\n",
      "Epoch 106/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2840 - val_loss: -0.2857\n",
      "Epoch 107/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2843 - val_loss: -0.2823\n",
      "Epoch 108/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2826 - val_loss: -0.2771\n",
      "Epoch 109/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2835 - val_loss: -0.2802\n",
      "Epoch 110/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2833\n",
      "Epoch 111/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2834 - val_loss: -0.2755\n",
      "Epoch 112/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2832 - val_loss: -0.2823\n",
      "Epoch 113/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2835 - val_loss: -0.2834\n",
      "Epoch 114/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2838 - val_loss: -0.2826\n",
      "Epoch 115/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2841 - val_loss: -0.2844\n",
      "Epoch 116/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2843 - val_loss: -0.2853\n",
      "Epoch 117/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2842 - val_loss: -0.2860\n",
      "Epoch 118/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2840 - val_loss: -0.2841\n",
      "Epoch 119/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2840 - val_loss: -0.2853\n",
      "Epoch 120/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2848 - val_loss: -0.2861\n",
      "Epoch 121/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2847 - val_loss: -0.2799\n",
      "Epoch 122/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2849 - val_loss: -0.2839\n",
      "Epoch 123/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2847 - val_loss: -0.2842\n",
      "Epoch 124/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2840 - val_loss: -0.2835\n",
      "Epoch 125/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2838 - val_loss: -0.2833\n",
      "Epoch 126/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2850 - val_loss: -0.2852\n",
      "Epoch 127/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2849 - val_loss: -0.2871\n",
      "Epoch 128/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2846 - val_loss: -0.2733\n",
      "Epoch 129/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2847 - val_loss: -0.2855\n",
      "Epoch 130/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2849 - val_loss: -0.2844\n",
      "Epoch 131/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2858 - val_loss: -0.2855\n",
      "Epoch 132/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2838 - val_loss: -0.2847\n",
      "Epoch 133/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2858 - val_loss: -0.2880\n",
      "Epoch 134/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2867 - val_loss: -0.2870\n",
      "Epoch 135/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2847 - val_loss: -0.2854\n",
      "Epoch 136/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2850 - val_loss: -0.2837\n",
      "Epoch 137/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2862 - val_loss: -0.2797\n",
      "Epoch 138/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2847 - val_loss: -0.2798\n",
      "Epoch 139/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2866 - val_loss: -0.2804\n",
      "Epoch 140/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2856 - val_loss: -0.2875\n",
      "Epoch 141/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2813\n",
      "Epoch 142/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2847 - val_loss: -0.2843\n",
      "Epoch 143/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2857 - val_loss: -0.2848\n",
      "Epoch 144/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2851 - val_loss: -0.2829\n",
      "Epoch 145/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2864 - val_loss: -0.2813\n",
      "Epoch 146/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2859 - val_loss: -0.2839\n",
      "Epoch 147/10000\n",
      "298851/298851 [==============================] - 3s 12us/sample - loss: -0.2859 - val_loss: -0.2840\n",
      "Epoch 148/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2835\n",
      "Epoch 149/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2859 - val_loss: -0.2806\n",
      "Epoch 150/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2817\n",
      "Epoch 151/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2859 - val_loss: -0.2883\n",
      "Epoch 152/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2868 - val_loss: -0.2842\n",
      "Epoch 153/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2880 - val_loss: -0.2883\n",
      "Epoch 154/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2860 - val_loss: -0.2827\n",
      "Epoch 155/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2875 - val_loss: -0.2856\n",
      "Epoch 156/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2861 - val_loss: -0.2841\n",
      "Epoch 157/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2873 - val_loss: -0.2762\n",
      "Epoch 158/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2872 - val_loss: -0.2841\n",
      "Epoch 159/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.2863 - val_loss: -0.2804\n",
      "Epoch 160/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2876 - val_loss: -0.2786\n",
      "Epoch 161/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2878 - val_loss: -0.2810\n",
      "Epoch 162/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2859 - val_loss: -0.2820\n",
      "Epoch 163/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2875 - val_loss: -0.2883\n",
      "Epoch 164/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2875 - val_loss: -0.2855\n",
      "Epoch 165/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2867 - val_loss: -0.2793\n",
      "Epoch 166/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2869 - val_loss: -0.2844\n",
      "Epoch 167/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2876 - val_loss: -0.2879\n",
      "Epoch 168/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2874 - val_loss: -0.2845\n",
      "Epoch 169/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2882 - val_loss: -0.2878\n",
      "Epoch 170/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2872 - val_loss: -0.2880\n",
      "Epoch 171/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2875 - val_loss: -0.2803\n",
      "Epoch 172/10000\n",
      "298851/298851 [==============================] - 4s 12us/sample - loss: -0.2884 - val_loss: -0.2819\n",
      "Epoch 173/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2879 - val_loss: -0.2821\n",
      "Epoch 174/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2877 - val_loss: -0.2869\n",
      "Epoch 175/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2877 - val_loss: -0.2830\n",
      "Epoch 176/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2877 - val_loss: -0.2849\n",
      "Epoch 177/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2886 - val_loss: -0.2844\n",
      "Epoch 178/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2868 - val_loss: -0.2881\n",
      "Epoch 179/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2878 - val_loss: -0.2810\n",
      "Epoch 180/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2872 - val_loss: -0.2884\n",
      "Epoch 181/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2882 - val_loss: -0.2872\n",
      "Epoch 182/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2880 - val_loss: -0.2810\n",
      "Epoch 183/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2885 - val_loss: -0.2884\n",
      "Epoch 184/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2888 - val_loss: -0.2836\n",
      "Epoch 185/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2888 - val_loss: -0.2856\n",
      "Epoch 186/10000\n",
      "298851/298851 [==============================] - 4s 12us/sample - loss: -0.2875 - val_loss: -0.2873\n",
      "Epoch 187/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2877 - val_loss: -0.2812\n",
      "Epoch 188/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2882 - val_loss: -0.2850\n",
      "Epoch 189/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2884 - val_loss: -0.2834\n",
      "Epoch 190/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2859\n",
      "Epoch 191/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2883 - val_loss: -0.2860\n",
      "Epoch 192/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2890 - val_loss: -0.2828\n",
      "Epoch 193/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2883 - val_loss: -0.2848\n",
      "Epoch 194/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2875 - val_loss: -0.2829\n",
      "Epoch 195/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2876 - val_loss: -0.2856\n",
      "Epoch 196/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2794\n",
      "Epoch 197/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2883 - val_loss: -0.2846\n",
      "Epoch 198/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2848\n",
      "Epoch 199/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2908 - val_loss: -0.2816\n",
      "Epoch 200/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2883 - val_loss: -0.2853\n",
      "Epoch 201/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2898 - val_loss: -0.2851\n",
      "Epoch 202/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2888 - val_loss: -0.2847\n",
      "Epoch 203/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2843\n",
      "Epoch 204/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2901 - val_loss: -0.2847\n",
      "Epoch 205/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2899 - val_loss: -0.2732\n",
      "Epoch 206/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2886 - val_loss: -0.2878\n",
      "Epoch 207/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2879\n",
      "Epoch 208/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2894 - val_loss: -0.2862\n",
      "Epoch 209/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2901 - val_loss: -0.2878\n",
      "Epoch 210/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2902 - val_loss: -0.2868\n",
      "Epoch 211/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2898 - val_loss: -0.2865\n",
      "Epoch 212/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2888 - val_loss: -0.2854\n",
      "Epoch 213/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2830\n",
      "Epoch 214/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2887 - val_loss: -0.2883\n",
      "Epoch 215/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2897 - val_loss: -0.2789\n",
      "Epoch 216/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2897 - val_loss: -0.2816\n",
      "Epoch 217/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2874\n",
      "Epoch 218/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2902 - val_loss: -0.2852\n",
      "Epoch 219/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2897 - val_loss: -0.2854\n",
      "Epoch 220/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2893 - val_loss: -0.2779\n",
      "Epoch 221/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2900 - val_loss: -0.2847\n",
      "Epoch 222/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2893 - val_loss: -0.2844\n",
      "Epoch 223/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2900 - val_loss: -0.2883\n",
      "Epoch 224/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2866\n",
      "Epoch 225/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2899 - val_loss: -0.2809\n",
      "Epoch 226/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2897 - val_loss: -0.2881\n",
      "Epoch 227/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2903 - val_loss: -0.2843\n",
      "Epoch 228/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2903 - val_loss: -0.2861\n",
      "Epoch 229/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2897 - val_loss: -0.2881\n",
      "Epoch 230/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2904 - val_loss: -0.2850\n",
      "Epoch 231/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2908 - val_loss: -0.2861\n",
      "Epoch 232/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2908 - val_loss: -0.2859\n",
      "Epoch 233/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2901 - val_loss: -0.2890\n",
      "Epoch 234/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2908 - val_loss: -0.2869\n",
      "Epoch 235/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2915 - val_loss: -0.2852\n",
      "Epoch 236/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2911 - val_loss: -0.2839\n",
      "Epoch 237/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2903 - val_loss: -0.2884\n",
      "Epoch 238/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2914 - val_loss: -0.2831\n",
      "Epoch 239/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2904 - val_loss: -0.2868\n",
      "Epoch 240/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2901 - val_loss: -0.2865\n",
      "Epoch 241/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2911 - val_loss: -0.2787\n",
      "Epoch 242/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2920 - val_loss: -0.2866\n",
      "Epoch 243/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2906 - val_loss: -0.2864\n",
      "Epoch 244/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2913 - val_loss: -0.2761\n",
      "Epoch 245/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2904 - val_loss: -0.2779\n",
      "Epoch 246/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2922 - val_loss: -0.2873\n",
      "Epoch 247/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2910 - val_loss: -0.2851\n",
      "Epoch 248/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2868\n",
      "Epoch 249/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2912 - val_loss: -0.2809\n",
      "Epoch 250/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2913 - val_loss: -0.2857\n",
      "Epoch 251/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2914 - val_loss: -0.2889\n",
      "Epoch 252/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2918 - val_loss: -0.2875\n",
      "Epoch 253/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2812\n",
      "Epoch 254/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2911 - val_loss: -0.2813\n",
      "Epoch 255/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2848\n",
      "Epoch 256/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2912 - val_loss: -0.2853\n",
      "Epoch 257/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2914 - val_loss: -0.2883\n",
      "Epoch 258/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2910 - val_loss: -0.2852\n",
      "Epoch 259/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2924 - val_loss: -0.2860\n",
      "Epoch 260/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2841\n",
      "Epoch 261/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2913 - val_loss: -0.2890\n",
      "Epoch 262/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2924 - val_loss: -0.2859\n",
      "Epoch 263/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2914 - val_loss: -0.2869\n",
      "Epoch 264/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2915 - val_loss: -0.2848\n",
      "Epoch 265/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2827\n",
      "Epoch 266/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2911 - val_loss: -0.2802\n",
      "Epoch 267/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2913 - val_loss: -0.2865\n",
      "Epoch 268/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2878\n",
      "Epoch 269/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2926 - val_loss: -0.2882\n",
      "Epoch 270/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2852\n",
      "Epoch 271/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2847\n",
      "Epoch 272/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2865\n",
      "Epoch 273/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2865\n",
      "Epoch 274/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2871\n",
      "Epoch 275/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2894\n",
      "Epoch 276/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2872\n",
      "Epoch 277/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2816\n",
      "Epoch 278/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2819\n",
      "Epoch 279/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2885\n",
      "Epoch 280/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2929 - val_loss: -0.2859\n",
      "Epoch 281/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2871\n",
      "Epoch 282/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2923 - val_loss: -0.2876\n",
      "Epoch 283/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2916 - val_loss: -0.2813\n",
      "Epoch 284/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2923 - val_loss: -0.2847\n",
      "Epoch 285/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2920 - val_loss: -0.2839\n",
      "Epoch 286/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2885\n",
      "Epoch 287/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2930 - val_loss: -0.2879\n",
      "Epoch 288/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2934 - val_loss: -0.2893\n",
      "Epoch 289/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2929 - val_loss: -0.2855\n",
      "Epoch 290/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2920 - val_loss: -0.2889\n",
      "Epoch 291/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2852\n",
      "Epoch 292/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2930 - val_loss: -0.2859\n",
      "Epoch 293/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2944 - val_loss: -0.2895\n",
      "Epoch 294/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2924 - val_loss: -0.2829\n",
      "Epoch 295/10000\n",
      "298851/298851 [==============================] - 3s 12us/sample - loss: -0.2932 - val_loss: -0.2858\n",
      "Epoch 296/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2933 - val_loss: -0.2891\n",
      "Epoch 297/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2943 - val_loss: -0.2849\n",
      "Epoch 298/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2820\n",
      "Epoch 299/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2933 - val_loss: -0.2818\n",
      "Epoch 300/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2743\n",
      "Epoch 301/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2933 - val_loss: -0.2861\n",
      "Epoch 302/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2930 - val_loss: -0.2878\n",
      "Epoch 303/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2875\n",
      "Epoch 304/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2931 - val_loss: -0.2868\n",
      "Epoch 305/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2937 - val_loss: -0.2879\n",
      "Epoch 306/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2932 - val_loss: -0.2852\n",
      "Epoch 307/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2888\n",
      "Epoch 308/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2838\n",
      "Epoch 309/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2866\n",
      "Epoch 310/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2856\n",
      "Epoch 311/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2874\n",
      "Epoch 312/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2946 - val_loss: -0.2856\n",
      "Epoch 313/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2865\n",
      "Epoch 314/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2871\n",
      "Epoch 315/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2857\n",
      "Epoch 316/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2940 - val_loss: -0.2879\n",
      "Epoch 317/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2867\n",
      "Epoch 318/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2943 - val_loss: -0.2886\n",
      "Epoch 319/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2941 - val_loss: -0.2890\n",
      "Epoch 320/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2949 - val_loss: -0.2890\n",
      "Epoch 321/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2942 - val_loss: -0.2803\n",
      "Epoch 322/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2846\n",
      "Epoch 323/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2897\n",
      "Epoch 324/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2816\n",
      "Epoch 325/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2934 - val_loss: -0.2871\n",
      "Epoch 326/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2840\n",
      "Epoch 327/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2940 - val_loss: -0.2865\n",
      "Epoch 328/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2844\n",
      "Epoch 329/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2788\n",
      "Epoch 330/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2945 - val_loss: -0.2854\n",
      "Epoch 331/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2871\n",
      "Epoch 332/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2940 - val_loss: -0.2844\n",
      "Epoch 333/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2957 - val_loss: -0.2889\n",
      "Epoch 334/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2951 - val_loss: -0.2881\n",
      "Epoch 335/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2863\n",
      "Epoch 336/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2869\n",
      "Epoch 337/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2947 - val_loss: -0.2863\n",
      "Epoch 338/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2963 - val_loss: -0.2860\n",
      "Epoch 339/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2948 - val_loss: -0.2806\n",
      "Epoch 340/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2850\n",
      "Epoch 341/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2874\n",
      "Epoch 342/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2957 - val_loss: -0.2890\n",
      "Epoch 343/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2878\n",
      "Epoch 344/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2962 - val_loss: -0.2879\n",
      "Epoch 345/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2951 - val_loss: -0.2875\n",
      "Epoch 346/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2955 - val_loss: -0.2869\n",
      "Epoch 347/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2952 - val_loss: -0.2824\n",
      "Epoch 348/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.2959 - val_loss: -0.2874\n",
      "Epoch 349/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2867\n",
      "Epoch 350/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2829\n",
      "Epoch 351/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2953 - val_loss: -0.2890\n",
      "Epoch 352/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2947 - val_loss: -0.2878\n",
      "Epoch 353/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2875\n",
      "Epoch 354/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2853\n",
      "Epoch 355/10000\n",
      "298851/298851 [==============================] - 4s 12us/sample - loss: -0.2963 - val_loss: -0.2880\n",
      "Epoch 356/10000\n",
      "298851/298851 [==============================] - 4s 14us/sample - loss: -0.2959 - val_loss: -0.2846\n",
      "Epoch 357/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2959 - val_loss: -0.2850\n",
      "Epoch 358/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2961 - val_loss: -0.2857\n",
      "Epoch 359/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2961 - val_loss: -0.2875\n",
      "Epoch 360/10000\n",
      "298851/298851 [==============================] - 4s 12us/sample - loss: -0.2963 - val_loss: -0.2853\n",
      "Epoch 361/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2835\n",
      "Epoch 362/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2852\n",
      "Epoch 363/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2868\n",
      "Epoch 364/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2957 - val_loss: -0.2832\n",
      "Epoch 365/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2961 - val_loss: -0.2810\n",
      "Epoch 366/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2967 - val_loss: -0.2871\n",
      "Epoch 367/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2882\n",
      "Epoch 368/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2964 - val_loss: -0.2878\n",
      "Epoch 369/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2976 - val_loss: -0.2859\n",
      "Epoch 370/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2964 - val_loss: -0.2882\n",
      "Epoch 371/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2964 - val_loss: -0.2847\n",
      "Epoch 372/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2972 - val_loss: -0.2875\n",
      "Epoch 373/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2967 - val_loss: -0.2878\n",
      "Epoch 374/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2881\n",
      "Epoch 375/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2855\n",
      "Epoch 376/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2965 - val_loss: -0.2859\n",
      "Epoch 377/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2971 - val_loss: -0.2862\n",
      "Epoch 378/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2969 - val_loss: -0.2841\n",
      "Epoch 379/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2964 - val_loss: -0.2843\n",
      "Epoch 380/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2971 - val_loss: -0.2824\n",
      "Epoch 381/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2960 - val_loss: -0.2898\n",
      "Epoch 382/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2970 - val_loss: -0.2835\n",
      "Epoch 383/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2961 - val_loss: -0.2832\n",
      "Epoch 384/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2874\n",
      "Epoch 385/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2982 - val_loss: -0.2886\n",
      "Epoch 386/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2981 - val_loss: -0.2874\n",
      "Epoch 387/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2976 - val_loss: -0.2858\n",
      "Epoch 388/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2974 - val_loss: -0.2871\n",
      "Epoch 389/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2982 - val_loss: -0.2884\n",
      "Epoch 390/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2978 - val_loss: -0.2842\n",
      "Epoch 391/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2982 - val_loss: -0.2854\n",
      "Epoch 392/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2979 - val_loss: -0.2891\n",
      "Epoch 393/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2967 - val_loss: -0.2880\n",
      "Epoch 394/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2981 - val_loss: -0.2885\n",
      "Epoch 395/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2975 - val_loss: -0.2857\n",
      "Epoch 396/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2986 - val_loss: -0.2858\n",
      "Epoch 397/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2981 - val_loss: -0.2881\n",
      "Epoch 398/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2860\n",
      "Epoch 399/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2982 - val_loss: -0.2877\n",
      "Epoch 400/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2976 - val_loss: -0.2892\n",
      "Epoch 401/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2973 - val_loss: -0.2889\n",
      "Epoch 402/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2975 - val_loss: -0.2882\n",
      "Epoch 403/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2975 - val_loss: -0.2889\n",
      "Epoch 404/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2965 - val_loss: -0.2885\n",
      "Epoch 405/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2984 - val_loss: -0.2855\n",
      "Epoch 406/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2970 - val_loss: -0.2778\n",
      "Epoch 407/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2971 - val_loss: -0.2878\n",
      "Epoch 408/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.2990 - val_loss: -0.2848\n",
      "Epoch 409/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2977 - val_loss: -0.2869\n",
      "Epoch 410/10000\n",
      "298851/298851 [==============================] - 4s 13us/sample - loss: -0.2981 - val_loss: -0.2832\n",
      "Epoch 411/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2987 - val_loss: -0.2789\n",
      "Epoch 412/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2979 - val_loss: -0.2856\n",
      "Epoch 413/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2989 - val_loss: -0.2871\n",
      "Epoch 414/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2875\n",
      "Epoch 415/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2982 - val_loss: -0.2862\n",
      "Epoch 416/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2981 - val_loss: -0.2861\n",
      "Epoch 417/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3003 - val_loss: -0.2871\n",
      "Epoch 418/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2982 - val_loss: -0.2881\n",
      "Epoch 419/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2997 - val_loss: -0.2859\n",
      "Epoch 420/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2879\n",
      "Epoch 421/10000\n",
      "298851/298851 [==============================] - 2s 8us/sample - loss: -0.2993 - val_loss: -0.2855\n",
      "Epoch 422/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2991 - val_loss: -0.2851\n",
      "Epoch 423/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2989 - val_loss: -0.2877\n",
      "Epoch 424/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2996 - val_loss: -0.2872\n",
      "Epoch 425/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2993 - val_loss: -0.2885\n",
      "Epoch 426/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2995 - val_loss: -0.2806\n",
      "Epoch 427/10000\n",
      "298851/298851 [==============================] - 3s 8us/sample - loss: -0.2986 - val_loss: -0.2862\n",
      "Epoch 428/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2974 - val_loss: -0.2831\n",
      "Epoch 429/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2867\n",
      "Epoch 430/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2997 - val_loss: -0.2879\n",
      "Epoch 431/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2871\n",
      "Epoch 432/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2990 - val_loss: -0.2857\n",
      "Epoch 433/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2996 - val_loss: -0.2868\n",
      "Epoch 434/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2839\n",
      "Epoch 435/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2995 - val_loss: -0.2890\n",
      "Epoch 436/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2999 - val_loss: -0.2869\n",
      "Epoch 437/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2878\n",
      "Epoch 438/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2993 - val_loss: -0.2852\n",
      "Epoch 439/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2997 - val_loss: -0.2860\n",
      "Epoch 440/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2989 - val_loss: -0.2851\n",
      "Epoch 441/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2987 - val_loss: -0.2823\n",
      "Epoch 442/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3003 - val_loss: -0.2876\n",
      "Epoch 443/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2994 - val_loss: -0.2867\n",
      "Epoch 444/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2989 - val_loss: -0.2853\n",
      "Epoch 445/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2862\n",
      "Epoch 446/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3001 - val_loss: -0.2855\n",
      "Epoch 447/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3004 - val_loss: -0.2865\n",
      "Epoch 448/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2997 - val_loss: -0.2854\n",
      "Epoch 449/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2994 - val_loss: -0.2816\n",
      "Epoch 450/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2983 - val_loss: -0.2833\n",
      "Epoch 451/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3011 - val_loss: -0.2823\n",
      "Epoch 452/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3003 - val_loss: -0.2839\n",
      "Epoch 453/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2993 - val_loss: -0.2794\n",
      "Epoch 454/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.2992 - val_loss: -0.2809\n",
      "Epoch 455/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3003 - val_loss: -0.2881\n",
      "Epoch 456/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3007 - val_loss: -0.2864\n",
      "Epoch 457/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3009 - val_loss: -0.2880\n",
      "Epoch 458/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3006 - val_loss: -0.2878\n",
      "Epoch 459/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3011 - val_loss: -0.2887\n",
      "Epoch 460/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2883\n",
      "Epoch 461/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3018 - val_loss: -0.2858\n",
      "Epoch 462/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3014 - val_loss: -0.2841\n",
      "Epoch 463/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3005 - val_loss: -0.2843\n",
      "Epoch 464/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3011 - val_loss: -0.2874\n",
      "Epoch 465/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2999 - val_loss: -0.2872\n",
      "Epoch 466/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3010 - val_loss: -0.2859\n",
      "Epoch 467/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3013 - val_loss: -0.2873\n",
      "Epoch 468/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3009 - val_loss: -0.2875\n",
      "Epoch 469/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.2995 - val_loss: -0.2837\n",
      "Epoch 470/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3008 - val_loss: -0.2869\n",
      "Epoch 471/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3005 - val_loss: -0.2851\n",
      "Epoch 472/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3004 - val_loss: -0.2831\n",
      "Epoch 473/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3012 - val_loss: -0.2854\n",
      "Epoch 474/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3012 - val_loss: -0.2875\n",
      "Epoch 475/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3025 - val_loss: -0.2847\n",
      "Epoch 476/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3010 - val_loss: -0.2862\n",
      "Epoch 477/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3014 - val_loss: -0.2879\n",
      "Epoch 478/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3023 - val_loss: -0.2841\n",
      "Epoch 479/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3026 - val_loss: -0.2858\n",
      "Epoch 480/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3007 - val_loss: -0.2851\n",
      "Epoch 481/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3014 - val_loss: -0.2805\n",
      "Epoch 482/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3017 - val_loss: -0.2844\n",
      "Epoch 483/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3013 - val_loss: -0.2826\n",
      "Epoch 484/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3024 - val_loss: -0.2841\n",
      "Epoch 485/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3016 - val_loss: -0.2839\n",
      "Epoch 486/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3015 - val_loss: -0.2854\n",
      "Epoch 487/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3023 - val_loss: -0.2865\n",
      "Epoch 488/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3030 - val_loss: -0.2866\n",
      "Epoch 489/10000\n",
      "298851/298851 [==============================] - 3s 11us/sample - loss: -0.3018 - val_loss: -0.2824\n",
      "Epoch 490/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3018 - val_loss: -0.2851\n",
      "Epoch 491/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3013 - val_loss: -0.2849\n",
      "Epoch 492/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3024 - val_loss: -0.2875\n",
      "Epoch 493/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3025 - val_loss: -0.2852\n",
      "Epoch 494/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3023 - val_loss: -0.2861\n",
      "Epoch 495/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3022 - val_loss: -0.2867\n",
      "Epoch 496/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3023 - val_loss: -0.2834\n",
      "Epoch 497/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3023 - val_loss: -0.2870\n",
      "Epoch 498/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3013 - val_loss: -0.2869\n",
      "Epoch 499/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3028 - val_loss: -0.2840\n",
      "Epoch 500/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3022 - val_loss: -0.2795\n",
      "Epoch 501/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3023 - val_loss: -0.2862\n",
      "Epoch 502/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3030 - val_loss: -0.2836\n",
      "Epoch 503/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3026 - val_loss: -0.2871\n",
      "Epoch 504/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3029 - val_loss: -0.2834\n",
      "Epoch 505/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3025 - val_loss: -0.2839\n",
      "Epoch 506/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3023 - val_loss: -0.2840\n",
      "Epoch 507/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3030 - val_loss: -0.2819\n",
      "Epoch 508/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3025 - val_loss: -0.2864\n",
      "Epoch 509/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3030 - val_loss: -0.2864\n",
      "Epoch 510/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3030 - val_loss: -0.2876\n",
      "Epoch 511/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3029 - val_loss: -0.2865\n",
      "Epoch 512/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3038 - val_loss: -0.2850\n",
      "Epoch 513/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3019 - val_loss: -0.2852\n",
      "Epoch 514/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3045 - val_loss: -0.2858\n",
      "Epoch 515/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3044 - val_loss: -0.2851\n",
      "Epoch 516/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3040 - val_loss: -0.2855\n",
      "Epoch 517/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3028 - val_loss: -0.2865\n",
      "Epoch 518/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3037 - val_loss: -0.2863\n",
      "Epoch 519/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3038 - val_loss: -0.2839\n",
      "Epoch 520/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3034 - val_loss: -0.2864\n",
      "Epoch 521/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3030 - val_loss: -0.2868\n",
      "Epoch 522/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3039 - val_loss: -0.2868\n",
      "Epoch 523/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3042 - val_loss: -0.2870\n",
      "Epoch 524/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3035 - val_loss: -0.2843\n",
      "Epoch 525/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3035 - val_loss: -0.2866\n",
      "Epoch 526/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3036 - val_loss: -0.2852\n",
      "Epoch 527/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3042 - val_loss: -0.2815\n",
      "Epoch 528/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3029 - val_loss: -0.2868\n",
      "Epoch 529/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3049 - val_loss: -0.2847\n",
      "Epoch 530/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3040 - val_loss: -0.2821\n",
      "Epoch 531/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3040 - val_loss: -0.2847\n",
      "Epoch 532/10000\n",
      "298851/298851 [==============================] - 4s 12us/sample - loss: -0.3047 - val_loss: -0.2833\n",
      "Epoch 533/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3045 - val_loss: -0.2825\n",
      "Epoch 534/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3033 - val_loss: -0.2870\n",
      "Epoch 535/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3060 - val_loss: -0.2840\n",
      "Epoch 536/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3040 - val_loss: -0.2814\n",
      "Epoch 537/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3045 - val_loss: -0.2812\n",
      "Epoch 538/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3039 - val_loss: -0.2865\n",
      "Epoch 539/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3031 - val_loss: -0.2834\n",
      "Epoch 540/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3050 - val_loss: -0.2811\n",
      "Epoch 541/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3056 - val_loss: -0.2810\n",
      "Epoch 542/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3046 - val_loss: -0.2729\n",
      "Epoch 543/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3043 - val_loss: -0.2822\n",
      "Epoch 544/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3038 - val_loss: -0.2858\n",
      "Epoch 545/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3053 - val_loss: -0.2762\n",
      "Epoch 546/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3036 - val_loss: -0.2852\n",
      "Epoch 547/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2849\n",
      "Epoch 548/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3058 - val_loss: -0.2855\n",
      "Epoch 549/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3054 - val_loss: -0.2841\n",
      "Epoch 550/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3048 - val_loss: -0.2822\n",
      "Epoch 551/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3039 - val_loss: -0.2808\n",
      "Epoch 552/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2836\n",
      "Epoch 553/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3049 - val_loss: -0.2844\n",
      "Epoch 554/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3053 - val_loss: -0.2847\n",
      "Epoch 555/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3055 - val_loss: -0.2852\n",
      "Epoch 556/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3046 - val_loss: -0.2804\n",
      "Epoch 557/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3054 - val_loss: -0.2835\n",
      "Epoch 558/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2824\n",
      "Epoch 559/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3048 - val_loss: -0.2822\n",
      "Epoch 560/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3059 - val_loss: -0.2834\n",
      "Epoch 561/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3044 - val_loss: -0.2823\n",
      "Epoch 562/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3048 - val_loss: -0.2834\n",
      "Epoch 563/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3057 - val_loss: -0.2849\n",
      "Epoch 564/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3069 - val_loss: -0.2842\n",
      "Epoch 565/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3060 - val_loss: -0.2819\n",
      "Epoch 566/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3057 - val_loss: -0.2828\n",
      "Epoch 567/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3062 - val_loss: -0.2832\n",
      "Epoch 568/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2794\n",
      "Epoch 569/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3059 - val_loss: -0.2811\n",
      "Epoch 570/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3051 - val_loss: -0.2835\n",
      "Epoch 571/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3071 - val_loss: -0.2834\n",
      "Epoch 572/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3074 - val_loss: -0.2767\n",
      "Epoch 573/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2815\n",
      "Epoch 574/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3068 - val_loss: -0.2821\n",
      "Epoch 575/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3069 - val_loss: -0.2857\n",
      "Epoch 576/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3071 - val_loss: -0.2843\n",
      "Epoch 577/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3068 - val_loss: -0.2817\n",
      "Epoch 578/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3059 - val_loss: -0.2822\n",
      "Epoch 579/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3071 - val_loss: -0.2831\n",
      "Epoch 580/10000\n",
      "298851/298851 [==============================] - 3s 10us/sample - loss: -0.3061 - val_loss: -0.2810\n",
      "Epoch 581/10000\n",
      "298851/298851 [==============================] - 3s 9us/sample - loss: -0.3061 - val_loss: -0.2795\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1205.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7510, 6, 1)\n",
      "y_test.shape:  (7510, 1)\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:22,249 WARNING Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1205.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1211.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7871, 6, 1)\n",
      "y_test.shape:  (7871, 1)\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:25,659 WARNING Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1211.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1219.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7913, 6, 1)\n",
      "y_test.shape:  (7913, 1)\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:29,261 WARNING Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1219.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1230.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7691, 6, 1)\n",
      "y_test.shape:  (7691, 1)\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:32,938 WARNING Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1230.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1239.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7753, 6, 1)\n",
      "y_test.shape:  (7753, 1)\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:36,365 WARNING Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1239.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1271.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7960, 6, 1)\n",
      "y_test.shape:  (7960, 1)\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:40,240 WARNING Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1271.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1286.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7981, 6, 1)\n",
      "y_test.shape:  (7981, 1)\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:43,987 WARNING Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1286.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1311.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7515, 6, 1)\n",
      "y_test.shape:  (7515, 1)\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:47,565 WARNING Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1311.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1330.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7907, 6, 1)\n",
      "y_test.shape:  (7907, 1)\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:51,293 WARNING Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1330.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1336.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7747, 6, 1)\n",
      "y_test.shape:  (7747, 1)\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:54,999 WARNING Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1336.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1343.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7373, 6, 1)\n",
      "y_test.shape:  (7373, 1)\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:26:58,803 WARNING Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1343.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1345.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7535, 6, 1)\n",
      "y_test.shape:  (7535, 1)\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:27:02,492 WARNING Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1345.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1348.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7785, 6, 1)\n",
      "y_test.shape:  (7785, 1)\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:27:06,710 WARNING Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1348.csv\n",
      "2025-01-21 14:27:09,573 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (301478, 6, 1)\n",
      "y_train.shape:  (301478, 1)\n",
      "x_valid.shape:  (75346, 6, 1)\n",
      "y_valid.shape:  (75346, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:28:00,165 WARNING Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 14:28:00,323 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 301478 samples, validate on 75346 samples\n",
      "Epoch 1/10000\n",
      "301478/301478 [==============================] - ETA: 0s - loss: 0.1540"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301478/301478 [==============================] - 4s 14us/sample - loss: 0.1540 - val_loss: -0.1388\n",
      "Epoch 2/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.0498 - val_loss: -0.1659\n",
      "Epoch 3/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.0959 - val_loss: -0.1610\n",
      "Epoch 4/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.1219 - val_loss: -0.1560\n",
      "Epoch 5/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.1466 - val_loss: -0.2225\n",
      "Epoch 6/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.1642 - val_loss: -0.2460\n",
      "Epoch 7/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.1769 - val_loss: -0.2473\n",
      "Epoch 8/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.1868 - val_loss: -0.2462\n",
      "Epoch 9/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.1969 - val_loss: -0.2439\n",
      "Epoch 10/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2015 - val_loss: -0.2144\n",
      "Epoch 11/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2099 - val_loss: -0.2519\n",
      "Epoch 12/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2130 - val_loss: -0.2588\n",
      "Epoch 13/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2228 - val_loss: -0.2564\n",
      "Epoch 14/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2264 - val_loss: -0.2678\n",
      "Epoch 15/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2332 - val_loss: -0.2537\n",
      "Epoch 16/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2375 - val_loss: -0.2641\n",
      "Epoch 17/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2429 - val_loss: -0.2711\n",
      "Epoch 18/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2465 - val_loss: -0.2747\n",
      "Epoch 19/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2499 - val_loss: -0.2682\n",
      "Epoch 20/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2525 - val_loss: -0.2707\n",
      "Epoch 21/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2535 - val_loss: -0.2828\n",
      "Epoch 22/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2558 - val_loss: -0.2755\n",
      "Epoch 23/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2596 - val_loss: -0.2759\n",
      "Epoch 24/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2590 - val_loss: -0.2730\n",
      "Epoch 25/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2619 - val_loss: -0.2617\n",
      "Epoch 26/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2634 - val_loss: -0.2742\n",
      "Epoch 27/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2646 - val_loss: -0.2770\n",
      "Epoch 28/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2662 - val_loss: -0.2800\n",
      "Epoch 29/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2671 - val_loss: -0.2639\n",
      "Epoch 30/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2683 - val_loss: -0.2743\n",
      "Epoch 31/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2695 - val_loss: -0.2757\n",
      "Epoch 32/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2692 - val_loss: -0.2808\n",
      "Epoch 33/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2718 - val_loss: -0.2772\n",
      "Epoch 34/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2695 - val_loss: -0.2792\n",
      "Epoch 35/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2727 - val_loss: -0.2791\n",
      "Epoch 36/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2720 - val_loss: -0.2701\n",
      "Epoch 37/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2723 - val_loss: -0.2864\n",
      "Epoch 38/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2727 - val_loss: -0.2706\n",
      "Epoch 39/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2726 - val_loss: -0.2773\n",
      "Epoch 40/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2748\n",
      "Epoch 41/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2739 - val_loss: -0.2732\n",
      "Epoch 42/10000\n",
      "301478/301478 [==============================] - 3s 12us/sample - loss: -0.2761 - val_loss: -0.2815\n",
      "Epoch 43/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2755 - val_loss: -0.2587\n",
      "Epoch 44/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2847\n",
      "Epoch 45/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2762 - val_loss: -0.2851\n",
      "Epoch 46/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2773 - val_loss: -0.2834\n",
      "Epoch 47/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2787 - val_loss: -0.2838\n",
      "Epoch 48/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2792 - val_loss: -0.2766\n",
      "Epoch 49/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2791 - val_loss: -0.2834\n",
      "Epoch 50/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2788 - val_loss: -0.2848\n",
      "Epoch 51/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2803 - val_loss: -0.2861\n",
      "Epoch 52/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2794 - val_loss: -0.2806\n",
      "Epoch 53/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2792 - val_loss: -0.2873\n",
      "Epoch 54/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2804 - val_loss: -0.2773\n",
      "Epoch 55/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2811 - val_loss: -0.2859\n",
      "Epoch 56/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2814 - val_loss: -0.2876\n",
      "Epoch 57/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2817 - val_loss: -0.2861\n",
      "Epoch 58/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2826 - val_loss: -0.2878\n",
      "Epoch 59/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2818 - val_loss: -0.2906\n",
      "Epoch 60/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2826 - val_loss: -0.2873\n",
      "Epoch 61/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2821 - val_loss: -0.2882\n",
      "Epoch 62/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2817 - val_loss: -0.2910\n",
      "Epoch 63/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2833 - val_loss: -0.2867\n",
      "Epoch 64/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2827 - val_loss: -0.2793\n",
      "Epoch 65/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2839 - val_loss: -0.2887\n",
      "Epoch 66/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2820 - val_loss: -0.2764\n",
      "Epoch 67/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2850 - val_loss: -0.2901\n",
      "Epoch 68/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2845 - val_loss: -0.2808\n",
      "Epoch 69/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2847 - val_loss: -0.2887\n",
      "Epoch 70/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2856 - val_loss: -0.2885\n",
      "Epoch 71/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2836 - val_loss: -0.2845\n",
      "Epoch 72/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2844 - val_loss: -0.2864\n",
      "Epoch 73/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2852 - val_loss: -0.2877\n",
      "Epoch 74/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2845 - val_loss: -0.2917\n",
      "Epoch 75/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2856 - val_loss: -0.2895\n",
      "Epoch 76/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2866 - val_loss: -0.2890\n",
      "Epoch 77/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2851 - val_loss: -0.2909\n",
      "Epoch 78/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2855 - val_loss: -0.2896\n",
      "Epoch 79/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2863 - val_loss: -0.2836\n",
      "Epoch 80/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2845 - val_loss: -0.2897\n",
      "Epoch 81/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2857 - val_loss: -0.2749\n",
      "Epoch 82/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2862 - val_loss: -0.2920\n",
      "Epoch 83/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2868 - val_loss: -0.2846\n",
      "Epoch 84/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2872 - val_loss: -0.2739\n",
      "Epoch 85/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2876 - val_loss: -0.2885\n",
      "Epoch 86/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2870 - val_loss: -0.2873\n",
      "Epoch 87/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2874 - val_loss: -0.2867\n",
      "Epoch 88/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2868 - val_loss: -0.2897\n",
      "Epoch 89/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2860 - val_loss: -0.2887\n",
      "Epoch 90/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2880 - val_loss: -0.2894\n",
      "Epoch 91/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2877 - val_loss: -0.2903\n",
      "Epoch 92/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2880 - val_loss: -0.2895\n",
      "Epoch 93/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2887 - val_loss: -0.2886\n",
      "Epoch 94/10000\n",
      "301478/301478 [==============================] - 4s 12us/sample - loss: -0.2874 - val_loss: -0.2851\n",
      "Epoch 95/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2888 - val_loss: -0.2940\n",
      "Epoch 96/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2882 - val_loss: -0.2875\n",
      "Epoch 97/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2881 - val_loss: -0.2861\n",
      "Epoch 98/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2893 - val_loss: -0.2903\n",
      "Epoch 99/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2898 - val_loss: -0.2935\n",
      "Epoch 100/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2892 - val_loss: -0.2865\n",
      "Epoch 101/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2882 - val_loss: -0.2928\n",
      "Epoch 102/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2880 - val_loss: -0.2896\n",
      "Epoch 103/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2888 - val_loss: -0.2881\n",
      "Epoch 104/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2897 - val_loss: -0.2913\n",
      "Epoch 105/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2885 - val_loss: -0.2867\n",
      "Epoch 106/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2884 - val_loss: -0.2885\n",
      "Epoch 107/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2880 - val_loss: -0.2913\n",
      "Epoch 108/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2899 - val_loss: -0.2890\n",
      "Epoch 109/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2890 - val_loss: -0.2889\n",
      "Epoch 110/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2906 - val_loss: -0.2896\n",
      "Epoch 111/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2898 - val_loss: -0.2913\n",
      "Epoch 112/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2897 - val_loss: -0.2833\n",
      "Epoch 113/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2900 - val_loss: -0.2894\n",
      "Epoch 114/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2897 - val_loss: -0.2918\n",
      "Epoch 115/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2902 - val_loss: -0.2872\n",
      "Epoch 116/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2896 - val_loss: -0.2826\n",
      "Epoch 117/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2901 - val_loss: -0.2860\n",
      "Epoch 118/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2898 - val_loss: -0.2840\n",
      "Epoch 119/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2899 - val_loss: -0.2889\n",
      "Epoch 120/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2894 - val_loss: -0.2741\n",
      "Epoch 121/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2907 - val_loss: -0.2923\n",
      "Epoch 122/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2905 - val_loss: -0.2870\n",
      "Epoch 123/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2915 - val_loss: -0.2886\n",
      "Epoch 124/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2900 - val_loss: -0.2879\n",
      "Epoch 125/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2900 - val_loss: -0.2917\n",
      "Epoch 126/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2892\n",
      "Epoch 127/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2899 - val_loss: -0.2889\n",
      "Epoch 128/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2907 - val_loss: -0.2882\n",
      "Epoch 129/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2910 - val_loss: -0.2911\n",
      "Epoch 130/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2906 - val_loss: -0.2946\n",
      "Epoch 131/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2900 - val_loss: -0.2913\n",
      "Epoch 132/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2904 - val_loss: -0.2896\n",
      "Epoch 133/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2913 - val_loss: -0.2914\n",
      "Epoch 134/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2933\n",
      "Epoch 135/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2910\n",
      "Epoch 136/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2905 - val_loss: -0.2866\n",
      "Epoch 137/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2921\n",
      "Epoch 138/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2918 - val_loss: -0.2902\n",
      "Epoch 139/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2903 - val_loss: -0.2856\n",
      "Epoch 140/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2912 - val_loss: -0.2906\n",
      "Epoch 141/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2921 - val_loss: -0.2925\n",
      "Epoch 142/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2920 - val_loss: -0.2897\n",
      "Epoch 143/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2913 - val_loss: -0.2895\n",
      "Epoch 144/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2896\n",
      "Epoch 145/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2909 - val_loss: -0.2781\n",
      "Epoch 146/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2911 - val_loss: -0.2885\n",
      "Epoch 147/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2889\n",
      "Epoch 148/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2923 - val_loss: -0.2908\n",
      "Epoch 149/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2914 - val_loss: -0.2912\n",
      "Epoch 150/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2876\n",
      "Epoch 151/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2919 - val_loss: -0.2855\n",
      "Epoch 152/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2911 - val_loss: -0.2903\n",
      "Epoch 153/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2929 - val_loss: -0.2814\n",
      "Epoch 154/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2927 - val_loss: -0.2872\n",
      "Epoch 155/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2917 - val_loss: -0.2907\n",
      "Epoch 156/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2940\n",
      "Epoch 157/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2915\n",
      "Epoch 158/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2924 - val_loss: -0.2941\n",
      "Epoch 159/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2877\n",
      "Epoch 160/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2924 - val_loss: -0.2845\n",
      "Epoch 161/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2900\n",
      "Epoch 162/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2929 - val_loss: -0.2900\n",
      "Epoch 163/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2929 - val_loss: -0.2912\n",
      "Epoch 164/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2924 - val_loss: -0.2904\n",
      "Epoch 165/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2926 - val_loss: -0.2858\n",
      "Epoch 166/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2927 - val_loss: -0.2891\n",
      "Epoch 167/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2848\n",
      "Epoch 168/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2936 - val_loss: -0.2908\n",
      "Epoch 169/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2937\n",
      "Epoch 170/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2930 - val_loss: -0.2921\n",
      "Epoch 171/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2924 - val_loss: -0.2841\n",
      "Epoch 172/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2872\n",
      "Epoch 173/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2863\n",
      "Epoch 174/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2936 - val_loss: -0.2880\n",
      "Epoch 175/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2934 - val_loss: -0.2956\n",
      "Epoch 176/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2943 - val_loss: -0.2893\n",
      "Epoch 177/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2933 - val_loss: -0.2912\n",
      "Epoch 178/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2922\n",
      "Epoch 179/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2943 - val_loss: -0.2934\n",
      "Epoch 180/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2930 - val_loss: -0.2897\n",
      "Epoch 181/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2935 - val_loss: -0.2911\n",
      "Epoch 182/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2942 - val_loss: -0.2898\n",
      "Epoch 183/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2933 - val_loss: -0.2912\n",
      "Epoch 184/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2949 - val_loss: -0.2889\n",
      "Epoch 185/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2941 - val_loss: -0.2908\n",
      "Epoch 186/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2941 - val_loss: -0.2903\n",
      "Epoch 187/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2935 - val_loss: -0.2944\n",
      "Epoch 188/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2943 - val_loss: -0.2920\n",
      "Epoch 189/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2945 - val_loss: -0.2920\n",
      "Epoch 190/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2936 - val_loss: -0.2891\n",
      "Epoch 191/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2944 - val_loss: -0.2900\n",
      "Epoch 192/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2928 - val_loss: -0.2912\n",
      "Epoch 193/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2941 - val_loss: -0.2932\n",
      "Epoch 194/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2946 - val_loss: -0.2899\n",
      "Epoch 195/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2951 - val_loss: -0.2880\n",
      "Epoch 196/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2944 - val_loss: -0.2906\n",
      "Epoch 197/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2951 - val_loss: -0.2933\n",
      "Epoch 198/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2936\n",
      "Epoch 199/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2888\n",
      "Epoch 200/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2785\n",
      "Epoch 201/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2916\n",
      "Epoch 202/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2941 - val_loss: -0.2895\n",
      "Epoch 203/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2912\n",
      "Epoch 204/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2947 - val_loss: -0.2903\n",
      "Epoch 205/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2954 - val_loss: -0.2885\n",
      "Epoch 206/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2955 - val_loss: -0.2945\n",
      "Epoch 207/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2953 - val_loss: -0.2930\n",
      "Epoch 208/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2960 - val_loss: -0.2896\n",
      "Epoch 209/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2947 - val_loss: -0.2890\n",
      "Epoch 210/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2951 - val_loss: -0.2901\n",
      "Epoch 211/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2949 - val_loss: -0.2885\n",
      "Epoch 212/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2947 - val_loss: -0.2921\n",
      "Epoch 213/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2852\n",
      "Epoch 214/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2956 - val_loss: -0.2889\n",
      "Epoch 215/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2951 - val_loss: -0.2938\n",
      "Epoch 216/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2948 - val_loss: -0.2931\n",
      "Epoch 217/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2963 - val_loss: -0.2917\n",
      "Epoch 218/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2952 - val_loss: -0.2716\n",
      "Epoch 219/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2865\n",
      "Epoch 220/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2954 - val_loss: -0.2936\n",
      "Epoch 221/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2907\n",
      "Epoch 222/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2925\n",
      "Epoch 223/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.2953 - val_loss: -0.2917\n",
      "Epoch 224/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2926\n",
      "Epoch 225/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2896\n",
      "Epoch 226/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2957 - val_loss: -0.2906\n",
      "Epoch 227/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2952 - val_loss: -0.2909\n",
      "Epoch 228/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2955 - val_loss: -0.2952\n",
      "Epoch 229/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2972 - val_loss: -0.2927\n",
      "Epoch 230/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2971 - val_loss: -0.2920\n",
      "Epoch 231/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2961 - val_loss: -0.2847\n",
      "Epoch 232/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2857\n",
      "Epoch 233/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2967 - val_loss: -0.2861\n",
      "Epoch 234/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2973 - val_loss: -0.2938\n",
      "Epoch 235/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2975 - val_loss: -0.2882\n",
      "Epoch 236/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2966 - val_loss: -0.2917\n",
      "Epoch 237/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2966 - val_loss: -0.2890\n",
      "Epoch 238/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2966 - val_loss: -0.2904\n",
      "Epoch 239/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2975 - val_loss: -0.2955\n",
      "Epoch 240/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2960 - val_loss: -0.2908\n",
      "Epoch 241/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2974 - val_loss: -0.2893\n",
      "Epoch 242/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2970 - val_loss: -0.2898\n",
      "Epoch 243/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2962 - val_loss: -0.2878\n",
      "Epoch 244/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2975 - val_loss: -0.2927\n",
      "Epoch 245/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2958 - val_loss: -0.2915\n",
      "Epoch 246/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2970 - val_loss: -0.2924\n",
      "Epoch 247/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2973 - val_loss: -0.2929\n",
      "Epoch 248/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2969 - val_loss: -0.2938\n",
      "Epoch 249/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2970 - val_loss: -0.2900\n",
      "Epoch 250/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2973 - val_loss: -0.2907\n",
      "Epoch 251/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2979 - val_loss: -0.2910\n",
      "Epoch 252/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2970 - val_loss: -0.2936\n",
      "Epoch 253/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2983 - val_loss: -0.2885\n",
      "Epoch 254/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2975 - val_loss: -0.2934\n",
      "Epoch 255/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2981 - val_loss: -0.2884\n",
      "Epoch 256/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2971 - val_loss: -0.2940\n",
      "Epoch 257/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2981 - val_loss: -0.2953\n",
      "Epoch 258/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2977 - val_loss: -0.2956\n",
      "Epoch 259/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2976 - val_loss: -0.2943\n",
      "Epoch 260/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2985 - val_loss: -0.2901\n",
      "Epoch 261/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2984 - val_loss: -0.2929\n",
      "Epoch 262/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2975 - val_loss: -0.2868\n",
      "Epoch 263/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2977 - val_loss: -0.2891\n",
      "Epoch 264/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2978 - val_loss: -0.2903\n",
      "Epoch 265/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2952\n",
      "Epoch 266/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2991 - val_loss: -0.2957\n",
      "Epoch 267/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2984 - val_loss: -0.2950\n",
      "Epoch 268/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2971 - val_loss: -0.2947\n",
      "Epoch 269/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2987 - val_loss: -0.2922\n",
      "Epoch 270/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2992 - val_loss: -0.2925\n",
      "Epoch 271/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2968 - val_loss: -0.2920\n",
      "Epoch 272/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2980 - val_loss: -0.2919\n",
      "Epoch 273/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2978 - val_loss: -0.2902\n",
      "Epoch 274/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2978 - val_loss: -0.2918\n",
      "Epoch 275/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2981 - val_loss: -0.2908\n",
      "Epoch 276/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2983 - val_loss: -0.2935\n",
      "Epoch 277/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2971 - val_loss: -0.2942\n",
      "Epoch 278/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2986 - val_loss: -0.2929\n",
      "Epoch 279/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2927\n",
      "Epoch 280/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2929\n",
      "Epoch 281/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2983 - val_loss: -0.2933\n",
      "Epoch 282/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2985 - val_loss: -0.2944\n",
      "Epoch 283/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2996 - val_loss: -0.2939\n",
      "Epoch 284/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2922\n",
      "Epoch 285/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2930\n",
      "Epoch 286/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2991 - val_loss: -0.2932\n",
      "Epoch 287/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2989 - val_loss: -0.2932\n",
      "Epoch 288/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2853\n",
      "Epoch 289/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2984 - val_loss: -0.2898\n",
      "Epoch 290/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2995 - val_loss: -0.2908\n",
      "Epoch 291/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.2992 - val_loss: -0.2959\n",
      "Epoch 292/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2991 - val_loss: -0.2957\n",
      "Epoch 293/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2994 - val_loss: -0.2945\n",
      "Epoch 294/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2985 - val_loss: -0.2907\n",
      "Epoch 295/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2996 - val_loss: -0.2938\n",
      "Epoch 296/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2994 - val_loss: -0.2917\n",
      "Epoch 297/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2984 - val_loss: -0.2832\n",
      "Epoch 298/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2990 - val_loss: -0.2907\n",
      "Epoch 299/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3003 - val_loss: -0.2934\n",
      "Epoch 300/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2991 - val_loss: -0.2912\n",
      "Epoch 301/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3001 - val_loss: -0.2901\n",
      "Epoch 302/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3001 - val_loss: -0.2922\n",
      "Epoch 303/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2995 - val_loss: -0.2961\n",
      "Epoch 304/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2988 - val_loss: -0.2956\n",
      "Epoch 305/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3013 - val_loss: -0.2933\n",
      "Epoch 306/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3005 - val_loss: -0.2933\n",
      "Epoch 307/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.2999 - val_loss: -0.2960\n",
      "Epoch 308/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.2996 - val_loss: -0.2908\n",
      "Epoch 309/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3007 - val_loss: -0.2943\n",
      "Epoch 310/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3002 - val_loss: -0.2922\n",
      "Epoch 311/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2994 - val_loss: -0.2927\n",
      "Epoch 312/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3001 - val_loss: -0.2925\n",
      "Epoch 313/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3008 - val_loss: -0.2873\n",
      "Epoch 314/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3005 - val_loss: -0.2926\n",
      "Epoch 315/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3007 - val_loss: -0.2939\n",
      "Epoch 316/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.2989 - val_loss: -0.2807\n",
      "Epoch 317/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3001 - val_loss: -0.2938\n",
      "Epoch 318/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3003 - val_loss: -0.2920\n",
      "Epoch 319/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3015 - val_loss: -0.2933\n",
      "Epoch 320/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3003 - val_loss: -0.2939\n",
      "Epoch 321/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3006 - val_loss: -0.2940\n",
      "Epoch 322/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3006 - val_loss: -0.2879\n",
      "Epoch 323/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3010 - val_loss: -0.2941\n",
      "Epoch 324/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3010 - val_loss: -0.2936\n",
      "Epoch 325/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3014 - val_loss: -0.2929\n",
      "Epoch 326/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3015 - val_loss: -0.2920\n",
      "Epoch 327/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3001 - val_loss: -0.2940\n",
      "Epoch 328/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3010 - val_loss: -0.2919\n",
      "Epoch 329/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3012 - val_loss: -0.2923\n",
      "Epoch 330/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3014 - val_loss: -0.2919\n",
      "Epoch 331/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3004 - val_loss: -0.2922\n",
      "Epoch 332/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3006 - val_loss: -0.2932\n",
      "Epoch 333/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3012 - val_loss: -0.2870\n",
      "Epoch 334/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3003 - val_loss: -0.2908\n",
      "Epoch 335/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3017 - val_loss: -0.2932\n",
      "Epoch 336/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3007 - val_loss: -0.2937\n",
      "Epoch 337/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3015 - val_loss: -0.2909\n",
      "Epoch 338/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3012 - val_loss: -0.2911\n",
      "Epoch 339/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3014 - val_loss: -0.2917\n",
      "Epoch 340/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3004 - val_loss: -0.2951\n",
      "Epoch 341/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3009 - val_loss: -0.2938\n",
      "Epoch 342/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3006 - val_loss: -0.2937\n",
      "Epoch 343/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3019 - val_loss: -0.2907\n",
      "Epoch 344/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3004 - val_loss: -0.2903\n",
      "Epoch 345/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3013 - val_loss: -0.2913\n",
      "Epoch 346/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3026 - val_loss: -0.2929\n",
      "Epoch 347/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3011 - val_loss: -0.2798\n",
      "Epoch 348/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3028 - val_loss: -0.2907\n",
      "Epoch 349/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3031 - val_loss: -0.2922\n",
      "Epoch 350/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3013 - val_loss: -0.2919\n",
      "Epoch 351/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3019 - val_loss: -0.2876\n",
      "Epoch 352/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3029 - val_loss: -0.2915\n",
      "Epoch 353/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3022 - val_loss: -0.2938\n",
      "Epoch 354/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3034 - val_loss: -0.2915\n",
      "Epoch 355/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3028 - val_loss: -0.2931\n",
      "Epoch 356/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3029 - val_loss: -0.2877\n",
      "Epoch 357/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3016 - val_loss: -0.2926\n",
      "Epoch 358/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3019 - val_loss: -0.2920\n",
      "Epoch 359/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3028 - val_loss: -0.2878\n",
      "Epoch 360/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3025 - val_loss: -0.2953\n",
      "Epoch 361/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3032 - val_loss: -0.2928\n",
      "Epoch 362/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3031 - val_loss: -0.2846\n",
      "Epoch 363/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3027 - val_loss: -0.2922\n",
      "Epoch 364/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3022 - val_loss: -0.2908\n",
      "Epoch 365/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3023 - val_loss: -0.2913\n",
      "Epoch 366/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3024 - val_loss: -0.2936\n",
      "Epoch 367/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3038 - val_loss: -0.2899\n",
      "Epoch 368/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3020 - val_loss: -0.2926\n",
      "Epoch 369/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3026 - val_loss: -0.2912\n",
      "Epoch 370/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3046 - val_loss: -0.2908\n",
      "Epoch 371/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3030 - val_loss: -0.2918\n",
      "Epoch 372/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3037 - val_loss: -0.2918\n",
      "Epoch 373/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3030 - val_loss: -0.2924\n",
      "Epoch 374/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3036 - val_loss: -0.2887\n",
      "Epoch 375/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3028 - val_loss: -0.2935\n",
      "Epoch 376/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3035 - val_loss: -0.2915\n",
      "Epoch 377/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3029 - val_loss: -0.2918\n",
      "Epoch 378/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3033 - val_loss: -0.2890\n",
      "Epoch 379/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3033 - val_loss: -0.2934\n",
      "Epoch 380/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3044 - val_loss: -0.2949\n",
      "Epoch 381/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3041 - val_loss: -0.2938\n",
      "Epoch 382/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3040 - val_loss: -0.2869\n",
      "Epoch 383/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3037 - val_loss: -0.2942\n",
      "Epoch 384/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3035 - val_loss: -0.2947\n",
      "Epoch 385/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3035 - val_loss: -0.2900\n",
      "Epoch 386/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3034 - val_loss: -0.2897\n",
      "Epoch 387/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3035 - val_loss: -0.2932\n",
      "Epoch 388/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3039 - val_loss: -0.2939\n",
      "Epoch 389/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3043 - val_loss: -0.2906\n",
      "Epoch 390/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3036 - val_loss: -0.2923\n",
      "Epoch 391/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3046 - val_loss: -0.2909\n",
      "Epoch 392/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3038 - val_loss: -0.2926\n",
      "Epoch 393/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3042 - val_loss: -0.2895\n",
      "Epoch 394/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3042 - val_loss: -0.2894\n",
      "Epoch 395/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3036 - val_loss: -0.2929\n",
      "Epoch 396/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3047 - val_loss: -0.2932\n",
      "Epoch 397/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3038 - val_loss: -0.2902\n",
      "Epoch 398/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3036 - val_loss: -0.2890\n",
      "Epoch 399/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3045 - val_loss: -0.2930\n",
      "Epoch 400/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3050 - val_loss: -0.2957\n",
      "Epoch 401/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3049 - val_loss: -0.2894\n",
      "Epoch 402/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3034 - val_loss: -0.2913\n",
      "Epoch 403/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3052 - val_loss: -0.2858\n",
      "Epoch 404/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3055 - val_loss: -0.2913\n",
      "Epoch 405/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3050 - val_loss: -0.2884\n",
      "Epoch 406/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3049 - val_loss: -0.2923\n",
      "Epoch 407/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3051 - val_loss: -0.2937\n",
      "Epoch 408/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3053 - val_loss: -0.2923\n",
      "Epoch 409/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3050 - val_loss: -0.2932\n",
      "Epoch 410/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3056 - val_loss: -0.2928\n",
      "Epoch 411/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3047 - val_loss: -0.2923\n",
      "Epoch 412/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3033 - val_loss: -0.2904\n",
      "Epoch 413/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3036 - val_loss: -0.2922\n",
      "Epoch 414/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3046 - val_loss: -0.2920\n",
      "Epoch 415/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3056 - val_loss: -0.2900\n",
      "Epoch 416/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3043 - val_loss: -0.2899\n",
      "Epoch 417/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3049 - val_loss: -0.2897\n",
      "Epoch 418/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3054 - val_loss: -0.2888\n",
      "Epoch 419/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3056 - val_loss: -0.2911\n",
      "Epoch 420/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3061 - val_loss: -0.2896\n",
      "Epoch 421/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3055 - val_loss: -0.2937\n",
      "Epoch 422/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3050 - val_loss: -0.2913\n",
      "Epoch 423/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3050 - val_loss: -0.2899\n",
      "Epoch 424/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3052 - val_loss: -0.2918\n",
      "Epoch 425/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3059 - val_loss: -0.2923\n",
      "Epoch 426/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3064 - val_loss: -0.2928\n",
      "Epoch 427/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3055 - val_loss: -0.2894\n",
      "Epoch 428/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3051 - val_loss: -0.2850\n",
      "Epoch 429/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3056 - val_loss: -0.2922\n",
      "Epoch 430/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3057 - val_loss: -0.2940\n",
      "Epoch 431/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3057 - val_loss: -0.2933\n",
      "Epoch 432/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3067 - val_loss: -0.2884\n",
      "Epoch 433/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3066 - val_loss: -0.2919\n",
      "Epoch 434/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3055 - val_loss: -0.2942\n",
      "Epoch 435/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3062 - val_loss: -0.2917\n",
      "Epoch 436/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3066 - val_loss: -0.2905\n",
      "Epoch 437/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3052 - val_loss: -0.2886\n",
      "Epoch 438/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3064 - val_loss: -0.2906\n",
      "Epoch 439/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3060 - val_loss: -0.2927\n",
      "Epoch 440/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3065 - val_loss: -0.2895\n",
      "Epoch 441/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3061 - val_loss: -0.2887\n",
      "Epoch 442/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3071 - val_loss: -0.2877\n",
      "Epoch 443/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3065 - val_loss: -0.2909\n",
      "Epoch 444/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3058 - val_loss: -0.2908\n",
      "Epoch 445/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3067 - val_loss: -0.2919\n",
      "Epoch 446/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3053 - val_loss: -0.2889\n",
      "Epoch 447/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3062 - val_loss: -0.2911\n",
      "Epoch 448/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3063 - val_loss: -0.2861\n",
      "Epoch 449/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3075 - val_loss: -0.2850\n",
      "Epoch 450/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3062 - val_loss: -0.2893\n",
      "Epoch 451/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3075 - val_loss: -0.2883\n",
      "Epoch 452/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3050 - val_loss: -0.2931\n",
      "Epoch 453/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3072 - val_loss: -0.2881\n",
      "Epoch 454/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3080 - val_loss: -0.2905\n",
      "Epoch 455/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3066 - val_loss: -0.2779\n",
      "Epoch 456/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3083 - val_loss: -0.2946\n",
      "Epoch 457/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3067 - val_loss: -0.2909\n",
      "Epoch 458/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3075 - val_loss: -0.2898\n",
      "Epoch 459/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3074 - val_loss: -0.2905\n",
      "Epoch 460/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3075 - val_loss: -0.2874\n",
      "Epoch 461/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3082 - val_loss: -0.2924\n",
      "Epoch 462/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3077 - val_loss: -0.2901\n",
      "Epoch 463/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3077 - val_loss: -0.2918\n",
      "Epoch 464/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3068 - val_loss: -0.2898\n",
      "Epoch 465/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3078 - val_loss: -0.2852\n",
      "Epoch 466/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3076 - val_loss: -0.2915\n",
      "Epoch 467/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3069 - val_loss: -0.2917\n",
      "Epoch 468/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3090 - val_loss: -0.2911\n",
      "Epoch 469/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3080 - val_loss: -0.2903\n",
      "Epoch 470/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3087 - val_loss: -0.2880\n",
      "Epoch 471/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3072 - val_loss: -0.2941\n",
      "Epoch 472/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3076 - val_loss: -0.2869\n",
      "Epoch 473/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3082 - val_loss: -0.2915\n",
      "Epoch 474/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3081 - val_loss: -0.2888\n",
      "Epoch 475/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3073 - val_loss: -0.2879\n",
      "Epoch 476/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3087 - val_loss: -0.2901\n",
      "Epoch 477/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3082 - val_loss: -0.2904\n",
      "Epoch 478/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3080 - val_loss: -0.2904\n",
      "Epoch 479/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3085 - val_loss: -0.2920\n",
      "Epoch 480/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3081 - val_loss: -0.2898\n",
      "Epoch 481/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3082 - val_loss: -0.2907\n",
      "Epoch 482/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3075 - val_loss: -0.2854\n",
      "Epoch 483/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3087 - val_loss: -0.2905\n",
      "Epoch 484/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3089 - val_loss: -0.2892\n",
      "Epoch 485/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.3081 - val_loss: -0.2909\n",
      "Epoch 486/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3081 - val_loss: -0.2900\n",
      "Epoch 487/10000\n",
      "301478/301478 [==============================] - 3s 8us/sample - loss: -0.3094 - val_loss: -0.2866\n",
      "Epoch 488/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3071 - val_loss: -0.2842\n",
      "Epoch 489/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3090 - val_loss: -0.2896\n",
      "Epoch 490/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3091 - val_loss: -0.2883\n",
      "Epoch 491/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3077 - val_loss: -0.2917\n",
      "Epoch 492/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3088 - val_loss: -0.2910\n",
      "Epoch 493/10000\n",
      "301478/301478 [==============================] - 3s 10us/sample - loss: -0.3088 - val_loss: -0.2866\n",
      "Epoch 494/10000\n",
      "301478/301478 [==============================] - 2s 8us/sample - loss: -0.3087 - val_loss: -0.2873\n",
      "Epoch 495/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3094 - val_loss: -0.2895\n",
      "Epoch 496/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.3086 - val_loss: -0.2899\n",
      "Epoch 497/10000\n",
      "301478/301478 [==============================] - 5s 15us/sample - loss: -0.3091 - val_loss: -0.2903\n",
      "Epoch 498/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.3095 - val_loss: -0.2880\n",
      "Epoch 499/10000\n",
      "301478/301478 [==============================] - 3s 12us/sample - loss: -0.3099 - val_loss: -0.2907\n",
      "Epoch 500/10000\n",
      "301478/301478 [==============================] - 3s 11us/sample - loss: -0.3088 - val_loss: -0.2913\n",
      "Epoch 501/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3084 - val_loss: -0.2925\n",
      "Epoch 502/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3097 - val_loss: -0.2906\n",
      "Epoch 503/10000\n",
      "301478/301478 [==============================] - 3s 9us/sample - loss: -0.3097 - val_loss: -0.2908\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1361.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7703, 6, 1)\n",
      "y_test.shape:  (7703, 1)\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:23,730 WARNING Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1361.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1362.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6423, 6, 1)\n",
      "y_test.shape:  (6423, 1)\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:26,958 WARNING Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1362.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1363.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7708, 6, 1)\n",
      "y_test.shape:  (7708, 1)\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:30,304 WARNING Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1363.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1377.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7833, 6, 1)\n",
      "y_test.shape:  (7833, 1)\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:33,759 WARNING Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1377.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1381.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7791, 6, 1)\n",
      "y_test.shape:  (7791, 1)\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:37,179 WARNING Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1381.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1386.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7438, 6, 1)\n",
      "y_test.shape:  (7438, 1)\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:40,541 WARNING Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1386.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1408.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7444, 6, 1)\n",
      "y_test.shape:  (7444, 1)\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:43,824 WARNING Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1408.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1422.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7246, 6, 1)\n",
      "y_test.shape:  (7246, 1)\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:47,201 WARNING Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1422.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1427.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7431, 6, 1)\n",
      "y_test.shape:  (7431, 1)\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:50,554 WARNING Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1427.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1433.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7716, 6, 1)\n",
      "y_test.shape:  (7716, 1)\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:54,100 WARNING Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1433.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1435.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7096, 6, 1)\n",
      "y_test.shape:  (7096, 1)\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:50:57,573 WARNING Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1435.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1457.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7818, 6, 1)\n",
      "y_test.shape:  (7818, 1)\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:51:01,080 WARNING Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1457.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1459.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7607, 6, 1)\n",
      "y_test.shape:  (7607, 1)\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:51:04,671 WARNING Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1459.csv\n",
      "2025-01-21 14:51:07,378 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (312874, 6, 1)\n",
      "y_train.shape:  (312874, 1)\n",
      "x_valid.shape:  (78194, 6, 1)\n",
      "y_valid.shape:  (78194, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 14:51:51,476 WARNING Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 14:51:51,604 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 312874 samples, validate on 78194 samples\n",
      "Epoch 1/10000\n",
      "311296/312874 [============================>.] - ETA: 0s - loss: 0.1632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312874/312874 [==============================] - 5s 15us/sample - loss: 0.1623 - val_loss: -0.0962\n",
      "Epoch 2/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.0491 - val_loss: -0.1582\n",
      "Epoch 3/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.0892 - val_loss: -0.1524\n",
      "Epoch 4/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.1148 - val_loss: -0.1828\n",
      "Epoch 5/10000\n",
      "312874/312874 [==============================] - 3s 11us/sample - loss: -0.1289 - val_loss: -0.2014\n",
      "Epoch 6/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.1510 - val_loss: -0.2151\n",
      "Epoch 7/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.1641 - val_loss: -0.2213\n",
      "Epoch 8/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.1753 - val_loss: -0.2221\n",
      "Epoch 9/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.1855 - val_loss: -0.2325\n",
      "Epoch 10/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.1935 - val_loss: -0.2396\n",
      "Epoch 11/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.1985 - val_loss: -0.2428\n",
      "Epoch 12/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2071 - val_loss: -0.2121\n",
      "Epoch 13/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2125 - val_loss: -0.2310\n",
      "Epoch 14/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2169 - val_loss: -0.2525\n",
      "Epoch 15/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2245 - val_loss: -0.2562\n",
      "Epoch 16/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2267 - val_loss: -0.2522\n",
      "Epoch 17/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2321 - val_loss: -0.2628\n",
      "Epoch 18/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2359 - val_loss: -0.2534\n",
      "Epoch 19/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2388 - val_loss: -0.2552\n",
      "Epoch 20/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2419 - val_loss: -0.2568\n",
      "Epoch 21/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2434 - val_loss: -0.2509\n",
      "Epoch 22/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2464 - val_loss: -0.2582\n",
      "Epoch 23/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2488 - val_loss: -0.2629\n",
      "Epoch 24/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2491 - val_loss: -0.2586\n",
      "Epoch 25/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2521 - val_loss: -0.2554\n",
      "Epoch 26/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2513 - val_loss: -0.2373\n",
      "Epoch 27/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2542 - val_loss: -0.2576\n",
      "Epoch 28/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2568 - val_loss: -0.2596\n",
      "Epoch 29/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2567 - val_loss: -0.2618\n",
      "Epoch 30/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2577 - val_loss: -0.2661\n",
      "Epoch 31/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2579 - val_loss: -0.2597\n",
      "Epoch 32/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2598 - val_loss: -0.2616\n",
      "Epoch 33/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2603 - val_loss: -0.2639\n",
      "Epoch 34/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2610 - val_loss: -0.2552\n",
      "Epoch 35/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2613 - val_loss: -0.2637\n",
      "Epoch 36/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2634 - val_loss: -0.2584\n",
      "Epoch 37/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2619 - val_loss: -0.2582\n",
      "Epoch 38/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2637 - val_loss: -0.2658\n",
      "Epoch 39/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2635 - val_loss: -0.2687\n",
      "Epoch 40/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2622 - val_loss: -0.2592\n",
      "Epoch 41/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2636 - val_loss: -0.2629\n",
      "Epoch 42/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2643 - val_loss: -0.2623\n",
      "Epoch 43/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2661 - val_loss: -0.2553\n",
      "Epoch 44/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2667 - val_loss: -0.2608\n",
      "Epoch 45/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2666 - val_loss: -0.2591\n",
      "Epoch 46/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2654 - val_loss: -0.2601\n",
      "Epoch 47/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2676 - val_loss: -0.2707\n",
      "Epoch 48/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2673 - val_loss: -0.2556\n",
      "Epoch 49/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2681 - val_loss: -0.2598\n",
      "Epoch 50/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2662 - val_loss: -0.2668\n",
      "Epoch 51/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2685 - val_loss: -0.2659\n",
      "Epoch 52/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2684 - val_loss: -0.2703\n",
      "Epoch 53/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2690 - val_loss: -0.2691\n",
      "Epoch 54/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2696 - val_loss: -0.2699\n",
      "Epoch 55/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2701 - val_loss: -0.2697\n",
      "Epoch 56/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2701 - val_loss: -0.2681\n",
      "Epoch 57/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2702 - val_loss: -0.2671\n",
      "Epoch 58/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2701 - val_loss: -0.2600\n",
      "Epoch 59/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2701 - val_loss: -0.2679\n",
      "Epoch 60/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2718 - val_loss: -0.2612\n",
      "Epoch 61/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2715 - val_loss: -0.2684\n",
      "Epoch 62/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2716 - val_loss: -0.2605\n",
      "Epoch 63/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2709 - val_loss: -0.2666\n",
      "Epoch 64/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2720 - val_loss: -0.2678\n",
      "Epoch 65/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2726 - val_loss: -0.2631\n",
      "Epoch 66/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2725 - val_loss: -0.2707\n",
      "Epoch 67/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2720 - val_loss: -0.2668\n",
      "Epoch 68/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2716 - val_loss: -0.2643\n",
      "Epoch 69/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2726 - val_loss: -0.2658\n",
      "Epoch 70/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2731 - val_loss: -0.2711\n",
      "Epoch 71/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2731 - val_loss: -0.2718\n",
      "Epoch 72/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2726 - val_loss: -0.2683\n",
      "Epoch 73/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2732 - val_loss: -0.2624\n",
      "Epoch 74/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2740 - val_loss: -0.2703\n",
      "Epoch 75/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2739 - val_loss: -0.2680\n",
      "Epoch 76/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2724 - val_loss: -0.2708\n",
      "Epoch 77/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2756 - val_loss: -0.2700\n",
      "Epoch 78/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2759 - val_loss: -0.2707\n",
      "Epoch 79/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2726\n",
      "Epoch 80/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2744 - val_loss: -0.2737\n",
      "Epoch 81/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2708\n",
      "Epoch 82/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2736 - val_loss: -0.2673\n",
      "Epoch 83/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2750 - val_loss: -0.2708\n",
      "Epoch 84/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2748 - val_loss: -0.2718\n",
      "Epoch 85/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2759 - val_loss: -0.2748\n",
      "Epoch 86/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2751 - val_loss: -0.2699\n",
      "Epoch 87/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2757 - val_loss: -0.2743\n",
      "Epoch 88/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2704\n",
      "Epoch 89/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2749 - val_loss: -0.2719\n",
      "Epoch 90/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2739 - val_loss: -0.2743\n",
      "Epoch 91/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2755 - val_loss: -0.2666\n",
      "Epoch 92/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2765 - val_loss: -0.2714\n",
      "Epoch 93/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2757 - val_loss: -0.2729\n",
      "Epoch 94/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2768 - val_loss: -0.2734\n",
      "Epoch 95/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2770 - val_loss: -0.2694\n",
      "Epoch 96/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2756 - val_loss: -0.2699\n",
      "Epoch 97/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2765 - val_loss: -0.2712\n",
      "Epoch 98/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2766 - val_loss: -0.2659\n",
      "Epoch 99/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2768 - val_loss: -0.2611\n",
      "Epoch 100/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2773 - val_loss: -0.2716\n",
      "Epoch 101/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2778 - val_loss: -0.2717\n",
      "Epoch 102/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2763 - val_loss: -0.2736\n",
      "Epoch 103/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2768 - val_loss: -0.2731\n",
      "Epoch 104/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2758 - val_loss: -0.2725\n",
      "Epoch 105/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2768 - val_loss: -0.2751\n",
      "Epoch 106/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2754 - val_loss: -0.2721\n",
      "Epoch 107/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2766 - val_loss: -0.2723\n",
      "Epoch 108/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2770 - val_loss: -0.2760\n",
      "Epoch 109/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2783 - val_loss: -0.2744\n",
      "Epoch 110/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2771 - val_loss: -0.2661\n",
      "Epoch 111/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2767 - val_loss: -0.2747\n",
      "Epoch 112/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2774 - val_loss: -0.2715\n",
      "Epoch 113/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2766 - val_loss: -0.2736\n",
      "Epoch 114/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2779 - val_loss: -0.2728\n",
      "Epoch 115/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2769 - val_loss: -0.2688\n",
      "Epoch 116/10000\n",
      "312874/312874 [==============================] - 3s 11us/sample - loss: -0.2785 - val_loss: -0.2695\n",
      "Epoch 117/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2776 - val_loss: -0.2737\n",
      "Epoch 118/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2786 - val_loss: -0.2738\n",
      "Epoch 119/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2783 - val_loss: -0.2702\n",
      "Epoch 120/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2776 - val_loss: -0.2751\n",
      "Epoch 121/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2781 - val_loss: -0.2735\n",
      "Epoch 122/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2733\n",
      "Epoch 123/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2777 - val_loss: -0.2741\n",
      "Epoch 124/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2786 - val_loss: -0.2744\n",
      "Epoch 125/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2783 - val_loss: -0.2751\n",
      "Epoch 126/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2781 - val_loss: -0.2726\n",
      "Epoch 127/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2782 - val_loss: -0.2732\n",
      "Epoch 128/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2795 - val_loss: -0.2761\n",
      "Epoch 129/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2784 - val_loss: -0.2692\n",
      "Epoch 130/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2798 - val_loss: -0.2739\n",
      "Epoch 131/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2783 - val_loss: -0.2660\n",
      "Epoch 132/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2788 - val_loss: -0.2723\n",
      "Epoch 133/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2794 - val_loss: -0.2748\n",
      "Epoch 134/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2797 - val_loss: -0.2745\n",
      "Epoch 135/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2789 - val_loss: -0.2763\n",
      "Epoch 136/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2800 - val_loss: -0.2766\n",
      "Epoch 137/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2792 - val_loss: -0.2703\n",
      "Epoch 138/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2788 - val_loss: -0.2709\n",
      "Epoch 139/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2790 - val_loss: -0.2752\n",
      "Epoch 140/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2786 - val_loss: -0.2576\n",
      "Epoch 141/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2785 - val_loss: -0.2726\n",
      "Epoch 142/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2798 - val_loss: -0.2736\n",
      "Epoch 143/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2798 - val_loss: -0.2723\n",
      "Epoch 144/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2795 - val_loss: -0.2752\n",
      "Epoch 145/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2787 - val_loss: -0.2757\n",
      "Epoch 146/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2800 - val_loss: -0.2751\n",
      "Epoch 147/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2790 - val_loss: -0.2753\n",
      "Epoch 148/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2790 - val_loss: -0.2780\n",
      "Epoch 149/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2798 - val_loss: -0.2711\n",
      "Epoch 150/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2812 - val_loss: -0.2708\n",
      "Epoch 151/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2813 - val_loss: -0.2724\n",
      "Epoch 152/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2795 - val_loss: -0.2709\n",
      "Epoch 153/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2801 - val_loss: -0.2709\n",
      "Epoch 154/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2805 - val_loss: -0.2737\n",
      "Epoch 155/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2810 - val_loss: -0.2748\n",
      "Epoch 156/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2806 - val_loss: -0.2764\n",
      "Epoch 157/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2807 - val_loss: -0.2767\n",
      "Epoch 158/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2807 - val_loss: -0.2656\n",
      "Epoch 159/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2806 - val_loss: -0.2758\n",
      "Epoch 160/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2819 - val_loss: -0.2749\n",
      "Epoch 161/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2810 - val_loss: -0.2740\n",
      "Epoch 162/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2806 - val_loss: -0.2743\n",
      "Epoch 163/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2805 - val_loss: -0.2711\n",
      "Epoch 164/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2807 - val_loss: -0.2703\n",
      "Epoch 165/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2799 - val_loss: -0.2725\n",
      "Epoch 166/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2813 - val_loss: -0.2768\n",
      "Epoch 167/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2813 - val_loss: -0.2741\n",
      "Epoch 168/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2814 - val_loss: -0.2714\n",
      "Epoch 169/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2817 - val_loss: -0.2711\n",
      "Epoch 170/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2814 - val_loss: -0.2721\n",
      "Epoch 171/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2816 - val_loss: -0.2735\n",
      "Epoch 172/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2808 - val_loss: -0.2759\n",
      "Epoch 173/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2809 - val_loss: -0.2746\n",
      "Epoch 174/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2805 - val_loss: -0.2698\n",
      "Epoch 175/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2821 - val_loss: -0.2717\n",
      "Epoch 176/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2819 - val_loss: -0.2729\n",
      "Epoch 177/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2823 - val_loss: -0.2562\n",
      "Epoch 178/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2814 - val_loss: -0.2746\n",
      "Epoch 179/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2819 - val_loss: -0.2718\n",
      "Epoch 180/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2814 - val_loss: -0.2738\n",
      "Epoch 181/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2820 - val_loss: -0.2747\n",
      "Epoch 182/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2822 - val_loss: -0.2764\n",
      "Epoch 183/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2809 - val_loss: -0.2761\n",
      "Epoch 184/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2814 - val_loss: -0.2714\n",
      "Epoch 185/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2809 - val_loss: -0.2742\n",
      "Epoch 186/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2821 - val_loss: -0.2749\n",
      "Epoch 187/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2818 - val_loss: -0.2735\n",
      "Epoch 188/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2830 - val_loss: -0.2767\n",
      "Epoch 189/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2827 - val_loss: -0.2752\n",
      "Epoch 190/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2830 - val_loss: -0.2734\n",
      "Epoch 191/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2832 - val_loss: -0.2743\n",
      "Epoch 192/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2834 - val_loss: -0.2766\n",
      "Epoch 193/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2822 - val_loss: -0.2739\n",
      "Epoch 194/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2834 - val_loss: -0.2778\n",
      "Epoch 195/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2735\n",
      "Epoch 196/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2827 - val_loss: -0.2750\n",
      "Epoch 197/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2822 - val_loss: -0.2697\n",
      "Epoch 198/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2822 - val_loss: -0.2728\n",
      "Epoch 199/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2817 - val_loss: -0.2720\n",
      "Epoch 200/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2818 - val_loss: -0.2728\n",
      "Epoch 201/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2831 - val_loss: -0.2735\n",
      "Epoch 202/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2832 - val_loss: -0.2751\n",
      "Epoch 203/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2828 - val_loss: -0.2730\n",
      "Epoch 204/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2824 - val_loss: -0.2665\n",
      "Epoch 205/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2830 - val_loss: -0.2744\n",
      "Epoch 206/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2833 - val_loss: -0.2636\n",
      "Epoch 207/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2823 - val_loss: -0.2741\n",
      "Epoch 208/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2847 - val_loss: -0.2759\n",
      "Epoch 209/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2777\n",
      "Epoch 210/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2835 - val_loss: -0.2694\n",
      "Epoch 211/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2764\n",
      "Epoch 212/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2822 - val_loss: -0.2716\n",
      "Epoch 213/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2740\n",
      "Epoch 214/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2845 - val_loss: -0.2709\n",
      "Epoch 215/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2840 - val_loss: -0.2742\n",
      "Epoch 216/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2756\n",
      "Epoch 217/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2840 - val_loss: -0.2766\n",
      "Epoch 218/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2829 - val_loss: -0.2669\n",
      "Epoch 219/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2836 - val_loss: -0.2743\n",
      "Epoch 220/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2844 - val_loss: -0.2693\n",
      "Epoch 221/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2838 - val_loss: -0.2725\n",
      "Epoch 222/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2852 - val_loss: -0.2736\n",
      "Epoch 223/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2846 - val_loss: -0.2731\n",
      "Epoch 224/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2829 - val_loss: -0.2735\n",
      "Epoch 225/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2843 - val_loss: -0.2740\n",
      "Epoch 226/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2844 - val_loss: -0.2701\n",
      "Epoch 227/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2833 - val_loss: -0.2765\n",
      "Epoch 228/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2854 - val_loss: -0.2749\n",
      "Epoch 229/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2848 - val_loss: -0.2737\n",
      "Epoch 230/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2841 - val_loss: -0.2743\n",
      "Epoch 231/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2849 - val_loss: -0.2737\n",
      "Epoch 232/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2855 - val_loss: -0.2711\n",
      "Epoch 233/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2844 - val_loss: -0.2758\n",
      "Epoch 234/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2843 - val_loss: -0.2729\n",
      "Epoch 235/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2851 - val_loss: -0.2708\n",
      "Epoch 236/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2841 - val_loss: -0.2771\n",
      "Epoch 237/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2843 - val_loss: -0.2713\n",
      "Epoch 238/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2849 - val_loss: -0.2637\n",
      "Epoch 239/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2847 - val_loss: -0.2655\n",
      "Epoch 240/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2832 - val_loss: -0.2712\n",
      "Epoch 241/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2856 - val_loss: -0.2734\n",
      "Epoch 242/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2857 - val_loss: -0.2723\n",
      "Epoch 243/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2845 - val_loss: -0.2752\n",
      "Epoch 244/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2857 - val_loss: -0.2764\n",
      "Epoch 245/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2854 - val_loss: -0.2737\n",
      "Epoch 246/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2841 - val_loss: -0.2769\n",
      "Epoch 247/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2856 - val_loss: -0.2751\n",
      "Epoch 248/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2859 - val_loss: -0.2726\n",
      "Epoch 249/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2857 - val_loss: -0.2739\n",
      "Epoch 250/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2857 - val_loss: -0.2758\n",
      "Epoch 251/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2854 - val_loss: -0.2746\n",
      "Epoch 252/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2853 - val_loss: -0.2739\n",
      "Epoch 253/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2863 - val_loss: -0.2709\n",
      "Epoch 254/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2869 - val_loss: -0.2773\n",
      "Epoch 255/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2853 - val_loss: -0.2642\n",
      "Epoch 256/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2869 - val_loss: -0.2768\n",
      "Epoch 257/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2850 - val_loss: -0.2744\n",
      "Epoch 258/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2860 - val_loss: -0.2776\n",
      "Epoch 259/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2868 - val_loss: -0.2772\n",
      "Epoch 260/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2871 - val_loss: -0.2755\n",
      "Epoch 261/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2868 - val_loss: -0.2696\n",
      "Epoch 262/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2865 - val_loss: -0.2774\n",
      "Epoch 263/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2864 - val_loss: -0.2750\n",
      "Epoch 264/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2872 - val_loss: -0.2744\n",
      "Epoch 265/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2864 - val_loss: -0.2742\n",
      "Epoch 266/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2868 - val_loss: -0.2718\n",
      "Epoch 267/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2857 - val_loss: -0.2787\n",
      "Epoch 268/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2859 - val_loss: -0.2700\n",
      "Epoch 269/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2864 - val_loss: -0.2767\n",
      "Epoch 270/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2864 - val_loss: -0.2716\n",
      "Epoch 271/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2865 - val_loss: -0.2714\n",
      "Epoch 272/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2875 - val_loss: -0.2725\n",
      "Epoch 273/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2867 - val_loss: -0.2737\n",
      "Epoch 274/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2867 - val_loss: -0.2776\n",
      "Epoch 275/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2864 - val_loss: -0.2682\n",
      "Epoch 276/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2866 - val_loss: -0.2708\n",
      "Epoch 277/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2876 - val_loss: -0.2766\n",
      "Epoch 278/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2870 - val_loss: -0.2768\n",
      "Epoch 279/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2866 - val_loss: -0.2775\n",
      "Epoch 280/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2855 - val_loss: -0.2745\n",
      "Epoch 281/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2872 - val_loss: -0.2747\n",
      "Epoch 282/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2871 - val_loss: -0.2743\n",
      "Epoch 283/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2882 - val_loss: -0.2735\n",
      "Epoch 284/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2867 - val_loss: -0.2782\n",
      "Epoch 285/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2879 - val_loss: -0.2787\n",
      "Epoch 286/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2890 - val_loss: -0.2763\n",
      "Epoch 287/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2872 - val_loss: -0.2786\n",
      "Epoch 288/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2871 - val_loss: -0.2738\n",
      "Epoch 289/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2873 - val_loss: -0.2699\n",
      "Epoch 290/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2867 - val_loss: -0.2762\n",
      "Epoch 291/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2874 - val_loss: -0.2757\n",
      "Epoch 292/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2868 - val_loss: -0.2754\n",
      "Epoch 293/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2873 - val_loss: -0.2766\n",
      "Epoch 294/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2879 - val_loss: -0.2742\n",
      "Epoch 295/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2865 - val_loss: -0.2757\n",
      "Epoch 296/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2889 - val_loss: -0.2753\n",
      "Epoch 297/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2886 - val_loss: -0.2729\n",
      "Epoch 298/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2886 - val_loss: -0.2768\n",
      "Epoch 299/10000\n",
      "312874/312874 [==============================] - 2s 8us/sample - loss: -0.2873 - val_loss: -0.2767\n",
      "Epoch 300/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2881 - val_loss: -0.2737\n",
      "Epoch 301/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2877 - val_loss: -0.2719\n",
      "Epoch 302/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2751\n",
      "Epoch 303/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2873 - val_loss: -0.2747\n",
      "Epoch 304/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2873 - val_loss: -0.2750\n",
      "Epoch 305/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2883 - val_loss: -0.2766\n",
      "Epoch 306/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2876 - val_loss: -0.2717\n",
      "Epoch 307/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2883 - val_loss: -0.2718\n",
      "Epoch 308/10000\n",
      "312874/312874 [==============================] - 3s 11us/sample - loss: -0.2885 - val_loss: -0.2736\n",
      "Epoch 309/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2757\n",
      "Epoch 310/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2889 - val_loss: -0.2729\n",
      "Epoch 311/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2892 - val_loss: -0.2746\n",
      "Epoch 312/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2874 - val_loss: -0.2761\n",
      "Epoch 313/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2875 - val_loss: -0.2757\n",
      "Epoch 314/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2882 - val_loss: -0.2734\n",
      "Epoch 315/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2898 - val_loss: -0.2746\n",
      "Epoch 316/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2890 - val_loss: -0.2670\n",
      "Epoch 317/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2885 - val_loss: -0.2735\n",
      "Epoch 318/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2760\n",
      "Epoch 319/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2762\n",
      "Epoch 320/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2894 - val_loss: -0.2743\n",
      "Epoch 321/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2886 - val_loss: -0.2760\n",
      "Epoch 322/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2891 - val_loss: -0.2753\n",
      "Epoch 323/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2895 - val_loss: -0.2737\n",
      "Epoch 324/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2893 - val_loss: -0.2755\n",
      "Epoch 325/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2892 - val_loss: -0.2721\n",
      "Epoch 326/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2885 - val_loss: -0.2754\n",
      "Epoch 327/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2892 - val_loss: -0.2772\n",
      "Epoch 328/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2886 - val_loss: -0.2752\n",
      "Epoch 329/10000\n",
      "312874/312874 [==============================] - 4s 11us/sample - loss: -0.2890 - val_loss: -0.2701\n",
      "Epoch 330/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2770\n",
      "Epoch 331/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2891 - val_loss: -0.2741\n",
      "Epoch 332/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2762\n",
      "Epoch 333/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2899 - val_loss: -0.2740\n",
      "Epoch 334/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2688\n",
      "Epoch 335/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2900 - val_loss: -0.2765\n",
      "Epoch 336/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2886 - val_loss: -0.2750\n",
      "Epoch 337/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2891 - val_loss: -0.2682\n",
      "Epoch 338/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2775\n",
      "Epoch 339/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2895 - val_loss: -0.2746\n",
      "Epoch 340/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2902 - val_loss: -0.2696\n",
      "Epoch 341/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2899 - val_loss: -0.2687\n",
      "Epoch 342/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2898 - val_loss: -0.2702\n",
      "Epoch 343/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2727\n",
      "Epoch 344/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2892 - val_loss: -0.2761\n",
      "Epoch 345/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2895 - val_loss: -0.2764\n",
      "Epoch 346/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2709\n",
      "Epoch 347/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2899 - val_loss: -0.2741\n",
      "Epoch 348/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2898 - val_loss: -0.2742\n",
      "Epoch 349/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2903 - val_loss: -0.2744\n",
      "Epoch 350/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2765\n",
      "Epoch 351/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2901 - val_loss: -0.2724\n",
      "Epoch 352/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2894 - val_loss: -0.2763\n",
      "Epoch 353/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2906 - val_loss: -0.2739\n",
      "Epoch 354/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2904 - val_loss: -0.2743\n",
      "Epoch 355/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2912 - val_loss: -0.2771\n",
      "Epoch 356/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2904 - val_loss: -0.2738\n",
      "Epoch 357/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2898 - val_loss: -0.2713\n",
      "Epoch 358/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2904 - val_loss: -0.2745\n",
      "Epoch 359/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2903 - val_loss: -0.2712\n",
      "Epoch 360/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2907 - val_loss: -0.2735\n",
      "Epoch 361/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2912 - val_loss: -0.2764\n",
      "Epoch 362/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2919 - val_loss: -0.2757\n",
      "Epoch 363/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2905 - val_loss: -0.2731\n",
      "Epoch 364/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2914 - val_loss: -0.2764\n",
      "Epoch 365/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2900 - val_loss: -0.2714\n",
      "Epoch 366/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2914 - val_loss: -0.2784\n",
      "Epoch 367/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2707\n",
      "Epoch 368/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2761\n",
      "Epoch 369/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2907 - val_loss: -0.2682\n",
      "Epoch 370/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2743\n",
      "Epoch 371/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2914 - val_loss: -0.2747\n",
      "Epoch 372/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2915 - val_loss: -0.2746\n",
      "Epoch 373/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2914 - val_loss: -0.2741\n",
      "Epoch 374/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2920 - val_loss: -0.2701\n",
      "Epoch 375/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2921 - val_loss: -0.2723\n",
      "Epoch 376/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2730\n",
      "Epoch 377/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2909 - val_loss: -0.2734\n",
      "Epoch 378/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2914 - val_loss: -0.2741\n",
      "Epoch 379/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2913 - val_loss: -0.2747\n",
      "Epoch 380/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2741\n",
      "Epoch 381/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2917 - val_loss: -0.2757\n",
      "Epoch 382/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2911 - val_loss: -0.2750\n",
      "Epoch 383/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2915 - val_loss: -0.2771\n",
      "Epoch 384/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2918 - val_loss: -0.2759\n",
      "Epoch 385/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2928 - val_loss: -0.2756\n",
      "Epoch 386/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2933 - val_loss: -0.2765\n",
      "Epoch 387/10000\n",
      "312874/312874 [==============================] - 3s 11us/sample - loss: -0.2915 - val_loss: -0.2723\n",
      "Epoch 388/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2932 - val_loss: -0.2756\n",
      "Epoch 389/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2917 - val_loss: -0.2755\n",
      "Epoch 390/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2924 - val_loss: -0.2724\n",
      "Epoch 391/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2921 - val_loss: -0.2737\n",
      "Epoch 392/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2927 - val_loss: -0.2673\n",
      "Epoch 393/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2915 - val_loss: -0.2759\n",
      "Epoch 394/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2930 - val_loss: -0.2740\n",
      "Epoch 395/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2721\n",
      "Epoch 396/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2724\n",
      "Epoch 397/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2929 - val_loss: -0.2750\n",
      "Epoch 398/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2931 - val_loss: -0.2743\n",
      "Epoch 399/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2924 - val_loss: -0.2751\n",
      "Epoch 400/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2750\n",
      "Epoch 401/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2931 - val_loss: -0.2705\n",
      "Epoch 402/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2717\n",
      "Epoch 403/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2935 - val_loss: -0.2733\n",
      "Epoch 404/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2930 - val_loss: -0.2755\n",
      "Epoch 405/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2930 - val_loss: -0.2760\n",
      "Epoch 406/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2931 - val_loss: -0.2738\n",
      "Epoch 407/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2723\n",
      "Epoch 408/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2931 - val_loss: -0.2725\n",
      "Epoch 409/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2660\n",
      "Epoch 410/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2920 - val_loss: -0.2723\n",
      "Epoch 411/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2737\n",
      "Epoch 412/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2922 - val_loss: -0.2707\n",
      "Epoch 413/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2724\n",
      "Epoch 414/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2928 - val_loss: -0.2733\n",
      "Epoch 415/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2923 - val_loss: -0.2746\n",
      "Epoch 416/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2733\n",
      "Epoch 417/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2663\n",
      "Epoch 418/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2932 - val_loss: -0.2720\n",
      "Epoch 419/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2725\n",
      "Epoch 420/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2922 - val_loss: -0.2748\n",
      "Epoch 421/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2742\n",
      "Epoch 422/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2925 - val_loss: -0.2749\n",
      "Epoch 423/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2943 - val_loss: -0.2733\n",
      "Epoch 424/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2941 - val_loss: -0.2628\n",
      "Epoch 425/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2933 - val_loss: -0.2747\n",
      "Epoch 426/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2741\n",
      "Epoch 427/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2939 - val_loss: -0.2746\n",
      "Epoch 428/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2740\n",
      "Epoch 429/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2933 - val_loss: -0.2686\n",
      "Epoch 430/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2952 - val_loss: -0.2707\n",
      "Epoch 431/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2942 - val_loss: -0.2710\n",
      "Epoch 432/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2940 - val_loss: -0.2744\n",
      "Epoch 433/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2937 - val_loss: -0.2712\n",
      "Epoch 434/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2681\n",
      "Epoch 435/10000\n",
      "312874/312874 [==============================] - 3s 10us/sample - loss: -0.2946 - val_loss: -0.2696\n",
      "Epoch 436/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2940 - val_loss: -0.2757\n",
      "Epoch 437/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2938 - val_loss: -0.2705\n",
      "Epoch 438/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2953 - val_loss: -0.2716\n",
      "Epoch 439/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2946 - val_loss: -0.2735\n",
      "Epoch 440/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2946 - val_loss: -0.2737\n",
      "Epoch 441/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2935 - val_loss: -0.2729\n",
      "Epoch 442/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2951 - val_loss: -0.2719\n",
      "Epoch 443/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2940 - val_loss: -0.2741\n",
      "Epoch 444/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2939 - val_loss: -0.2694\n",
      "Epoch 445/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2957 - val_loss: -0.2742\n",
      "Epoch 446/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2943 - val_loss: -0.2702\n",
      "Epoch 447/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2950 - val_loss: -0.2650\n",
      "Epoch 448/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2955 - val_loss: -0.2676\n",
      "Epoch 449/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2944 - val_loss: -0.2713\n",
      "Epoch 450/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2724\n",
      "Epoch 451/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2938 - val_loss: -0.2754\n",
      "Epoch 452/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2955 - val_loss: -0.2748\n",
      "Epoch 453/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2948 - val_loss: -0.2586\n",
      "Epoch 454/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2745\n",
      "Epoch 455/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2717\n",
      "Epoch 456/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2960 - val_loss: -0.2705\n",
      "Epoch 457/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2942 - val_loss: -0.2721\n",
      "Epoch 458/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2949 - val_loss: -0.2729\n",
      "Epoch 459/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2678\n",
      "Epoch 460/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2950 - val_loss: -0.2742\n",
      "Epoch 461/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2957 - val_loss: -0.2720\n",
      "Epoch 462/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2962 - val_loss: -0.2684\n",
      "Epoch 463/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2948 - val_loss: -0.2716\n",
      "Epoch 464/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2963 - val_loss: -0.2697\n",
      "Epoch 465/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2968 - val_loss: -0.2729\n",
      "Epoch 466/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2956 - val_loss: -0.2742\n",
      "Epoch 467/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2958 - val_loss: -0.2725\n",
      "Epoch 468/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2963 - val_loss: -0.2670\n",
      "Epoch 469/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2964 - val_loss: -0.2734\n",
      "Epoch 470/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2953 - val_loss: -0.2710\n",
      "Epoch 471/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2963 - val_loss: -0.2724\n",
      "Epoch 472/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2970 - val_loss: -0.2723\n",
      "Epoch 473/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2967 - val_loss: -0.2724\n",
      "Epoch 474/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2967 - val_loss: -0.2743\n",
      "Epoch 475/10000\n",
      "312874/312874 [==============================] - 3s 8us/sample - loss: -0.2962 - val_loss: -0.2710\n",
      "Epoch 476/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2966 - val_loss: -0.2731\n",
      "Epoch 477/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2956 - val_loss: -0.2747\n",
      "Epoch 478/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2965 - val_loss: -0.2729\n",
      "Epoch 479/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2965 - val_loss: -0.2740\n",
      "Epoch 480/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2951 - val_loss: -0.2672\n",
      "Epoch 481/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2977 - val_loss: -0.2659\n",
      "Epoch 482/10000\n",
      "312874/312874 [==============================] - 3s 11us/sample - loss: -0.2968 - val_loss: -0.2725\n",
      "Epoch 483/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2970 - val_loss: -0.2714\n",
      "Epoch 484/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2961 - val_loss: -0.2747\n",
      "Epoch 485/10000\n",
      "312874/312874 [==============================] - 3s 9us/sample - loss: -0.2966 - val_loss: -0.2708\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6249, 6, 1)\n",
      "y_test.shape:  (6249, 1)\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:08,700 WARNING Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7855, 6, 1)\n",
      "y_test.shape:  (7855, 1)\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:12,205 WARNING Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7971, 6, 1)\n",
      "y_test.shape:  (7971, 1)\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:16,042 WARNING Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6893, 6, 1)\n",
      "y_test.shape:  (6893, 1)\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:19,838 WARNING Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7379, 6, 1)\n",
      "y_test.shape:  (7379, 1)\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:23,507 WARNING Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7674, 6, 1)\n",
      "y_test.shape:  (7674, 1)\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:27,312 WARNING Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7709, 6, 1)\n",
      "y_test.shape:  (7709, 1)\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:31,128 WARNING Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7886, 6, 1)\n",
      "y_test.shape:  (7886, 1)\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:34,936 WARNING Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7581, 6, 1)\n",
      "y_test.shape:  (7581, 1)\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:38,779 WARNING Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7927, 6, 1)\n",
      "y_test.shape:  (7927, 1)\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:43,009 WARNING Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7886, 6, 1)\n",
      "y_test.shape:  (7886, 1)\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:14:47,140 WARNING Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1726.csv\n",
      "2025-01-21 15:14:50,251 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "2025-01-21 15:14:50,252 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (298618, 12, 1)\n",
      "y_train.shape:  (298618, 1)\n",
      "x_valid.shape:  (74625, 12, 1)\n",
      "y_valid.shape:  (74625, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:15:31,435 WARNING Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 15:15:31,532 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 298618 samples, validate on 74625 samples\n",
      "Epoch 1/10000\n",
      "296960/298618 [============================>.] - ETA: 0s - loss: 0.1391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298618/298618 [==============================] - 7s 23us/sample - loss: 0.1381 - val_loss: -0.1071\n",
      "Epoch 2/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.0644 - val_loss: -0.1702\n",
      "Epoch 3/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.1083 - val_loss: -0.2134\n",
      "Epoch 4/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.1392 - val_loss: -0.2232\n",
      "Epoch 5/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.1582 - val_loss: -0.2091\n",
      "Epoch 6/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.1694 - val_loss: -0.2342\n",
      "Epoch 7/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.1865 - val_loss: -0.2496\n",
      "Epoch 8/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.1952 - val_loss: -0.2496\n",
      "Epoch 9/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2038 - val_loss: -0.2563\n",
      "Epoch 10/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2123 - val_loss: -0.2541\n",
      "Epoch 11/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2185 - val_loss: -0.2399\n",
      "Epoch 12/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2254 - val_loss: -0.2697\n",
      "Epoch 13/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2301 - val_loss: -0.2652\n",
      "Epoch 14/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2359 - val_loss: -0.2531\n",
      "Epoch 15/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2405 - val_loss: -0.2570\n",
      "Epoch 16/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2439 - val_loss: -0.2561\n",
      "Epoch 17/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2498 - val_loss: -0.2695\n",
      "Epoch 18/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.2548 - val_loss: -0.2771\n",
      "Epoch 19/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2564 - val_loss: -0.2688\n",
      "Epoch 20/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2588 - val_loss: -0.2667\n",
      "Epoch 21/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2633 - val_loss: -0.2799\n",
      "Epoch 22/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.2653 - val_loss: -0.2728\n",
      "Epoch 23/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2674 - val_loss: -0.2747\n",
      "Epoch 24/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2679 - val_loss: -0.2711\n",
      "Epoch 25/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2709 - val_loss: -0.2817\n",
      "Epoch 26/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2735 - val_loss: -0.2795\n",
      "Epoch 27/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2754 - val_loss: -0.2773\n",
      "Epoch 28/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2741 - val_loss: -0.2789\n",
      "Epoch 29/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2776 - val_loss: -0.2725\n",
      "Epoch 30/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2778 - val_loss: -0.2793\n",
      "Epoch 31/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2808 - val_loss: -0.2841\n",
      "Epoch 32/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2805 - val_loss: -0.2752\n",
      "Epoch 33/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2822 - val_loss: -0.2821\n",
      "Epoch 34/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2836 - val_loss: -0.2820\n",
      "Epoch 35/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2843 - val_loss: -0.2763\n",
      "Epoch 36/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2841 - val_loss: -0.2907\n",
      "Epoch 37/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2853 - val_loss: -0.2835\n",
      "Epoch 38/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2863 - val_loss: -0.2908\n",
      "Epoch 39/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2868 - val_loss: -0.2865\n",
      "Epoch 40/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2875 - val_loss: -0.2870\n",
      "Epoch 41/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2875 - val_loss: -0.2881\n",
      "Epoch 42/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2881 - val_loss: -0.2891\n",
      "Epoch 43/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2903 - val_loss: -0.2912\n",
      "Epoch 44/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2902 - val_loss: -0.2872\n",
      "Epoch 45/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2887 - val_loss: -0.2886\n",
      "Epoch 46/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2897 - val_loss: -0.2828\n",
      "Epoch 47/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2908 - val_loss: -0.2857\n",
      "Epoch 48/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2909 - val_loss: -0.2913\n",
      "Epoch 49/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2919 - val_loss: -0.2822\n",
      "Epoch 50/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2915 - val_loss: -0.2917\n",
      "Epoch 51/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.2928 - val_loss: -0.2890\n",
      "Epoch 52/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2937 - val_loss: -0.2895\n",
      "Epoch 53/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2920 - val_loss: -0.2872\n",
      "Epoch 54/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2934 - val_loss: -0.2875\n",
      "Epoch 55/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2924 - val_loss: -0.2881\n",
      "Epoch 56/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2933 - val_loss: -0.2867\n",
      "Epoch 57/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2959 - val_loss: -0.2852\n",
      "Epoch 58/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2946 - val_loss: -0.2908\n",
      "Epoch 59/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2944 - val_loss: -0.2920\n",
      "Epoch 60/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.2944 - val_loss: -0.2639\n",
      "Epoch 61/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2949 - val_loss: -0.2929\n",
      "Epoch 62/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2961 - val_loss: -0.2898\n",
      "Epoch 63/10000\n",
      "298618/298618 [==============================] - 4s 13us/sample - loss: -0.2968 - val_loss: -0.2893\n",
      "Epoch 64/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2962 - val_loss: -0.2859\n",
      "Epoch 65/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2945 - val_loss: -0.2851\n",
      "Epoch 66/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2973 - val_loss: -0.2899\n",
      "Epoch 67/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2959 - val_loss: -0.2933\n",
      "Epoch 68/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2977 - val_loss: -0.2895\n",
      "Epoch 69/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2964 - val_loss: -0.2913\n",
      "Epoch 70/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2978 - val_loss: -0.2934\n",
      "Epoch 71/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2976 - val_loss: -0.2904\n",
      "Epoch 72/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2970 - val_loss: -0.2875\n",
      "Epoch 73/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2998 - val_loss: -0.2902\n",
      "Epoch 74/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2988 - val_loss: -0.2903\n",
      "Epoch 75/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.2996 - val_loss: -0.2870\n",
      "Epoch 76/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2991 - val_loss: -0.2922\n",
      "Epoch 77/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3011 - val_loss: -0.2872\n",
      "Epoch 78/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.2998 - val_loss: -0.2931\n",
      "Epoch 79/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.2994 - val_loss: -0.2941\n",
      "Epoch 80/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3008 - val_loss: -0.2900\n",
      "Epoch 81/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3006 - val_loss: -0.2921\n",
      "Epoch 82/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3000 - val_loss: -0.2827\n",
      "Epoch 83/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3024 - val_loss: -0.2918\n",
      "Epoch 84/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3013 - val_loss: -0.2915\n",
      "Epoch 85/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3024 - val_loss: -0.2894\n",
      "Epoch 86/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3018 - val_loss: -0.2920\n",
      "Epoch 87/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3021 - val_loss: -0.2916\n",
      "Epoch 88/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3023 - val_loss: -0.2918\n",
      "Epoch 89/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3031 - val_loss: -0.2942\n",
      "Epoch 90/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3019 - val_loss: -0.2899\n",
      "Epoch 91/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3047 - val_loss: -0.2942\n",
      "Epoch 92/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3039 - val_loss: -0.2931\n",
      "Epoch 93/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3042 - val_loss: -0.2870\n",
      "Epoch 94/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3041 - val_loss: -0.2919\n",
      "Epoch 95/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3041 - val_loss: -0.2910\n",
      "Epoch 96/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3037 - val_loss: -0.2922\n",
      "Epoch 97/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3038 - val_loss: -0.2909\n",
      "Epoch 98/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3050 - val_loss: -0.2931\n",
      "Epoch 99/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3049 - val_loss: -0.2912\n",
      "Epoch 100/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3052 - val_loss: -0.2883\n",
      "Epoch 101/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3066 - val_loss: -0.2916\n",
      "Epoch 102/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3042 - val_loss: -0.2931\n",
      "Epoch 103/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3050 - val_loss: -0.2936\n",
      "Epoch 104/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3061 - val_loss: -0.2859\n",
      "Epoch 105/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3070 - val_loss: -0.2912\n",
      "Epoch 106/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3063 - val_loss: -0.2961\n",
      "Epoch 107/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3079 - val_loss: -0.2895\n",
      "Epoch 108/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3061 - val_loss: -0.2898\n",
      "Epoch 109/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3073 - val_loss: -0.2922\n",
      "Epoch 110/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3084 - val_loss: -0.2905\n",
      "Epoch 111/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3090 - val_loss: -0.2914\n",
      "Epoch 112/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3086 - val_loss: -0.2898\n",
      "Epoch 113/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3082 - val_loss: -0.2910\n",
      "Epoch 114/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3093 - val_loss: -0.2866\n",
      "Epoch 115/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3084 - val_loss: -0.2895\n",
      "Epoch 116/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3091 - val_loss: -0.2801\n",
      "Epoch 117/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3079 - val_loss: -0.2899\n",
      "Epoch 118/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3098 - val_loss: -0.2900\n",
      "Epoch 119/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3106 - val_loss: -0.2929\n",
      "Epoch 120/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3110 - val_loss: -0.2927\n",
      "Epoch 121/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3102 - val_loss: -0.2911\n",
      "Epoch 122/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3112 - val_loss: -0.2894\n",
      "Epoch 123/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3114 - val_loss: -0.2886\n",
      "Epoch 124/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3104 - val_loss: -0.2877\n",
      "Epoch 125/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3125 - val_loss: -0.2895\n",
      "Epoch 126/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3122 - val_loss: -0.2889\n",
      "Epoch 127/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3120 - val_loss: -0.2897\n",
      "Epoch 128/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3122 - val_loss: -0.2905\n",
      "Epoch 129/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3135 - val_loss: -0.2875\n",
      "Epoch 130/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3133 - val_loss: -0.2842\n",
      "Epoch 131/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3135 - val_loss: -0.2861\n",
      "Epoch 132/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3134 - val_loss: -0.2839\n",
      "Epoch 133/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3159 - val_loss: -0.2840\n",
      "Epoch 134/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3148 - val_loss: -0.2881\n",
      "Epoch 135/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3144 - val_loss: -0.2887\n",
      "Epoch 136/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3150 - val_loss: -0.2837\n",
      "Epoch 137/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3166 - val_loss: -0.2867\n",
      "Epoch 138/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3159 - val_loss: -0.2862\n",
      "Epoch 139/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3163 - val_loss: -0.2890\n",
      "Epoch 140/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3164 - val_loss: -0.2889\n",
      "Epoch 141/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3156 - val_loss: -0.2795\n",
      "Epoch 142/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3167 - val_loss: -0.2817\n",
      "Epoch 143/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3190 - val_loss: -0.2851\n",
      "Epoch 144/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3184 - val_loss: -0.2820\n",
      "Epoch 145/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3183 - val_loss: -0.2847\n",
      "Epoch 146/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3188 - val_loss: -0.2831\n",
      "Epoch 147/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3203 - val_loss: -0.2869\n",
      "Epoch 148/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3196 - val_loss: -0.2836\n",
      "Epoch 149/10000\n",
      "298618/298618 [==============================] - 5s 18us/sample - loss: -0.3203 - val_loss: -0.2855\n",
      "Epoch 150/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3205 - val_loss: -0.2828\n",
      "Epoch 151/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3207 - val_loss: -0.2842\n",
      "Epoch 152/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3197 - val_loss: -0.2869\n",
      "Epoch 153/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3200 - val_loss: -0.2864\n",
      "Epoch 154/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3198 - val_loss: -0.2875\n",
      "Epoch 155/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3229 - val_loss: -0.2864\n",
      "Epoch 156/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3228 - val_loss: -0.2819\n",
      "Epoch 157/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3220 - val_loss: -0.2798\n",
      "Epoch 158/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3240 - val_loss: -0.2852\n",
      "Epoch 159/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3250 - val_loss: -0.2856\n",
      "Epoch 160/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3250 - val_loss: -0.2847\n",
      "Epoch 161/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3245 - val_loss: -0.2815\n",
      "Epoch 162/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3238 - val_loss: -0.2854\n",
      "Epoch 163/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3253 - val_loss: -0.2755\n",
      "Epoch 164/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3262 - val_loss: -0.2824\n",
      "Epoch 165/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3259 - val_loss: -0.2776\n",
      "Epoch 166/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3274 - val_loss: -0.2844\n",
      "Epoch 167/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3269 - val_loss: -0.2784\n",
      "Epoch 168/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3280 - val_loss: -0.2783\n",
      "Epoch 169/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3271 - val_loss: -0.2807\n",
      "Epoch 170/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3292 - val_loss: -0.2798\n",
      "Epoch 171/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3287 - val_loss: -0.2809\n",
      "Epoch 172/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3293 - val_loss: -0.2783\n",
      "Epoch 173/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3296 - val_loss: -0.2789\n",
      "Epoch 174/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3297 - val_loss: -0.2814\n",
      "Epoch 175/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3294 - val_loss: -0.2731\n",
      "Epoch 176/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3314 - val_loss: -0.2824\n",
      "Epoch 177/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3301 - val_loss: -0.2819\n",
      "Epoch 178/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3331 - val_loss: -0.2754\n",
      "Epoch 179/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3332 - val_loss: -0.2753\n",
      "Epoch 180/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3314 - val_loss: -0.2804\n",
      "Epoch 181/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3337 - val_loss: -0.2768\n",
      "Epoch 182/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3338 - val_loss: -0.2645\n",
      "Epoch 183/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3335 - val_loss: -0.2773\n",
      "Epoch 184/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3347 - val_loss: -0.2752\n",
      "Epoch 185/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3353 - val_loss: -0.2746\n",
      "Epoch 186/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3363 - val_loss: -0.2750\n",
      "Epoch 187/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3368 - val_loss: -0.2758\n",
      "Epoch 188/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3366 - val_loss: -0.2742\n",
      "Epoch 189/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3385 - val_loss: -0.2728\n",
      "Epoch 190/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3376 - val_loss: -0.2684\n",
      "Epoch 191/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3376 - val_loss: -0.2654\n",
      "Epoch 192/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3382 - val_loss: -0.2618\n",
      "Epoch 193/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3391 - val_loss: -0.2711\n",
      "Epoch 194/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3407 - val_loss: -0.2676\n",
      "Epoch 195/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3410 - val_loss: -0.2733\n",
      "Epoch 196/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3409 - val_loss: -0.2728\n",
      "Epoch 197/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3413 - val_loss: -0.2713\n",
      "Epoch 198/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3424 - val_loss: -0.2637\n",
      "Epoch 199/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3420 - val_loss: -0.2522\n",
      "Epoch 200/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3423 - val_loss: -0.2700\n",
      "Epoch 201/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3441 - val_loss: -0.2622\n",
      "Epoch 202/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3437 - val_loss: -0.2672\n",
      "Epoch 203/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3442 - val_loss: -0.2686\n",
      "Epoch 204/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3450 - val_loss: -0.2645\n",
      "Epoch 205/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3461 - val_loss: -0.2634\n",
      "Epoch 206/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3462 - val_loss: -0.2638\n",
      "Epoch 207/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3474 - val_loss: -0.2636\n",
      "Epoch 208/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3480 - val_loss: -0.2660\n",
      "Epoch 209/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3483 - val_loss: -0.2641\n",
      "Epoch 210/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3485 - val_loss: -0.2590\n",
      "Epoch 211/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3480 - val_loss: -0.2659\n",
      "Epoch 212/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3496 - val_loss: -0.2617\n",
      "Epoch 213/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3506 - val_loss: -0.2541\n",
      "Epoch 214/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3519 - val_loss: -0.2577\n",
      "Epoch 215/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3521 - val_loss: -0.2623\n",
      "Epoch 216/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3517 - val_loss: -0.2617\n",
      "Epoch 217/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3533 - val_loss: -0.2607\n",
      "Epoch 218/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3534 - val_loss: -0.2569\n",
      "Epoch 219/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3533 - val_loss: -0.2616\n",
      "Epoch 220/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3560 - val_loss: -0.2564\n",
      "Epoch 221/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3546 - val_loss: -0.2562\n",
      "Epoch 222/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3554 - val_loss: -0.2578\n",
      "Epoch 223/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3561 - val_loss: -0.2460\n",
      "Epoch 224/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3576 - val_loss: -0.2563\n",
      "Epoch 225/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3568 - val_loss: -0.2508\n",
      "Epoch 226/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3579 - val_loss: -0.2522\n",
      "Epoch 227/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3601 - val_loss: -0.2511\n",
      "Epoch 228/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3593 - val_loss: -0.2616\n",
      "Epoch 229/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3613 - val_loss: -0.2470\n",
      "Epoch 230/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3614 - val_loss: -0.2419\n",
      "Epoch 231/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3612 - val_loss: -0.2538\n",
      "Epoch 232/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3618 - val_loss: -0.2496\n",
      "Epoch 233/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3629 - val_loss: -0.2510\n",
      "Epoch 234/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3624 - val_loss: -0.2486\n",
      "Epoch 235/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3630 - val_loss: -0.2449\n",
      "Epoch 236/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3648 - val_loss: -0.2469\n",
      "Epoch 237/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3650 - val_loss: -0.2503\n",
      "Epoch 238/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3654 - val_loss: -0.2494\n",
      "Epoch 239/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3662 - val_loss: -0.2465\n",
      "Epoch 240/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3669 - val_loss: -0.2419\n",
      "Epoch 241/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3685 - val_loss: -0.2429\n",
      "Epoch 242/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3677 - val_loss: -0.2381\n",
      "Epoch 243/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3686 - val_loss: -0.2457\n",
      "Epoch 244/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3700 - val_loss: -0.2420\n",
      "Epoch 245/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3697 - val_loss: -0.2377\n",
      "Epoch 246/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3710 - val_loss: -0.2360\n",
      "Epoch 247/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3728 - val_loss: -0.2399\n",
      "Epoch 248/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3726 - val_loss: -0.2311\n",
      "Epoch 249/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3745 - val_loss: -0.2362\n",
      "Epoch 250/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3740 - val_loss: -0.2238\n",
      "Epoch 251/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3746 - val_loss: -0.2338\n",
      "Epoch 252/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3749 - val_loss: -0.2434\n",
      "Epoch 253/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3755 - val_loss: -0.2334\n",
      "Epoch 254/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3767 - val_loss: -0.2316\n",
      "Epoch 255/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3785 - val_loss: -0.2339\n",
      "Epoch 256/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3781 - val_loss: -0.2280\n",
      "Epoch 257/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3793 - val_loss: -0.2300\n",
      "Epoch 258/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3785 - val_loss: -0.2329\n",
      "Epoch 259/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3804 - val_loss: -0.2250\n",
      "Epoch 260/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3813 - val_loss: -0.2238\n",
      "Epoch 261/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3813 - val_loss: -0.2327\n",
      "Epoch 262/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3811 - val_loss: -0.2209\n",
      "Epoch 263/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3827 - val_loss: -0.2220\n",
      "Epoch 264/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3829 - val_loss: -0.2246\n",
      "Epoch 265/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3861 - val_loss: -0.2202\n",
      "Epoch 266/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3853 - val_loss: -0.2214\n",
      "Epoch 267/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3858 - val_loss: -0.2217\n",
      "Epoch 268/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3873 - val_loss: -0.2183\n",
      "Epoch 269/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3855 - val_loss: -0.2201\n",
      "Epoch 270/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3876 - val_loss: -0.2209\n",
      "Epoch 271/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3892 - val_loss: -0.2185\n",
      "Epoch 272/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3887 - val_loss: -0.2157\n",
      "Epoch 273/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3910 - val_loss: -0.2115\n",
      "Epoch 274/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3901 - val_loss: -0.2157\n",
      "Epoch 275/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3930 - val_loss: -0.2161\n",
      "Epoch 276/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3928 - val_loss: -0.2067\n",
      "Epoch 277/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3939 - val_loss: -0.2071\n",
      "Epoch 278/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3966 - val_loss: -0.2086\n",
      "Epoch 279/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3948 - val_loss: -0.2059\n",
      "Epoch 280/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.3954 - val_loss: -0.2082\n",
      "Epoch 281/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.3939 - val_loss: -0.2143\n",
      "Epoch 282/10000\n",
      "298618/298618 [==============================] - 5s 17us/sample - loss: -0.3966 - val_loss: -0.2036\n",
      "Epoch 283/10000\n",
      "298618/298618 [==============================] - 5s 15us/sample - loss: -0.3982 - val_loss: -0.1974\n",
      "Epoch 284/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3989 - val_loss: -0.1984\n",
      "Epoch 285/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3994 - val_loss: -0.2068\n",
      "Epoch 286/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3984 - val_loss: -0.2066\n",
      "Epoch 287/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.3999 - val_loss: -0.2014\n",
      "Epoch 288/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.4020 - val_loss: -0.1958\n",
      "Epoch 289/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.4003 - val_loss: -0.2021\n",
      "Epoch 290/10000\n",
      "298618/298618 [==============================] - 5s 16us/sample - loss: -0.4036 - val_loss: -0.2018\n",
      "Epoch 291/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4034 - val_loss: -0.2035\n",
      "Epoch 292/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4038 - val_loss: -0.1944\n",
      "Epoch 293/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4056 - val_loss: -0.1940\n",
      "Epoch 294/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4065 - val_loss: -0.2027\n",
      "Epoch 295/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4069 - val_loss: -0.1868\n",
      "Epoch 296/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4073 - val_loss: -0.1992\n",
      "Epoch 297/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4075 - val_loss: -0.1900\n",
      "Epoch 298/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4073 - val_loss: -0.2025\n",
      "Epoch 299/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4098 - val_loss: -0.1968\n",
      "Epoch 300/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4098 - val_loss: -0.1996\n",
      "Epoch 301/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.4100 - val_loss: -0.1910\n",
      "Epoch 302/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4122 - val_loss: -0.2002\n",
      "Epoch 303/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.4121 - val_loss: -0.1875\n",
      "Epoch 304/10000\n",
      "298618/298618 [==============================] - 4s 15us/sample - loss: -0.4118 - val_loss: -0.1924\n",
      "Epoch 305/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4135 - val_loss: -0.1825\n",
      "Epoch 306/10000\n",
      "298618/298618 [==============================] - 4s 14us/sample - loss: -0.4152 - val_loss: -0.1905\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5080, 12, 1)\n",
      "y_test.shape:  (5080, 1)\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:37:37,787 WARNING Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7640, 12, 1)\n",
      "y_test.shape:  (7640, 1)\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:37:41,768 WARNING Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7895, 12, 1)\n",
      "y_test.shape:  (7895, 1)\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:37:46,166 WARNING Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7952, 12, 1)\n",
      "y_test.shape:  (7952, 1)\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:37:50,795 WARNING Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7432, 12, 1)\n",
      "y_test.shape:  (7432, 1)\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:37:55,512 WARNING Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7821, 12, 1)\n",
      "y_test.shape:  (7821, 1)\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:00,074 WARNING Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7746, 12, 1)\n",
      "y_test.shape:  (7746, 1)\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:04,730 WARNING Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7829, 12, 1)\n",
      "y_test.shape:  (7829, 1)\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:09,301 WARNING Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7920, 12, 1)\n",
      "y_test.shape:  (7920, 1)\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:13,987 WARNING Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7753, 12, 1)\n",
      "y_test.shape:  (7753, 1)\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:18,585 WARNING Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4908, 12, 1)\n",
      "y_test.shape:  (4908, 1)\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:23,282 WARNING Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7606, 12, 1)\n",
      "y_test.shape:  (7606, 1)\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:27,414 WARNING Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7330, 12, 1)\n",
      "y_test.shape:  (7330, 1)\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:38:32,144 WARNING Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  25.csv\n",
      "2025-01-21 15:38:35,833 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (298332, 12, 1)\n",
      "y_train.shape:  (298332, 1)\n",
      "x_valid.shape:  (74556, 12, 1)\n",
      "y_valid.shape:  (74556, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 15:39:18,139 WARNING Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 15:39:18,259 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 298332 samples, validate on 74556 samples\n",
      "Epoch 1/10000\n",
      "297984/298332 [============================>.] - ETA: 0s - loss: 0.1431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298332/298332 [==============================] - 7s 22us/sample - loss: 0.1428 - val_loss: -0.1108\n",
      "Epoch 2/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.0522 - val_loss: -0.1586\n",
      "Epoch 3/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.0965 - val_loss: -0.1712\n",
      "Epoch 4/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1253 - val_loss: -0.2144\n",
      "Epoch 5/10000\n",
      "298332/298332 [==============================] - 5s 18us/sample - loss: -0.1453 - val_loss: -0.2267\n",
      "Epoch 6/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1589 - val_loss: -0.2303\n",
      "Epoch 7/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1724 - val_loss: -0.2317\n",
      "Epoch 8/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1826 - val_loss: -0.2394\n",
      "Epoch 9/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1914 - val_loss: -0.2428\n",
      "Epoch 10/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.1975 - val_loss: -0.2554\n",
      "Epoch 11/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2039 - val_loss: -0.2380\n",
      "Epoch 12/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2105 - val_loss: -0.2360\n",
      "Epoch 13/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2186 - val_loss: -0.2510\n",
      "Epoch 14/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2232 - val_loss: -0.2632\n",
      "Epoch 15/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2274 - val_loss: -0.2624\n",
      "Epoch 16/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2305 - val_loss: -0.2564\n",
      "Epoch 17/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2353 - val_loss: -0.2656\n",
      "Epoch 18/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2391 - val_loss: -0.2637\n",
      "Epoch 19/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2413 - val_loss: -0.2597\n",
      "Epoch 20/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2457 - val_loss: -0.2547\n",
      "Epoch 21/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2476 - val_loss: -0.2664\n",
      "Epoch 22/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2518 - val_loss: -0.2617\n",
      "Epoch 23/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2524 - val_loss: -0.2704\n",
      "Epoch 24/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2546 - val_loss: -0.2688\n",
      "Epoch 25/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2563 - val_loss: -0.2653\n",
      "Epoch 26/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2586 - val_loss: -0.2624\n",
      "Epoch 27/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2599 - val_loss: -0.2667\n",
      "Epoch 28/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2619 - val_loss: -0.2710\n",
      "Epoch 29/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2633 - val_loss: -0.2615\n",
      "Epoch 30/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2620 - val_loss: -0.2716\n",
      "Epoch 31/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2632 - val_loss: -0.2710\n",
      "Epoch 32/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2661 - val_loss: -0.2706\n",
      "Epoch 33/10000\n",
      "298332/298332 [==============================] - 4s 13us/sample - loss: -0.2663 - val_loss: -0.2709\n",
      "Epoch 34/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2677 - val_loss: -0.2635\n",
      "Epoch 35/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2672 - val_loss: -0.2726\n",
      "Epoch 36/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2693 - val_loss: -0.2627\n",
      "Epoch 37/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2686 - val_loss: -0.2724\n",
      "Epoch 38/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2716 - val_loss: -0.2708\n",
      "Epoch 39/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2708 - val_loss: -0.2753\n",
      "Epoch 40/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2706 - val_loss: -0.2647\n",
      "Epoch 41/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2731 - val_loss: -0.2714\n",
      "Epoch 42/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2714 - val_loss: -0.2741\n",
      "Epoch 43/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2722 - val_loss: -0.2739\n",
      "Epoch 44/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2745 - val_loss: -0.2706\n",
      "Epoch 45/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2732 - val_loss: -0.2712\n",
      "Epoch 46/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2749 - val_loss: -0.2678\n",
      "Epoch 47/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2738 - val_loss: -0.2718\n",
      "Epoch 48/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2746 - val_loss: -0.2687\n",
      "Epoch 49/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2754 - val_loss: -0.2766\n",
      "Epoch 50/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2770 - val_loss: -0.2779\n",
      "Epoch 51/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2765 - val_loss: -0.2752\n",
      "Epoch 52/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2754 - val_loss: -0.2748\n",
      "Epoch 53/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2776 - val_loss: -0.2795\n",
      "Epoch 54/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2772 - val_loss: -0.2690\n",
      "Epoch 55/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2788 - val_loss: -0.2761\n",
      "Epoch 56/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2779 - val_loss: -0.2748\n",
      "Epoch 57/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2779 - val_loss: -0.2781\n",
      "Epoch 58/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2785 - val_loss: -0.2799\n",
      "Epoch 59/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2790 - val_loss: -0.2732\n",
      "Epoch 60/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2792 - val_loss: -0.2737\n",
      "Epoch 61/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2801 - val_loss: -0.2682\n",
      "Epoch 62/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2793 - val_loss: -0.2659\n",
      "Epoch 63/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2786 - val_loss: -0.2782\n",
      "Epoch 64/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2812 - val_loss: -0.2711\n",
      "Epoch 65/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2810 - val_loss: -0.2757\n",
      "Epoch 66/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2809 - val_loss: -0.2780\n",
      "Epoch 67/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2817 - val_loss: -0.2811\n",
      "Epoch 68/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2818 - val_loss: -0.2792\n",
      "Epoch 69/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2813 - val_loss: -0.2823\n",
      "Epoch 70/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2807 - val_loss: -0.2810\n",
      "Epoch 71/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2819 - val_loss: -0.2780\n",
      "Epoch 72/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2833 - val_loss: -0.2772\n",
      "Epoch 73/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2829 - val_loss: -0.2706\n",
      "Epoch 74/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2829 - val_loss: -0.2752\n",
      "Epoch 75/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2832 - val_loss: -0.2772\n",
      "Epoch 76/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2840 - val_loss: -0.2766\n",
      "Epoch 77/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2842 - val_loss: -0.2778\n",
      "Epoch 78/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2846 - val_loss: -0.2798\n",
      "Epoch 79/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2861 - val_loss: -0.2802\n",
      "Epoch 80/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2852 - val_loss: -0.2792\n",
      "Epoch 81/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2855 - val_loss: -0.2700\n",
      "Epoch 82/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2860 - val_loss: -0.2786\n",
      "Epoch 83/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2859 - val_loss: -0.2825\n",
      "Epoch 84/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2862 - val_loss: -0.2733\n",
      "Epoch 85/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2873 - val_loss: -0.2776\n",
      "Epoch 86/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2864 - val_loss: -0.2766\n",
      "Epoch 87/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2887 - val_loss: -0.2763\n",
      "Epoch 88/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2875 - val_loss: -0.2795\n",
      "Epoch 89/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2883 - val_loss: -0.2741\n",
      "Epoch 90/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2895 - val_loss: -0.2785\n",
      "Epoch 91/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2885 - val_loss: -0.2786\n",
      "Epoch 92/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2885 - val_loss: -0.2743\n",
      "Epoch 93/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2892 - val_loss: -0.2758\n",
      "Epoch 94/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2900 - val_loss: -0.2794\n",
      "Epoch 95/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2896 - val_loss: -0.2747\n",
      "Epoch 96/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2904 - val_loss: -0.2774\n",
      "Epoch 97/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2905 - val_loss: -0.2808\n",
      "Epoch 98/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2915 - val_loss: -0.2775\n",
      "Epoch 99/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2901 - val_loss: -0.2750\n",
      "Epoch 100/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2907 - val_loss: -0.2769\n",
      "Epoch 101/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2919 - val_loss: -0.2774\n",
      "Epoch 102/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2927 - val_loss: -0.2783\n",
      "Epoch 103/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2929 - val_loss: -0.2743\n",
      "Epoch 104/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.2928 - val_loss: -0.2765\n",
      "Epoch 105/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2927 - val_loss: -0.2739\n",
      "Epoch 106/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2932 - val_loss: -0.2791\n",
      "Epoch 107/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2938 - val_loss: -0.2758\n",
      "Epoch 108/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.2943 - val_loss: -0.2801\n",
      "Epoch 109/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2939 - val_loss: -0.2763\n",
      "Epoch 110/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2939 - val_loss: -0.2781\n",
      "Epoch 111/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2950 - val_loss: -0.2784\n",
      "Epoch 112/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2945 - val_loss: -0.2772\n",
      "Epoch 113/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2950 - val_loss: -0.2739\n",
      "Epoch 114/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2962 - val_loss: -0.2742\n",
      "Epoch 115/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2951 - val_loss: -0.2781\n",
      "Epoch 116/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2967 - val_loss: -0.2720\n",
      "Epoch 117/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2967 - val_loss: -0.2792\n",
      "Epoch 118/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2975 - val_loss: -0.2758\n",
      "Epoch 119/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2970 - val_loss: -0.2715\n",
      "Epoch 120/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2971 - val_loss: -0.2776\n",
      "Epoch 121/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2983 - val_loss: -0.2786\n",
      "Epoch 122/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.2974 - val_loss: -0.2757\n",
      "Epoch 123/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2999 - val_loss: -0.2675\n",
      "Epoch 124/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2981 - val_loss: -0.2770\n",
      "Epoch 125/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3000 - val_loss: -0.2782\n",
      "Epoch 126/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.2988 - val_loss: -0.2760\n",
      "Epoch 127/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.2994 - val_loss: -0.2732\n",
      "Epoch 128/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3007 - val_loss: -0.2721\n",
      "Epoch 129/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3017 - val_loss: -0.2759\n",
      "Epoch 130/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3014 - val_loss: -0.2757\n",
      "Epoch 131/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3016 - val_loss: -0.2716\n",
      "Epoch 132/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3019 - val_loss: -0.2749\n",
      "Epoch 133/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3019 - val_loss: -0.2686\n",
      "Epoch 134/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3013 - val_loss: -0.2761\n",
      "Epoch 135/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3021 - val_loss: -0.2726\n",
      "Epoch 136/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3035 - val_loss: -0.2695\n",
      "Epoch 137/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3054 - val_loss: -0.2763\n",
      "Epoch 138/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3040 - val_loss: -0.2725\n",
      "Epoch 139/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3047 - val_loss: -0.2766\n",
      "Epoch 140/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3042 - val_loss: -0.2718\n",
      "Epoch 141/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3065 - val_loss: -0.2723\n",
      "Epoch 142/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3050 - val_loss: -0.2696\n",
      "Epoch 143/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3065 - val_loss: -0.2723\n",
      "Epoch 144/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3062 - val_loss: -0.2743\n",
      "Epoch 145/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3075 - val_loss: -0.2713\n",
      "Epoch 146/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3069 - val_loss: -0.2694\n",
      "Epoch 147/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3085 - val_loss: -0.2736\n",
      "Epoch 148/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3076 - val_loss: -0.2701\n",
      "Epoch 149/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3083 - val_loss: -0.2727\n",
      "Epoch 150/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3082 - val_loss: -0.2735\n",
      "Epoch 151/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3106 - val_loss: -0.2710\n",
      "Epoch 152/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3106 - val_loss: -0.2674\n",
      "Epoch 153/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3103 - val_loss: -0.2708\n",
      "Epoch 154/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3119 - val_loss: -0.2717\n",
      "Epoch 155/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3116 - val_loss: -0.2620\n",
      "Epoch 156/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3120 - val_loss: -0.2686\n",
      "Epoch 157/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3117 - val_loss: -0.2708\n",
      "Epoch 158/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3126 - val_loss: -0.2693\n",
      "Epoch 159/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3132 - val_loss: -0.2684\n",
      "Epoch 160/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3143 - val_loss: -0.2679\n",
      "Epoch 161/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3130 - val_loss: -0.2694\n",
      "Epoch 162/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3160 - val_loss: -0.2703\n",
      "Epoch 163/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3149 - val_loss: -0.2677\n",
      "Epoch 164/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3151 - val_loss: -0.2648\n",
      "Epoch 165/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3169 - val_loss: -0.2666\n",
      "Epoch 166/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3171 - val_loss: -0.2675\n",
      "Epoch 167/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3162 - val_loss: -0.2655\n",
      "Epoch 168/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3173 - val_loss: -0.2672\n",
      "Epoch 169/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3177 - val_loss: -0.2627\n",
      "Epoch 170/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3180 - val_loss: -0.2653\n",
      "Epoch 171/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3186 - val_loss: -0.2635\n",
      "Epoch 172/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3198 - val_loss: -0.2695\n",
      "Epoch 173/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3205 - val_loss: -0.2646\n",
      "Epoch 174/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3201 - val_loss: -0.2619\n",
      "Epoch 175/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3215 - val_loss: -0.2617\n",
      "Epoch 176/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3211 - val_loss: -0.2666\n",
      "Epoch 177/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3222 - val_loss: -0.2632\n",
      "Epoch 178/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3231 - val_loss: -0.2645\n",
      "Epoch 179/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3231 - val_loss: -0.2645\n",
      "Epoch 180/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3240 - val_loss: -0.2637\n",
      "Epoch 181/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3253 - val_loss: -0.2585\n",
      "Epoch 182/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3249 - val_loss: -0.2483\n",
      "Epoch 183/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3244 - val_loss: -0.2597\n",
      "Epoch 184/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3261 - val_loss: -0.2590\n",
      "Epoch 185/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3252 - val_loss: -0.2620\n",
      "Epoch 186/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3269 - val_loss: -0.2585\n",
      "Epoch 187/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3268 - val_loss: -0.2620\n",
      "Epoch 188/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3277 - val_loss: -0.2567\n",
      "Epoch 189/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3286 - val_loss: -0.2587\n",
      "Epoch 190/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3281 - val_loss: -0.2590\n",
      "Epoch 191/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3293 - val_loss: -0.2560\n",
      "Epoch 192/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3301 - val_loss: -0.2559\n",
      "Epoch 193/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3304 - val_loss: -0.2606\n",
      "Epoch 194/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3311 - val_loss: -0.2611\n",
      "Epoch 195/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3323 - val_loss: -0.2554\n",
      "Epoch 196/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3332 - val_loss: -0.2517\n",
      "Epoch 197/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3336 - val_loss: -0.2511\n",
      "Epoch 198/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3337 - val_loss: -0.2562\n",
      "Epoch 199/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3346 - val_loss: -0.2508\n",
      "Epoch 200/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3360 - val_loss: -0.2440\n",
      "Epoch 201/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3355 - val_loss: -0.2539\n",
      "Epoch 202/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3368 - val_loss: -0.2504\n",
      "Epoch 203/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3374 - val_loss: -0.2491\n",
      "Epoch 204/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3379 - val_loss: -0.2541\n",
      "Epoch 205/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3386 - val_loss: -0.2475\n",
      "Epoch 206/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3393 - val_loss: -0.2460\n",
      "Epoch 207/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3398 - val_loss: -0.2472\n",
      "Epoch 208/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3406 - val_loss: -0.2480\n",
      "Epoch 209/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3409 - val_loss: -0.2400\n",
      "Epoch 210/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3412 - val_loss: -0.2482\n",
      "Epoch 211/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3432 - val_loss: -0.2421\n",
      "Epoch 212/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3435 - val_loss: -0.2376\n",
      "Epoch 213/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3431 - val_loss: -0.2440\n",
      "Epoch 214/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3440 - val_loss: -0.2433\n",
      "Epoch 215/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3454 - val_loss: -0.2436\n",
      "Epoch 216/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3468 - val_loss: -0.2370\n",
      "Epoch 217/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3460 - val_loss: -0.2378\n",
      "Epoch 218/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3466 - val_loss: -0.2457\n",
      "Epoch 219/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3487 - val_loss: -0.2383\n",
      "Epoch 220/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3478 - val_loss: -0.2437\n",
      "Epoch 221/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3484 - val_loss: -0.2340\n",
      "Epoch 222/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3495 - val_loss: -0.2350\n",
      "Epoch 223/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3496 - val_loss: -0.2314\n",
      "Epoch 224/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3511 - val_loss: -0.2339\n",
      "Epoch 225/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.3519 - val_loss: -0.2399\n",
      "Epoch 226/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3529 - val_loss: -0.2344\n",
      "Epoch 227/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3529 - val_loss: -0.2336\n",
      "Epoch 228/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3535 - val_loss: -0.2337\n",
      "Epoch 229/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3550 - val_loss: -0.2284\n",
      "Epoch 230/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3557 - val_loss: -0.2312\n",
      "Epoch 231/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3547 - val_loss: -0.2327\n",
      "Epoch 232/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3574 - val_loss: -0.2296\n",
      "Epoch 233/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3586 - val_loss: -0.2329\n",
      "Epoch 234/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3576 - val_loss: -0.2260\n",
      "Epoch 235/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3601 - val_loss: -0.2179\n",
      "Epoch 236/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3592 - val_loss: -0.2279\n",
      "Epoch 237/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3615 - val_loss: -0.2232\n",
      "Epoch 238/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3617 - val_loss: -0.2299\n",
      "Epoch 239/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3613 - val_loss: -0.2239\n",
      "Epoch 240/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3618 - val_loss: -0.2207\n",
      "Epoch 241/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3637 - val_loss: -0.2172\n",
      "Epoch 242/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3644 - val_loss: -0.2206\n",
      "Epoch 243/10000\n",
      "298332/298332 [==============================] - 6s 19us/sample - loss: -0.3658 - val_loss: -0.2206\n",
      "Epoch 244/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3657 - val_loss: -0.2159\n",
      "Epoch 245/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3672 - val_loss: -0.2210\n",
      "Epoch 246/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3672 - val_loss: -0.2113\n",
      "Epoch 247/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3678 - val_loss: -0.2194\n",
      "Epoch 248/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3682 - val_loss: -0.2147\n",
      "Epoch 249/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3691 - val_loss: -0.2160\n",
      "Epoch 250/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3702 - val_loss: -0.2175\n",
      "Epoch 251/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3710 - val_loss: -0.2179\n",
      "Epoch 252/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3698 - val_loss: -0.2129\n",
      "Epoch 253/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3731 - val_loss: -0.2083\n",
      "Epoch 254/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3731 - val_loss: -0.2108\n",
      "Epoch 255/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3744 - val_loss: -0.2097\n",
      "Epoch 256/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3750 - val_loss: -0.1997\n",
      "Epoch 257/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3744 - val_loss: -0.2047\n",
      "Epoch 258/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3760 - val_loss: -0.2098\n",
      "Epoch 259/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3775 - val_loss: -0.1978\n",
      "Epoch 260/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3784 - val_loss: -0.1951\n",
      "Epoch 261/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3770 - val_loss: -0.2098\n",
      "Epoch 262/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3791 - val_loss: -0.1961\n",
      "Epoch 263/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3787 - val_loss: -0.2013\n",
      "Epoch 264/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3811 - val_loss: -0.1919\n",
      "Epoch 265/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3812 - val_loss: -0.1974\n",
      "Epoch 266/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3826 - val_loss: -0.1968\n",
      "Epoch 267/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3830 - val_loss: -0.1897\n",
      "Epoch 268/10000\n",
      "298332/298332 [==============================] - 5s 17us/sample - loss: -0.3839 - val_loss: -0.1936\n",
      "Epoch 269/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3845 - val_loss: -0.2007\n",
      "Epoch 270/10000\n",
      "298332/298332 [==============================] - 5s 15us/sample - loss: -0.3843 - val_loss: -0.1889\n",
      "Epoch 271/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3856 - val_loss: -0.1915\n",
      "Epoch 272/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3857 - val_loss: -0.2045\n",
      "Epoch 273/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3883 - val_loss: -0.1948\n",
      "Epoch 274/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3881 - val_loss: -0.1953\n",
      "Epoch 275/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3880 - val_loss: -0.1959\n",
      "Epoch 276/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3897 - val_loss: -0.1965\n",
      "Epoch 277/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3901 - val_loss: -0.1875\n",
      "Epoch 278/10000\n",
      "298332/298332 [==============================] - 4s 15us/sample - loss: -0.3903 - val_loss: -0.1858\n",
      "Epoch 279/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3908 - val_loss: -0.1889\n",
      "Epoch 280/10000\n",
      "298332/298332 [==============================] - 4s 14us/sample - loss: -0.3932 - val_loss: -0.1924\n",
      "Epoch 281/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3934 - val_loss: -0.1875\n",
      "Epoch 282/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3950 - val_loss: -0.1834\n",
      "Epoch 283/10000\n",
      "298332/298332 [==============================] - 5s 16us/sample - loss: -0.3958 - val_loss: -0.1847\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1010.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7486, 12, 1)\n",
      "y_test.shape:  (7486, 1)\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:08,440 WARNING Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1010.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1015.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5708, 12, 1)\n",
      "y_test.shape:  (5708, 1)\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:13,729 WARNING Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1015.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1043.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7754, 12, 1)\n",
      "y_test.shape:  (7754, 1)\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:18,382 WARNING Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1043.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1082.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7854, 12, 1)\n",
      "y_test.shape:  (7854, 1)\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:23,497 WARNING Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1082.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7773, 12, 1)\n",
      "y_test.shape:  (7773, 1)\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:29,225 WARNING Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1121.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7850, 12, 1)\n",
      "y_test.shape:  (7850, 1)\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:34,620 WARNING Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1121.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1127.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7061, 12, 1)\n",
      "y_test.shape:  (7061, 1)\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:39,582 WARNING Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1127.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1139.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5249, 12, 1)\n",
      "y_test.shape:  (5249, 1)\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:44,337 WARNING Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1139.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1143.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7597, 12, 1)\n",
      "y_test.shape:  (7597, 1)\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:49,675 WARNING Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1143.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1171.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7750, 12, 1)\n",
      "y_test.shape:  (7750, 1)\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:00:54,878 WARNING Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1171.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1194.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7941, 12, 1)\n",
      "y_test.shape:  (7941, 1)\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:01:00,568 WARNING Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1194.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1201.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7863, 12, 1)\n",
      "y_test.shape:  (7863, 1)\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:01:06,332 WARNING Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1201.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\252.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7381, 12, 1)\n",
      "y_test.shape:  (7381, 1)\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:01:12,052 WARNING Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  252.csv\n",
      "2025-01-21 16:01:16,227 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (294757, 12, 1)\n",
      "y_train.shape:  (294757, 1)\n",
      "x_valid.shape:  (73664, 12, 1)\n",
      "y_valid.shape:  (73664, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:01:57,033 WARNING Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 16:01:57,124 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 294757 samples, validate on 73664 samples\n",
      "Epoch 1/10000\n",
      "292864/294757 [============================>.] - ETA: 0s - loss: 0.1376"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294757/294757 [==============================] - 7s 23us/sample - loss: 0.1363 - val_loss: -0.1242\n",
      "Epoch 2/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.0731 - val_loss: -0.1831\n",
      "Epoch 3/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.1182 - val_loss: -0.2120\n",
      "Epoch 4/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.1416 - val_loss: -0.2194\n",
      "Epoch 5/10000\n",
      "294757/294757 [==============================] - 6s 20us/sample - loss: -0.1576 - val_loss: -0.2076\n",
      "Epoch 6/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.1710 - val_loss: -0.2438\n",
      "Epoch 7/10000\n",
      "294757/294757 [==============================] - 4s 13us/sample - loss: -0.1832 - val_loss: -0.2382\n",
      "Epoch 8/10000\n",
      "294757/294757 [==============================] - 4s 13us/sample - loss: -0.1948 - val_loss: -0.2546\n",
      "Epoch 9/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2045 - val_loss: -0.2607\n",
      "Epoch 10/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2125 - val_loss: -0.2678\n",
      "Epoch 11/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2196 - val_loss: -0.2587\n",
      "Epoch 12/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2249 - val_loss: -0.2624\n",
      "Epoch 13/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2304 - val_loss: -0.2727\n",
      "Epoch 14/10000\n",
      "294757/294757 [==============================] - 4s 13us/sample - loss: -0.2368 - val_loss: -0.2764\n",
      "Epoch 15/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2386 - val_loss: -0.2694\n",
      "Epoch 16/10000\n",
      "294757/294757 [==============================] - 4s 13us/sample - loss: -0.2455 - val_loss: -0.2775\n",
      "Epoch 17/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2497 - val_loss: -0.2715\n",
      "Epoch 18/10000\n",
      "294757/294757 [==============================] - 4s 13us/sample - loss: -0.2542 - val_loss: -0.2740\n",
      "Epoch 19/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2569 - val_loss: -0.2862\n",
      "Epoch 20/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2598 - val_loss: -0.2854\n",
      "Epoch 21/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2634 - val_loss: -0.2817\n",
      "Epoch 22/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2653 - val_loss: -0.2843\n",
      "Epoch 23/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2676 - val_loss: -0.2810\n",
      "Epoch 24/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2711 - val_loss: -0.2894\n",
      "Epoch 25/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2711 - val_loss: -0.2872\n",
      "Epoch 26/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2744 - val_loss: -0.2886\n",
      "Epoch 27/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2759 - val_loss: -0.2899\n",
      "Epoch 28/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2779 - val_loss: -0.2841\n",
      "Epoch 29/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2796 - val_loss: -0.2899\n",
      "Epoch 30/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2796 - val_loss: -0.2887\n",
      "Epoch 31/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2804 - val_loss: -0.2894\n",
      "Epoch 32/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2826 - val_loss: -0.2916\n",
      "Epoch 33/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2825 - val_loss: -0.2843\n",
      "Epoch 34/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2840 - val_loss: -0.2936\n",
      "Epoch 35/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2854 - val_loss: -0.2953\n",
      "Epoch 36/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2850 - val_loss: -0.2890\n",
      "Epoch 37/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2871 - val_loss: -0.2901\n",
      "Epoch 38/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2857 - val_loss: -0.2894\n",
      "Epoch 39/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.2875 - val_loss: -0.2949\n",
      "Epoch 40/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2882 - val_loss: -0.2878\n",
      "Epoch 41/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2896 - val_loss: -0.2914\n",
      "Epoch 42/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2889 - val_loss: -0.2943\n",
      "Epoch 43/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2904 - val_loss: -0.2980\n",
      "Epoch 44/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2908 - val_loss: -0.2941\n",
      "Epoch 45/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2904 - val_loss: -0.2879\n",
      "Epoch 46/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2900 - val_loss: -0.2918\n",
      "Epoch 47/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2916 - val_loss: -0.2902\n",
      "Epoch 48/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2924 - val_loss: -0.2862\n",
      "Epoch 49/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2922 - val_loss: -0.2982\n",
      "Epoch 50/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2925 - val_loss: -0.2972\n",
      "Epoch 51/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2926 - val_loss: -0.2955\n",
      "Epoch 52/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2937 - val_loss: -0.2964\n",
      "Epoch 53/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2927 - val_loss: -0.2968\n",
      "Epoch 54/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2951 - val_loss: -0.2835\n",
      "Epoch 55/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2949 - val_loss: -0.2916\n",
      "Epoch 56/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2937 - val_loss: -0.2931\n",
      "Epoch 57/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2956 - val_loss: -0.2844\n",
      "Epoch 58/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.2942 - val_loss: -0.2941\n",
      "Epoch 59/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2953 - val_loss: -0.2868\n",
      "Epoch 60/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2977 - val_loss: -0.2988\n",
      "Epoch 61/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2950 - val_loss: -0.2982\n",
      "Epoch 62/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2968 - val_loss: -0.2992\n",
      "Epoch 63/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2971 - val_loss: -0.2969\n",
      "Epoch 64/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2971 - val_loss: -0.2949\n",
      "Epoch 65/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2986 - val_loss: -0.2951\n",
      "Epoch 66/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.2961 - val_loss: -0.2967\n",
      "Epoch 67/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.2978 - val_loss: -0.2943\n",
      "Epoch 68/10000\n",
      "294757/294757 [==============================] - 5s 18us/sample - loss: -0.2980 - val_loss: -0.3006\n",
      "Epoch 69/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.2985 - val_loss: -0.2960\n",
      "Epoch 70/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.2984 - val_loss: -0.2991\n",
      "Epoch 71/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2987 - val_loss: -0.2977\n",
      "Epoch 72/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2998 - val_loss: -0.2945\n",
      "Epoch 73/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2989 - val_loss: -0.2966\n",
      "Epoch 74/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3007 - val_loss: -0.2948\n",
      "Epoch 75/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.2988 - val_loss: -0.2992\n",
      "Epoch 76/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3009 - val_loss: -0.2946\n",
      "Epoch 77/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3007 - val_loss: -0.2979\n",
      "Epoch 78/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3023 - val_loss: -0.2950\n",
      "Epoch 79/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3011 - val_loss: -0.2980\n",
      "Epoch 80/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3032 - val_loss: -0.2994\n",
      "Epoch 81/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3019 - val_loss: -0.2995\n",
      "Epoch 82/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3036 - val_loss: -0.3007\n",
      "Epoch 83/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3027 - val_loss: -0.3015\n",
      "Epoch 84/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3022 - val_loss: -0.2881\n",
      "Epoch 85/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3030 - val_loss: -0.2986\n",
      "Epoch 86/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3044 - val_loss: -0.3009\n",
      "Epoch 87/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3021 - val_loss: -0.2954\n",
      "Epoch 88/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3031 - val_loss: -0.2983\n",
      "Epoch 89/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3039 - val_loss: -0.2983\n",
      "Epoch 90/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3051 - val_loss: -0.2994\n",
      "Epoch 91/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3049 - val_loss: -0.2991\n",
      "Epoch 92/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3039 - val_loss: -0.2979\n",
      "Epoch 93/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3050 - val_loss: -0.2969\n",
      "Epoch 94/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3052 - val_loss: -0.2934\n",
      "Epoch 95/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3062 - val_loss: -0.3015\n",
      "Epoch 96/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3058 - val_loss: -0.3015\n",
      "Epoch 97/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3061 - val_loss: -0.2969\n",
      "Epoch 98/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3063 - val_loss: -0.2974\n",
      "Epoch 99/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3055 - val_loss: -0.2987\n",
      "Epoch 100/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3078 - val_loss: -0.2918\n",
      "Epoch 101/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3065 - val_loss: -0.2984\n",
      "Epoch 102/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3078 - val_loss: -0.2915\n",
      "Epoch 103/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3080 - val_loss: -0.2935\n",
      "Epoch 104/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3066 - val_loss: -0.2777\n",
      "Epoch 105/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3089 - val_loss: -0.3002\n",
      "Epoch 106/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3072 - val_loss: -0.2969\n",
      "Epoch 107/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3081 - val_loss: -0.3017\n",
      "Epoch 108/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3100 - val_loss: -0.2993\n",
      "Epoch 109/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3098 - val_loss: -0.2987\n",
      "Epoch 110/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3097 - val_loss: -0.3016\n",
      "Epoch 111/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3090 - val_loss: -0.3003\n",
      "Epoch 112/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3089 - val_loss: -0.2947\n",
      "Epoch 113/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3095 - val_loss: -0.2977\n",
      "Epoch 114/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3106 - val_loss: -0.2979\n",
      "Epoch 115/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3108 - val_loss: -0.2984\n",
      "Epoch 116/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3115 - val_loss: -0.2987\n",
      "Epoch 117/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3121 - val_loss: -0.3015\n",
      "Epoch 118/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3100 - val_loss: -0.2976\n",
      "Epoch 119/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3118 - val_loss: -0.3012\n",
      "Epoch 120/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3119 - val_loss: -0.2968\n",
      "Epoch 121/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3130 - val_loss: -0.2980\n",
      "Epoch 122/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3121 - val_loss: -0.2987\n",
      "Epoch 123/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3115 - val_loss: -0.3017\n",
      "Epoch 124/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3133 - val_loss: -0.2961\n",
      "Epoch 125/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3141 - val_loss: -0.2996\n",
      "Epoch 126/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3144 - val_loss: -0.3007\n",
      "Epoch 127/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3128 - val_loss: -0.2966\n",
      "Epoch 128/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3141 - val_loss: -0.2983\n",
      "Epoch 129/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3138 - val_loss: -0.2989\n",
      "Epoch 130/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3145 - val_loss: -0.2997\n",
      "Epoch 131/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3162 - val_loss: -0.2972\n",
      "Epoch 132/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3160 - val_loss: -0.2976\n",
      "Epoch 133/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3159 - val_loss: -0.2977\n",
      "Epoch 134/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3174 - val_loss: -0.2952\n",
      "Epoch 135/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3148 - val_loss: -0.2921\n",
      "Epoch 136/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3184 - val_loss: -0.2956\n",
      "Epoch 137/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3180 - val_loss: -0.2959\n",
      "Epoch 138/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3166 - val_loss: -0.2971\n",
      "Epoch 139/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3199 - val_loss: -0.2980\n",
      "Epoch 140/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3180 - val_loss: -0.2955\n",
      "Epoch 141/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3183 - val_loss: -0.2928\n",
      "Epoch 142/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3184 - val_loss: -0.2943\n",
      "Epoch 143/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3191 - val_loss: -0.2956\n",
      "Epoch 144/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3191 - val_loss: -0.2954\n",
      "Epoch 145/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3195 - val_loss: -0.2969\n",
      "Epoch 146/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3201 - val_loss: -0.2965\n",
      "Epoch 147/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3208 - val_loss: -0.2897\n",
      "Epoch 148/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3197 - val_loss: -0.2923\n",
      "Epoch 149/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3212 - val_loss: -0.2906\n",
      "Epoch 150/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3215 - val_loss: -0.2976\n",
      "Epoch 151/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3210 - val_loss: -0.2942\n",
      "Epoch 152/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3215 - val_loss: -0.2865\n",
      "Epoch 153/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3209 - val_loss: -0.2949\n",
      "Epoch 154/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3246 - val_loss: -0.2955\n",
      "Epoch 155/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3243 - val_loss: -0.2938\n",
      "Epoch 156/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3225 - val_loss: -0.2915\n",
      "Epoch 157/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3245 - val_loss: -0.2929\n",
      "Epoch 158/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3249 - val_loss: -0.2939\n",
      "Epoch 159/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3253 - val_loss: -0.2919\n",
      "Epoch 160/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3253 - val_loss: -0.2900\n",
      "Epoch 161/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3254 - val_loss: -0.2935\n",
      "Epoch 162/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3265 - val_loss: -0.2905\n",
      "Epoch 163/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3271 - val_loss: -0.2927\n",
      "Epoch 164/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3270 - val_loss: -0.2933\n",
      "Epoch 165/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3276 - val_loss: -0.2920\n",
      "Epoch 166/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3275 - val_loss: -0.2940\n",
      "Epoch 167/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3284 - val_loss: -0.2921\n",
      "Epoch 168/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3292 - val_loss: -0.2863\n",
      "Epoch 169/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3294 - val_loss: -0.2961\n",
      "Epoch 170/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3305 - val_loss: -0.2880\n",
      "Epoch 171/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3292 - val_loss: -0.2904\n",
      "Epoch 172/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3296 - val_loss: -0.2874\n",
      "Epoch 173/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3306 - val_loss: -0.2907\n",
      "Epoch 174/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3314 - val_loss: -0.2907\n",
      "Epoch 175/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3313 - val_loss: -0.2928\n",
      "Epoch 176/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3326 - val_loss: -0.2884\n",
      "Epoch 177/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3328 - val_loss: -0.2872\n",
      "Epoch 178/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3323 - val_loss: -0.2866\n",
      "Epoch 179/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3339 - val_loss: -0.2842\n",
      "Epoch 180/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3344 - val_loss: -0.2880\n",
      "Epoch 181/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3336 - val_loss: -0.2836\n",
      "Epoch 182/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3346 - val_loss: -0.2867\n",
      "Epoch 183/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3363 - val_loss: -0.2857\n",
      "Epoch 184/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3356 - val_loss: -0.2843\n",
      "Epoch 185/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3363 - val_loss: -0.2874\n",
      "Epoch 186/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3374 - val_loss: -0.2875\n",
      "Epoch 187/10000\n",
      "294757/294757 [==============================] - 5s 18us/sample - loss: -0.3373 - val_loss: -0.2885\n",
      "Epoch 188/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3380 - val_loss: -0.2842\n",
      "Epoch 189/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3389 - val_loss: -0.2869\n",
      "Epoch 190/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3390 - val_loss: -0.2858\n",
      "Epoch 191/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3393 - val_loss: -0.2837\n",
      "Epoch 192/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3400 - val_loss: -0.2811\n",
      "Epoch 193/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3404 - val_loss: -0.2819\n",
      "Epoch 194/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3410 - val_loss: -0.2799\n",
      "Epoch 195/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3414 - val_loss: -0.2857\n",
      "Epoch 196/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3414 - val_loss: -0.2810\n",
      "Epoch 197/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3423 - val_loss: -0.2760\n",
      "Epoch 198/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3414 - val_loss: -0.2827\n",
      "Epoch 199/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3417 - val_loss: -0.2800\n",
      "Epoch 200/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3417 - val_loss: -0.2833\n",
      "Epoch 201/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3451 - val_loss: -0.2828\n",
      "Epoch 202/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3452 - val_loss: -0.2765\n",
      "Epoch 203/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3456 - val_loss: -0.2780\n",
      "Epoch 204/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3460 - val_loss: -0.2803\n",
      "Epoch 205/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3457 - val_loss: -0.2781\n",
      "Epoch 206/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3467 - val_loss: -0.2764\n",
      "Epoch 207/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3483 - val_loss: -0.2758\n",
      "Epoch 208/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3475 - val_loss: -0.2727\n",
      "Epoch 209/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3476 - val_loss: -0.2743\n",
      "Epoch 210/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3484 - val_loss: -0.2698\n",
      "Epoch 211/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3485 - val_loss: -0.2700\n",
      "Epoch 212/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3496 - val_loss: -0.2726\n",
      "Epoch 213/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3515 - val_loss: -0.2726\n",
      "Epoch 214/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3517 - val_loss: -0.2736\n",
      "Epoch 215/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3519 - val_loss: -0.2725\n",
      "Epoch 216/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3521 - val_loss: -0.2738\n",
      "Epoch 217/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3526 - val_loss: -0.2686\n",
      "Epoch 218/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3531 - val_loss: -0.2755\n",
      "Epoch 219/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3539 - val_loss: -0.2722\n",
      "Epoch 220/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3556 - val_loss: -0.2711\n",
      "Epoch 221/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3555 - val_loss: -0.2674\n",
      "Epoch 222/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3566 - val_loss: -0.2728\n",
      "Epoch 223/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3555 - val_loss: -0.2671\n",
      "Epoch 224/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3558 - val_loss: -0.2710\n",
      "Epoch 225/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3588 - val_loss: -0.2654\n",
      "Epoch 226/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3587 - val_loss: -0.2686\n",
      "Epoch 227/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3586 - val_loss: -0.2593\n",
      "Epoch 228/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3590 - val_loss: -0.2654\n",
      "Epoch 229/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3592 - val_loss: -0.2625\n",
      "Epoch 230/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3603 - val_loss: -0.2641\n",
      "Epoch 231/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3611 - val_loss: -0.2620\n",
      "Epoch 232/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3622 - val_loss: -0.2605\n",
      "Epoch 233/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3623 - val_loss: -0.2601\n",
      "Epoch 234/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3617 - val_loss: -0.2676\n",
      "Epoch 235/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3641 - val_loss: -0.2583\n",
      "Epoch 236/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3647 - val_loss: -0.2529\n",
      "Epoch 237/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3651 - val_loss: -0.2631\n",
      "Epoch 238/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3648 - val_loss: -0.2573\n",
      "Epoch 239/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3667 - val_loss: -0.2598\n",
      "Epoch 240/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3672 - val_loss: -0.2628\n",
      "Epoch 241/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3675 - val_loss: -0.2530\n",
      "Epoch 242/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3666 - val_loss: -0.2534\n",
      "Epoch 243/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3690 - val_loss: -0.2532\n",
      "Epoch 244/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3690 - val_loss: -0.2587\n",
      "Epoch 245/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3688 - val_loss: -0.2580\n",
      "Epoch 246/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3708 - val_loss: -0.2555\n",
      "Epoch 247/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3711 - val_loss: -0.2455\n",
      "Epoch 248/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3720 - val_loss: -0.2506\n",
      "Epoch 249/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3736 - val_loss: -0.2506\n",
      "Epoch 250/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3728 - val_loss: -0.2519\n",
      "Epoch 251/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3744 - val_loss: -0.2389\n",
      "Epoch 252/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3738 - val_loss: -0.2530\n",
      "Epoch 253/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3751 - val_loss: -0.2524\n",
      "Epoch 254/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3757 - val_loss: -0.2497\n",
      "Epoch 255/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3761 - val_loss: -0.2441\n",
      "Epoch 256/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3767 - val_loss: -0.2505\n",
      "Epoch 257/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3782 - val_loss: -0.2436\n",
      "Epoch 258/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3801 - val_loss: -0.2431\n",
      "Epoch 259/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3797 - val_loss: -0.2423\n",
      "Epoch 260/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3803 - val_loss: -0.2438\n",
      "Epoch 261/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3805 - val_loss: -0.2463\n",
      "Epoch 262/10000\n",
      "294757/294757 [==============================] - 5s 17us/sample - loss: -0.3812 - val_loss: -0.2394\n",
      "Epoch 263/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3806 - val_loss: -0.2537\n",
      "Epoch 264/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3841 - val_loss: -0.2425\n",
      "Epoch 265/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3825 - val_loss: -0.2322\n",
      "Epoch 266/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3823 - val_loss: -0.2284\n",
      "Epoch 267/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3840 - val_loss: -0.2348\n",
      "Epoch 268/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3849 - val_loss: -0.2401\n",
      "Epoch 269/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3859 - val_loss: -0.2364\n",
      "Epoch 270/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3853 - val_loss: -0.2410\n",
      "Epoch 271/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3881 - val_loss: -0.2250\n",
      "Epoch 272/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3876 - val_loss: -0.2351\n",
      "Epoch 273/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3889 - val_loss: -0.2238\n",
      "Epoch 274/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3898 - val_loss: -0.2359\n",
      "Epoch 275/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.3897 - val_loss: -0.2425\n",
      "Epoch 276/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.3910 - val_loss: -0.2368\n",
      "Epoch 277/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3907 - val_loss: -0.2361\n",
      "Epoch 278/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3917 - val_loss: -0.2301\n",
      "Epoch 279/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3922 - val_loss: -0.2346\n",
      "Epoch 280/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3920 - val_loss: -0.2262\n",
      "Epoch 281/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3922 - val_loss: -0.2295\n",
      "Epoch 282/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3931 - val_loss: -0.2257\n",
      "Epoch 283/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3943 - val_loss: -0.2239\n",
      "Epoch 284/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3969 - val_loss: -0.2262\n",
      "Epoch 285/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3966 - val_loss: -0.2230\n",
      "Epoch 286/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3976 - val_loss: -0.2203\n",
      "Epoch 287/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3960 - val_loss: -0.2203\n",
      "Epoch 288/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3991 - val_loss: -0.2184\n",
      "Epoch 289/10000\n",
      "294757/294757 [==============================] - 5s 15us/sample - loss: -0.3991 - val_loss: -0.2140\n",
      "Epoch 290/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.3990 - val_loss: -0.2244\n",
      "Epoch 291/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4000 - val_loss: -0.2074\n",
      "Epoch 292/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4000 - val_loss: -0.2189\n",
      "Epoch 293/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4016 - val_loss: -0.2175\n",
      "Epoch 294/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4017 - val_loss: -0.2189\n",
      "Epoch 295/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4042 - val_loss: -0.2130\n",
      "Epoch 296/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4048 - val_loss: -0.2265\n",
      "Epoch 297/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4049 - val_loss: -0.2185\n",
      "Epoch 298/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4054 - val_loss: -0.2201\n",
      "Epoch 299/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4058 - val_loss: -0.2153\n",
      "Epoch 300/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4049 - val_loss: -0.2109\n",
      "Epoch 301/10000\n",
      "294757/294757 [==============================] - 5s 16us/sample - loss: -0.4080 - val_loss: -0.2110\n",
      "Epoch 302/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4096 - val_loss: -0.2058\n",
      "Epoch 303/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4081 - val_loss: -0.2123\n",
      "Epoch 304/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4105 - val_loss: -0.2032\n",
      "Epoch 305/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4102 - val_loss: -0.2133\n",
      "Epoch 306/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4111 - val_loss: -0.2121\n",
      "Epoch 307/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4114 - val_loss: -0.2091\n",
      "Epoch 308/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4131 - val_loss: -0.1946\n",
      "Epoch 309/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4120 - val_loss: -0.2004\n",
      "Epoch 310/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4151 - val_loss: -0.2058\n",
      "Epoch 311/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4140 - val_loss: -0.1988\n",
      "Epoch 312/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4158 - val_loss: -0.2044\n",
      "Epoch 313/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4156 - val_loss: -0.2024\n",
      "Epoch 314/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4164 - val_loss: -0.1916\n",
      "Epoch 315/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4171 - val_loss: -0.1957\n",
      "Epoch 316/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4171 - val_loss: -0.1859\n",
      "Epoch 317/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4174 - val_loss: -0.2048\n",
      "Epoch 318/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4187 - val_loss: -0.1863\n",
      "Epoch 319/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4199 - val_loss: -0.1906\n",
      "Epoch 320/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4199 - val_loss: -0.1809\n",
      "Epoch 321/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4214 - val_loss: -0.1837\n",
      "Epoch 322/10000\n",
      "294757/294757 [==============================] - 4s 15us/sample - loss: -0.4221 - val_loss: -0.1914\n",
      "Epoch 323/10000\n",
      "294757/294757 [==============================] - 4s 14us/sample - loss: -0.4231 - val_loss: -0.1888\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1205.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7411, 12, 1)\n",
      "y_test.shape:  (7411, 1)\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:25:36,400 WARNING Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1205.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1211.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7841, 12, 1)\n",
      "y_test.shape:  (7841, 1)\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:25:41,752 WARNING Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1211.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1219.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7883, 12, 1)\n",
      "y_test.shape:  (7883, 1)\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:25:47,022 WARNING Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1219.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1230.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7589, 12, 1)\n",
      "y_test.shape:  (7589, 1)\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:25:52,403 WARNING Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1230.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1239.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7681, 12, 1)\n",
      "y_test.shape:  (7681, 1)\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:25:57,779 WARNING Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1239.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1271.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7942, 12, 1)\n",
      "y_test.shape:  (7942, 1)\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:03,414 WARNING Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1271.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1286.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7963, 12, 1)\n",
      "y_test.shape:  (7963, 1)\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:08,889 WARNING Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1286.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1311.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7384, 12, 1)\n",
      "y_test.shape:  (7384, 1)\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:14,469 WARNING Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1311.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1330.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7877, 12, 1)\n",
      "y_test.shape:  (7877, 1)\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:19,792 WARNING Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1330.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1336.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7669, 12, 1)\n",
      "y_test.shape:  (7669, 1)\n",
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:25,322 WARNING Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1336.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1343.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7292, 12, 1)\n",
      "y_test.shape:  (7292, 1)\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:30,825 WARNING Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1343.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1345.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7489, 12, 1)\n",
      "y_test.shape:  (7489, 1)\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:36,156 WARNING Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1345.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1348.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7713, 12, 1)\n",
      "y_test.shape:  (7713, 1)\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:26:41,594 WARNING Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1348.csv\n",
      "2025-01-21 16:26:46,262 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (297646, 12, 1)\n",
      "y_train.shape:  (297646, 1)\n",
      "x_valid.shape:  (74383, 12, 1)\n",
      "y_valid.shape:  (74383, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:27:27,417 WARNING Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 16:27:27,546 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 297646 samples, validate on 74383 samples\n",
      "Epoch 1/10000\n",
      "295936/297646 [============================>.] - ETA: 0s - loss: 0.1297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297646/297646 [==============================] - 7s 23us/sample - loss: 0.1290 - val_loss: -0.1335\n",
      "Epoch 2/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.0727 - val_loss: -0.2090\n",
      "Epoch 3/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.1184 - val_loss: -0.2246\n",
      "Epoch 4/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.1417 - val_loss: -0.2278\n",
      "Epoch 5/10000\n",
      "297646/297646 [==============================] - 6s 21us/sample - loss: -0.1629 - val_loss: -0.2541\n",
      "Epoch 6/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.1785 - val_loss: -0.2681\n",
      "Epoch 7/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.1902 - val_loss: -0.2614\n",
      "Epoch 8/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.1997 - val_loss: -0.2694\n",
      "Epoch 9/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2101 - val_loss: -0.2664\n",
      "Epoch 10/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2162 - val_loss: -0.2705\n",
      "Epoch 11/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2247 - val_loss: -0.2719\n",
      "Epoch 12/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2319 - val_loss: -0.2738\n",
      "Epoch 13/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2367 - val_loss: -0.2734\n",
      "Epoch 14/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2418 - val_loss: -0.2830\n",
      "Epoch 15/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2470 - val_loss: -0.2748\n",
      "Epoch 16/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2519 - val_loss: -0.2743\n",
      "Epoch 17/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2572 - val_loss: -0.2800\n",
      "Epoch 18/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2584 - val_loss: -0.2826\n",
      "Epoch 19/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2628 - val_loss: -0.2896\n",
      "Epoch 20/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2666 - val_loss: -0.2836\n",
      "Epoch 21/10000\n",
      "297646/297646 [==============================] - 4s 13us/sample - loss: -0.2694 - val_loss: -0.2797\n",
      "Epoch 22/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2714 - val_loss: -0.2847\n",
      "Epoch 23/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2720 - val_loss: -0.2943\n",
      "Epoch 24/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2762 - val_loss: -0.2772\n",
      "Epoch 25/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2768 - val_loss: -0.2899\n",
      "Epoch 26/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2771 - val_loss: -0.2947\n",
      "Epoch 27/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2797 - val_loss: -0.2897\n",
      "Epoch 28/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2824 - val_loss: -0.2933\n",
      "Epoch 29/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2837 - val_loss: -0.2932\n",
      "Epoch 30/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2833 - val_loss: -0.2967\n",
      "Epoch 31/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2850 - val_loss: -0.2977\n",
      "Epoch 32/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2861 - val_loss: -0.2976\n",
      "Epoch 33/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2883 - val_loss: -0.2982\n",
      "Epoch 34/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2887 - val_loss: -0.2674\n",
      "Epoch 35/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2896 - val_loss: -0.2968\n",
      "Epoch 36/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2907 - val_loss: -0.2893\n",
      "Epoch 37/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2902 - val_loss: -0.2907\n",
      "Epoch 38/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2912 - val_loss: -0.2989\n",
      "Epoch 39/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2917 - val_loss: -0.3002\n",
      "Epoch 40/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2929 - val_loss: -0.3040\n",
      "Epoch 41/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2932 - val_loss: -0.2981\n",
      "Epoch 42/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2945 - val_loss: -0.2879\n",
      "Epoch 43/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2946 - val_loss: -0.3034\n",
      "Epoch 44/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2944 - val_loss: -0.3005\n",
      "Epoch 45/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2963 - val_loss: -0.2969\n",
      "Epoch 46/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.2961 - val_loss: -0.2978\n",
      "Epoch 47/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2956 - val_loss: -0.2915\n",
      "Epoch 48/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2963 - val_loss: -0.2924\n",
      "Epoch 49/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2974 - val_loss: -0.2909\n",
      "Epoch 50/10000\n",
      "297646/297646 [==============================] - 5s 17us/sample - loss: -0.2975 - val_loss: -0.2945\n",
      "Epoch 51/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2991 - val_loss: -0.2921\n",
      "Epoch 52/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.2981 - val_loss: -0.3011\n",
      "Epoch 53/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2988 - val_loss: -0.3012\n",
      "Epoch 54/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3000 - val_loss: -0.2995\n",
      "Epoch 55/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.2999 - val_loss: -0.3026\n",
      "Epoch 56/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3007 - val_loss: -0.2965\n",
      "Epoch 57/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3007 - val_loss: -0.3006\n",
      "Epoch 58/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3005 - val_loss: -0.2920\n",
      "Epoch 59/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.2997 - val_loss: -0.2847\n",
      "Epoch 60/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3021 - val_loss: -0.3021\n",
      "Epoch 61/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3018 - val_loss: -0.3031\n",
      "Epoch 62/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3013 - val_loss: -0.3018\n",
      "Epoch 63/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3030 - val_loss: -0.3009\n",
      "Epoch 64/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3008 - val_loss: -0.3017\n",
      "Epoch 65/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3033 - val_loss: -0.3046\n",
      "Epoch 66/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3030 - val_loss: -0.3005\n",
      "Epoch 67/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3028 - val_loss: -0.2958\n",
      "Epoch 68/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3030 - val_loss: -0.2963\n",
      "Epoch 69/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3049 - val_loss: -0.2965\n",
      "Epoch 70/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3027 - val_loss: -0.3031\n",
      "Epoch 71/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3049 - val_loss: -0.3056\n",
      "Epoch 72/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3044 - val_loss: -0.3002\n",
      "Epoch 73/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3052 - val_loss: -0.3027\n",
      "Epoch 74/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3058 - val_loss: -0.3015\n",
      "Epoch 75/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3065 - val_loss: -0.2983\n",
      "Epoch 76/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3057 - val_loss: -0.2975\n",
      "Epoch 77/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3061 - val_loss: -0.3015\n",
      "Epoch 78/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3069 - val_loss: -0.3006\n",
      "Epoch 79/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3072 - val_loss: -0.3036\n",
      "Epoch 80/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3071 - val_loss: -0.2994\n",
      "Epoch 81/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3068 - val_loss: -0.3032\n",
      "Epoch 82/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3068 - val_loss: -0.3024\n",
      "Epoch 83/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3077 - val_loss: -0.3047\n",
      "Epoch 84/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3077 - val_loss: -0.3044\n",
      "Epoch 85/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3080 - val_loss: -0.3063\n",
      "Epoch 86/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3086 - val_loss: -0.3054\n",
      "Epoch 87/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3082 - val_loss: -0.2982\n",
      "Epoch 88/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3084 - val_loss: -0.2992\n",
      "Epoch 89/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3103 - val_loss: -0.3052\n",
      "Epoch 90/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3093 - val_loss: -0.3014\n",
      "Epoch 91/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3101 - val_loss: -0.2979\n",
      "Epoch 92/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3103 - val_loss: -0.3005\n",
      "Epoch 93/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3113 - val_loss: -0.3052\n",
      "Epoch 94/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3113 - val_loss: -0.3046\n",
      "Epoch 95/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3107 - val_loss: -0.3009\n",
      "Epoch 96/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3116 - val_loss: -0.3009\n",
      "Epoch 97/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3115 - val_loss: -0.3015\n",
      "Epoch 98/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3116 - val_loss: -0.3019\n",
      "Epoch 99/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3122 - val_loss: -0.3044\n",
      "Epoch 100/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3129 - val_loss: -0.3053\n",
      "Epoch 101/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3132 - val_loss: -0.3035\n",
      "Epoch 102/10000\n",
      "297646/297646 [==============================] - 5s 17us/sample - loss: -0.3132 - val_loss: -0.2953\n",
      "Epoch 103/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3121 - val_loss: -0.2990\n",
      "Epoch 104/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3142 - val_loss: -0.2984\n",
      "Epoch 105/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3158 - val_loss: -0.3021\n",
      "Epoch 106/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3142 - val_loss: -0.3022\n",
      "Epoch 107/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3146 - val_loss: -0.2919\n",
      "Epoch 108/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3148 - val_loss: -0.3037\n",
      "Epoch 109/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3149 - val_loss: -0.3027\n",
      "Epoch 110/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3160 - val_loss: -0.3016\n",
      "Epoch 111/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3155 - val_loss: -0.3027\n",
      "Epoch 112/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3150 - val_loss: -0.2987\n",
      "Epoch 113/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3168 - val_loss: -0.2990\n",
      "Epoch 114/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3164 - val_loss: -0.3030\n",
      "Epoch 115/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3178 - val_loss: -0.3042\n",
      "Epoch 116/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3174 - val_loss: -0.2983\n",
      "Epoch 117/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3185 - val_loss: -0.3007\n",
      "Epoch 118/10000\n",
      "297646/297646 [==============================] - 5s 17us/sample - loss: -0.3179 - val_loss: -0.3018\n",
      "Epoch 119/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3183 - val_loss: -0.3002\n",
      "Epoch 120/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3182 - val_loss: -0.2995\n",
      "Epoch 121/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3186 - val_loss: -0.3025\n",
      "Epoch 122/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3174 - val_loss: -0.3003\n",
      "Epoch 123/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3187 - val_loss: -0.2921\n",
      "Epoch 124/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3208 - val_loss: -0.3014\n",
      "Epoch 125/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3208 - val_loss: -0.2954\n",
      "Epoch 126/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3202 - val_loss: -0.2998\n",
      "Epoch 127/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3207 - val_loss: -0.2984\n",
      "Epoch 128/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3219 - val_loss: -0.3000\n",
      "Epoch 129/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3218 - val_loss: -0.2987\n",
      "Epoch 130/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3217 - val_loss: -0.2995\n",
      "Epoch 131/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3219 - val_loss: -0.3007\n",
      "Epoch 132/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3234 - val_loss: -0.2958\n",
      "Epoch 133/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3220 - val_loss: -0.2977\n",
      "Epoch 134/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3231 - val_loss: -0.2967\n",
      "Epoch 135/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3241 - val_loss: -0.3042\n",
      "Epoch 136/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3245 - val_loss: -0.2918\n",
      "Epoch 137/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3243 - val_loss: -0.2997\n",
      "Epoch 138/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3234 - val_loss: -0.2990\n",
      "Epoch 139/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3248 - val_loss: -0.2985\n",
      "Epoch 140/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3249 - val_loss: -0.2980\n",
      "Epoch 141/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3257 - val_loss: -0.3006\n",
      "Epoch 142/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3256 - val_loss: -0.2928\n",
      "Epoch 143/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3257 - val_loss: -0.2976\n",
      "Epoch 144/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3273 - val_loss: -0.2968\n",
      "Epoch 145/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3259 - val_loss: -0.2951\n",
      "Epoch 146/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3278 - val_loss: -0.2948\n",
      "Epoch 147/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3278 - val_loss: -0.2990\n",
      "Epoch 148/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3289 - val_loss: -0.2928\n",
      "Epoch 149/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3277 - val_loss: -0.2963\n",
      "Epoch 150/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3285 - val_loss: -0.2897\n",
      "Epoch 151/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3286 - val_loss: -0.2932\n",
      "Epoch 152/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3298 - val_loss: -0.2952\n",
      "Epoch 153/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3303 - val_loss: -0.2955\n",
      "Epoch 154/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3309 - val_loss: -0.2987\n",
      "Epoch 155/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3299 - val_loss: -0.2980\n",
      "Epoch 156/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3314 - val_loss: -0.2950\n",
      "Epoch 157/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3315 - val_loss: -0.2952\n",
      "Epoch 158/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3313 - val_loss: -0.2929\n",
      "Epoch 159/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3334 - val_loss: -0.2900\n",
      "Epoch 160/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3322 - val_loss: -0.2951\n",
      "Epoch 161/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3331 - val_loss: -0.2916\n",
      "Epoch 162/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3334 - val_loss: -0.2933\n",
      "Epoch 163/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3335 - val_loss: -0.2921\n",
      "Epoch 164/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3364 - val_loss: -0.2954\n",
      "Epoch 165/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3329 - val_loss: -0.2928\n",
      "Epoch 166/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3362 - val_loss: -0.2917\n",
      "Epoch 167/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3365 - val_loss: -0.2946\n",
      "Epoch 168/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3358 - val_loss: -0.2905\n",
      "Epoch 169/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3368 - val_loss: -0.2903\n",
      "Epoch 170/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3374 - val_loss: -0.2935\n",
      "Epoch 171/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3373 - val_loss: -0.2903\n",
      "Epoch 172/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3371 - val_loss: -0.2906\n",
      "Epoch 173/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3396 - val_loss: -0.2849\n",
      "Epoch 174/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3388 - val_loss: -0.2864\n",
      "Epoch 175/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3394 - val_loss: -0.2892\n",
      "Epoch 176/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3403 - val_loss: -0.2907\n",
      "Epoch 177/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3404 - val_loss: -0.2868\n",
      "Epoch 178/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3408 - val_loss: -0.2903\n",
      "Epoch 179/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3408 - val_loss: -0.2808\n",
      "Epoch 180/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3423 - val_loss: -0.2852\n",
      "Epoch 181/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3425 - val_loss: -0.2886\n",
      "Epoch 182/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3432 - val_loss: -0.2888\n",
      "Epoch 183/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3431 - val_loss: -0.2872\n",
      "Epoch 184/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3444 - val_loss: -0.2809\n",
      "Epoch 185/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3453 - val_loss: -0.2788\n",
      "Epoch 186/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3458 - val_loss: -0.2778\n",
      "Epoch 187/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3458 - val_loss: -0.2857\n",
      "Epoch 188/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3451 - val_loss: -0.2823\n",
      "Epoch 189/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3464 - val_loss: -0.2851\n",
      "Epoch 190/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3480 - val_loss: -0.2836\n",
      "Epoch 191/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3479 - val_loss: -0.2872\n",
      "Epoch 192/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3485 - val_loss: -0.2837\n",
      "Epoch 193/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3485 - val_loss: -0.2819\n",
      "Epoch 194/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3493 - val_loss: -0.2822\n",
      "Epoch 195/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3512 - val_loss: -0.2821\n",
      "Epoch 196/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3504 - val_loss: -0.2816\n",
      "Epoch 197/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3519 - val_loss: -0.2836\n",
      "Epoch 198/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3531 - val_loss: -0.2802\n",
      "Epoch 199/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3530 - val_loss: -0.2767\n",
      "Epoch 200/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3523 - val_loss: -0.2813\n",
      "Epoch 201/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3536 - val_loss: -0.2779\n",
      "Epoch 202/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3544 - val_loss: -0.2768\n",
      "Epoch 203/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3554 - val_loss: -0.2765\n",
      "Epoch 204/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3566 - val_loss: -0.2751\n",
      "Epoch 205/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3563 - val_loss: -0.2762\n",
      "Epoch 206/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3571 - val_loss: -0.2762\n",
      "Epoch 207/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3574 - val_loss: -0.2643\n",
      "Epoch 208/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3566 - val_loss: -0.2688\n",
      "Epoch 209/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3589 - val_loss: -0.2701\n",
      "Epoch 210/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3595 - val_loss: -0.2730\n",
      "Epoch 211/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3595 - val_loss: -0.2741\n",
      "Epoch 212/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3609 - val_loss: -0.2701\n",
      "Epoch 213/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3611 - val_loss: -0.2664\n",
      "Epoch 214/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3615 - val_loss: -0.2638\n",
      "Epoch 215/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3629 - val_loss: -0.2716\n",
      "Epoch 216/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3628 - val_loss: -0.2706\n",
      "Epoch 217/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3640 - val_loss: -0.2659\n",
      "Epoch 218/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3639 - val_loss: -0.2714\n",
      "Epoch 219/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3654 - val_loss: -0.2673\n",
      "Epoch 220/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3653 - val_loss: -0.2683\n",
      "Epoch 221/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3653 - val_loss: -0.2619\n",
      "Epoch 222/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3670 - val_loss: -0.2607\n",
      "Epoch 223/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3672 - val_loss: -0.2692\n",
      "Epoch 224/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3672 - val_loss: -0.2623\n",
      "Epoch 225/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3694 - val_loss: -0.2655\n",
      "Epoch 226/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3708 - val_loss: -0.2558\n",
      "Epoch 227/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3713 - val_loss: -0.2568\n",
      "Epoch 228/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3703 - val_loss: -0.2551\n",
      "Epoch 229/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3709 - val_loss: -0.2563\n",
      "Epoch 230/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3708 - val_loss: -0.2592\n",
      "Epoch 231/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3716 - val_loss: -0.2572\n",
      "Epoch 232/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3735 - val_loss: -0.2523\n",
      "Epoch 233/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3740 - val_loss: -0.2605\n",
      "Epoch 234/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3754 - val_loss: -0.2562\n",
      "Epoch 235/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3763 - val_loss: -0.2626\n",
      "Epoch 236/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3769 - val_loss: -0.2552\n",
      "Epoch 237/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3781 - val_loss: -0.2514\n",
      "Epoch 238/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3767 - val_loss: -0.2557\n",
      "Epoch 239/10000\n",
      "297646/297646 [==============================] - 5s 17us/sample - loss: -0.3773 - val_loss: -0.2505\n",
      "Epoch 240/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3785 - val_loss: -0.2507\n",
      "Epoch 241/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3781 - val_loss: -0.2568\n",
      "Epoch 242/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3799 - val_loss: -0.2419\n",
      "Epoch 243/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3810 - val_loss: -0.2452\n",
      "Epoch 244/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3840 - val_loss: -0.2383\n",
      "Epoch 245/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3832 - val_loss: -0.2432\n",
      "Epoch 246/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3825 - val_loss: -0.2477\n",
      "Epoch 247/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3855 - val_loss: -0.2441\n",
      "Epoch 248/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3847 - val_loss: -0.2375\n",
      "Epoch 249/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3850 - val_loss: -0.2450\n",
      "Epoch 250/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3850 - val_loss: -0.2434\n",
      "Epoch 251/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3859 - val_loss: -0.2457\n",
      "Epoch 252/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3866 - val_loss: -0.2376\n",
      "Epoch 253/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3881 - val_loss: -0.2296\n",
      "Epoch 254/10000\n",
      "297646/297646 [==============================] - 4s 14us/sample - loss: -0.3880 - val_loss: -0.2432\n",
      "Epoch 255/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3894 - val_loss: -0.2294\n",
      "Epoch 256/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3910 - val_loss: -0.2334\n",
      "Epoch 257/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3918 - val_loss: -0.2356\n",
      "Epoch 258/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3922 - val_loss: -0.2297\n",
      "Epoch 259/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3923 - val_loss: -0.2401\n",
      "Epoch 260/10000\n",
      "297646/297646 [==============================] - 5s 17us/sample - loss: -0.3930 - val_loss: -0.2324\n",
      "Epoch 261/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3940 - val_loss: -0.2303\n",
      "Epoch 262/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3955 - val_loss: -0.2271\n",
      "Epoch 263/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3943 - val_loss: -0.2252\n",
      "Epoch 264/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.3954 - val_loss: -0.2293\n",
      "Epoch 265/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3963 - val_loss: -0.2329\n",
      "Epoch 266/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3980 - val_loss: -0.2236\n",
      "Epoch 267/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3968 - val_loss: -0.2246\n",
      "Epoch 268/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3988 - val_loss: -0.2281\n",
      "Epoch 269/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.3989 - val_loss: -0.2205\n",
      "Epoch 270/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.3997 - val_loss: -0.2228\n",
      "Epoch 271/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4004 - val_loss: -0.2228\n",
      "Epoch 272/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.4022 - val_loss: -0.2185\n",
      "Epoch 273/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4021 - val_loss: -0.2187\n",
      "Epoch 274/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4035 - val_loss: -0.2242\n",
      "Epoch 275/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.4029 - val_loss: -0.2200\n",
      "Epoch 276/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4062 - val_loss: -0.2167\n",
      "Epoch 277/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4039 - val_loss: -0.2188\n",
      "Epoch 278/10000\n",
      "297646/297646 [==============================] - 5s 15us/sample - loss: -0.4060 - val_loss: -0.2021\n",
      "Epoch 279/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4064 - val_loss: -0.2247\n",
      "Epoch 280/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.4089 - val_loss: -0.2051\n",
      "Epoch 281/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4098 - val_loss: -0.2019\n",
      "Epoch 282/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.4080 - val_loss: -0.2142\n",
      "Epoch 283/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4078 - val_loss: -0.2092\n",
      "Epoch 284/10000\n",
      "297646/297646 [==============================] - 5s 16us/sample - loss: -0.4104 - val_loss: -0.2135\n",
      "Epoch 285/10000\n",
      "297646/297646 [==============================] - 4s 15us/sample - loss: -0.4106 - val_loss: -0.1961\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1361.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7659, 12, 1)\n",
      "y_test.shape:  (7659, 1)\n",
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:48:39,532 WARNING Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1361.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1362.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6251, 12, 1)\n",
      "y_test.shape:  (6251, 1)\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:48:45,537 WARNING Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1362.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1363.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7635, 12, 1)\n",
      "y_test.shape:  (7635, 1)\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:48:50,857 WARNING Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1363.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1377.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7785, 12, 1)\n",
      "y_test.shape:  (7785, 1)\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:48:56,754 WARNING Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1377.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1381.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7749, 12, 1)\n",
      "y_test.shape:  (7749, 1)\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:02,895 WARNING Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1381.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1386.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7360, 12, 1)\n",
      "y_test.shape:  (7360, 1)\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:08,603 WARNING Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1386.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1408.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7348, 12, 1)\n",
      "y_test.shape:  (7348, 1)\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:14,582 WARNING Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1408.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1422.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7096, 12, 1)\n",
      "y_test.shape:  (7096, 1)\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:20,108 WARNING Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1422.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1427.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7290, 12, 1)\n",
      "y_test.shape:  (7290, 1)\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:25,727 WARNING Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1427.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1433.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7644, 12, 1)\n",
      "y_test.shape:  (7644, 1)\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:31,685 WARNING Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1433.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1435.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7022, 12, 1)\n",
      "y_test.shape:  (7022, 1)\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:37,514 WARNING Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1435.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1457.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7752, 12, 1)\n",
      "y_test.shape:  (7752, 1)\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:43,629 WARNING Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1457.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1459.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7535, 12, 1)\n",
      "y_test.shape:  (7535, 1)\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:49:49,730 WARNING Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1459.csv\n",
      "2025-01-21 16:49:54,635 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (308851, 12, 1)\n",
      "y_train.shape:  (308851, 1)\n",
      "x_valid.shape:  (77188, 12, 1)\n",
      "y_valid.shape:  (77188, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 16:50:39,776 WARNING Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 16:50:39,899 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 308851 samples, validate on 77188 samples\n",
      "Epoch 1/10000\n",
      "307200/308851 [============================>.] - ETA: 0s - loss: 0.1295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308851/308851 [==============================] - 8s 25us/sample - loss: 0.1286 - val_loss: -0.1380\n",
      "Epoch 2/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.0697 - val_loss: -0.1824\n",
      "Epoch 3/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.1140 - val_loss: -0.1897\n",
      "Epoch 4/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.1374 - val_loss: -0.1915\n",
      "Epoch 5/10000\n",
      "308851/308851 [==============================] - 6s 20us/sample - loss: -0.1570 - val_loss: -0.2298\n",
      "Epoch 6/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.1718 - val_loss: -0.2357\n",
      "Epoch 7/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.1847 - val_loss: -0.2377\n",
      "Epoch 8/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.1951 - val_loss: -0.2465\n",
      "Epoch 9/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2029 - val_loss: -0.2543\n",
      "Epoch 10/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2107 - val_loss: -0.2542\n",
      "Epoch 11/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2181 - val_loss: -0.2622\n",
      "Epoch 12/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2232 - val_loss: -0.2526\n",
      "Epoch 13/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2281 - val_loss: -0.2680\n",
      "Epoch 14/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2326 - val_loss: -0.2702\n",
      "Epoch 15/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2396 - val_loss: -0.2654\n",
      "Epoch 16/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2433 - val_loss: -0.2661\n",
      "Epoch 17/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2481 - val_loss: -0.2668\n",
      "Epoch 18/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2509 - val_loss: -0.2394\n",
      "Epoch 19/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2521 - val_loss: -0.2777\n",
      "Epoch 20/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2561 - val_loss: -0.2734\n",
      "Epoch 21/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2606 - val_loss: -0.2620\n",
      "Epoch 22/10000\n",
      "308851/308851 [==============================] - 4s 13us/sample - loss: -0.2607 - val_loss: -0.2759\n",
      "Epoch 23/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2624 - val_loss: -0.2794\n",
      "Epoch 24/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2663 - val_loss: -0.2652\n",
      "Epoch 25/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2665 - val_loss: -0.2734\n",
      "Epoch 26/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2703 - val_loss: -0.2781\n",
      "Epoch 27/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2713 - val_loss: -0.2760\n",
      "Epoch 28/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.2733 - val_loss: -0.2790\n",
      "Epoch 29/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2734 - val_loss: -0.2760\n",
      "Epoch 30/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2744 - val_loss: -0.2714\n",
      "Epoch 31/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2769 - val_loss: -0.2809\n",
      "Epoch 32/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2777 - val_loss: -0.2786\n",
      "Epoch 33/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2789 - val_loss: -0.2816\n",
      "Epoch 34/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2788 - val_loss: -0.2803\n",
      "Epoch 35/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2800 - val_loss: -0.2800\n",
      "Epoch 36/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2785 - val_loss: -0.2781\n",
      "Epoch 37/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2804 - val_loss: -0.2653\n",
      "Epoch 38/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2818 - val_loss: -0.2784\n",
      "Epoch 39/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2832 - val_loss: -0.2844\n",
      "Epoch 40/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2823 - val_loss: -0.2858\n",
      "Epoch 41/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2832 - val_loss: -0.2798\n",
      "Epoch 42/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2839 - val_loss: -0.2873\n",
      "Epoch 43/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2853 - val_loss: -0.2858\n",
      "Epoch 44/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.2849 - val_loss: -0.2838\n",
      "Epoch 45/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2853 - val_loss: -0.2823\n",
      "Epoch 46/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2855 - val_loss: -0.2852\n",
      "Epoch 47/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2854 - val_loss: -0.2811\n",
      "Epoch 48/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2860 - val_loss: -0.2852\n",
      "Epoch 49/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2872 - val_loss: -0.2838\n",
      "Epoch 50/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2877 - val_loss: -0.2835\n",
      "Epoch 51/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2870 - val_loss: -0.2853\n",
      "Epoch 52/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2884 - val_loss: -0.2853\n",
      "Epoch 53/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2880 - val_loss: -0.2864\n",
      "Epoch 54/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2891 - val_loss: -0.2801\n",
      "Epoch 55/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2899 - val_loss: -0.2864\n",
      "Epoch 56/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2896 - val_loss: -0.2894\n",
      "Epoch 57/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2909 - val_loss: -0.2816\n",
      "Epoch 58/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2899 - val_loss: -0.2880\n",
      "Epoch 59/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2900 - val_loss: -0.2885\n",
      "Epoch 60/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2915 - val_loss: -0.2857\n",
      "Epoch 61/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2911 - val_loss: -0.2892\n",
      "Epoch 62/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2925 - val_loss: -0.2811\n",
      "Epoch 63/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2923 - val_loss: -0.2832\n",
      "Epoch 64/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2920 - val_loss: -0.2885\n",
      "Epoch 65/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2918 - val_loss: -0.2821\n",
      "Epoch 66/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2931 - val_loss: -0.2884\n",
      "Epoch 67/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2925 - val_loss: -0.2889\n",
      "Epoch 68/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2942 - val_loss: -0.2868\n",
      "Epoch 69/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2932 - val_loss: -0.2716\n",
      "Epoch 70/10000\n",
      "308851/308851 [==============================] - 6s 19us/sample - loss: -0.2936 - val_loss: -0.2892\n",
      "Epoch 71/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2938 - val_loss: -0.2883\n",
      "Epoch 72/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.2932 - val_loss: -0.2916\n",
      "Epoch 73/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2942 - val_loss: -0.2863\n",
      "Epoch 74/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2943 - val_loss: -0.2858\n",
      "Epoch 75/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2953 - val_loss: -0.2821\n",
      "Epoch 76/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2946 - val_loss: -0.2881\n",
      "Epoch 77/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2945 - val_loss: -0.2873\n",
      "Epoch 78/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2970 - val_loss: -0.2779\n",
      "Epoch 79/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.2963 - val_loss: -0.2871\n",
      "Epoch 80/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.2970 - val_loss: -0.2817\n",
      "Epoch 81/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2949 - val_loss: -0.2911\n",
      "Epoch 82/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2969 - val_loss: -0.2888\n",
      "Epoch 83/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2965 - val_loss: -0.2891\n",
      "Epoch 84/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2967 - val_loss: -0.2856\n",
      "Epoch 85/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2981 - val_loss: -0.2900\n",
      "Epoch 86/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2967 - val_loss: -0.2889\n",
      "Epoch 87/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2963 - val_loss: -0.2886\n",
      "Epoch 88/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2983 - val_loss: -0.2913\n",
      "Epoch 89/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2993 - val_loss: -0.2876\n",
      "Epoch 90/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.2988 - val_loss: -0.2879\n",
      "Epoch 91/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3001 - val_loss: -0.2934\n",
      "Epoch 92/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3010 - val_loss: -0.2902\n",
      "Epoch 93/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3002 - val_loss: -0.2895\n",
      "Epoch 94/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2995 - val_loss: -0.2902\n",
      "Epoch 95/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.2995 - val_loss: -0.2887\n",
      "Epoch 96/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3009 - val_loss: -0.2899\n",
      "Epoch 97/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3007 - val_loss: -0.2929\n",
      "Epoch 98/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3008 - val_loss: -0.2871\n",
      "Epoch 99/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3012 - val_loss: -0.2890\n",
      "Epoch 100/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3019 - val_loss: -0.2854\n",
      "Epoch 101/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3016 - val_loss: -0.2871\n",
      "Epoch 102/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3021 - val_loss: -0.2858\n",
      "Epoch 103/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3019 - val_loss: -0.2841\n",
      "Epoch 104/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3034 - val_loss: -0.2860\n",
      "Epoch 105/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3032 - val_loss: -0.2884\n",
      "Epoch 106/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3022 - val_loss: -0.2915\n",
      "Epoch 107/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3032 - val_loss: -0.2854\n",
      "Epoch 108/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3038 - val_loss: -0.2876\n",
      "Epoch 109/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3034 - val_loss: -0.2913\n",
      "Epoch 110/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3050 - val_loss: -0.2932\n",
      "Epoch 111/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3051 - val_loss: -0.2893\n",
      "Epoch 112/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3049 - val_loss: -0.2910\n",
      "Epoch 113/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3050 - val_loss: -0.2864\n",
      "Epoch 114/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3033 - val_loss: -0.2908\n",
      "Epoch 115/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3076 - val_loss: -0.2905\n",
      "Epoch 116/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3059 - val_loss: -0.2883\n",
      "Epoch 117/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3060 - val_loss: -0.2881\n",
      "Epoch 118/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3069 - val_loss: -0.2845\n",
      "Epoch 119/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3068 - val_loss: -0.2912\n",
      "Epoch 120/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3069 - val_loss: -0.2910\n",
      "Epoch 121/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3076 - val_loss: -0.2867\n",
      "Epoch 122/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3072 - val_loss: -0.2797\n",
      "Epoch 123/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3075 - val_loss: -0.2895\n",
      "Epoch 124/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3081 - val_loss: -0.2893\n",
      "Epoch 125/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3094 - val_loss: -0.2881\n",
      "Epoch 126/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3098 - val_loss: -0.2891\n",
      "Epoch 127/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3079 - val_loss: -0.2899\n",
      "Epoch 128/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3089 - val_loss: -0.2888\n",
      "Epoch 129/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3103 - val_loss: -0.2909\n",
      "Epoch 130/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3096 - val_loss: -0.2860\n",
      "Epoch 131/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3100 - val_loss: -0.2885\n",
      "Epoch 132/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3109 - val_loss: -0.2877\n",
      "Epoch 133/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3106 - val_loss: -0.2856\n",
      "Epoch 134/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3121 - val_loss: -0.2846\n",
      "Epoch 135/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3119 - val_loss: -0.2804\n",
      "Epoch 136/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3111 - val_loss: -0.2834\n",
      "Epoch 137/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3130 - val_loss: -0.2863\n",
      "Epoch 138/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3120 - val_loss: -0.2855\n",
      "Epoch 139/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3128 - val_loss: -0.2849\n",
      "Epoch 140/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3129 - val_loss: -0.2875\n",
      "Epoch 141/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3134 - val_loss: -0.2865\n",
      "Epoch 142/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3150 - val_loss: -0.2851\n",
      "Epoch 143/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3144 - val_loss: -0.2858\n",
      "Epoch 144/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3133 - val_loss: -0.2810\n",
      "Epoch 145/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3140 - val_loss: -0.2880\n",
      "Epoch 146/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3150 - val_loss: -0.2843\n",
      "Epoch 147/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3150 - val_loss: -0.2871\n",
      "Epoch 148/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3159 - val_loss: -0.2824\n",
      "Epoch 149/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3173 - val_loss: -0.2835\n",
      "Epoch 150/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3155 - val_loss: -0.2857\n",
      "Epoch 151/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3173 - val_loss: -0.2865\n",
      "Epoch 152/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3172 - val_loss: -0.2846\n",
      "Epoch 153/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3187 - val_loss: -0.2821\n",
      "Epoch 154/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3176 - val_loss: -0.2834\n",
      "Epoch 155/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3181 - val_loss: -0.2846\n",
      "Epoch 156/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3186 - val_loss: -0.2852\n",
      "Epoch 157/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3185 - val_loss: -0.2821\n",
      "Epoch 158/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3196 - val_loss: -0.2831\n",
      "Epoch 159/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3199 - val_loss: -0.2885\n",
      "Epoch 160/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3217 - val_loss: -0.2827\n",
      "Epoch 161/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3204 - val_loss: -0.2828\n",
      "Epoch 162/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3212 - val_loss: -0.2839\n",
      "Epoch 163/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3213 - val_loss: -0.2847\n",
      "Epoch 164/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3227 - val_loss: -0.2830\n",
      "Epoch 165/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3229 - val_loss: -0.2751\n",
      "Epoch 166/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3227 - val_loss: -0.2830\n",
      "Epoch 167/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3215 - val_loss: -0.2791\n",
      "Epoch 168/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3247 - val_loss: -0.2788\n",
      "Epoch 169/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3240 - val_loss: -0.2829\n",
      "Epoch 170/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3243 - val_loss: -0.2800\n",
      "Epoch 171/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3245 - val_loss: -0.2808\n",
      "Epoch 172/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3238 - val_loss: -0.2818\n",
      "Epoch 173/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3259 - val_loss: -0.2779\n",
      "Epoch 174/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3247 - val_loss: -0.2816\n",
      "Epoch 175/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3266 - val_loss: -0.2783\n",
      "Epoch 176/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3267 - val_loss: -0.2739\n",
      "Epoch 177/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3263 - val_loss: -0.2751\n",
      "Epoch 178/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3280 - val_loss: -0.2763\n",
      "Epoch 179/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3277 - val_loss: -0.2752\n",
      "Epoch 180/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3277 - val_loss: -0.2769\n",
      "Epoch 181/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3298 - val_loss: -0.2781\n",
      "Epoch 182/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3291 - val_loss: -0.2727\n",
      "Epoch 183/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3314 - val_loss: -0.2766\n",
      "Epoch 184/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3305 - val_loss: -0.2706\n",
      "Epoch 185/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3294 - val_loss: -0.2768\n",
      "Epoch 186/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3320 - val_loss: -0.2754\n",
      "Epoch 187/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3324 - val_loss: -0.2742\n",
      "Epoch 188/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3324 - val_loss: -0.2741\n",
      "Epoch 189/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3329 - val_loss: -0.2702\n",
      "Epoch 190/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3328 - val_loss: -0.2544\n",
      "Epoch 191/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3334 - val_loss: -0.2684\n",
      "Epoch 192/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3349 - val_loss: -0.2732\n",
      "Epoch 193/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3342 - val_loss: -0.2715\n",
      "Epoch 194/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3341 - val_loss: -0.2680\n",
      "Epoch 195/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3366 - val_loss: -0.2675\n",
      "Epoch 196/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3366 - val_loss: -0.2741\n",
      "Epoch 197/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3368 - val_loss: -0.2716\n",
      "Epoch 198/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3372 - val_loss: -0.2661\n",
      "Epoch 199/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3386 - val_loss: -0.2635\n",
      "Epoch 200/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3395 - val_loss: -0.2688\n",
      "Epoch 201/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3386 - val_loss: -0.2693\n",
      "Epoch 202/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3401 - val_loss: -0.2703\n",
      "Epoch 203/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3402 - val_loss: -0.2721\n",
      "Epoch 204/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3412 - val_loss: -0.2714\n",
      "Epoch 205/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3420 - val_loss: -0.2685\n",
      "Epoch 206/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3418 - val_loss: -0.2670\n",
      "Epoch 207/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3419 - val_loss: -0.2678\n",
      "Epoch 208/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3431 - val_loss: -0.2679\n",
      "Epoch 209/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3443 - val_loss: -0.2624\n",
      "Epoch 210/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3449 - val_loss: -0.2630\n",
      "Epoch 211/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3455 - val_loss: -0.2631\n",
      "Epoch 212/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3466 - val_loss: -0.2632\n",
      "Epoch 213/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3455 - val_loss: -0.2620\n",
      "Epoch 214/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3466 - val_loss: -0.2584\n",
      "Epoch 215/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3467 - val_loss: -0.2661\n",
      "Epoch 216/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3466 - val_loss: -0.2591\n",
      "Epoch 217/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3489 - val_loss: -0.2584\n",
      "Epoch 218/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3487 - val_loss: -0.2626\n",
      "Epoch 219/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3491 - val_loss: -0.2673\n",
      "Epoch 220/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3495 - val_loss: -0.2604\n",
      "Epoch 221/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3502 - val_loss: -0.2542\n",
      "Epoch 222/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3523 - val_loss: -0.2499\n",
      "Epoch 223/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3520 - val_loss: -0.2636\n",
      "Epoch 224/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3525 - val_loss: -0.2584\n",
      "Epoch 225/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3537 - val_loss: -0.2553\n",
      "Epoch 226/10000\n",
      "308851/308851 [==============================] - 5s 17us/sample - loss: -0.3541 - val_loss: -0.2502\n",
      "Epoch 227/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3548 - val_loss: -0.2548\n",
      "Epoch 228/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3570 - val_loss: -0.2539\n",
      "Epoch 229/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3566 - val_loss: -0.2461\n",
      "Epoch 230/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3572 - val_loss: -0.2563\n",
      "Epoch 231/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3563 - val_loss: -0.2532\n",
      "Epoch 232/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3575 - val_loss: -0.2548\n",
      "Epoch 233/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3569 - val_loss: -0.2502\n",
      "Epoch 234/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3594 - val_loss: -0.2532\n",
      "Epoch 235/10000\n",
      "308851/308851 [==============================] - 5s 18us/sample - loss: -0.3603 - val_loss: -0.2526\n",
      "Epoch 236/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3608 - val_loss: -0.2508\n",
      "Epoch 237/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3605 - val_loss: -0.2409\n",
      "Epoch 238/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3633 - val_loss: -0.2482\n",
      "Epoch 239/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3621 - val_loss: -0.2477\n",
      "Epoch 240/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3630 - val_loss: -0.2505\n",
      "Epoch 241/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3635 - val_loss: -0.2390\n",
      "Epoch 242/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3629 - val_loss: -0.2433\n",
      "Epoch 243/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3649 - val_loss: -0.2422\n",
      "Epoch 244/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3655 - val_loss: -0.2432\n",
      "Epoch 245/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3659 - val_loss: -0.2402\n",
      "Epoch 246/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3680 - val_loss: -0.2409\n",
      "Epoch 247/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3683 - val_loss: -0.2438\n",
      "Epoch 248/10000\n",
      "308851/308851 [==============================] - 4s 14us/sample - loss: -0.3681 - val_loss: -0.2418\n",
      "Epoch 249/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3688 - val_loss: -0.2438\n",
      "Epoch 250/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3707 - val_loss: -0.2392\n",
      "Epoch 251/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3715 - val_loss: -0.2336\n",
      "Epoch 252/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3716 - val_loss: -0.2345\n",
      "Epoch 253/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3728 - val_loss: -0.2435\n",
      "Epoch 254/10000\n",
      "308851/308851 [==============================] - 4s 15us/sample - loss: -0.3723 - val_loss: -0.2347\n",
      "Epoch 255/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3741 - val_loss: -0.2378\n",
      "Epoch 256/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3740 - val_loss: -0.2434\n",
      "Epoch 257/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3745 - val_loss: -0.2254\n",
      "Epoch 258/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3749 - val_loss: -0.2381\n",
      "Epoch 259/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3768 - val_loss: -0.2413\n",
      "Epoch 260/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3769 - val_loss: -0.2404\n",
      "Epoch 261/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3793 - val_loss: -0.2366\n",
      "Epoch 262/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3786 - val_loss: -0.2303\n",
      "Epoch 263/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3797 - val_loss: -0.2339\n",
      "Epoch 264/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3798 - val_loss: -0.2345\n",
      "Epoch 265/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3790 - val_loss: -0.2298\n",
      "Epoch 266/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3811 - val_loss: -0.2364\n",
      "Epoch 267/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3814 - val_loss: -0.2328\n",
      "Epoch 268/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3817 - val_loss: -0.2357\n",
      "Epoch 269/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3841 - val_loss: -0.2278\n",
      "Epoch 270/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3844 - val_loss: -0.2187\n",
      "Epoch 271/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3859 - val_loss: -0.2297\n",
      "Epoch 272/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3848 - val_loss: -0.2214\n",
      "Epoch 273/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3869 - val_loss: -0.2261\n",
      "Epoch 274/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3864 - val_loss: -0.2237\n",
      "Epoch 275/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3872 - val_loss: -0.2115\n",
      "Epoch 276/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3870 - val_loss: -0.2136\n",
      "Epoch 277/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3892 - val_loss: -0.2215\n",
      "Epoch 278/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3894 - val_loss: -0.2224\n",
      "Epoch 279/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3913 - val_loss: -0.2138\n",
      "Epoch 280/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3911 - val_loss: -0.2167\n",
      "Epoch 281/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3917 - val_loss: -0.2182\n",
      "Epoch 282/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3924 - val_loss: -0.2170\n",
      "Epoch 283/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3919 - val_loss: -0.2183\n",
      "Epoch 284/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3954 - val_loss: -0.2142\n",
      "Epoch 285/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3933 - val_loss: -0.2130\n",
      "Epoch 286/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3965 - val_loss: -0.2009\n",
      "Epoch 287/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3937 - val_loss: -0.2108\n",
      "Epoch 288/10000\n",
      "308851/308851 [==============================] - 5s 16us/sample - loss: -0.3971 - val_loss: -0.1962\n",
      "Epoch 289/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3982 - val_loss: -0.2124\n",
      "Epoch 290/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.3985 - val_loss: -0.2148\n",
      "Epoch 291/10000\n",
      "308851/308851 [==============================] - 5s 15us/sample - loss: -0.4009 - val_loss: -0.2069\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5854, 12, 1)\n",
      "y_test.shape:  (5854, 1)\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:24,760 WARNING Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7813, 12, 1)\n",
      "y_test.shape:  (7813, 1)\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:31,080 WARNING Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7953, 12, 1)\n",
      "y_test.shape:  (7953, 1)\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:37,402 WARNING Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6694, 12, 1)\n",
      "y_test.shape:  (6694, 1)\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:44,064 WARNING Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7349, 12, 1)\n",
      "y_test.shape:  (7349, 1)\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:50,268 WARNING Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7644, 12, 1)\n",
      "y_test.shape:  (7644, 1)\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:13:56,553 WARNING Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7649, 12, 1)\n",
      "y_test.shape:  (7649, 1)\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:14:02,462 WARNING Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7844, 12, 1)\n",
      "y_test.shape:  (7844, 1)\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:14:10,087 WARNING Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7563, 12, 1)\n",
      "y_test.shape:  (7563, 1)\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:14:17,688 WARNING Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7897, 12, 1)\n",
      "y_test.shape:  (7897, 1)\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:14:25,395 WARNING Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_12sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 12, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 12, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7856, 12, 1)\n",
      "y_test.shape:  (7856, 1)\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:14:32,141 WARNING Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_12sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1726.csv\n",
      "2025-01-21 17:14:37,758 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "2025-01-21 17:14:37,759 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (295613, 18, 1)\n",
      "y_train.shape:  (295613, 1)\n",
      "x_valid.shape:  (73879, 18, 1)\n",
      "y_valid.shape:  (73879, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:15:22,278 WARNING Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 17:15:22,444 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 295613 samples, validate on 73879 samples\n",
      "Epoch 1/10000\n",
      "293888/295613 [============================>.] - ETA: 0s - loss: 0.1068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295613/295613 [==============================] - 10s 33us/sample - loss: 0.1060 - val_loss: -0.1380\n",
      "Epoch 2/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.0887 - val_loss: -0.1757\n",
      "Epoch 3/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.1259 - val_loss: -0.2199\n",
      "Epoch 4/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.1498 - val_loss: -0.2235\n",
      "Epoch 5/10000\n",
      "295613/295613 [==============================] - 8s 28us/sample - loss: -0.1690 - val_loss: -0.2228\n",
      "Epoch 6/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.1810 - val_loss: -0.2441\n",
      "Epoch 7/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.1949 - val_loss: -0.2587\n",
      "Epoch 8/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2061 - val_loss: -0.2675\n",
      "Epoch 9/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.2134 - val_loss: -0.2703\n",
      "Epoch 10/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2221 - val_loss: -0.2685\n",
      "Epoch 11/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2257 - val_loss: -0.2674\n",
      "Epoch 12/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2322 - val_loss: -0.2781\n",
      "Epoch 13/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.2395 - val_loss: -0.2747\n",
      "Epoch 14/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2430 - val_loss: -0.2860\n",
      "Epoch 15/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.2504 - val_loss: -0.2762\n",
      "Epoch 16/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2499 - val_loss: -0.2805\n",
      "Epoch 17/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2553 - val_loss: -0.2755\n",
      "Epoch 18/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.2605 - val_loss: -0.2768\n",
      "Epoch 19/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2651 - val_loss: -0.2875\n",
      "Epoch 20/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.2643 - val_loss: -0.2754\n",
      "Epoch 21/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.2689 - val_loss: -0.2877\n",
      "Epoch 22/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2719 - val_loss: -0.2872\n",
      "Epoch 23/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2741 - val_loss: -0.2885\n",
      "Epoch 24/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2755 - val_loss: -0.2890\n",
      "Epoch 25/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.2793 - val_loss: -0.2879\n",
      "Epoch 26/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.2789 - val_loss: -0.2780\n",
      "Epoch 27/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2821 - val_loss: -0.2856\n",
      "Epoch 28/10000\n",
      "295613/295613 [==============================] - 5s 17us/sample - loss: -0.2828 - val_loss: -0.2913\n",
      "Epoch 29/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.2852 - val_loss: -0.2890\n",
      "Epoch 30/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2865 - val_loss: -0.2880\n",
      "Epoch 31/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2879 - val_loss: -0.2947\n",
      "Epoch 32/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.2895 - val_loss: -0.2875\n",
      "Epoch 33/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.2889 - val_loss: -0.2839\n",
      "Epoch 34/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.2901 - val_loss: -0.2793\n",
      "Epoch 35/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2913 - val_loss: -0.2892\n",
      "Epoch 36/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2914 - val_loss: -0.2944\n",
      "Epoch 37/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2926 - val_loss: -0.2954\n",
      "Epoch 38/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2941 - val_loss: -0.2962\n",
      "Epoch 39/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2937 - val_loss: -0.2957\n",
      "Epoch 40/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2968 - val_loss: -0.2856\n",
      "Epoch 41/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2957 - val_loss: -0.2943\n",
      "Epoch 42/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2961 - val_loss: -0.2938\n",
      "Epoch 43/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2963 - val_loss: -0.2980\n",
      "Epoch 44/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.2974 - val_loss: -0.2964\n",
      "Epoch 45/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2987 - val_loss: -0.2953\n",
      "Epoch 46/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.2997 - val_loss: -0.2949\n",
      "Epoch 47/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3007 - val_loss: -0.2971\n",
      "Epoch 48/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.2986 - val_loss: -0.2983\n",
      "Epoch 49/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3008 - val_loss: -0.2971\n",
      "Epoch 50/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3021 - val_loss: -0.2974\n",
      "Epoch 51/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3022 - val_loss: -0.2950\n",
      "Epoch 52/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3033 - val_loss: -0.2939\n",
      "Epoch 53/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3024 - val_loss: -0.2955\n",
      "Epoch 54/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3026 - val_loss: -0.2922\n",
      "Epoch 55/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3036 - val_loss: -0.2941\n",
      "Epoch 56/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3047 - val_loss: -0.2978\n",
      "Epoch 57/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3053 - val_loss: -0.2865\n",
      "Epoch 58/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3052 - val_loss: -0.2898\n",
      "Epoch 59/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3067 - val_loss: -0.2907\n",
      "Epoch 60/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3074 - val_loss: -0.2897\n",
      "Epoch 61/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3070 - val_loss: -0.2934\n",
      "Epoch 62/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3082 - val_loss: -0.2914\n",
      "Epoch 63/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3083 - val_loss: -0.2988\n",
      "Epoch 64/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3073 - val_loss: -0.2982\n",
      "Epoch 65/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.3086 - val_loss: -0.2955\n",
      "Epoch 66/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.3095 - val_loss: -0.2976\n",
      "Epoch 67/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3092 - val_loss: -0.2946\n",
      "Epoch 68/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.3105 - val_loss: -0.2962\n",
      "Epoch 69/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3121 - val_loss: -0.2946\n",
      "Epoch 70/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3126 - val_loss: -0.2973\n",
      "Epoch 71/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3118 - val_loss: -0.2926\n",
      "Epoch 72/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3126 - val_loss: -0.2922\n",
      "Epoch 73/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3128 - val_loss: -0.2998\n",
      "Epoch 74/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3124 - val_loss: -0.2958\n",
      "Epoch 75/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3135 - val_loss: -0.2962\n",
      "Epoch 76/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3154 - val_loss: -0.2729\n",
      "Epoch 77/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3132 - val_loss: -0.2971\n",
      "Epoch 78/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3171 - val_loss: -0.2946\n",
      "Epoch 79/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3166 - val_loss: -0.2972\n",
      "Epoch 80/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3184 - val_loss: -0.2898\n",
      "Epoch 81/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3169 - val_loss: -0.2901\n",
      "Epoch 82/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3184 - val_loss: -0.2949\n",
      "Epoch 83/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3178 - val_loss: -0.2955\n",
      "Epoch 84/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3195 - val_loss: -0.2949\n",
      "Epoch 85/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3198 - val_loss: -0.2854\n",
      "Epoch 86/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3190 - val_loss: -0.2918\n",
      "Epoch 87/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3208 - val_loss: -0.2967\n",
      "Epoch 88/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3212 - val_loss: -0.2951\n",
      "Epoch 89/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3222 - val_loss: -0.2940\n",
      "Epoch 90/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3231 - val_loss: -0.2934\n",
      "Epoch 91/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3237 - val_loss: -0.2971\n",
      "Epoch 92/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3242 - val_loss: -0.2813\n",
      "Epoch 93/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3241 - val_loss: -0.2908\n",
      "Epoch 94/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3264 - val_loss: -0.2837\n",
      "Epoch 95/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3261 - val_loss: -0.2906\n",
      "Epoch 96/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.3262 - val_loss: -0.2877\n",
      "Epoch 97/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3283 - val_loss: -0.2801\n",
      "Epoch 98/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3270 - val_loss: -0.2856\n",
      "Epoch 99/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3299 - val_loss: -0.2887\n",
      "Epoch 100/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3298 - val_loss: -0.2897\n",
      "Epoch 101/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3306 - val_loss: -0.2905\n",
      "Epoch 102/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3315 - val_loss: -0.2900\n",
      "Epoch 103/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3323 - val_loss: -0.2883\n",
      "Epoch 104/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3322 - val_loss: -0.2800\n",
      "Epoch 105/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3333 - val_loss: -0.2874\n",
      "Epoch 106/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3351 - val_loss: -0.2885\n",
      "Epoch 107/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.3364 - val_loss: -0.2860\n",
      "Epoch 108/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.3376 - val_loss: -0.2837\n",
      "Epoch 109/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3375 - val_loss: -0.2836\n",
      "Epoch 110/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3394 - val_loss: -0.2833\n",
      "Epoch 111/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.3408 - val_loss: -0.2797\n",
      "Epoch 112/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3410 - val_loss: -0.2846\n",
      "Epoch 113/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3429 - val_loss: -0.2816\n",
      "Epoch 114/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3424 - val_loss: -0.2813\n",
      "Epoch 115/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3442 - val_loss: -0.2798\n",
      "Epoch 116/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3458 - val_loss: -0.2734\n",
      "Epoch 117/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3464 - val_loss: -0.2823\n",
      "Epoch 118/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3476 - val_loss: -0.2747\n",
      "Epoch 119/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3490 - val_loss: -0.2745\n",
      "Epoch 120/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3498 - val_loss: -0.2714\n",
      "Epoch 121/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3509 - val_loss: -0.2716\n",
      "Epoch 122/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3518 - val_loss: -0.2706\n",
      "Epoch 123/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.3526 - val_loss: -0.2668\n",
      "Epoch 124/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3539 - val_loss: -0.2717\n",
      "Epoch 125/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.3558 - val_loss: -0.2700\n",
      "Epoch 126/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3546 - val_loss: -0.2716\n",
      "Epoch 127/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3596 - val_loss: -0.2637\n",
      "Epoch 128/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.3594 - val_loss: -0.2690\n",
      "Epoch 129/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3592 - val_loss: -0.2654\n",
      "Epoch 130/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3625 - val_loss: -0.2569\n",
      "Epoch 131/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.3648 - val_loss: -0.2611\n",
      "Epoch 132/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3624 - val_loss: -0.2700\n",
      "Epoch 133/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3660 - val_loss: -0.2523\n",
      "Epoch 134/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3666 - val_loss: -0.2562\n",
      "Epoch 135/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3684 - val_loss: -0.2603\n",
      "Epoch 136/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3702 - val_loss: -0.2487\n",
      "Epoch 137/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3712 - val_loss: -0.2589\n",
      "Epoch 138/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3722 - val_loss: -0.2515\n",
      "Epoch 139/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3757 - val_loss: -0.2525\n",
      "Epoch 140/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3763 - val_loss: -0.2384\n",
      "Epoch 141/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3787 - val_loss: -0.2454\n",
      "Epoch 142/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3786 - val_loss: -0.2411\n",
      "Epoch 143/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3798 - val_loss: -0.2381\n",
      "Epoch 144/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3804 - val_loss: -0.2338\n",
      "Epoch 145/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3820 - val_loss: -0.2414\n",
      "Epoch 146/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3846 - val_loss: -0.2429\n",
      "Epoch 147/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3862 - val_loss: -0.2313\n",
      "Epoch 148/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.3873 - val_loss: -0.2320\n",
      "Epoch 149/10000\n",
      "295613/295613 [==============================] - 5s 19us/sample - loss: -0.3907 - val_loss: -0.2337\n",
      "Epoch 150/10000\n",
      "295613/295613 [==============================] - 5s 18us/sample - loss: -0.3916 - val_loss: -0.2286\n",
      "Epoch 151/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.3946 - val_loss: -0.2322\n",
      "Epoch 152/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3958 - val_loss: -0.2172\n",
      "Epoch 153/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.3967 - val_loss: -0.2254\n",
      "Epoch 154/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.3993 - val_loss: -0.2064\n",
      "Epoch 155/10000\n",
      "295613/295613 [==============================] - 8s 25us/sample - loss: -0.4004 - val_loss: -0.2059\n",
      "Epoch 156/10000\n",
      "295613/295613 [==============================] - 9s 29us/sample - loss: -0.4031 - val_loss: -0.2187\n",
      "Epoch 157/10000\n",
      "295613/295613 [==============================] - 8s 29us/sample - loss: -0.4034 - val_loss: -0.2113\n",
      "Epoch 158/10000\n",
      "295613/295613 [==============================] - 8s 25us/sample - loss: -0.4072 - val_loss: -0.2188\n",
      "Epoch 159/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4070 - val_loss: -0.2189\n",
      "Epoch 160/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4103 - val_loss: -0.2075\n",
      "Epoch 161/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4116 - val_loss: -0.1955\n",
      "Epoch 162/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4138 - val_loss: -0.2073\n",
      "Epoch 163/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4142 - val_loss: -0.1948\n",
      "Epoch 164/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4177 - val_loss: -0.1929\n",
      "Epoch 165/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4192 - val_loss: -0.1973\n",
      "Epoch 166/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4221 - val_loss: -0.1884\n",
      "Epoch 167/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4232 - val_loss: -0.1871\n",
      "Epoch 168/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4235 - val_loss: -0.1765\n",
      "Epoch 169/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4252 - val_loss: -0.1848\n",
      "Epoch 170/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4292 - val_loss: -0.1770\n",
      "Epoch 171/10000\n",
      "295613/295613 [==============================] - 8s 26us/sample - loss: -0.4302 - val_loss: -0.1684\n",
      "Epoch 172/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4307 - val_loss: -0.1600\n",
      "Epoch 173/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4359 - val_loss: -0.1555\n",
      "Epoch 174/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4373 - val_loss: -0.1706\n",
      "Epoch 175/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.4379 - val_loss: -0.1532\n",
      "Epoch 176/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.4385 - val_loss: -0.1592\n",
      "Epoch 177/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.4418 - val_loss: -0.1386\n",
      "Epoch 178/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4443 - val_loss: -0.1746\n",
      "Epoch 179/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4462 - val_loss: -0.1642\n",
      "Epoch 180/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4471 - val_loss: -0.1559\n",
      "Epoch 181/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4498 - val_loss: -0.1506\n",
      "Epoch 182/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4516 - val_loss: -0.1578\n",
      "Epoch 183/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.4546 - val_loss: -0.1365\n",
      "Epoch 184/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.4532 - val_loss: -0.1291\n",
      "Epoch 185/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4572 - val_loss: -0.1387\n",
      "Epoch 186/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4592 - val_loss: -0.1198\n",
      "Epoch 187/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4630 - val_loss: -0.1184\n",
      "Epoch 188/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4634 - val_loss: -0.1093\n",
      "Epoch 189/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4658 - val_loss: -0.1172\n",
      "Epoch 190/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4665 - val_loss: -0.1095\n",
      "Epoch 191/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4701 - val_loss: -0.1098\n",
      "Epoch 192/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4723 - val_loss: -0.1148\n",
      "Epoch 193/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4733 - val_loss: -0.0914\n",
      "Epoch 194/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.4752 - val_loss: -0.0734\n",
      "Epoch 195/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4756 - val_loss: -0.1022\n",
      "Epoch 196/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4792 - val_loss: -0.0927\n",
      "Epoch 197/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4838 - val_loss: -0.0946\n",
      "Epoch 198/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4837 - val_loss: -0.0828\n",
      "Epoch 199/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4869 - val_loss: -0.0715\n",
      "Epoch 200/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4877 - val_loss: -0.0611\n",
      "Epoch 201/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4888 - val_loss: -0.0681\n",
      "Epoch 202/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.4911 - val_loss: -0.0677\n",
      "Epoch 203/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.4927 - val_loss: -0.0743\n",
      "Epoch 204/10000\n",
      "295613/295613 [==============================] - 7s 24us/sample - loss: -0.4951 - val_loss: -0.0560\n",
      "Epoch 205/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.4970 - val_loss: -0.0530\n",
      "Epoch 206/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.4983 - val_loss: -0.0551\n",
      "Epoch 207/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5037 - val_loss: -0.0458\n",
      "Epoch 208/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5040 - val_loss: -0.0564\n",
      "Epoch 209/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5065 - val_loss: -0.0434\n",
      "Epoch 210/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5088 - val_loss: -0.0344\n",
      "Epoch 211/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5109 - val_loss: -0.0242\n",
      "Epoch 212/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5113 - val_loss: -0.0355\n",
      "Epoch 213/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5151 - val_loss: -0.0233\n",
      "Epoch 214/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5160 - val_loss: -0.0350\n",
      "Epoch 215/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5175 - val_loss: -0.0132\n",
      "Epoch 216/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.5188 - val_loss: -0.0262\n",
      "Epoch 217/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.5217 - val_loss: -0.0034\n",
      "Epoch 218/10000\n",
      "295613/295613 [==============================] - 7s 25us/sample - loss: -0.5224 - val_loss: -0.0133\n",
      "Epoch 219/10000\n",
      "295613/295613 [==============================] - 9s 29us/sample - loss: -0.5252 - val_loss: -8.3077e-04\n",
      "Epoch 220/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.5270 - val_loss: -0.0015\n",
      "Epoch 221/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5289 - val_loss: -0.0123\n",
      "Epoch 222/10000\n",
      "295613/295613 [==============================] - 6s 20us/sample - loss: -0.5295 - val_loss: 0.0218\n",
      "Epoch 223/10000\n",
      "295613/295613 [==============================] - 6s 19us/sample - loss: -0.5318 - val_loss: 0.0234\n",
      "Epoch 224/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5361 - val_loss: 0.0172\n",
      "Epoch 225/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5372 - val_loss: 0.0400\n",
      "Epoch 226/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5399 - val_loss: 0.0389\n",
      "Epoch 227/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5411 - val_loss: 0.0334\n",
      "Epoch 228/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5437 - val_loss: 0.0607\n",
      "Epoch 229/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5436 - val_loss: 0.0342\n",
      "Epoch 230/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5453 - val_loss: 0.0445\n",
      "Epoch 231/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5460 - val_loss: 0.0374\n",
      "Epoch 232/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5505 - val_loss: 0.0608\n",
      "Epoch 233/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5523 - val_loss: 0.0600\n",
      "Epoch 234/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5535 - val_loss: 0.0646\n",
      "Epoch 235/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.5565 - val_loss: 0.0952\n",
      "Epoch 236/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5576 - val_loss: 0.0652\n",
      "Epoch 237/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5591 - val_loss: 0.0661\n",
      "Epoch 238/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5595 - val_loss: 0.0626\n",
      "Epoch 239/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5619 - val_loss: 0.0882\n",
      "Epoch 240/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5638 - val_loss: 0.0809\n",
      "Epoch 241/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5670 - val_loss: 0.0870\n",
      "Epoch 242/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5672 - val_loss: 0.0797\n",
      "Epoch 243/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5699 - val_loss: 0.0931\n",
      "Epoch 244/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5725 - val_loss: 0.0837\n",
      "Epoch 245/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5719 - val_loss: 0.1024\n",
      "Epoch 246/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5771 - val_loss: 0.0918\n",
      "Epoch 247/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5752 - val_loss: 0.1014\n",
      "Epoch 248/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5783 - val_loss: 0.1087\n",
      "Epoch 249/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5801 - val_loss: 0.1383\n",
      "Epoch 250/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5831 - val_loss: 0.0987\n",
      "Epoch 251/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.5833 - val_loss: 0.1247\n",
      "Epoch 252/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5854 - val_loss: 0.1547\n",
      "Epoch 253/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5873 - val_loss: 0.1532\n",
      "Epoch 254/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5902 - val_loss: 0.1246\n",
      "Epoch 255/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5907 - val_loss: 0.1186\n",
      "Epoch 256/10000\n",
      "295613/295613 [==============================] - 6s 21us/sample - loss: -0.5915 - val_loss: 0.1670\n",
      "Epoch 257/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5929 - val_loss: 0.1636\n",
      "Epoch 258/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5955 - val_loss: 0.1643\n",
      "Epoch 259/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.5977 - val_loss: 0.1743\n",
      "Epoch 260/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5989 - val_loss: 0.2065\n",
      "Epoch 261/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.5990 - val_loss: 0.1488\n",
      "Epoch 262/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.6038 - val_loss: 0.1964\n",
      "Epoch 263/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.6037 - val_loss: 0.2001\n",
      "Epoch 264/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.6058 - val_loss: 0.2205\n",
      "Epoch 265/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.6079 - val_loss: 0.1981\n",
      "Epoch 266/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.6080 - val_loss: 0.2108\n",
      "Epoch 267/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.6119 - val_loss: 0.2135\n",
      "Epoch 268/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.6113 - val_loss: 0.1790\n",
      "Epoch 269/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.6129 - val_loss: 0.2260\n",
      "Epoch 270/10000\n",
      "295613/295613 [==============================] - 6s 22us/sample - loss: -0.6148 - val_loss: 0.2057\n",
      "Epoch 271/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.6178 - val_loss: 0.2231\n",
      "Epoch 272/10000\n",
      "295613/295613 [==============================] - 7s 23us/sample - loss: -0.6191 - val_loss: 0.2093\n",
      "Epoch 273/10000\n",
      "295613/295613 [==============================] - 7s 22us/sample - loss: -0.6198 - val_loss: 0.2258\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4541, 18, 1)\n",
      "y_test.shape:  (4541, 1)\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:16,852 WARNING Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7586, 18, 1)\n",
      "y_test.shape:  (7586, 1)\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:23,434 WARNING Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7859, 18, 1)\n",
      "y_test.shape:  (7859, 1)\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:31,543 WARNING Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7934, 18, 1)\n",
      "y_test.shape:  (7934, 1)\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:39,772 WARNING Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7408, 18, 1)\n",
      "y_test.shape:  (7408, 1)\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:47,649 WARNING Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7773, 18, 1)\n",
      "y_test.shape:  (7773, 1)\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:43:55,369 WARNING Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7686, 18, 1)\n",
      "y_test.shape:  (7686, 1)\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:03,384 WARNING Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7787, 18, 1)\n",
      "y_test.shape:  (7787, 1)\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:11,148 WARNING Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7896, 18, 1)\n",
      "y_test.shape:  (7896, 1)\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:18,706 WARNING Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7711, 18, 1)\n",
      "y_test.shape:  (7711, 1)\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:26,367 WARNING Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4295, 18, 1)\n",
      "y_test.shape:  (4295, 1)\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:33,882 WARNING Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7528, 18, 1)\n",
      "y_test.shape:  (7528, 1)\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:40,333 WARNING Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7222, 18, 1)\n",
      "y_test.shape:  (7222, 1)\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:44:47,680 WARNING Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  25.csv\n",
      "2025-01-21 17:44:54,131 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (294832, 18, 1)\n",
      "y_train.shape:  (294832, 1)\n",
      "x_valid.shape:  (73684, 18, 1)\n",
      "y_valid.shape:  (73684, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 17:45:38,133 WARNING Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 17:45:38,278 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 294832 samples, validate on 73684 samples\n",
      "Epoch 1/10000\n",
      "292864/294832 [============================>.] - ETA: 0s - loss: 0.1708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294832/294832 [==============================] - 10s 36us/sample - loss: 0.1694 - val_loss: -0.1303\n",
      "Epoch 2/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.0533 - val_loss: -0.1788\n",
      "Epoch 3/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.0975 - val_loss: -0.1830\n",
      "Epoch 4/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.1297 - val_loss: -0.1861\n",
      "Epoch 5/10000\n",
      "294832/294832 [==============================] - 9s 31us/sample - loss: -0.1443 - val_loss: -0.2050\n",
      "Epoch 6/10000\n",
      "294832/294832 [==============================] - 6s 19us/sample - loss: -0.1608 - val_loss: -0.2254\n",
      "Epoch 7/10000\n",
      "294832/294832 [==============================] - 5s 18us/sample - loss: -0.1760 - val_loss: -0.2405\n",
      "Epoch 8/10000\n",
      "294832/294832 [==============================] - 5s 18us/sample - loss: -0.1855 - val_loss: -0.2350\n",
      "Epoch 9/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.1963 - val_loss: -0.2459\n",
      "Epoch 10/10000\n",
      "294832/294832 [==============================] - 6s 19us/sample - loss: -0.2050 - val_loss: -0.2571\n",
      "Epoch 11/10000\n",
      "294832/294832 [==============================] - 5s 19us/sample - loss: -0.2128 - val_loss: -0.2621\n",
      "Epoch 12/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.2200 - val_loss: -0.2575\n",
      "Epoch 13/10000\n",
      "294832/294832 [==============================] - 9s 30us/sample - loss: -0.2215 - val_loss: -0.2624\n",
      "Epoch 14/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2279 - val_loss: -0.2614\n",
      "Epoch 15/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2322 - val_loss: -0.2604\n",
      "Epoch 16/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.2373 - val_loss: -0.2732\n",
      "Epoch 17/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2407 - val_loss: -0.2738\n",
      "Epoch 18/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2439 - val_loss: -0.2635\n",
      "Epoch 19/10000\n",
      "294832/294832 [==============================] - 6s 19us/sample - loss: -0.2464 - val_loss: -0.2678\n",
      "Epoch 20/10000\n",
      "294832/294832 [==============================] - 6s 19us/sample - loss: -0.2512 - val_loss: -0.2588\n",
      "Epoch 21/10000\n",
      "294832/294832 [==============================] - 6s 19us/sample - loss: -0.2534 - val_loss: -0.2793\n",
      "Epoch 22/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2558 - val_loss: -0.2645\n",
      "Epoch 23/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2581 - val_loss: -0.2774\n",
      "Epoch 24/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2617 - val_loss: -0.2764\n",
      "Epoch 25/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2626 - val_loss: -0.2716\n",
      "Epoch 26/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2634 - val_loss: -0.2765\n",
      "Epoch 27/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2659 - val_loss: -0.2761\n",
      "Epoch 28/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2668 - val_loss: -0.2670\n",
      "Epoch 29/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2683 - val_loss: -0.2765\n",
      "Epoch 30/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2715 - val_loss: -0.2793\n",
      "Epoch 31/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2702 - val_loss: -0.2799\n",
      "Epoch 32/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2728 - val_loss: -0.2773\n",
      "Epoch 33/10000\n",
      "294832/294832 [==============================] - 7s 25us/sample - loss: -0.2722 - val_loss: -0.2810\n",
      "Epoch 34/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.2730 - val_loss: -0.2653\n",
      "Epoch 35/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2750 - val_loss: -0.2821\n",
      "Epoch 36/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2759 - val_loss: -0.2766\n",
      "Epoch 37/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2780 - val_loss: -0.2832\n",
      "Epoch 38/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2780 - val_loss: -0.2761\n",
      "Epoch 39/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2783 - val_loss: -0.2799\n",
      "Epoch 40/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2794 - val_loss: -0.2774\n",
      "Epoch 41/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2801 - val_loss: -0.2796\n",
      "Epoch 42/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2812 - val_loss: -0.2832\n",
      "Epoch 43/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2825 - val_loss: -0.2825\n",
      "Epoch 44/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2809 - val_loss: -0.2776\n",
      "Epoch 45/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2826 - val_loss: -0.2824\n",
      "Epoch 46/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2830 - val_loss: -0.2721\n",
      "Epoch 47/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2849 - val_loss: -0.2753\n",
      "Epoch 48/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2850 - val_loss: -0.2806\n",
      "Epoch 49/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2863 - val_loss: -0.2844\n",
      "Epoch 50/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2871 - val_loss: -0.2774\n",
      "Epoch 51/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2872 - val_loss: -0.2843\n",
      "Epoch 52/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2866 - val_loss: -0.2836\n",
      "Epoch 53/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2877 - val_loss: -0.2808\n",
      "Epoch 54/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2899 - val_loss: -0.2815\n",
      "Epoch 55/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2889 - val_loss: -0.2837\n",
      "Epoch 56/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2905 - val_loss: -0.2812\n",
      "Epoch 57/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2895 - val_loss: -0.2864\n",
      "Epoch 58/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2910 - val_loss: -0.2770\n",
      "Epoch 59/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.2907 - val_loss: -0.2849\n",
      "Epoch 60/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2906 - val_loss: -0.2837\n",
      "Epoch 61/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2936 - val_loss: -0.2855\n",
      "Epoch 62/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2951 - val_loss: -0.2839\n",
      "Epoch 63/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2944 - val_loss: -0.2795\n",
      "Epoch 64/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2945 - val_loss: -0.2843\n",
      "Epoch 65/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.2948 - val_loss: -0.2806\n",
      "Epoch 66/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2958 - val_loss: -0.2833\n",
      "Epoch 67/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.2959 - val_loss: -0.2855\n",
      "Epoch 68/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2960 - val_loss: -0.2850\n",
      "Epoch 69/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2995 - val_loss: -0.2818\n",
      "Epoch 70/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.2981 - val_loss: -0.2838\n",
      "Epoch 71/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.2988 - val_loss: -0.2871\n",
      "Epoch 72/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3001 - val_loss: -0.2846\n",
      "Epoch 73/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3020 - val_loss: -0.2826\n",
      "Epoch 74/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3020 - val_loss: -0.2838\n",
      "Epoch 75/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3029 - val_loss: -0.2824\n",
      "Epoch 76/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3027 - val_loss: -0.2858\n",
      "Epoch 77/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3024 - val_loss: -0.2804\n",
      "Epoch 78/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3050 - val_loss: -0.2815\n",
      "Epoch 79/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3063 - val_loss: -0.2815\n",
      "Epoch 80/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.3066 - val_loss: -0.2793\n",
      "Epoch 81/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3061 - val_loss: -0.2761\n",
      "Epoch 82/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3083 - val_loss: -0.2794\n",
      "Epoch 83/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3087 - val_loss: -0.2787\n",
      "Epoch 84/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3094 - val_loss: -0.2796\n",
      "Epoch 85/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3111 - val_loss: -0.2811\n",
      "Epoch 86/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3116 - val_loss: -0.2731\n",
      "Epoch 87/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3113 - val_loss: -0.2701\n",
      "Epoch 88/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3118 - val_loss: -0.2749\n",
      "Epoch 89/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3163 - val_loss: -0.2682\n",
      "Epoch 90/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3151 - val_loss: -0.2747\n",
      "Epoch 91/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3170 - val_loss: -0.2751\n",
      "Epoch 92/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.3162 - val_loss: -0.2736\n",
      "Epoch 93/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3170 - val_loss: -0.2742\n",
      "Epoch 94/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3192 - val_loss: -0.2696\n",
      "Epoch 95/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.3195 - val_loss: -0.2654\n",
      "Epoch 96/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.3216 - val_loss: -0.2726\n",
      "Epoch 97/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3224 - val_loss: -0.2674\n",
      "Epoch 98/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3239 - val_loss: -0.2674\n",
      "Epoch 99/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3247 - val_loss: -0.2632\n",
      "Epoch 100/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3264 - val_loss: -0.2639\n",
      "Epoch 101/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3271 - val_loss: -0.2653\n",
      "Epoch 102/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3277 - val_loss: -0.2612\n",
      "Epoch 103/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3293 - val_loss: -0.2620\n",
      "Epoch 104/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3310 - val_loss: -0.2621\n",
      "Epoch 105/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3306 - val_loss: -0.2612\n",
      "Epoch 106/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3321 - val_loss: -0.2573\n",
      "Epoch 107/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3359 - val_loss: -0.2552\n",
      "Epoch 108/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3348 - val_loss: -0.2641\n",
      "Epoch 109/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3369 - val_loss: -0.2603\n",
      "Epoch 110/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3393 - val_loss: -0.2523\n",
      "Epoch 111/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3405 - val_loss: -0.2495\n",
      "Epoch 112/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3418 - val_loss: -0.2570\n",
      "Epoch 113/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3429 - val_loss: -0.2508\n",
      "Epoch 114/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3445 - val_loss: -0.2486\n",
      "Epoch 115/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3472 - val_loss: -0.2506\n",
      "Epoch 116/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3477 - val_loss: -0.2440\n",
      "Epoch 117/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3492 - val_loss: -0.2386\n",
      "Epoch 118/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3502 - val_loss: -0.2450\n",
      "Epoch 119/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3527 - val_loss: -0.2465\n",
      "Epoch 120/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3538 - val_loss: -0.2417\n",
      "Epoch 121/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3565 - val_loss: -0.2216\n",
      "Epoch 122/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3560 - val_loss: -0.2333\n",
      "Epoch 123/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3581 - val_loss: -0.2246\n",
      "Epoch 124/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.3610 - val_loss: -0.2288\n",
      "Epoch 125/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3632 - val_loss: -0.2216\n",
      "Epoch 126/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3635 - val_loss: -0.2273\n",
      "Epoch 127/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3662 - val_loss: -0.2285\n",
      "Epoch 128/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3663 - val_loss: -0.2205\n",
      "Epoch 129/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3689 - val_loss: -0.2160\n",
      "Epoch 130/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3717 - val_loss: -0.2183\n",
      "Epoch 131/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3726 - val_loss: -0.2082\n",
      "Epoch 132/10000\n",
      "294832/294832 [==============================] - 6s 20us/sample - loss: -0.3755 - val_loss: -0.2007\n",
      "Epoch 133/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3764 - val_loss: -0.2079\n",
      "Epoch 134/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3777 - val_loss: -0.2051\n",
      "Epoch 135/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3802 - val_loss: -0.2153\n",
      "Epoch 136/10000\n",
      "294832/294832 [==============================] - 8s 26us/sample - loss: -0.3824 - val_loss: -0.1973\n",
      "Epoch 137/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3833 - val_loss: -0.1957\n",
      "Epoch 138/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3862 - val_loss: -0.1816\n",
      "Epoch 139/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3867 - val_loss: -0.1755\n",
      "Epoch 140/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3898 - val_loss: -0.1757\n",
      "Epoch 141/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.3916 - val_loss: -0.1848\n",
      "Epoch 142/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.3919 - val_loss: -0.1943\n",
      "Epoch 143/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3953 - val_loss: -0.1784\n",
      "Epoch 144/10000\n",
      "294832/294832 [==============================] - 6s 21us/sample - loss: -0.3989 - val_loss: -0.1833\n",
      "Epoch 145/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4003 - val_loss: -0.1743\n",
      "Epoch 146/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4016 - val_loss: -0.1769\n",
      "Epoch 147/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4027 - val_loss: -0.1744\n",
      "Epoch 148/10000\n",
      "294832/294832 [==============================] - 8s 28us/sample - loss: -0.4094 - val_loss: -0.1654\n",
      "Epoch 149/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4067 - val_loss: -0.1601\n",
      "Epoch 150/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4104 - val_loss: -0.1445\n",
      "Epoch 151/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4109 - val_loss: -0.1490\n",
      "Epoch 152/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4134 - val_loss: -0.1613\n",
      "Epoch 153/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4157 - val_loss: -0.1416\n",
      "Epoch 154/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4190 - val_loss: -0.1455\n",
      "Epoch 155/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4199 - val_loss: -0.1483\n",
      "Epoch 156/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4206 - val_loss: -0.1381\n",
      "Epoch 157/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4241 - val_loss: -0.1281\n",
      "Epoch 158/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4269 - val_loss: -0.1428\n",
      "Epoch 159/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4281 - val_loss: -0.1116\n",
      "Epoch 160/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4291 - val_loss: -0.1366\n",
      "Epoch 161/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4323 - val_loss: -0.1213\n",
      "Epoch 162/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4362 - val_loss: -0.1161\n",
      "Epoch 163/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4366 - val_loss: -0.1343\n",
      "Epoch 164/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4372 - val_loss: -0.0933\n",
      "Epoch 165/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4407 - val_loss: -0.1241\n",
      "Epoch 166/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4423 - val_loss: -0.0935\n",
      "Epoch 167/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4442 - val_loss: -0.0887\n",
      "Epoch 168/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4481 - val_loss: -0.0983\n",
      "Epoch 169/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4477 - val_loss: -0.0932\n",
      "Epoch 170/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4525 - val_loss: -0.0860\n",
      "Epoch 171/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4543 - val_loss: -0.0804\n",
      "Epoch 172/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4559 - val_loss: -0.0690\n",
      "Epoch 173/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4576 - val_loss: -0.0669\n",
      "Epoch 174/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4621 - val_loss: -0.0605\n",
      "Epoch 175/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4628 - val_loss: -0.0864\n",
      "Epoch 176/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4646 - val_loss: -0.0679\n",
      "Epoch 177/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4668 - val_loss: -0.0701\n",
      "Epoch 178/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.4691 - val_loss: -0.0470\n",
      "Epoch 179/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4715 - val_loss: -0.0486\n",
      "Epoch 180/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4717 - val_loss: -0.0363\n",
      "Epoch 181/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4751 - val_loss: -0.0398\n",
      "Epoch 182/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4774 - val_loss: -0.0352\n",
      "Epoch 183/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4801 - val_loss: -0.0163\n",
      "Epoch 184/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4790 - val_loss: -0.0301\n",
      "Epoch 185/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4844 - val_loss: -0.0162\n",
      "Epoch 186/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4847 - val_loss: -0.0169\n",
      "Epoch 187/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4871 - val_loss: -0.0066\n",
      "Epoch 188/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.4894 - val_loss: -0.0157\n",
      "Epoch 189/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.4933 - val_loss: -0.0093\n",
      "Epoch 190/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4938 - val_loss: -0.0085\n",
      "Epoch 191/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.4969 - val_loss: -0.0092\n",
      "Epoch 192/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.4980 - val_loss: 0.0116\n",
      "Epoch 193/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5008 - val_loss: -0.0050\n",
      "Epoch 194/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5025 - val_loss: 0.0180\n",
      "Epoch 195/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5041 - val_loss: 0.0180\n",
      "Epoch 196/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5042 - val_loss: 0.0198\n",
      "Epoch 197/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5098 - val_loss: 0.0039\n",
      "Epoch 198/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5097 - val_loss: 0.0407\n",
      "Epoch 199/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5125 - val_loss: 0.0457\n",
      "Epoch 200/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5141 - val_loss: 0.0310\n",
      "Epoch 201/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5162 - val_loss: 0.0255\n",
      "Epoch 202/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5194 - val_loss: 0.0730\n",
      "Epoch 203/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5213 - val_loss: 0.0803\n",
      "Epoch 204/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5210 - val_loss: 0.0514\n",
      "Epoch 205/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.5238 - val_loss: 0.0685\n",
      "Epoch 206/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5253 - val_loss: 0.0684\n",
      "Epoch 207/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5283 - val_loss: 0.0512\n",
      "Epoch 208/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5309 - val_loss: 0.0847\n",
      "Epoch 209/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5323 - val_loss: 0.0789\n",
      "Epoch 210/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5340 - val_loss: 0.0852\n",
      "Epoch 211/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5371 - val_loss: 0.0796\n",
      "Epoch 212/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5397 - val_loss: 0.0982\n",
      "Epoch 213/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5385 - val_loss: 0.0928\n",
      "Epoch 214/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5417 - val_loss: 0.0850\n",
      "Epoch 215/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5436 - val_loss: 0.1063\n",
      "Epoch 216/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5463 - val_loss: 0.1261\n",
      "Epoch 217/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5493 - val_loss: 0.1277\n",
      "Epoch 218/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5496 - val_loss: 0.1061\n",
      "Epoch 219/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5523 - val_loss: 0.1315\n",
      "Epoch 220/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5533 - val_loss: 0.1349\n",
      "Epoch 221/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5568 - val_loss: 0.1350\n",
      "Epoch 222/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.5589 - val_loss: 0.0965\n",
      "Epoch 223/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5609 - val_loss: 0.1521\n",
      "Epoch 224/10000\n",
      "294832/294832 [==============================] - 6s 22us/sample - loss: -0.5615 - val_loss: 0.1542\n",
      "Epoch 225/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5639 - val_loss: 0.1445\n",
      "Epoch 226/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5638 - val_loss: 0.1676\n",
      "Epoch 227/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5658 - val_loss: 0.1692\n",
      "Epoch 228/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.5689 - val_loss: 0.1601\n",
      "Epoch 229/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5692 - val_loss: 0.1444\n",
      "Epoch 230/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5718 - val_loss: 0.1778\n",
      "Epoch 231/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5734 - val_loss: 0.1834\n",
      "Epoch 232/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5745 - val_loss: 0.1401\n",
      "Epoch 233/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5784 - val_loss: 0.1935\n",
      "Epoch 234/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.5777 - val_loss: 0.2013\n",
      "Epoch 235/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5810 - val_loss: 0.2347\n",
      "Epoch 236/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5833 - val_loss: 0.2058\n",
      "Epoch 237/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5856 - val_loss: 0.1966\n",
      "Epoch 238/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5876 - val_loss: 0.2289\n",
      "Epoch 239/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5849 - val_loss: 0.2069\n",
      "Epoch 240/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5912 - val_loss: 0.1921\n",
      "Epoch 241/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5920 - val_loss: 0.2423\n",
      "Epoch 242/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5952 - val_loss: 0.2230\n",
      "Epoch 243/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5971 - val_loss: 0.2351\n",
      "Epoch 244/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.5967 - val_loss: 0.2361\n",
      "Epoch 245/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6004 - val_loss: 0.2350\n",
      "Epoch 246/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6029 - val_loss: 0.2612\n",
      "Epoch 247/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6034 - val_loss: 0.2703\n",
      "Epoch 248/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6051 - val_loss: 0.2389\n",
      "Epoch 249/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6047 - val_loss: 0.2401\n",
      "Epoch 250/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6068 - val_loss: 0.2504\n",
      "Epoch 251/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6112 - val_loss: 0.2805\n",
      "Epoch 252/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6105 - val_loss: 0.2726\n",
      "Epoch 253/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6116 - val_loss: 0.2596\n",
      "Epoch 254/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6148 - val_loss: 0.2783\n",
      "Epoch 255/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6163 - val_loss: 0.3142\n",
      "Epoch 256/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6166 - val_loss: 0.2729\n",
      "Epoch 257/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6165 - val_loss: 0.2487\n",
      "Epoch 258/10000\n",
      "294832/294832 [==============================] - 7s 22us/sample - loss: -0.6209 - val_loss: 0.3064\n",
      "Epoch 259/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6226 - val_loss: 0.3221\n",
      "Epoch 260/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6237 - val_loss: 0.2918\n",
      "Epoch 261/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.6255 - val_loss: 0.3274\n",
      "Epoch 262/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6268 - val_loss: 0.2905\n",
      "Epoch 263/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6282 - val_loss: 0.3554\n",
      "Epoch 264/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6317 - val_loss: 0.3116\n",
      "Epoch 265/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6304 - val_loss: 0.3337\n",
      "Epoch 266/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6351 - val_loss: 0.3326\n",
      "Epoch 267/10000\n",
      "294832/294832 [==============================] - 7s 24us/sample - loss: -0.6365 - val_loss: 0.3189\n",
      "Epoch 268/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6367 - val_loss: 0.3648\n",
      "Epoch 269/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6380 - val_loss: 0.3479\n",
      "Epoch 270/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6401 - val_loss: 0.3800\n",
      "Epoch 271/10000\n",
      "294832/294832 [==============================] - 7s 23us/sample - loss: -0.6413 - val_loss: 0.3627\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1010.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7425, 18, 1)\n",
      "y_test.shape:  (7425, 1)\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:10,153 WARNING Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1010.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1015.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5288, 18, 1)\n",
      "y_test.shape:  (5288, 1)\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:18,383 WARNING Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1015.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1043.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7706, 18, 1)\n",
      "y_test.shape:  (7706, 1)\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:25,854 WARNING Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1043.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1082.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7818, 18, 1)\n",
      "y_test.shape:  (7818, 1)\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:34,277 WARNING Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1082.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7719, 18, 1)\n",
      "y_test.shape:  (7719, 1)\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:42,708 WARNING Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1121.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7820, 18, 1)\n",
      "y_test.shape:  (7820, 1)\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:51,482 WARNING Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1121.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1127.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7031, 18, 1)\n",
      "y_test.shape:  (7031, 1)\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:15:59,749 WARNING Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1127.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1139.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5054, 18, 1)\n",
      "y_test.shape:  (5054, 1)\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:07,521 WARNING Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1139.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1143.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7536, 18, 1)\n",
      "y_test.shape:  (7536, 1)\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:14,624 WARNING Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1143.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1171.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7686, 18, 1)\n",
      "y_test.shape:  (7686, 1)\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:22,708 WARNING Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1171.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1194.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7923, 18, 1)\n",
      "y_test.shape:  (7923, 1)\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:30,782 WARNING Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1194.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1201.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7833, 18, 1)\n",
      "y_test.shape:  (7833, 1)\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:38,871 WARNING Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1201.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\252.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7363, 18, 1)\n",
      "y_test.shape:  (7363, 1)\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:16:46,855 WARNING Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  252.csv\n",
      "2025-01-21 18:16:53,857 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (291017, 18, 1)\n",
      "y_train.shape:  (291017, 1)\n",
      "x_valid.shape:  (72730, 18, 1)\n",
      "y_valid.shape:  (72730, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:17:37,504 WARNING Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 18:17:37,643 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 291017 samples, validate on 72730 samples\n",
      "Epoch 1/10000\n",
      "290816/291017 [============================>.] - ETA: 0s - loss: 0.1584"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291017/291017 [==============================] - 11s 37us/sample - loss: 0.1582 - val_loss: -0.1256\n",
      "Epoch 2/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.0634 - val_loss: -0.2015\n",
      "Epoch 3/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.1075 - val_loss: -0.2221\n",
      "Epoch 4/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.1338 - val_loss: -0.1385\n",
      "Epoch 5/10000\n",
      "291017/291017 [==============================] - 9s 30us/sample - loss: -0.1549 - val_loss: -0.2049\n",
      "Epoch 6/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.1702 - val_loss: -0.2417\n",
      "Epoch 7/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.1856 - val_loss: -0.2471\n",
      "Epoch 8/10000\n",
      "291017/291017 [==============================] - 6s 20us/sample - loss: -0.1961 - val_loss: -0.2696\n",
      "Epoch 9/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2073 - val_loss: -0.2594\n",
      "Epoch 10/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.2164 - val_loss: -0.2727\n",
      "Epoch 11/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.2237 - val_loss: -0.2807\n",
      "Epoch 12/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2293 - val_loss: -0.2687\n",
      "Epoch 13/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2364 - val_loss: -0.2789\n",
      "Epoch 14/10000\n",
      "291017/291017 [==============================] - 6s 21us/sample - loss: -0.2427 - val_loss: -0.2773\n",
      "Epoch 15/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2469 - val_loss: -0.2406\n",
      "Epoch 16/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2498 - val_loss: -0.2830\n",
      "Epoch 17/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2568 - val_loss: -0.2826\n",
      "Epoch 18/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2595 - val_loss: -0.2911\n",
      "Epoch 19/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2632 - val_loss: -0.2889\n",
      "Epoch 20/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2681 - val_loss: -0.2462\n",
      "Epoch 21/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2693 - val_loss: -0.2929\n",
      "Epoch 22/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2728 - val_loss: -0.2664\n",
      "Epoch 23/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2743 - val_loss: -0.2743\n",
      "Epoch 24/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2764 - val_loss: -0.2938\n",
      "Epoch 25/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2791 - val_loss: -0.2969\n",
      "Epoch 26/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2789 - val_loss: -0.2907\n",
      "Epoch 27/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2816 - val_loss: -0.2962\n",
      "Epoch 28/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2830 - val_loss: -0.2998\n",
      "Epoch 29/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2823 - val_loss: -0.2972\n",
      "Epoch 30/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2873 - val_loss: -0.2986\n",
      "Epoch 31/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.2872 - val_loss: -0.2815\n",
      "Epoch 32/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2876 - val_loss: -0.2968\n",
      "Epoch 33/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2881 - val_loss: -0.3024\n",
      "Epoch 34/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2896 - val_loss: -0.2992\n",
      "Epoch 35/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2910 - val_loss: -0.2899\n",
      "Epoch 36/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2922 - val_loss: -0.2992\n",
      "Epoch 37/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2904 - val_loss: -0.2945\n",
      "Epoch 38/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2943 - val_loss: -0.3050\n",
      "Epoch 39/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.2946 - val_loss: -0.2838\n",
      "Epoch 40/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2932 - val_loss: -0.2932\n",
      "Epoch 41/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2946 - val_loss: -0.2979\n",
      "Epoch 42/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2960 - val_loss: -0.3019\n",
      "Epoch 43/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2961 - val_loss: -0.2924\n",
      "Epoch 44/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2962 - val_loss: -0.2994\n",
      "Epoch 45/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2969 - val_loss: -0.2971\n",
      "Epoch 46/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2955 - val_loss: -0.3009\n",
      "Epoch 47/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2985 - val_loss: -0.2992\n",
      "Epoch 48/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2980 - val_loss: -0.3012\n",
      "Epoch 49/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3006 - val_loss: -0.2938\n",
      "Epoch 50/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2996 - val_loss: -0.3018\n",
      "Epoch 51/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.2994 - val_loss: -0.3041\n",
      "Epoch 52/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.2992 - val_loss: -0.3057\n",
      "Epoch 53/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3015 - val_loss: -0.3054\n",
      "Epoch 54/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3017 - val_loss: -0.3055\n",
      "Epoch 55/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3016 - val_loss: -0.3038\n",
      "Epoch 56/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3018 - val_loss: -0.3008\n",
      "Epoch 57/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3024 - val_loss: -0.2952\n",
      "Epoch 58/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3030 - val_loss: -0.3038\n",
      "Epoch 59/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3039 - val_loss: -0.3034\n",
      "Epoch 60/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3027 - val_loss: -0.3036\n",
      "Epoch 61/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3045 - val_loss: -0.3009\n",
      "Epoch 62/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3048 - val_loss: -0.2931\n",
      "Epoch 63/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3052 - val_loss: -0.3060\n",
      "Epoch 64/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3048 - val_loss: -0.3035\n",
      "Epoch 65/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3051 - val_loss: -0.3038\n",
      "Epoch 66/10000\n",
      "291017/291017 [==============================] - 7s 25us/sample - loss: -0.3063 - val_loss: -0.2922\n",
      "Epoch 67/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3075 - val_loss: -0.2993\n",
      "Epoch 68/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3074 - val_loss: -0.2943\n",
      "Epoch 69/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3073 - val_loss: -0.3023\n",
      "Epoch 70/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3092 - val_loss: -0.3039\n",
      "Epoch 71/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3090 - val_loss: -0.3027\n",
      "Epoch 72/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3093 - val_loss: -0.3015\n",
      "Epoch 73/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3097 - val_loss: -0.3031\n",
      "Epoch 74/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3100 - val_loss: -0.3029\n",
      "Epoch 75/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3110 - val_loss: -0.3021\n",
      "Epoch 76/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3120 - val_loss: -0.2984\n",
      "Epoch 77/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3104 - val_loss: -0.3009\n",
      "Epoch 78/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3114 - val_loss: -0.3064\n",
      "Epoch 79/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3107 - val_loss: -0.3036\n",
      "Epoch 80/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3133 - val_loss: -0.3021\n",
      "Epoch 81/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3130 - val_loss: -0.3014\n",
      "Epoch 82/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3129 - val_loss: -0.2944\n",
      "Epoch 83/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3152 - val_loss: -0.2958\n",
      "Epoch 84/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3149 - val_loss: -0.2998\n",
      "Epoch 85/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3157 - val_loss: -0.2891\n",
      "Epoch 86/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3156 - val_loss: -0.3008\n",
      "Epoch 87/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3170 - val_loss: -0.2940\n",
      "Epoch 88/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3158 - val_loss: -0.2990\n",
      "Epoch 89/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3179 - val_loss: -0.2958\n",
      "Epoch 90/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3176 - val_loss: -0.3001\n",
      "Epoch 91/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3195 - val_loss: -0.3025\n",
      "Epoch 92/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3198 - val_loss: -0.3007\n",
      "Epoch 93/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3198 - val_loss: -0.2920\n",
      "Epoch 94/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3188 - val_loss: -0.2973\n",
      "Epoch 95/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3219 - val_loss: -0.3005\n",
      "Epoch 96/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3221 - val_loss: -0.3011\n",
      "Epoch 97/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3234 - val_loss: -0.2999\n",
      "Epoch 98/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3231 - val_loss: -0.2978\n",
      "Epoch 99/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3248 - val_loss: -0.2982\n",
      "Epoch 100/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3240 - val_loss: -0.2989\n",
      "Epoch 101/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3257 - val_loss: -0.2978\n",
      "Epoch 102/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3253 - val_loss: -0.2993\n",
      "Epoch 103/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3254 - val_loss: -0.2933\n",
      "Epoch 104/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3270 - val_loss: -0.2909\n",
      "Epoch 105/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3284 - val_loss: -0.2946\n",
      "Epoch 106/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3277 - val_loss: -0.2994\n",
      "Epoch 107/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3291 - val_loss: -0.2973\n",
      "Epoch 108/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3295 - val_loss: -0.2947\n",
      "Epoch 109/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3295 - val_loss: -0.2954\n",
      "Epoch 110/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3314 - val_loss: -0.2953\n",
      "Epoch 111/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3317 - val_loss: -0.2831\n",
      "Epoch 112/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3335 - val_loss: -0.2913\n",
      "Epoch 113/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3320 - val_loss: -0.2914\n",
      "Epoch 114/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3344 - val_loss: -0.2885\n",
      "Epoch 115/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3352 - val_loss: -0.2910\n",
      "Epoch 116/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3354 - val_loss: -0.2890\n",
      "Epoch 117/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3391 - val_loss: -0.2920\n",
      "Epoch 118/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3385 - val_loss: -0.2889\n",
      "Epoch 119/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3390 - val_loss: -0.2855\n",
      "Epoch 120/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3396 - val_loss: -0.2865\n",
      "Epoch 121/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3397 - val_loss: -0.2848\n",
      "Epoch 122/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3407 - val_loss: -0.2870\n",
      "Epoch 123/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3424 - val_loss: -0.2820\n",
      "Epoch 124/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3441 - val_loss: -0.2837\n",
      "Epoch 125/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3459 - val_loss: -0.2814\n",
      "Epoch 126/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3462 - val_loss: -0.2818\n",
      "Epoch 127/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3468 - val_loss: -0.2837\n",
      "Epoch 128/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3480 - val_loss: -0.2792\n",
      "Epoch 129/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3494 - val_loss: -0.2793\n",
      "Epoch 130/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3481 - val_loss: -0.2851\n",
      "Epoch 131/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3502 - val_loss: -0.2749\n",
      "Epoch 132/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3522 - val_loss: -0.2729\n",
      "Epoch 133/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3534 - val_loss: -0.2728\n",
      "Epoch 134/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3549 - val_loss: -0.2734\n",
      "Epoch 135/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3558 - val_loss: -0.2728\n",
      "Epoch 136/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3551 - val_loss: -0.2692\n",
      "Epoch 137/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3591 - val_loss: -0.2722\n",
      "Epoch 138/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3594 - val_loss: -0.2742\n",
      "Epoch 139/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3609 - val_loss: -0.2670\n",
      "Epoch 140/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3618 - val_loss: -0.2542\n",
      "Epoch 141/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3617 - val_loss: -0.2667\n",
      "Epoch 142/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3633 - val_loss: -0.2668\n",
      "Epoch 143/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3650 - val_loss: -0.2673\n",
      "Epoch 144/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3664 - val_loss: -0.2533\n",
      "Epoch 145/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3680 - val_loss: -0.2561\n",
      "Epoch 146/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.3671 - val_loss: -0.2657\n",
      "Epoch 147/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3709 - val_loss: -0.2568\n",
      "Epoch 148/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3701 - val_loss: -0.2618\n",
      "Epoch 149/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3726 - val_loss: -0.2550\n",
      "Epoch 150/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3744 - val_loss: -0.2500\n",
      "Epoch 151/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3754 - val_loss: -0.2476\n",
      "Epoch 152/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3756 - val_loss: -0.2489\n",
      "Epoch 153/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3744 - val_loss: -0.2434\n",
      "Epoch 154/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3782 - val_loss: -0.2466\n",
      "Epoch 155/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3805 - val_loss: -0.2487\n",
      "Epoch 156/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3797 - val_loss: -0.2218\n",
      "Epoch 157/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.3830 - val_loss: -0.2419\n",
      "Epoch 158/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3841 - val_loss: -0.2379\n",
      "Epoch 159/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3839 - val_loss: -0.2467\n",
      "Epoch 160/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3878 - val_loss: -0.2375\n",
      "Epoch 161/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.3894 - val_loss: -0.2348\n",
      "Epoch 162/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3894 - val_loss: -0.2294\n",
      "Epoch 163/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3914 - val_loss: -0.2253\n",
      "Epoch 164/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3931 - val_loss: -0.2314\n",
      "Epoch 165/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3954 - val_loss: -0.2270\n",
      "Epoch 166/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3963 - val_loss: -0.2266\n",
      "Epoch 167/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.3982 - val_loss: -0.2195\n",
      "Epoch 168/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4005 - val_loss: -0.2172\n",
      "Epoch 169/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4016 - val_loss: -0.2205\n",
      "Epoch 170/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4021 - val_loss: -0.2176\n",
      "Epoch 171/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4043 - val_loss: -0.2301\n",
      "Epoch 172/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4050 - val_loss: -0.2015\n",
      "Epoch 173/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4081 - val_loss: -0.2069\n",
      "Epoch 174/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4088 - val_loss: -0.2047\n",
      "Epoch 175/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4099 - val_loss: -0.2056\n",
      "Epoch 176/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4124 - val_loss: -0.2050\n",
      "Epoch 177/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4128 - val_loss: -0.1994\n",
      "Epoch 178/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.4137 - val_loss: -0.2005\n",
      "Epoch 179/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.4174 - val_loss: -0.1914\n",
      "Epoch 180/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4183 - val_loss: -0.1957\n",
      "Epoch 181/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.4192 - val_loss: -0.1831\n",
      "Epoch 182/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4209 - val_loss: -0.1773\n",
      "Epoch 183/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4227 - val_loss: -0.1879\n",
      "Epoch 184/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.4242 - val_loss: -0.1774\n",
      "Epoch 185/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4261 - val_loss: -0.1814\n",
      "Epoch 186/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4268 - val_loss: -0.1820\n",
      "Epoch 187/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4298 - val_loss: -0.1660\n",
      "Epoch 188/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4298 - val_loss: -0.1676\n",
      "Epoch 189/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4310 - val_loss: -0.1567\n",
      "Epoch 190/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4342 - val_loss: -0.1749\n",
      "Epoch 191/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4340 - val_loss: -0.1676\n",
      "Epoch 192/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4381 - val_loss: -0.1783\n",
      "Epoch 193/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4369 - val_loss: -0.1648\n",
      "Epoch 194/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4393 - val_loss: -0.1440\n",
      "Epoch 195/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4424 - val_loss: -0.1428\n",
      "Epoch 196/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4432 - val_loss: -0.1587\n",
      "Epoch 197/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4459 - val_loss: -0.1557\n",
      "Epoch 198/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4474 - val_loss: -0.1462\n",
      "Epoch 199/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4483 - val_loss: -0.1415\n",
      "Epoch 200/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4494 - val_loss: -0.1297\n",
      "Epoch 201/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4531 - val_loss: -0.1286\n",
      "Epoch 202/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4532 - val_loss: -0.1286\n",
      "Epoch 203/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4570 - val_loss: -0.1375\n",
      "Epoch 204/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4587 - val_loss: -0.1158\n",
      "Epoch 205/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4567 - val_loss: -0.1257\n",
      "Epoch 206/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4603 - val_loss: -0.1146\n",
      "Epoch 207/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.4627 - val_loss: -0.1225\n",
      "Epoch 208/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4635 - val_loss: -0.1262\n",
      "Epoch 209/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4651 - val_loss: -0.1133\n",
      "Epoch 210/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.4667 - val_loss: -0.0966\n",
      "Epoch 211/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4688 - val_loss: -0.1093\n",
      "Epoch 212/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.4710 - val_loss: -0.1007\n",
      "Epoch 213/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4712 - val_loss: -0.1076\n",
      "Epoch 214/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4720 - val_loss: -0.1056\n",
      "Epoch 215/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4759 - val_loss: -0.1019\n",
      "Epoch 216/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4768 - val_loss: -0.0905\n",
      "Epoch 217/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4769 - val_loss: -0.0947\n",
      "Epoch 218/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.4808 - val_loss: -0.0653\n",
      "Epoch 219/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4814 - val_loss: -0.1135\n",
      "Epoch 220/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4828 - val_loss: -0.0807\n",
      "Epoch 221/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.4840 - val_loss: -0.0691\n",
      "Epoch 222/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4857 - val_loss: -0.0788\n",
      "Epoch 223/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4890 - val_loss: -0.0423\n",
      "Epoch 224/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4904 - val_loss: -0.0539\n",
      "Epoch 225/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4909 - val_loss: -0.0598\n",
      "Epoch 226/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4939 - val_loss: -0.0634\n",
      "Epoch 227/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4920 - val_loss: -0.0636\n",
      "Epoch 228/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4954 - val_loss: -0.0415\n",
      "Epoch 229/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4964 - val_loss: -0.0557\n",
      "Epoch 230/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5012 - val_loss: -0.0578\n",
      "Epoch 231/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.4993 - val_loss: -0.0459\n",
      "Epoch 232/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.5018 - val_loss: -0.0499\n",
      "Epoch 233/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5026 - val_loss: -0.0409\n",
      "Epoch 234/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5054 - val_loss: -0.0397\n",
      "Epoch 235/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5059 - val_loss: -0.0443\n",
      "Epoch 236/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5096 - val_loss: -0.0116\n",
      "Epoch 237/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5109 - val_loss: -0.0183\n",
      "Epoch 238/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.5106 - val_loss: -0.0079\n",
      "Epoch 239/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5120 - val_loss: -0.0126\n",
      "Epoch 240/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.5148 - val_loss: -0.0110\n",
      "Epoch 241/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5159 - val_loss: 0.0020\n",
      "Epoch 242/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.5181 - val_loss: 0.0087\n",
      "Epoch 243/10000\n",
      "291017/291017 [==============================] - 6s 22us/sample - loss: -0.5178 - val_loss: -0.0066\n",
      "Epoch 244/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5218 - val_loss: -0.0111\n",
      "Epoch 245/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5233 - val_loss: -0.0075\n",
      "Epoch 246/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5264 - val_loss: 0.0210\n",
      "Epoch 247/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5241 - val_loss: -0.0026\n",
      "Epoch 248/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5261 - val_loss: 0.0052\n",
      "Epoch 249/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.5302 - val_loss: 0.0096\n",
      "Epoch 250/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5299 - val_loss: 0.0230\n",
      "Epoch 251/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5322 - val_loss: -0.0040\n",
      "Epoch 252/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5334 - val_loss: 0.0246\n",
      "Epoch 253/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5348 - val_loss: 0.0369\n",
      "Epoch 254/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5338 - val_loss: 0.0485\n",
      "Epoch 255/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5376 - val_loss: 0.0296\n",
      "Epoch 256/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5405 - val_loss: 0.0554\n",
      "Epoch 257/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.5410 - val_loss: 0.0635\n",
      "Epoch 258/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5413 - val_loss: 0.0561\n",
      "Epoch 259/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5430 - val_loss: 0.0469\n",
      "Epoch 260/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5441 - val_loss: 0.0581\n",
      "Epoch 261/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5465 - val_loss: 0.0677\n",
      "Epoch 262/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5501 - val_loss: 0.0748\n",
      "Epoch 263/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5495 - val_loss: 0.0673\n",
      "Epoch 264/10000\n",
      "291017/291017 [==============================] - 7s 25us/sample - loss: -0.5517 - val_loss: 0.0686\n",
      "Epoch 265/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5541 - val_loss: 0.0372\n",
      "Epoch 266/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5541 - val_loss: 0.0785\n",
      "Epoch 267/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5551 - val_loss: 0.0764\n",
      "Epoch 268/10000\n",
      "291017/291017 [==============================] - 7s 25us/sample - loss: -0.5568 - val_loss: 0.0807\n",
      "Epoch 269/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5591 - val_loss: 0.1016\n",
      "Epoch 270/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5626 - val_loss: 0.0935\n",
      "Epoch 271/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5628 - val_loss: 0.1046\n",
      "Epoch 272/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5616 - val_loss: 0.1139\n",
      "Epoch 273/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5637 - val_loss: 0.0779\n",
      "Epoch 274/10000\n",
      "291017/291017 [==============================] - 7s 24us/sample - loss: -0.5639 - val_loss: 0.0876\n",
      "Epoch 275/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5679 - val_loss: 0.1359\n",
      "Epoch 276/10000\n",
      "291017/291017 [==============================] - 7s 22us/sample - loss: -0.5686 - val_loss: 0.1348\n",
      "Epoch 277/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5709 - val_loss: 0.0990\n",
      "Epoch 278/10000\n",
      "291017/291017 [==============================] - 7s 23us/sample - loss: -0.5727 - val_loss: 0.1233\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1205.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7321, 18, 1)\n",
      "y_test.shape:  (7321, 1)\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:48:33,210 WARNING Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1205.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1211.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7811, 18, 1)\n",
      "y_test.shape:  (7811, 1)\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:48:41,984 WARNING Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1211.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1219.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7853, 18, 1)\n",
      "y_test.shape:  (7853, 1)\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:48:50,625 WARNING Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1219.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1230.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7493, 18, 1)\n",
      "y_test.shape:  (7493, 1)\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:48:59,088 WARNING Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1230.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1239.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7614, 18, 1)\n",
      "y_test.shape:  (7614, 1)\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:07,486 WARNING Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1239.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1271.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7924, 18, 1)\n",
      "y_test.shape:  (7924, 1)\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:15,719 WARNING Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1271.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1286.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7945, 18, 1)\n",
      "y_test.shape:  (7945, 1)\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:24,014 WARNING Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1286.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1311.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7262, 18, 1)\n",
      "y_test.shape:  (7262, 1)\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:32,541 WARNING Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1311.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1330.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7847, 18, 1)\n",
      "y_test.shape:  (7847, 1)\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:40,901 WARNING Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1330.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1336.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7591, 18, 1)\n",
      "y_test.shape:  (7591, 1)\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:49,251 WARNING Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1336.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1343.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7222, 18, 1)\n",
      "y_test.shape:  (7222, 1)\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:49:55,388 WARNING Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1343.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1345.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7447, 18, 1)\n",
      "y_test.shape:  (7447, 1)\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:50:04,032 WARNING Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1345.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1348.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7641, 18, 1)\n",
      "y_test.shape:  (7641, 1)\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:50:12,101 WARNING Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1348.csv\n",
      "2025-01-21 18:50:19,501 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (294154, 18, 1)\n",
      "y_train.shape:  (294154, 1)\n",
      "x_valid.shape:  (73514, 18, 1)\n",
      "y_valid.shape:  (73514, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 18:51:02,420 WARNING Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 18:51:02,527 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 294154 samples, validate on 73514 samples\n",
      "Epoch 1/10000\n",
      "294154/294154 [==============================] - ETA: 0s - loss: 0.1856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294154/294154 [==============================] - 11s 39us/sample - loss: 0.1856 - val_loss: -0.1020\n",
      "Epoch 2/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.0701 - val_loss: -0.1823\n",
      "Epoch 3/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.1129 - val_loss: -0.2011\n",
      "Epoch 4/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.1382 - val_loss: -0.2252\n",
      "Epoch 5/10000\n",
      "294154/294154 [==============================] - 9s 32us/sample - loss: -0.1583 - val_loss: -0.2243\n",
      "Epoch 6/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.1737 - val_loss: -0.2359\n",
      "Epoch 7/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.1895 - val_loss: -0.2558\n",
      "Epoch 8/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2016 - val_loss: -0.2729\n",
      "Epoch 9/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2109 - val_loss: -0.2769\n",
      "Epoch 10/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2222 - val_loss: -0.2751\n",
      "Epoch 11/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2270 - val_loss: -0.2173\n",
      "Epoch 12/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2357 - val_loss: -0.2900\n",
      "Epoch 13/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2419 - val_loss: -0.2699\n",
      "Epoch 14/10000\n",
      "294154/294154 [==============================] - 6s 21us/sample - loss: -0.2465 - val_loss: -0.2931\n",
      "Epoch 15/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.2530 - val_loss: -0.2872\n",
      "Epoch 16/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.2557 - val_loss: -0.2650\n",
      "Epoch 17/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.2618 - val_loss: -0.2935\n",
      "Epoch 18/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.2656 - val_loss: -0.2945\n",
      "Epoch 19/10000\n",
      "294154/294154 [==============================] - 6s 22us/sample - loss: -0.2685 - val_loss: -0.2919\n",
      "Epoch 20/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.2709 - val_loss: -0.2932\n",
      "Epoch 21/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.2758 - val_loss: -0.2920\n",
      "Epoch 22/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.2779 - val_loss: -0.2904\n",
      "Epoch 23/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.2809 - val_loss: -0.2978\n",
      "Epoch 24/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2826 - val_loss: -0.2993\n",
      "Epoch 25/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.2836 - val_loss: -0.2926\n",
      "Epoch 26/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2863 - val_loss: -0.3044\n",
      "Epoch 27/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2886 - val_loss: -0.2827\n",
      "Epoch 28/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2900 - val_loss: -0.3037\n",
      "Epoch 29/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2897 - val_loss: -0.2963\n",
      "Epoch 30/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2919 - val_loss: -0.2986\n",
      "Epoch 31/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.2947 - val_loss: -0.3018\n",
      "Epoch 32/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.2938 - val_loss: -0.2970\n",
      "Epoch 33/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.2970 - val_loss: -0.3027\n",
      "Epoch 34/10000\n",
      "294154/294154 [==============================] - 7s 25us/sample - loss: -0.2961 - val_loss: -0.3035\n",
      "Epoch 35/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.2971 - val_loss: -0.3056\n",
      "Epoch 36/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.2989 - val_loss: -0.3040\n",
      "Epoch 37/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.2980 - val_loss: -0.3081\n",
      "Epoch 38/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3005 - val_loss: -0.3025\n",
      "Epoch 39/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3007 - val_loss: -0.2992\n",
      "Epoch 40/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3032 - val_loss: -0.3057\n",
      "Epoch 41/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3024 - val_loss: -0.3073\n",
      "Epoch 42/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3021 - val_loss: -0.2984\n",
      "Epoch 43/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3029 - val_loss: -0.3039\n",
      "Epoch 44/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3049 - val_loss: -0.2968\n",
      "Epoch 45/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3049 - val_loss: -0.2940\n",
      "Epoch 46/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3064 - val_loss: -0.3048\n",
      "Epoch 47/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3072 - val_loss: -0.3034\n",
      "Epoch 48/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3070 - val_loss: -0.3024\n",
      "Epoch 49/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3072 - val_loss: -0.3047\n",
      "Epoch 50/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3070 - val_loss: -0.3047\n",
      "Epoch 51/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3074 - val_loss: -0.3121\n",
      "Epoch 52/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3091 - val_loss: -0.3075\n",
      "Epoch 53/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3102 - val_loss: -0.3062\n",
      "Epoch 54/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3087 - val_loss: -0.3035\n",
      "Epoch 55/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.3104 - val_loss: -0.3054\n",
      "Epoch 56/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3126 - val_loss: -0.3024\n",
      "Epoch 57/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3100 - val_loss: -0.2958\n",
      "Epoch 58/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3111 - val_loss: -0.2971\n",
      "Epoch 59/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.3104 - val_loss: -0.2980\n",
      "Epoch 60/10000\n",
      "294154/294154 [==============================] - 7s 25us/sample - loss: -0.3136 - val_loss: -0.3094\n",
      "Epoch 61/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3132 - val_loss: -0.3050\n",
      "Epoch 62/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3127 - val_loss: -0.3026\n",
      "Epoch 63/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3143 - val_loss: -0.3081\n",
      "Epoch 64/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3150 - val_loss: -0.3055\n",
      "Epoch 65/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3145 - val_loss: -0.3054\n",
      "Epoch 66/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3154 - val_loss: -0.3069\n",
      "Epoch 67/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3161 - val_loss: -0.3092\n",
      "Epoch 68/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3170 - val_loss: -0.3048\n",
      "Epoch 69/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3182 - val_loss: -0.3037\n",
      "Epoch 70/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3178 - val_loss: -0.3031\n",
      "Epoch 71/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3177 - val_loss: -0.3023\n",
      "Epoch 72/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3181 - val_loss: -0.3083\n",
      "Epoch 73/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3181 - val_loss: -0.3113\n",
      "Epoch 74/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3196 - val_loss: -0.3043\n",
      "Epoch 75/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3199 - val_loss: -0.3108\n",
      "Epoch 76/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3199 - val_loss: -0.3046\n",
      "Epoch 77/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3211 - val_loss: -0.3076\n",
      "Epoch 78/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3210 - val_loss: -0.3068\n",
      "Epoch 79/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3224 - val_loss: -0.3074\n",
      "Epoch 80/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.3225 - val_loss: -0.3063\n",
      "Epoch 81/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3238 - val_loss: -0.2928\n",
      "Epoch 82/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3238 - val_loss: -0.3074\n",
      "Epoch 83/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3237 - val_loss: -0.3033\n",
      "Epoch 84/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3230 - val_loss: -0.3059\n",
      "Epoch 85/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3251 - val_loss: -0.3067\n",
      "Epoch 86/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3263 - val_loss: -0.3045\n",
      "Epoch 87/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3272 - val_loss: -0.3023\n",
      "Epoch 88/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3261 - val_loss: -0.2998\n",
      "Epoch 89/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3261 - val_loss: -0.3020\n",
      "Epoch 90/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3277 - val_loss: -0.3053\n",
      "Epoch 91/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3284 - val_loss: -0.3055\n",
      "Epoch 92/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3301 - val_loss: -0.3042\n",
      "Epoch 93/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3312 - val_loss: -0.3051\n",
      "Epoch 94/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3309 - val_loss: -0.2986\n",
      "Epoch 95/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3319 - val_loss: -0.3065\n",
      "Epoch 96/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3319 - val_loss: -0.3015\n",
      "Epoch 97/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3336 - val_loss: -0.3015\n",
      "Epoch 98/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3332 - val_loss: -0.2971\n",
      "Epoch 99/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3339 - val_loss: -0.2969\n",
      "Epoch 100/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3367 - val_loss: -0.2974\n",
      "Epoch 101/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3367 - val_loss: -0.2973\n",
      "Epoch 102/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3376 - val_loss: -0.3005\n",
      "Epoch 103/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3386 - val_loss: -0.3021\n",
      "Epoch 104/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3395 - val_loss: -0.2966\n",
      "Epoch 105/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3389 - val_loss: -0.2831\n",
      "Epoch 106/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3408 - val_loss: -0.2942\n",
      "Epoch 107/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3415 - val_loss: -0.2956\n",
      "Epoch 108/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3412 - val_loss: -0.2973\n",
      "Epoch 109/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3415 - val_loss: -0.2955\n",
      "Epoch 110/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3446 - val_loss: -0.2951\n",
      "Epoch 111/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3467 - val_loss: -0.2922\n",
      "Epoch 112/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3471 - val_loss: -0.2914\n",
      "Epoch 113/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3479 - val_loss: -0.2963\n",
      "Epoch 114/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3489 - val_loss: -0.2886\n",
      "Epoch 115/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3503 - val_loss: -0.2916\n",
      "Epoch 116/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3515 - val_loss: -0.2905\n",
      "Epoch 117/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3517 - val_loss: -0.2847\n",
      "Epoch 118/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3532 - val_loss: -0.2818\n",
      "Epoch 119/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3544 - val_loss: -0.2848\n",
      "Epoch 120/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3552 - val_loss: -0.2882\n",
      "Epoch 121/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3574 - val_loss: -0.2839\n",
      "Epoch 122/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3572 - val_loss: -0.2808\n",
      "Epoch 123/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3573 - val_loss: -0.2871\n",
      "Epoch 124/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3615 - val_loss: -0.2797\n",
      "Epoch 125/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3616 - val_loss: -0.2812\n",
      "Epoch 126/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3635 - val_loss: -0.2812\n",
      "Epoch 127/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3652 - val_loss: -0.2729\n",
      "Epoch 128/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3662 - val_loss: -0.2633\n",
      "Epoch 129/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3666 - val_loss: -0.2697\n",
      "Epoch 130/10000\n",
      "294154/294154 [==============================] - 7s 22us/sample - loss: -0.3681 - val_loss: -0.2720\n",
      "Epoch 131/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3689 - val_loss: -0.2645\n",
      "Epoch 132/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3700 - val_loss: -0.2716\n",
      "Epoch 133/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3719 - val_loss: -0.2716\n",
      "Epoch 134/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3739 - val_loss: -0.2576\n",
      "Epoch 135/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3743 - val_loss: -0.2599\n",
      "Epoch 136/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3770 - val_loss: -0.2670\n",
      "Epoch 137/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3790 - val_loss: -0.2507\n",
      "Epoch 138/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3807 - val_loss: -0.2547\n",
      "Epoch 139/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3825 - val_loss: -0.2598\n",
      "Epoch 140/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3823 - val_loss: -0.2515\n",
      "Epoch 141/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3834 - val_loss: -0.2416\n",
      "Epoch 142/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3861 - val_loss: -0.2529\n",
      "Epoch 143/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3877 - val_loss: -0.2413\n",
      "Epoch 144/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3887 - val_loss: -0.2525\n",
      "Epoch 145/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3912 - val_loss: -0.2539\n",
      "Epoch 146/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3935 - val_loss: -0.2434\n",
      "Epoch 147/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3933 - val_loss: -0.2284\n",
      "Epoch 148/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.3963 - val_loss: -0.2353\n",
      "Epoch 149/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.3985 - val_loss: -0.2298\n",
      "Epoch 150/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4019 - val_loss: -0.2249\n",
      "Epoch 151/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4006 - val_loss: -0.2258\n",
      "Epoch 152/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4014 - val_loss: -0.2209\n",
      "Epoch 153/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4050 - val_loss: -0.2211\n",
      "Epoch 154/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4081 - val_loss: -0.2193\n",
      "Epoch 155/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4082 - val_loss: -0.2187\n",
      "Epoch 156/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4109 - val_loss: -0.2152\n",
      "Epoch 157/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4124 - val_loss: -0.2192\n",
      "Epoch 158/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4135 - val_loss: -0.2089\n",
      "Epoch 159/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4164 - val_loss: -0.1988\n",
      "Epoch 160/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4182 - val_loss: -0.1879\n",
      "Epoch 161/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4197 - val_loss: -0.2042\n",
      "Epoch 162/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4199 - val_loss: -0.1953\n",
      "Epoch 163/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4244 - val_loss: -0.1960\n",
      "Epoch 164/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4249 - val_loss: -0.1996\n",
      "Epoch 165/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4275 - val_loss: -0.1927\n",
      "Epoch 166/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4294 - val_loss: -0.1921\n",
      "Epoch 167/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4315 - val_loss: -0.1841\n",
      "Epoch 168/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4323 - val_loss: -0.1885\n",
      "Epoch 169/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4358 - val_loss: -0.1849\n",
      "Epoch 170/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4367 - val_loss: -0.1798\n",
      "Epoch 171/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4390 - val_loss: -0.1734\n",
      "Epoch 172/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4409 - val_loss: -0.1823\n",
      "Epoch 173/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4418 - val_loss: -0.1720\n",
      "Epoch 174/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4455 - val_loss: -0.1520\n",
      "Epoch 175/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4466 - val_loss: -0.1495\n",
      "Epoch 176/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4474 - val_loss: -0.1686\n",
      "Epoch 177/10000\n",
      "294154/294154 [==============================] - 7s 25us/sample - loss: -0.4491 - val_loss: -0.1550\n",
      "Epoch 178/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4520 - val_loss: -0.1391\n",
      "Epoch 179/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4545 - val_loss: -0.1407\n",
      "Epoch 180/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4563 - val_loss: -0.1367\n",
      "Epoch 181/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4587 - val_loss: -0.1454\n",
      "Epoch 182/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4605 - val_loss: -0.1343\n",
      "Epoch 183/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4631 - val_loss: -0.1396\n",
      "Epoch 184/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4646 - val_loss: -0.1292\n",
      "Epoch 185/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4656 - val_loss: -0.1460\n",
      "Epoch 186/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.4688 - val_loss: -0.1047\n",
      "Epoch 187/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4704 - val_loss: -0.1314\n",
      "Epoch 188/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4727 - val_loss: -0.1105\n",
      "Epoch 189/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4731 - val_loss: -0.1023\n",
      "Epoch 190/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4751 - val_loss: -0.1244\n",
      "Epoch 191/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4782 - val_loss: -0.0903\n",
      "Epoch 192/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4783 - val_loss: -0.1181\n",
      "Epoch 193/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4829 - val_loss: -0.0958\n",
      "Epoch 194/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4833 - val_loss: -0.0876\n",
      "Epoch 195/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4870 - val_loss: -0.0966\n",
      "Epoch 196/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4879 - val_loss: -0.0763\n",
      "Epoch 197/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4904 - val_loss: -0.0739\n",
      "Epoch 198/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4919 - val_loss: -0.0857\n",
      "Epoch 199/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4951 - val_loss: -0.0790\n",
      "Epoch 200/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4980 - val_loss: -0.0791\n",
      "Epoch 201/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4996 - val_loss: -0.0731\n",
      "Epoch 202/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.4982 - val_loss: -0.0695\n",
      "Epoch 203/10000\n",
      "294154/294154 [==============================] - 7s 25us/sample - loss: -0.5018 - val_loss: -0.0503\n",
      "Epoch 204/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5034 - val_loss: -0.0702\n",
      "Epoch 205/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5076 - val_loss: -0.0361\n",
      "Epoch 206/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5068 - val_loss: -0.0253\n",
      "Epoch 207/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5093 - val_loss: -0.0526\n",
      "Epoch 208/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5128 - val_loss: -0.0342\n",
      "Epoch 209/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5148 - val_loss: -0.0205\n",
      "Epoch 210/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5168 - val_loss: -0.0440\n",
      "Epoch 211/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5190 - val_loss: -0.0376\n",
      "Epoch 212/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5206 - val_loss: -0.0177\n",
      "Epoch 213/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5225 - val_loss: -0.0279\n",
      "Epoch 214/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5255 - val_loss: -0.0055\n",
      "Epoch 215/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5276 - val_loss: -0.0065\n",
      "Epoch 216/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5277 - val_loss: 0.0193\n",
      "Epoch 217/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5295 - val_loss: -0.0319\n",
      "Epoch 218/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5311 - val_loss: 0.0065\n",
      "Epoch 219/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5339 - val_loss: -0.0126\n",
      "Epoch 220/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5352 - val_loss: 0.0120\n",
      "Epoch 221/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5358 - val_loss: 0.0026\n",
      "Epoch 222/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5385 - val_loss: 0.0383\n",
      "Epoch 223/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5409 - val_loss: 0.0335\n",
      "Epoch 224/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5434 - val_loss: 0.0073\n",
      "Epoch 225/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5473 - val_loss: 0.0062\n",
      "Epoch 226/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5469 - val_loss: 0.0321\n",
      "Epoch 227/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5491 - val_loss: 0.0670\n",
      "Epoch 228/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5507 - val_loss: 0.0616\n",
      "Epoch 229/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5522 - val_loss: 0.0460\n",
      "Epoch 230/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5547 - val_loss: 0.0143\n",
      "Epoch 231/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5573 - val_loss: 0.0346\n",
      "Epoch 232/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5588 - val_loss: 0.0395\n",
      "Epoch 233/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5609 - val_loss: 0.0435\n",
      "Epoch 234/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5621 - val_loss: 0.1089\n",
      "Epoch 235/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5640 - val_loss: 0.0214\n",
      "Epoch 236/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5653 - val_loss: 0.0765\n",
      "Epoch 237/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5674 - val_loss: 0.0612\n",
      "Epoch 238/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5688 - val_loss: 0.0711\n",
      "Epoch 239/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5712 - val_loss: 0.0760\n",
      "Epoch 240/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5735 - val_loss: 0.0515\n",
      "Epoch 241/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5748 - val_loss: 0.0849\n",
      "Epoch 242/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5772 - val_loss: 0.0915\n",
      "Epoch 243/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5786 - val_loss: 0.1140\n",
      "Epoch 244/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5804 - val_loss: 0.1060\n",
      "Epoch 245/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5853 - val_loss: 0.1066\n",
      "Epoch 246/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5868 - val_loss: 0.1124\n",
      "Epoch 247/10000\n",
      "294154/294154 [==============================] - 7s 25us/sample - loss: -0.5862 - val_loss: 0.1036\n",
      "Epoch 248/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5888 - val_loss: 0.1422\n",
      "Epoch 249/10000\n",
      "294154/294154 [==============================] - 7s 24us/sample - loss: -0.5894 - val_loss: 0.1305\n",
      "Epoch 250/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5902 - val_loss: 0.1659\n",
      "Epoch 251/10000\n",
      "294154/294154 [==============================] - 7s 23us/sample - loss: -0.5937 - val_loss: 0.1493\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1361.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7617, 18, 1)\n",
      "y_test.shape:  (7617, 1)\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:19:40,273 WARNING Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1361.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1362.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6091, 18, 1)\n",
      "y_test.shape:  (6091, 1)\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:19:49,126 WARNING Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1362.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1363.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7567, 18, 1)\n",
      "y_test.shape:  (7567, 1)\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:19:57,493 WARNING Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1363.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1377.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7737, 18, 1)\n",
      "y_test.shape:  (7737, 1)\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:06,658 WARNING Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1377.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1381.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7707, 18, 1)\n",
      "y_test.shape:  (7707, 1)\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:15,524 WARNING Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1381.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1386.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7282, 18, 1)\n",
      "y_test.shape:  (7282, 1)\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:24,393 WARNING Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1386.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1408.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7252, 18, 1)\n",
      "y_test.shape:  (7252, 1)\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:33,017 WARNING Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1408.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1422.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6964, 18, 1)\n",
      "y_test.shape:  (6964, 1)\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:41,486 WARNING Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1422.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1427.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7154, 18, 1)\n",
      "y_test.shape:  (7154, 1)\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:49,864 WARNING Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1427.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1433.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7580, 18, 1)\n",
      "y_test.shape:  (7580, 1)\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:20:58,710 WARNING Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1433.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1435.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6950, 18, 1)\n",
      "y_test.shape:  (6950, 1)\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:21:07,624 WARNING Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1435.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1457.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7686, 18, 1)\n",
      "y_test.shape:  (7686, 1)\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:21:16,060 WARNING Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1457.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1459.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7463, 18, 1)\n",
      "y_test.shape:  (7463, 1)\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:21:24,987 WARNING Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1459.csv\n",
      "2025-01-21 19:21:32,958 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (305180, 18, 1)\n",
      "y_train.shape:  (305180, 1)\n",
      "x_valid.shape:  (76269, 18, 1)\n",
      "y_valid.shape:  (76269, 1)\n",
      "x_test.shape:  (0, 18, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:22:19,251 WARNING Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 19:22:19,395 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 305180 samples, validate on 76269 samples\n",
      "Epoch 1/10000\n",
      "304128/305180 [============================>.] - ETA: 0s - loss: 0.1498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305180/305180 [==============================] - 12s 40us/sample - loss: 0.1491 - val_loss: -0.0821\n",
      "Epoch 2/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.0640 - val_loss: 0.0410\n",
      "Epoch 3/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.1003 - val_loss: -0.1788\n",
      "Epoch 4/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.1355 - val_loss: -0.2074\n",
      "Epoch 5/10000\n",
      "305180/305180 [==============================] - 10s 33us/sample - loss: -0.1543 - val_loss: -0.2209\n",
      "Epoch 6/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.1707 - val_loss: -0.1989\n",
      "Epoch 7/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.1853 - val_loss: -0.2412\n",
      "Epoch 8/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.1977 - val_loss: -0.2092\n",
      "Epoch 9/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2047 - val_loss: -0.2528\n",
      "Epoch 10/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2134 - val_loss: -0.2173\n",
      "Epoch 11/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2217 - val_loss: -0.2639\n",
      "Epoch 12/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2267 - val_loss: -0.2728\n",
      "Epoch 13/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2341 - val_loss: -0.2705\n",
      "Epoch 14/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2362 - val_loss: -0.2306\n",
      "Epoch 15/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2406 - val_loss: -0.2794\n",
      "Epoch 16/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2480 - val_loss: -0.2803\n",
      "Epoch 17/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2508 - val_loss: -0.2549\n",
      "Epoch 18/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2528 - val_loss: -0.2690\n",
      "Epoch 19/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2582 - val_loss: -0.2828\n",
      "Epoch 20/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2612 - val_loss: -0.2812\n",
      "Epoch 21/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2629 - val_loss: -0.2599\n",
      "Epoch 22/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.2654 - val_loss: -0.2664\n",
      "Epoch 23/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2666 - val_loss: -0.2823\n",
      "Epoch 24/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2677 - val_loss: -0.2261\n",
      "Epoch 25/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2676 - val_loss: -0.2915\n",
      "Epoch 26/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2753 - val_loss: -0.2537\n",
      "Epoch 27/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2748 - val_loss: -0.2712\n",
      "Epoch 28/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2761 - val_loss: -0.2845\n",
      "Epoch 29/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2763 - val_loss: -0.2500\n",
      "Epoch 30/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2768 - val_loss: -0.2523\n",
      "Epoch 31/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2800 - val_loss: -0.2831\n",
      "Epoch 32/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2810 - val_loss: -0.2861\n",
      "Epoch 33/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2830 - val_loss: -0.2835\n",
      "Epoch 34/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2836 - val_loss: -0.2875\n",
      "Epoch 35/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2841 - val_loss: -0.2889\n",
      "Epoch 36/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2851 - val_loss: -0.2916\n",
      "Epoch 37/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2865 - val_loss: -0.2927\n",
      "Epoch 38/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2859 - val_loss: -0.2759\n",
      "Epoch 39/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2864 - val_loss: -0.2877\n",
      "Epoch 40/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.2879 - val_loss: -0.2887\n",
      "Epoch 41/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2875 - val_loss: -0.2709\n",
      "Epoch 42/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2875 - val_loss: -0.2751\n",
      "Epoch 43/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2885 - val_loss: -0.2870\n",
      "Epoch 44/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2909 - val_loss: -0.2896\n",
      "Epoch 45/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2916 - val_loss: -0.2827\n",
      "Epoch 46/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2921 - val_loss: -0.2855\n",
      "Epoch 47/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2899 - val_loss: -0.2899\n",
      "Epoch 48/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2916 - val_loss: -0.2901\n",
      "Epoch 49/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2931 - val_loss: -0.2890\n",
      "Epoch 50/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2941 - val_loss: -0.2764\n",
      "Epoch 51/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2949 - val_loss: -0.2871\n",
      "Epoch 52/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2952 - val_loss: -0.2931\n",
      "Epoch 53/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2962 - val_loss: -0.2952\n",
      "Epoch 54/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2967 - val_loss: -0.2871\n",
      "Epoch 55/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2973 - val_loss: -0.2737\n",
      "Epoch 56/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.2967 - val_loss: -0.2824\n",
      "Epoch 57/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2977 - val_loss: -0.2806\n",
      "Epoch 58/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2981 - val_loss: -0.2815\n",
      "Epoch 59/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2978 - val_loss: -0.2929\n",
      "Epoch 60/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2999 - val_loss: -0.2713\n",
      "Epoch 61/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.2982 - val_loss: -0.2859\n",
      "Epoch 62/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3003 - val_loss: -0.2875\n",
      "Epoch 63/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3014 - val_loss: -0.2732\n",
      "Epoch 64/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3016 - val_loss: -0.2967\n",
      "Epoch 65/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3020 - val_loss: -0.2875\n",
      "Epoch 66/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3031 - val_loss: -0.2770\n",
      "Epoch 67/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3031 - val_loss: -0.2953\n",
      "Epoch 68/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3057 - val_loss: -0.2907\n",
      "Epoch 69/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3049 - val_loss: -0.2976\n",
      "Epoch 70/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3031 - val_loss: -0.2809\n",
      "Epoch 71/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3039 - val_loss: -0.2874\n",
      "Epoch 72/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3056 - val_loss: -0.2962\n",
      "Epoch 73/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3065 - val_loss: -0.2798\n",
      "Epoch 74/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3067 - val_loss: -0.2938\n",
      "Epoch 75/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3081 - val_loss: -0.2966\n",
      "Epoch 76/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3093 - val_loss: -0.2910\n",
      "Epoch 77/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3084 - val_loss: -0.2908\n",
      "Epoch 78/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3112 - val_loss: -0.2887\n",
      "Epoch 79/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3103 - val_loss: -0.2825\n",
      "Epoch 80/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3110 - val_loss: -0.2840\n",
      "Epoch 81/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3117 - val_loss: -0.2858\n",
      "Epoch 82/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3127 - val_loss: -0.2764\n",
      "Epoch 83/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3124 - val_loss: -0.2660\n",
      "Epoch 84/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3150 - val_loss: -0.2942\n",
      "Epoch 85/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3163 - val_loss: -0.2899\n",
      "Epoch 86/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3148 - val_loss: -0.2901\n",
      "Epoch 87/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3142 - val_loss: -0.2805\n",
      "Epoch 88/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3190 - val_loss: -0.2838\n",
      "Epoch 89/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3178 - val_loss: -0.2882\n",
      "Epoch 90/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3198 - val_loss: -0.2808\n",
      "Epoch 91/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3190 - val_loss: -0.2922\n",
      "Epoch 92/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3217 - val_loss: -0.2913\n",
      "Epoch 93/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3217 - val_loss: -0.2877\n",
      "Epoch 94/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3218 - val_loss: -0.2753\n",
      "Epoch 95/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3237 - val_loss: -0.2798\n",
      "Epoch 96/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3237 - val_loss: -0.2837\n",
      "Epoch 97/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3255 - val_loss: -0.2811\n",
      "Epoch 98/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3257 - val_loss: -0.2843\n",
      "Epoch 99/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3276 - val_loss: -0.2820\n",
      "Epoch 100/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3285 - val_loss: -0.2780\n",
      "Epoch 101/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3303 - val_loss: -0.2434\n",
      "Epoch 102/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3292 - val_loss: -0.2794\n",
      "Epoch 103/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3309 - val_loss: -0.2814\n",
      "Epoch 104/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3315 - val_loss: -0.2839\n",
      "Epoch 105/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3324 - val_loss: -0.2777\n",
      "Epoch 106/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3342 - val_loss: -0.2645\n",
      "Epoch 107/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3357 - val_loss: -0.2744\n",
      "Epoch 108/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3371 - val_loss: -0.2744\n",
      "Epoch 109/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3385 - val_loss: -0.2736\n",
      "Epoch 110/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3401 - val_loss: -0.2745\n",
      "Epoch 111/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3425 - val_loss: -0.2656\n",
      "Epoch 112/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3431 - val_loss: -0.2500\n",
      "Epoch 113/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3434 - val_loss: -0.2647\n",
      "Epoch 114/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3470 - val_loss: -0.2628\n",
      "Epoch 115/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3455 - val_loss: -0.2655\n",
      "Epoch 116/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3487 - val_loss: -0.2631\n",
      "Epoch 117/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3506 - val_loss: -0.2593\n",
      "Epoch 118/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3522 - val_loss: -0.2574\n",
      "Epoch 119/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3515 - val_loss: -0.2548\n",
      "Epoch 120/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3532 - val_loss: -0.2546\n",
      "Epoch 121/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3559 - val_loss: -0.2582\n",
      "Epoch 122/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3568 - val_loss: -0.2394\n",
      "Epoch 123/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3587 - val_loss: -0.2527\n",
      "Epoch 124/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3603 - val_loss: -0.2486\n",
      "Epoch 125/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3623 - val_loss: -0.2465\n",
      "Epoch 126/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3622 - val_loss: -0.2437\n",
      "Epoch 127/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3630 - val_loss: -0.2597\n",
      "Epoch 128/10000\n",
      "305180/305180 [==============================] - 7s 22us/sample - loss: -0.3621 - val_loss: -0.2408\n",
      "Epoch 129/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3666 - val_loss: -0.2294\n",
      "Epoch 130/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3665 - val_loss: -0.2301\n",
      "Epoch 131/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3694 - val_loss: -0.2305\n",
      "Epoch 132/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3731 - val_loss: -0.2418\n",
      "Epoch 133/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3746 - val_loss: -0.2225\n",
      "Epoch 134/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3766 - val_loss: -0.2387\n",
      "Epoch 135/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.3794 - val_loss: -0.2291\n",
      "Epoch 136/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3783 - val_loss: -0.2249\n",
      "Epoch 137/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3824 - val_loss: -0.2156\n",
      "Epoch 138/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3828 - val_loss: -0.2078\n",
      "Epoch 139/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3852 - val_loss: -0.2275\n",
      "Epoch 140/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3879 - val_loss: -0.2195\n",
      "Epoch 141/10000\n",
      "305180/305180 [==============================] - 7s 25us/sample - loss: -0.3902 - val_loss: -0.2099\n",
      "Epoch 142/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3888 - val_loss: -0.2060\n",
      "Epoch 143/10000\n",
      "305180/305180 [==============================] - 7s 25us/sample - loss: -0.3920 - val_loss: -0.2120\n",
      "Epoch 144/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3963 - val_loss: -0.2040\n",
      "Epoch 145/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3972 - val_loss: -0.2036\n",
      "Epoch 146/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3979 - val_loss: -0.1963\n",
      "Epoch 147/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.3983 - val_loss: -0.2076\n",
      "Epoch 148/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.4017 - val_loss: -0.1868\n",
      "Epoch 149/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.3993 - val_loss: -0.1797\n",
      "Epoch 150/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4059 - val_loss: -0.1906\n",
      "Epoch 151/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4080 - val_loss: -0.1750\n",
      "Epoch 152/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4129 - val_loss: -0.1864\n",
      "Epoch 153/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4073 - val_loss: -0.1608\n",
      "Epoch 154/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4104 - val_loss: -0.1710\n",
      "Epoch 155/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4160 - val_loss: -0.1763\n",
      "Epoch 156/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4198 - val_loss: -0.1746\n",
      "Epoch 157/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4202 - val_loss: -0.1666\n",
      "Epoch 158/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4236 - val_loss: -0.1621\n",
      "Epoch 159/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4249 - val_loss: -0.1668\n",
      "Epoch 160/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4264 - val_loss: -0.1483\n",
      "Epoch 161/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4292 - val_loss: -0.1428\n",
      "Epoch 162/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4306 - val_loss: -0.1311\n",
      "Epoch 163/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4329 - val_loss: -0.1293\n",
      "Epoch 164/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4362 - val_loss: -0.1272\n",
      "Epoch 165/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4351 - val_loss: -0.1358\n",
      "Epoch 166/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4380 - val_loss: -0.1283\n",
      "Epoch 167/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4396 - val_loss: -0.1327\n",
      "Epoch 168/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4437 - val_loss: -0.1265\n",
      "Epoch 169/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4446 - val_loss: -0.1231\n",
      "Epoch 170/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4414 - val_loss: -0.1200\n",
      "Epoch 171/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4463 - val_loss: -0.1407\n",
      "Epoch 172/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4519 - val_loss: -0.1116\n",
      "Epoch 173/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4518 - val_loss: -0.0978\n",
      "Epoch 174/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4518 - val_loss: -0.1013\n",
      "Epoch 175/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4567 - val_loss: -0.1102\n",
      "Epoch 176/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4600 - val_loss: -0.0974\n",
      "Epoch 177/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4620 - val_loss: -0.0785\n",
      "Epoch 178/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4651 - val_loss: -0.1151\n",
      "Epoch 179/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4655 - val_loss: -0.0584\n",
      "Epoch 180/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4696 - val_loss: -0.0869\n",
      "Epoch 181/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.4656 - val_loss: -0.1007\n",
      "Epoch 182/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4637 - val_loss: -0.0535\n",
      "Epoch 183/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4733 - val_loss: -0.0755\n",
      "Epoch 184/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4774 - val_loss: -0.0718\n",
      "Epoch 185/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4768 - val_loss: -0.0580\n",
      "Epoch 186/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4786 - val_loss: -0.0622\n",
      "Epoch 187/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4834 - val_loss: -0.0373\n",
      "Epoch 188/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4830 - val_loss: -0.0389\n",
      "Epoch 189/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4872 - val_loss: -0.0102\n",
      "Epoch 190/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4869 - val_loss: -0.0412\n",
      "Epoch 191/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4922 - val_loss: -0.0277\n",
      "Epoch 192/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.4923 - val_loss: -0.0572\n",
      "Epoch 193/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4942 - val_loss: -0.0385\n",
      "Epoch 194/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4906 - val_loss: -0.0191\n",
      "Epoch 195/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5000 - val_loss: 0.0110\n",
      "Epoch 196/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.4995 - val_loss: -0.0127\n",
      "Epoch 197/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5035 - val_loss: -0.0320\n",
      "Epoch 198/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5070 - val_loss: -0.0058\n",
      "Epoch 199/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5005 - val_loss: -0.0174\n",
      "Epoch 200/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5087 - val_loss: 0.0020\n",
      "Epoch 201/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5073 - val_loss: 0.0268\n",
      "Epoch 202/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5131 - val_loss: 0.0188\n",
      "Epoch 203/10000\n",
      "305180/305180 [==============================] - 8s 26us/sample - loss: -0.5179 - val_loss: 0.0258\n",
      "Epoch 204/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.5184 - val_loss: 0.0167\n",
      "Epoch 205/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5184 - val_loss: 0.0268\n",
      "Epoch 206/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5190 - val_loss: 0.0095\n",
      "Epoch 207/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5237 - val_loss: 0.0324\n",
      "Epoch 208/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5238 - val_loss: 0.0295\n",
      "Epoch 209/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5233 - val_loss: 0.0284\n",
      "Epoch 210/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5318 - val_loss: 0.0382\n",
      "Epoch 211/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5312 - val_loss: 0.0244\n",
      "Epoch 212/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5293 - val_loss: 0.0773\n",
      "Epoch 213/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5327 - val_loss: 0.0589\n",
      "Epoch 214/10000\n",
      "305180/305180 [==============================] - 7s 25us/sample - loss: -0.5362 - val_loss: 0.0609\n",
      "Epoch 215/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5408 - val_loss: 0.0894\n",
      "Epoch 216/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5413 - val_loss: 0.0803\n",
      "Epoch 217/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5428 - val_loss: 0.0756\n",
      "Epoch 218/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5337 - val_loss: 0.0579\n",
      "Epoch 219/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5351 - val_loss: 0.0666\n",
      "Epoch 220/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5522 - val_loss: 0.0748\n",
      "Epoch 221/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5517 - val_loss: 0.1107\n",
      "Epoch 222/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5547 - val_loss: 0.0732\n",
      "Epoch 223/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5545 - val_loss: 0.1048\n",
      "Epoch 224/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5584 - val_loss: 0.0924\n",
      "Epoch 225/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5503 - val_loss: 0.0857\n",
      "Epoch 226/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5611 - val_loss: 0.1344\n",
      "Epoch 227/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5561 - val_loss: 0.0963\n",
      "Epoch 228/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5480 - val_loss: 0.1257\n",
      "Epoch 229/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5625 - val_loss: 0.0938\n",
      "Epoch 230/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5590 - val_loss: 0.1327\n",
      "Epoch 231/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.5708 - val_loss: 0.1281\n",
      "Epoch 232/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5740 - val_loss: 0.1649\n",
      "Epoch 233/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.5735 - val_loss: 0.1367\n",
      "Epoch 234/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5772 - val_loss: 0.1495\n",
      "Epoch 235/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5789 - val_loss: 0.1583\n",
      "Epoch 236/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5786 - val_loss: 0.1372\n",
      "Epoch 237/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5778 - val_loss: 0.1733\n",
      "Epoch 238/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5822 - val_loss: 0.1666\n",
      "Epoch 239/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5868 - val_loss: 0.1577\n",
      "Epoch 240/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5843 - val_loss: 0.1837\n",
      "Epoch 241/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5904 - val_loss: 0.1933\n",
      "Epoch 242/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5848 - val_loss: 0.1840\n",
      "Epoch 243/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5838 - val_loss: 0.2296\n",
      "Epoch 244/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.5859 - val_loss: 0.1784\n",
      "Epoch 245/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.5962 - val_loss: 0.1911\n",
      "Epoch 246/10000\n",
      "305180/305180 [==============================] - 8s 25us/sample - loss: -0.5989 - val_loss: 0.2208\n",
      "Epoch 247/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6033 - val_loss: 0.2214\n",
      "Epoch 248/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6030 - val_loss: 0.2186\n",
      "Epoch 249/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6016 - val_loss: 0.1969\n",
      "Epoch 250/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6021 - val_loss: 0.2158\n",
      "Epoch 251/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6053 - val_loss: 0.2293\n",
      "Epoch 252/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6076 - val_loss: 0.2244\n",
      "Epoch 253/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6083 - val_loss: 0.2220\n",
      "Epoch 254/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6133 - val_loss: 0.2921\n",
      "Epoch 255/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6135 - val_loss: 0.2544\n",
      "Epoch 256/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6171 - val_loss: 0.2508\n",
      "Epoch 257/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6151 - val_loss: 0.2484\n",
      "Epoch 258/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6180 - val_loss: 0.2510\n",
      "Epoch 259/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6176 - val_loss: 0.2455\n",
      "Epoch 260/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6179 - val_loss: 0.2948\n",
      "Epoch 261/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6236 - val_loss: 0.2643\n",
      "Epoch 262/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6283 - val_loss: 0.3277\n",
      "Epoch 263/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6249 - val_loss: 0.2563\n",
      "Epoch 264/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6280 - val_loss: 0.2861\n",
      "Epoch 265/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6301 - val_loss: 0.3128\n",
      "Epoch 266/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6332 - val_loss: 0.3390\n",
      "Epoch 267/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6327 - val_loss: 0.3203\n",
      "Epoch 268/10000\n",
      "305180/305180 [==============================] - 7s 23us/sample - loss: -0.6335 - val_loss: 0.3511\n",
      "Epoch 269/10000\n",
      "305180/305180 [==============================] - 7s 24us/sample - loss: -0.6410 - val_loss: 0.3488\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5492, 18, 1)\n",
      "y_test.shape:  (5492, 1)\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:54:44,427 WARNING Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7771, 18, 1)\n",
      "y_test.shape:  (7771, 1)\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:54:53,007 WARNING Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7935, 18, 1)\n",
      "y_test.shape:  (7935, 1)\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:02,553 WARNING Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6505, 18, 1)\n",
      "y_test.shape:  (6505, 1)\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:12,774 WARNING Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7319, 18, 1)\n",
      "y_test.shape:  (7319, 1)\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:21,913 WARNING Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7614, 18, 1)\n",
      "y_test.shape:  (7614, 1)\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:31,521 WARNING Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7593, 18, 1)\n",
      "y_test.shape:  (7593, 1)\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:41,113 WARNING Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7802, 18, 1)\n",
      "y_test.shape:  (7802, 1)\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:55:50,789 WARNING Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7545, 18, 1)\n",
      "y_test.shape:  (7545, 1)\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:56:00,413 WARNING Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7867, 18, 1)\n",
      "y_test.shape:  (7867, 1)\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:56:09,653 WARNING Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 18,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_18sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 18, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 18, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7826, 18, 1)\n",
      "y_test.shape:  (7826, 1)\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:56:19,200 WARNING Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_18sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1726.csv\n",
      "2025-01-21 19:56:28,123 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "2025-01-21 19:56:28,123 ERROR c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (292759, 24, 1)\n",
      "y_train.shape:  (292759, 1)\n",
      "x_valid.shape:  (73171, 24, 1)\n",
      "y_valid.shape:  (73171, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 19:57:13,015 WARNING Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 19:57:13,140 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 292759 samples, validate on 73171 samples\n",
      "Epoch 1/10000\n",
      "292759/292759 [==============================] - ETA: 0s - loss: 0.1434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292759/292759 [==============================] - 14s 48us/sample - loss: 0.1434 - val_loss: -0.1456\n",
      "Epoch 2/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.0632 - val_loss: -0.1702\n",
      "Epoch 3/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.1066 - val_loss: -0.2226\n",
      "Epoch 4/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.1361 - val_loss: -0.2276\n",
      "Epoch 5/10000\n",
      "292759/292759 [==============================] - 12s 40us/sample - loss: -0.1600 - val_loss: -0.2440\n",
      "Epoch 6/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.1795 - val_loss: -0.2202\n",
      "Epoch 7/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.1879 - val_loss: -0.2626\n",
      "Epoch 8/10000\n",
      "292759/292759 [==============================] - 8s 27us/sample - loss: -0.2030 - val_loss: -0.2594\n",
      "Epoch 9/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2131 - val_loss: -0.2724\n",
      "Epoch 10/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.2224 - val_loss: -0.2684\n",
      "Epoch 11/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2280 - val_loss: -0.2684\n",
      "Epoch 12/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2357 - val_loss: -0.2514\n",
      "Epoch 13/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2378 - val_loss: -0.2755\n",
      "Epoch 14/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2475 - val_loss: -0.2663\n",
      "Epoch 15/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2530 - val_loss: -0.2850\n",
      "Epoch 16/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2560 - val_loss: -0.2746\n",
      "Epoch 17/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2617 - val_loss: -0.2849\n",
      "Epoch 18/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2636 - val_loss: -0.2874\n",
      "Epoch 19/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.2689 - val_loss: -0.2822\n",
      "Epoch 20/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.2699 - val_loss: -0.2885\n",
      "Epoch 21/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.2726 - val_loss: -0.2805\n",
      "Epoch 22/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2777 - val_loss: -0.2911\n",
      "Epoch 23/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2777 - val_loss: -0.2964\n",
      "Epoch 24/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2822 - val_loss: -0.2959\n",
      "Epoch 25/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2838 - val_loss: -0.2874\n",
      "Epoch 26/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.2852 - val_loss: -0.2959\n",
      "Epoch 27/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2853 - val_loss: -0.2939\n",
      "Epoch 28/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2892 - val_loss: -0.2932\n",
      "Epoch 29/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2893 - val_loss: -0.2942\n",
      "Epoch 30/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.2907 - val_loss: -0.2910\n",
      "Epoch 31/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2927 - val_loss: -0.2938\n",
      "Epoch 32/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.2939 - val_loss: -0.2943\n",
      "Epoch 33/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.2951 - val_loss: -0.2993\n",
      "Epoch 34/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.2956 - val_loss: -0.2965\n",
      "Epoch 35/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.2957 - val_loss: -0.2896\n",
      "Epoch 36/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.2956 - val_loss: -0.2989\n",
      "Epoch 37/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.2981 - val_loss: -0.2969\n",
      "Epoch 38/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3014 - val_loss: -0.2941\n",
      "Epoch 39/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3011 - val_loss: -0.2916\n",
      "Epoch 40/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3021 - val_loss: -0.2972\n",
      "Epoch 41/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3027 - val_loss: -0.2905\n",
      "Epoch 42/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3021 - val_loss: -0.2981\n",
      "Epoch 43/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3026 - val_loss: -0.3022\n",
      "Epoch 44/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3059 - val_loss: -0.2950\n",
      "Epoch 45/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3044 - val_loss: -0.3004\n",
      "Epoch 46/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3073 - val_loss: -0.2911\n",
      "Epoch 47/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3069 - val_loss: -0.2990\n",
      "Epoch 48/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3050 - val_loss: -0.2930\n",
      "Epoch 49/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3065 - val_loss: -0.2985\n",
      "Epoch 50/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3087 - val_loss: -0.2997\n",
      "Epoch 51/10000\n",
      "292759/292759 [==============================] - 8s 28us/sample - loss: -0.3105 - val_loss: -0.3002\n",
      "Epoch 52/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3093 - val_loss: -0.2952\n",
      "Epoch 53/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3090 - val_loss: -0.3017\n",
      "Epoch 54/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3128 - val_loss: -0.2984\n",
      "Epoch 55/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3134 - val_loss: -0.3003\n",
      "Epoch 56/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3131 - val_loss: -0.3011\n",
      "Epoch 57/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3133 - val_loss: -0.2982\n",
      "Epoch 58/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3147 - val_loss: -0.3008\n",
      "Epoch 59/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3154 - val_loss: -0.3014\n",
      "Epoch 60/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3155 - val_loss: -0.2956\n",
      "Epoch 61/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3168 - val_loss: -0.2932\n",
      "Epoch 62/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3189 - val_loss: -0.2958\n",
      "Epoch 63/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3193 - val_loss: -0.2956\n",
      "Epoch 64/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3205 - val_loss: -0.2956\n",
      "Epoch 65/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3210 - val_loss: -0.2979\n",
      "Epoch 66/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3228 - val_loss: -0.2855\n",
      "Epoch 67/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3224 - val_loss: -0.2998\n",
      "Epoch 68/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3246 - val_loss: -0.2981\n",
      "Epoch 69/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3276 - val_loss: -0.2942\n",
      "Epoch 70/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3284 - val_loss: -0.2979\n",
      "Epoch 71/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3285 - val_loss: -0.2940\n",
      "Epoch 72/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3296 - val_loss: -0.2919\n",
      "Epoch 73/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3309 - val_loss: -0.2899\n",
      "Epoch 74/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3328 - val_loss: -0.2959\n",
      "Epoch 75/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3338 - val_loss: -0.2961\n",
      "Epoch 76/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3356 - val_loss: -0.2928\n",
      "Epoch 77/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3356 - val_loss: -0.2917\n",
      "Epoch 78/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3373 - val_loss: -0.2854\n",
      "Epoch 79/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3395 - val_loss: -0.2850\n",
      "Epoch 80/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3415 - val_loss: -0.2899\n",
      "Epoch 81/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3417 - val_loss: -0.2844\n",
      "Epoch 82/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3449 - val_loss: -0.2839\n",
      "Epoch 83/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.3464 - val_loss: -0.2830\n",
      "Epoch 84/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3490 - val_loss: -0.2803\n",
      "Epoch 85/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3490 - val_loss: -0.2829\n",
      "Epoch 86/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3514 - val_loss: -0.2752\n",
      "Epoch 87/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.3537 - val_loss: -0.2771\n",
      "Epoch 88/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3557 - val_loss: -0.2795\n",
      "Epoch 89/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3583 - val_loss: -0.2591\n",
      "Epoch 90/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3591 - val_loss: -0.2681\n",
      "Epoch 91/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3630 - val_loss: -0.2663\n",
      "Epoch 92/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3652 - val_loss: -0.2521\n",
      "Epoch 93/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3674 - val_loss: -0.2612\n",
      "Epoch 94/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3728 - val_loss: -0.2588\n",
      "Epoch 95/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3740 - val_loss: -0.2551\n",
      "Epoch 96/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3757 - val_loss: -0.2667\n",
      "Epoch 97/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3787 - val_loss: -0.2575\n",
      "Epoch 98/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3813 - val_loss: -0.2442\n",
      "Epoch 99/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3834 - val_loss: -0.2395\n",
      "Epoch 100/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3896 - val_loss: -0.2302\n",
      "Epoch 101/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.3917 - val_loss: -0.2424\n",
      "Epoch 102/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3940 - val_loss: -0.2335\n",
      "Epoch 103/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.3976 - val_loss: -0.2280\n",
      "Epoch 104/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4012 - val_loss: -0.2160\n",
      "Epoch 105/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4031 - val_loss: -0.2200\n",
      "Epoch 106/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4078 - val_loss: -0.1983\n",
      "Epoch 107/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4100 - val_loss: -0.2130\n",
      "Epoch 108/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4147 - val_loss: -0.1939\n",
      "Epoch 109/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4180 - val_loss: -0.1858\n",
      "Epoch 110/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4229 - val_loss: -0.1943\n",
      "Epoch 111/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4265 - val_loss: -0.1933\n",
      "Epoch 112/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4299 - val_loss: -0.1822\n",
      "Epoch 113/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4334 - val_loss: -0.1843\n",
      "Epoch 114/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4369 - val_loss: -0.1569\n",
      "Epoch 115/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.4415 - val_loss: -0.1529\n",
      "Epoch 116/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4441 - val_loss: -0.1408\n",
      "Epoch 117/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4497 - val_loss: -0.1228\n",
      "Epoch 118/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4539 - val_loss: -0.1497\n",
      "Epoch 119/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4565 - val_loss: -0.1295\n",
      "Epoch 120/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4608 - val_loss: -0.1299\n",
      "Epoch 121/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4650 - val_loss: -0.1352\n",
      "Epoch 122/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4682 - val_loss: -0.1229\n",
      "Epoch 123/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4720 - val_loss: -0.1216\n",
      "Epoch 124/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.4773 - val_loss: -0.1077\n",
      "Epoch 125/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4806 - val_loss: -0.0884\n",
      "Epoch 126/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4859 - val_loss: -0.0686\n",
      "Epoch 127/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4895 - val_loss: -0.0599\n",
      "Epoch 128/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.4928 - val_loss: -0.0639\n",
      "Epoch 129/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.4983 - val_loss: -0.0721\n",
      "Epoch 130/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5001 - val_loss: -0.0489\n",
      "Epoch 131/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5056 - val_loss: -0.0140\n",
      "Epoch 132/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.5102 - val_loss: -0.0345\n",
      "Epoch 133/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5127 - val_loss: -0.0299\n",
      "Epoch 134/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5193 - val_loss: 0.0022\n",
      "Epoch 135/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5232 - val_loss: -0.0294\n",
      "Epoch 136/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5261 - val_loss: 0.0140\n",
      "Epoch 137/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5286 - val_loss: 0.0223\n",
      "Epoch 138/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5345 - val_loss: 0.0350\n",
      "Epoch 139/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.5378 - val_loss: 0.0380\n",
      "Epoch 140/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5436 - val_loss: 0.0476\n",
      "Epoch 141/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5466 - val_loss: 0.0489\n",
      "Epoch 142/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.5508 - val_loss: 0.0300\n",
      "Epoch 143/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.5532 - val_loss: 0.0394\n",
      "Epoch 144/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.5588 - val_loss: 0.0506\n",
      "Epoch 145/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5602 - val_loss: 0.0662\n",
      "Epoch 146/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5673 - val_loss: 0.0913\n",
      "Epoch 147/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5681 - val_loss: 0.1217\n",
      "Epoch 148/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.5722 - val_loss: 0.1526\n",
      "Epoch 149/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5773 - val_loss: 0.0987\n",
      "Epoch 150/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5820 - val_loss: 0.1571\n",
      "Epoch 151/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5860 - val_loss: 0.1817\n",
      "Epoch 152/10000\n",
      "292759/292759 [==============================] - 9s 32us/sample - loss: -0.5902 - val_loss: 0.1723\n",
      "Epoch 153/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5931 - val_loss: 0.1725\n",
      "Epoch 154/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.5984 - val_loss: 0.2091\n",
      "Epoch 155/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6003 - val_loss: 0.1858\n",
      "Epoch 156/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.6046 - val_loss: 0.1902\n",
      "Epoch 157/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6112 - val_loss: 0.1980\n",
      "Epoch 158/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6116 - val_loss: 0.2459\n",
      "Epoch 159/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6170 - val_loss: 0.2194\n",
      "Epoch 160/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.6212 - val_loss: 0.2208\n",
      "Epoch 161/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6237 - val_loss: 0.2614\n",
      "Epoch 162/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.6277 - val_loss: 0.2927\n",
      "Epoch 163/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6329 - val_loss: 0.2524\n",
      "Epoch 164/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6342 - val_loss: 0.3144\n",
      "Epoch 165/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6388 - val_loss: 0.3077\n",
      "Epoch 166/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6410 - val_loss: 0.3023\n",
      "Epoch 167/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6459 - val_loss: 0.3258\n",
      "Epoch 168/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.6489 - val_loss: 0.3370\n",
      "Epoch 169/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.6548 - val_loss: 0.3154\n",
      "Epoch 170/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6565 - val_loss: 0.3330\n",
      "Epoch 171/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6626 - val_loss: 0.3524\n",
      "Epoch 172/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6619 - val_loss: 0.3635\n",
      "Epoch 173/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6685 - val_loss: 0.3413\n",
      "Epoch 174/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6704 - val_loss: 0.4200\n",
      "Epoch 175/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6752 - val_loss: 0.4039\n",
      "Epoch 176/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6791 - val_loss: 0.3934\n",
      "Epoch 177/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6802 - val_loss: 0.4343\n",
      "Epoch 178/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6866 - val_loss: 0.4455\n",
      "Epoch 179/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6877 - val_loss: 0.4652\n",
      "Epoch 180/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6923 - val_loss: 0.4740\n",
      "Epoch 181/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.6934 - val_loss: 0.4939\n",
      "Epoch 182/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.6976 - val_loss: 0.4479\n",
      "Epoch 183/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.7004 - val_loss: 0.4187\n",
      "Epoch 184/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7055 - val_loss: 0.4827\n",
      "Epoch 185/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.7083 - val_loss: 0.4941\n",
      "Epoch 186/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7125 - val_loss: 0.5407\n",
      "Epoch 187/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7151 - val_loss: 0.5392\n",
      "Epoch 188/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7174 - val_loss: 0.5396\n",
      "Epoch 189/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7207 - val_loss: 0.5352\n",
      "Epoch 190/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7227 - val_loss: 0.5443\n",
      "Epoch 191/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7277 - val_loss: 0.5579\n",
      "Epoch 192/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7311 - val_loss: 0.5938\n",
      "Epoch 193/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7347 - val_loss: 0.5868\n",
      "Epoch 194/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7371 - val_loss: 0.5610\n",
      "Epoch 195/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7368 - val_loss: 0.6178\n",
      "Epoch 196/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7435 - val_loss: 0.6076\n",
      "Epoch 197/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7474 - val_loss: 0.7040\n",
      "Epoch 198/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7466 - val_loss: 0.5980\n",
      "Epoch 199/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.7510 - val_loss: 0.6879\n",
      "Epoch 200/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7556 - val_loss: 0.6672\n",
      "Epoch 201/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7552 - val_loss: 0.6963\n",
      "Epoch 202/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7624 - val_loss: 0.7088\n",
      "Epoch 203/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7656 - val_loss: 0.6771\n",
      "Epoch 204/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7662 - val_loss: 0.7697\n",
      "Epoch 205/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7700 - val_loss: 0.7474\n",
      "Epoch 206/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7740 - val_loss: 0.7131\n",
      "Epoch 207/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.7756 - val_loss: 0.7159\n",
      "Epoch 208/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.7778 - val_loss: 0.6873\n",
      "Epoch 209/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7849 - val_loss: 0.7678\n",
      "Epoch 210/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7850 - val_loss: 0.7425\n",
      "Epoch 211/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7865 - val_loss: 0.7552\n",
      "Epoch 212/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7886 - val_loss: 0.7578\n",
      "Epoch 213/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.7934 - val_loss: 0.7895\n",
      "Epoch 214/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.7957 - val_loss: 0.8744\n",
      "Epoch 215/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8014 - val_loss: 0.8186\n",
      "Epoch 216/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8019 - val_loss: 0.8522\n",
      "Epoch 217/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8056 - val_loss: 0.8458\n",
      "Epoch 218/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8049 - val_loss: 0.8114\n",
      "Epoch 219/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8103 - val_loss: 0.8633\n",
      "Epoch 220/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8145 - val_loss: 0.8692\n",
      "Epoch 221/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8164 - val_loss: 0.8925\n",
      "Epoch 222/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.8178 - val_loss: 0.8790\n",
      "Epoch 223/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8201 - val_loss: 0.9112\n",
      "Epoch 224/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.8233 - val_loss: 0.9706\n",
      "Epoch 225/10000\n",
      "292759/292759 [==============================] - 8s 29us/sample - loss: -0.8252 - val_loss: 0.8973\n",
      "Epoch 226/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8279 - val_loss: 0.8605\n",
      "Epoch 227/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8299 - val_loss: 0.9365\n",
      "Epoch 228/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8358 - val_loss: 0.9999\n",
      "Epoch 229/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8339 - val_loss: 0.9115\n",
      "Epoch 230/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8404 - val_loss: 0.9478\n",
      "Epoch 231/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8437 - val_loss: 0.9717\n",
      "Epoch 232/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8445 - val_loss: 0.9545\n",
      "Epoch 233/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8447 - val_loss: 1.0753\n",
      "Epoch 234/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8500 - val_loss: 0.9666\n",
      "Epoch 235/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8516 - val_loss: 1.0146\n",
      "Epoch 236/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8528 - val_loss: 1.1182\n",
      "Epoch 237/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8604 - val_loss: 0.9856\n",
      "Epoch 238/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8598 - val_loss: 1.0423\n",
      "Epoch 239/10000\n",
      "292759/292759 [==============================] - 9s 29us/sample - loss: -0.8611 - val_loss: 1.0640\n",
      "Epoch 240/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8623 - val_loss: 1.1016\n",
      "Epoch 241/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8667 - val_loss: 1.0601\n",
      "Epoch 242/10000\n",
      "292759/292759 [==============================] - 9s 31us/sample - loss: -0.8679 - val_loss: 1.1285\n",
      "Epoch 243/10000\n",
      "292759/292759 [==============================] - 9s 30us/sample - loss: -0.8718 - val_loss: 1.1865\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4062, 24, 1)\n",
      "y_test.shape:  (4062, 1)\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:32:18,283 WARNING Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7532, 24, 1)\n",
      "y_test.shape:  (7532, 1)\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:32:27,352 WARNING Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7823, 24, 1)\n",
      "y_test.shape:  (7823, 1)\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:32:38,109 WARNING Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7916, 24, 1)\n",
      "y_test.shape:  (7916, 1)\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:32:49,258 WARNING Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7384, 24, 1)\n",
      "y_test.shape:  (7384, 1)\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:32:59,554 WARNING Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7729, 24, 1)\n",
      "y_test.shape:  (7729, 1)\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:33:09,484 WARNING Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7626, 24, 1)\n",
      "y_test.shape:  (7626, 1)\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:33:19,800 WARNING Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7745, 24, 1)\n",
      "y_test.shape:  (7745, 1)\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:33:30,026 WARNING Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7872, 24, 1)\n",
      "y_test.shape:  (7872, 1)\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:33:40,075 WARNING Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7669, 24, 1)\n",
      "y_test.shape:  (7669, 1)\n",
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:33:50,682 WARNING Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (3805, 24, 1)\n",
      "y_test.shape:  (3805, 1)\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:34:00,675 WARNING Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7458, 24, 1)\n",
      "y_test.shape:  (7458, 1)\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:34:09,159 WARNING Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7117, 24, 1)\n",
      "y_test.shape:  (7117, 1)\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:34:19,184 WARNING Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  25.csv\n",
      "2025-01-21 20:34:28,289 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold2_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold2_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (291604, 24, 1)\n",
      "y_train.shape:  (291604, 1)\n",
      "x_valid.shape:  (72879, 24, 1)\n",
      "y_valid.shape:  (72879, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 20:35:12,202 WARNING Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 20:35:12,335 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 291604 samples, validate on 72879 samples\n",
      "Epoch 1/10000\n",
      "290816/291604 [============================>.] - ETA: 0s - loss: 0.2064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291604/291604 [==============================] - 14s 49us/sample - loss: 0.2059 - val_loss: -0.1083\n",
      "Epoch 2/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.0362 - val_loss: -0.1832\n",
      "Epoch 3/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.0900 - val_loss: -0.1846\n",
      "Epoch 4/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.1167 - val_loss: -0.1974\n",
      "Epoch 5/10000\n",
      "291604/291604 [==============================] - 12s 40us/sample - loss: -0.1360 - val_loss: -0.1814\n",
      "Epoch 6/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.1552 - val_loss: -0.2349\n",
      "Epoch 7/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.1704 - val_loss: -0.2522\n",
      "Epoch 8/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.1831 - val_loss: -0.2265\n",
      "Epoch 9/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.1978 - val_loss: -0.2442\n",
      "Epoch 10/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2060 - val_loss: -0.2473\n",
      "Epoch 11/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2097 - val_loss: -0.2593\n",
      "Epoch 12/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.2180 - val_loss: -0.2576\n",
      "Epoch 13/10000\n",
      "291604/291604 [==============================] - 8s 27us/sample - loss: -0.2248 - val_loss: -0.2605\n",
      "Epoch 14/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2289 - val_loss: -0.2659\n",
      "Epoch 15/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2353 - val_loss: -0.2712\n",
      "Epoch 16/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2411 - val_loss: -0.2667\n",
      "Epoch 17/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2429 - val_loss: -0.2704\n",
      "Epoch 18/10000\n",
      "291604/291604 [==============================] - 8s 28us/sample - loss: -0.2483 - val_loss: -0.2717\n",
      "Epoch 19/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2512 - val_loss: -0.2751\n",
      "Epoch 20/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2539 - val_loss: -0.2768\n",
      "Epoch 21/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2559 - val_loss: -0.2772\n",
      "Epoch 22/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2597 - val_loss: -0.2732\n",
      "Epoch 23/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2612 - val_loss: -0.2745\n",
      "Epoch 24/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2636 - val_loss: -0.2768\n",
      "Epoch 25/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2656 - val_loss: -0.2795\n",
      "Epoch 26/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2675 - val_loss: -0.2812\n",
      "Epoch 27/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2680 - val_loss: -0.2769\n",
      "Epoch 28/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2702 - val_loss: -0.2773\n",
      "Epoch 29/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2719 - val_loss: -0.2628\n",
      "Epoch 30/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2725 - val_loss: -0.2777\n",
      "Epoch 31/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2737 - val_loss: -0.2824\n",
      "Epoch 32/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2752 - val_loss: -0.2778\n",
      "Epoch 33/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2775 - val_loss: -0.2753\n",
      "Epoch 34/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2796 - val_loss: -0.2828\n",
      "Epoch 35/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2798 - val_loss: -0.2864\n",
      "Epoch 36/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2800 - val_loss: -0.2855\n",
      "Epoch 37/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2827 - val_loss: -0.2828\n",
      "Epoch 38/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2808 - val_loss: -0.2717\n",
      "Epoch 39/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2828 - val_loss: -0.2786\n",
      "Epoch 40/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2837 - val_loss: -0.2852\n",
      "Epoch 41/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2858 - val_loss: -0.2877\n",
      "Epoch 42/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2851 - val_loss: -0.2857\n",
      "Epoch 43/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2865 - val_loss: -0.2774\n",
      "Epoch 44/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2860 - val_loss: -0.2810\n",
      "Epoch 45/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.2872 - val_loss: -0.2865\n",
      "Epoch 46/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2890 - val_loss: -0.2852\n",
      "Epoch 47/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2897 - val_loss: -0.2845\n",
      "Epoch 48/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.2909 - val_loss: -0.2829\n",
      "Epoch 49/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2919 - val_loss: -0.2815\n",
      "Epoch 50/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2923 - val_loss: -0.2854\n",
      "Epoch 51/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2924 - val_loss: -0.2834\n",
      "Epoch 52/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2946 - val_loss: -0.2848\n",
      "Epoch 53/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2943 - val_loss: -0.2891\n",
      "Epoch 54/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2955 - val_loss: -0.2861\n",
      "Epoch 55/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2959 - val_loss: -0.2843\n",
      "Epoch 56/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2970 - val_loss: -0.2837\n",
      "Epoch 57/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2987 - val_loss: -0.2875\n",
      "Epoch 58/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.2973 - val_loss: -0.2837\n",
      "Epoch 59/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.2997 - val_loss: -0.2801\n",
      "Epoch 60/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3014 - val_loss: -0.2809\n",
      "Epoch 61/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3021 - val_loss: -0.2815\n",
      "Epoch 62/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3031 - val_loss: -0.2801\n",
      "Epoch 63/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3031 - val_loss: -0.2738\n",
      "Epoch 64/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.3045 - val_loss: -0.2822\n",
      "Epoch 65/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3049 - val_loss: -0.2829\n",
      "Epoch 66/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3059 - val_loss: -0.2831\n",
      "Epoch 67/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3067 - val_loss: -0.2807\n",
      "Epoch 68/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3078 - val_loss: -0.2768\n",
      "Epoch 69/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3098 - val_loss: -0.2740\n",
      "Epoch 70/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3107 - val_loss: -0.2807\n",
      "Epoch 71/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3120 - val_loss: -0.2753\n",
      "Epoch 72/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.3137 - val_loss: -0.2757\n",
      "Epoch 73/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3130 - val_loss: -0.2773\n",
      "Epoch 74/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3156 - val_loss: -0.2783\n",
      "Epoch 75/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3163 - val_loss: -0.2729\n",
      "Epoch 76/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3185 - val_loss: -0.2793\n",
      "Epoch 77/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3189 - val_loss: -0.2773\n",
      "Epoch 78/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3189 - val_loss: -0.2800\n",
      "Epoch 79/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3220 - val_loss: -0.2756\n",
      "Epoch 80/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3233 - val_loss: -0.2746\n",
      "Epoch 81/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3238 - val_loss: -0.2724\n",
      "Epoch 82/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3264 - val_loss: -0.2734\n",
      "Epoch 83/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.3283 - val_loss: -0.2703\n",
      "Epoch 84/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3296 - val_loss: -0.2756\n",
      "Epoch 85/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3308 - val_loss: -0.2718\n",
      "Epoch 86/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3323 - val_loss: -0.2634\n",
      "Epoch 87/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3352 - val_loss: -0.2702\n",
      "Epoch 88/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3362 - val_loss: -0.2375\n",
      "Epoch 89/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3376 - val_loss: -0.2602\n",
      "Epoch 90/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3400 - val_loss: -0.2536\n",
      "Epoch 91/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3418 - val_loss: -0.2580\n",
      "Epoch 92/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3436 - val_loss: -0.2539\n",
      "Epoch 93/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.3445 - val_loss: -0.2503\n",
      "Epoch 94/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3491 - val_loss: -0.2575\n",
      "Epoch 95/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3513 - val_loss: -0.2445\n",
      "Epoch 96/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3529 - val_loss: -0.2450\n",
      "Epoch 97/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3537 - val_loss: -0.2396\n",
      "Epoch 98/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3577 - val_loss: -0.2402\n",
      "Epoch 99/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3598 - val_loss: -0.2395\n",
      "Epoch 100/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.3615 - val_loss: -0.2364\n",
      "Epoch 101/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3636 - val_loss: -0.2293\n",
      "Epoch 102/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3670 - val_loss: -0.2335\n",
      "Epoch 103/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3704 - val_loss: -0.2226\n",
      "Epoch 104/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3733 - val_loss: -0.2102\n",
      "Epoch 105/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3765 - val_loss: -0.2161\n",
      "Epoch 106/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.3791 - val_loss: -0.2125\n",
      "Epoch 107/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.3808 - val_loss: -0.2015\n",
      "Epoch 108/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.3838 - val_loss: -0.2186\n",
      "Epoch 109/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3881 - val_loss: -0.1950\n",
      "Epoch 110/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3909 - val_loss: -0.1949\n",
      "Epoch 111/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3924 - val_loss: -0.2038\n",
      "Epoch 112/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.3964 - val_loss: -0.1980\n",
      "Epoch 113/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4001 - val_loss: -0.1858\n",
      "Epoch 114/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4032 - val_loss: -0.1821\n",
      "Epoch 115/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.4086 - val_loss: -0.1621\n",
      "Epoch 116/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.4096 - val_loss: -0.1521\n",
      "Epoch 117/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4145 - val_loss: -0.1666\n",
      "Epoch 118/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4160 - val_loss: -0.1633\n",
      "Epoch 119/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4205 - val_loss: -0.1484\n",
      "Epoch 120/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.4225 - val_loss: -0.1520\n",
      "Epoch 121/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4267 - val_loss: -0.1327\n",
      "Epoch 122/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4306 - val_loss: -0.1203\n",
      "Epoch 123/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4334 - val_loss: -0.1234\n",
      "Epoch 124/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4379 - val_loss: -0.1151\n",
      "Epoch 125/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4419 - val_loss: -0.1302\n",
      "Epoch 126/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4455 - val_loss: -0.1126\n",
      "Epoch 127/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4485 - val_loss: -0.0939\n",
      "Epoch 128/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4523 - val_loss: -0.0977\n",
      "Epoch 129/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4579 - val_loss: -0.0850\n",
      "Epoch 130/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4600 - val_loss: -0.0870\n",
      "Epoch 131/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4634 - val_loss: -0.0839\n",
      "Epoch 132/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4659 - val_loss: -0.0670\n",
      "Epoch 133/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4734 - val_loss: -0.0658\n",
      "Epoch 134/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4757 - val_loss: -0.0403\n",
      "Epoch 135/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.4790 - val_loss: -0.0373\n",
      "Epoch 136/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4827 - val_loss: -0.0212\n",
      "Epoch 137/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4867 - val_loss: -0.0120\n",
      "Epoch 138/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4887 - val_loss: -0.0249\n",
      "Epoch 139/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4933 - val_loss: -0.0149\n",
      "Epoch 140/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.4990 - val_loss: 0.0155\n",
      "Epoch 141/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5021 - val_loss: 0.0011\n",
      "Epoch 142/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5062 - val_loss: 0.0250\n",
      "Epoch 143/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5097 - val_loss: 0.0406\n",
      "Epoch 144/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5128 - val_loss: 0.0248\n",
      "Epoch 145/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5185 - val_loss: 0.0398\n",
      "Epoch 146/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5223 - val_loss: 0.0502\n",
      "Epoch 147/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5267 - val_loss: 0.0575\n",
      "Epoch 148/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5296 - val_loss: 0.0510\n",
      "Epoch 149/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5332 - val_loss: 0.0648\n",
      "Epoch 150/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.5359 - val_loss: 0.0657\n",
      "Epoch 151/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5416 - val_loss: 0.0971\n",
      "Epoch 152/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.5429 - val_loss: 0.1293\n",
      "Epoch 153/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5466 - val_loss: 0.0959\n",
      "Epoch 154/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.5524 - val_loss: 0.1034\n",
      "Epoch 155/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5541 - val_loss: 0.1623\n",
      "Epoch 156/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5604 - val_loss: 0.1574\n",
      "Epoch 157/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.5617 - val_loss: 0.1486\n",
      "Epoch 158/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5663 - val_loss: 0.1775\n",
      "Epoch 159/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5697 - val_loss: 0.1695\n",
      "Epoch 160/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5720 - val_loss: 0.1880\n",
      "Epoch 161/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5774 - val_loss: 0.1526\n",
      "Epoch 162/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5802 - val_loss: 0.2067\n",
      "Epoch 163/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.5857 - val_loss: 0.2115\n",
      "Epoch 164/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.5863 - val_loss: 0.2247\n",
      "Epoch 165/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5916 - val_loss: 0.2076\n",
      "Epoch 166/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5964 - val_loss: 0.2569\n",
      "Epoch 167/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.5981 - val_loss: 0.2893\n",
      "Epoch 168/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6015 - val_loss: 0.2727\n",
      "Epoch 169/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6068 - val_loss: 0.2749\n",
      "Epoch 170/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6076 - val_loss: 0.2253\n",
      "Epoch 171/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.6125 - val_loss: 0.2730\n",
      "Epoch 172/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6152 - val_loss: 0.2932\n",
      "Epoch 173/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6184 - val_loss: 0.2910\n",
      "Epoch 174/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6226 - val_loss: 0.2925\n",
      "Epoch 175/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6232 - val_loss: 0.3038\n",
      "Epoch 176/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6275 - val_loss: 0.3243\n",
      "Epoch 177/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6313 - val_loss: 0.3474\n",
      "Epoch 178/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.6349 - val_loss: 0.3298\n",
      "Epoch 179/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6376 - val_loss: 0.3495\n",
      "Epoch 180/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6432 - val_loss: 0.3663\n",
      "Epoch 181/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6458 - val_loss: 0.3679\n",
      "Epoch 182/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.6482 - val_loss: 0.3574\n",
      "Epoch 183/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.6518 - val_loss: 0.3823\n",
      "Epoch 184/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.6567 - val_loss: 0.4022\n",
      "Epoch 185/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.6568 - val_loss: 0.3720\n",
      "Epoch 186/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6601 - val_loss: 0.4646\n",
      "Epoch 187/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6623 - val_loss: 0.4969\n",
      "Epoch 188/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6676 - val_loss: 0.4449\n",
      "Epoch 189/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.6693 - val_loss: 0.4896\n",
      "Epoch 190/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6730 - val_loss: 0.4798\n",
      "Epoch 191/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.6784 - val_loss: 0.4665\n",
      "Epoch 192/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6770 - val_loss: 0.4519\n",
      "Epoch 193/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6856 - val_loss: 0.4813\n",
      "Epoch 194/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6860 - val_loss: 0.5191\n",
      "Epoch 195/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6889 - val_loss: 0.5297\n",
      "Epoch 196/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.6899 - val_loss: 0.4653\n",
      "Epoch 197/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.6928 - val_loss: 0.5194\n",
      "Epoch 198/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.6977 - val_loss: 0.5930\n",
      "Epoch 199/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.7004 - val_loss: 0.5569\n",
      "Epoch 200/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7004 - val_loss: 0.5722\n",
      "Epoch 201/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7065 - val_loss: 0.5613\n",
      "Epoch 202/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.7083 - val_loss: 0.6797\n",
      "Epoch 203/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7114 - val_loss: 0.6494\n",
      "Epoch 204/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7142 - val_loss: 0.5573\n",
      "Epoch 205/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7169 - val_loss: 0.6133\n",
      "Epoch 206/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.7188 - val_loss: 0.6388\n",
      "Epoch 207/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7228 - val_loss: 0.6540\n",
      "Epoch 208/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7249 - val_loss: 0.6564\n",
      "Epoch 209/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7283 - val_loss: 0.6403\n",
      "Epoch 210/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7306 - val_loss: 0.5950\n",
      "Epoch 211/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7309 - val_loss: 0.6608\n",
      "Epoch 212/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7380 - val_loss: 0.6541\n",
      "Epoch 213/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7399 - val_loss: 0.6255\n",
      "Epoch 214/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7409 - val_loss: 0.6110\n",
      "Epoch 215/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7454 - val_loss: 0.6977\n",
      "Epoch 216/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.7450 - val_loss: 0.7004\n",
      "Epoch 217/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7487 - val_loss: 0.6658\n",
      "Epoch 218/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7517 - val_loss: 0.7701\n",
      "Epoch 219/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.7566 - val_loss: 0.7992\n",
      "Epoch 220/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7598 - val_loss: 0.7654\n",
      "Epoch 221/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7576 - val_loss: 0.7609\n",
      "Epoch 222/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7642 - val_loss: 0.7945\n",
      "Epoch 223/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7634 - val_loss: 0.7456\n",
      "Epoch 224/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7704 - val_loss: 0.7359\n",
      "Epoch 225/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.7708 - val_loss: 0.8218\n",
      "Epoch 226/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7735 - val_loss: 0.8296\n",
      "Epoch 227/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7743 - val_loss: 0.7926\n",
      "Epoch 228/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7790 - val_loss: 0.9351\n",
      "Epoch 229/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7820 - val_loss: 0.7844\n",
      "Epoch 230/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7814 - val_loss: 0.8496\n",
      "Epoch 231/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7866 - val_loss: 0.8672\n",
      "Epoch 232/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7902 - val_loss: 0.8458\n",
      "Epoch 233/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7901 - val_loss: 0.8766\n",
      "Epoch 234/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.7911 - val_loss: 0.9236\n",
      "Epoch 235/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.7942 - val_loss: 0.8841\n",
      "Epoch 236/10000\n",
      "291604/291604 [==============================] - 9s 29us/sample - loss: -0.7982 - val_loss: 0.8953\n",
      "Epoch 237/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8025 - val_loss: 0.9368\n",
      "Epoch 238/10000\n",
      "291604/291604 [==============================] - 8s 29us/sample - loss: -0.8034 - val_loss: 0.8972\n",
      "Epoch 239/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8021 - val_loss: 0.9223\n",
      "Epoch 240/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8073 - val_loss: 0.8898\n",
      "Epoch 241/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8115 - val_loss: 1.0101\n",
      "Epoch 242/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8121 - val_loss: 0.9241\n",
      "Epoch 243/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8167 - val_loss: 0.9817\n",
      "Epoch 244/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.8153 - val_loss: 0.9581\n",
      "Epoch 245/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.8196 - val_loss: 0.9644\n",
      "Epoch 246/10000\n",
      "291604/291604 [==============================] - 9s 32us/sample - loss: -0.8175 - val_loss: 0.9473\n",
      "Epoch 247/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.8240 - val_loss: 0.9530\n",
      "Epoch 248/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8247 - val_loss: 0.9521\n",
      "Epoch 249/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8285 - val_loss: 1.0372\n",
      "Epoch 250/10000\n",
      "291604/291604 [==============================] - 9s 31us/sample - loss: -0.8293 - val_loss: 0.9943\n",
      "Epoch 251/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8325 - val_loss: 1.0061\n",
      "Epoch 252/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8349 - val_loss: 0.9832\n",
      "Epoch 253/10000\n",
      "291604/291604 [==============================] - 9s 30us/sample - loss: -0.8351 - val_loss: 1.0226\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1010.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7365, 24, 1)\n",
      "y_test.shape:  (7365, 1)\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:12:10,872 WARNING Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1010.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1015.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4897, 24, 1)\n",
      "y_test.shape:  (4897, 1)\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:12:21,587 WARNING Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1015.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1043.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7660, 24, 1)\n",
      "y_test.shape:  (7660, 1)\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:12:31,347 WARNING Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1043.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1082.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7782, 24, 1)\n",
      "y_test.shape:  (7782, 1)\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:12:42,115 WARNING Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1082.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7665, 24, 1)\n",
      "y_test.shape:  (7665, 1)\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:12:52,815 WARNING Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1121.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7790, 24, 1)\n",
      "y_test.shape:  (7790, 1)\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:03,541 WARNING Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1121.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1127.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7001, 24, 1)\n",
      "y_test.shape:  (7001, 1)\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:14,075 WARNING Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1127.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1139.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (4868, 24, 1)\n",
      "y_test.shape:  (4868, 1)\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:24,278 WARNING Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1139.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1143.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7478, 24, 1)\n",
      "y_test.shape:  (7478, 1)\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:33,465 WARNING Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1143.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1171.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7626, 24, 1)\n",
      "y_test.shape:  (7626, 1)\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:44,141 WARNING Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1171.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1194.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7905, 24, 1)\n",
      "y_test.shape:  (7905, 1)\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:13:54,738 WARNING Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1194.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1201.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7803, 24, 1)\n",
      "y_test.shape:  (7803, 1)\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:14:05,175 WARNING Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1201.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\252.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7345, 24, 1)\n",
      "y_test.shape:  (7345, 1)\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:14:15,779 WARNING Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  252.csv\n",
      "2025-01-21 21:14:25,225 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold3_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold3_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (287559, 24, 1)\n",
      "y_train.shape:  (287559, 1)\n",
      "x_valid.shape:  (71871, 24, 1)\n",
      "y_valid.shape:  (71871, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:15:08,695 WARNING Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 21:15:08,829 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 287559 samples, validate on 71871 samples\n",
      "Epoch 1/10000\n",
      "287559/287559 [==============================] - ETA: 0s - loss: 0.1243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287559/287559 [==============================] - 15s 52us/sample - loss: 0.1243 - val_loss: -0.1246\n",
      "Epoch 2/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.0695 - val_loss: -0.1943\n",
      "Epoch 3/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.1131 - val_loss: -0.2167\n",
      "Epoch 4/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.1349 - val_loss: -0.2113\n",
      "Epoch 5/10000\n",
      "287559/287559 [==============================] - 12s 41us/sample - loss: -0.1579 - val_loss: -0.2358\n",
      "Epoch 6/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.1757 - val_loss: -0.2413\n",
      "Epoch 7/10000\n",
      "287559/287559 [==============================] - 8s 27us/sample - loss: -0.1853 - val_loss: -0.2610\n",
      "Epoch 8/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.2005 - val_loss: -0.2503\n",
      "Epoch 9/10000\n",
      "287559/287559 [==============================] - 8s 27us/sample - loss: -0.2126 - val_loss: -0.2792\n",
      "Epoch 10/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.2192 - val_loss: -0.2847\n",
      "Epoch 11/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.2252 - val_loss: -0.2859\n",
      "Epoch 12/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.2327 - val_loss: -0.2373\n",
      "Epoch 13/10000\n",
      "287559/287559 [==============================] - 8s 28us/sample - loss: -0.2397 - val_loss: -0.2837\n",
      "Epoch 14/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2449 - val_loss: -0.2846\n",
      "Epoch 15/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2467 - val_loss: -0.2903\n",
      "Epoch 16/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2544 - val_loss: -0.2908\n",
      "Epoch 17/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2572 - val_loss: -0.2950\n",
      "Epoch 18/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2608 - val_loss: -0.2929\n",
      "Epoch 19/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2622 - val_loss: -0.2767\n",
      "Epoch 20/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2675 - val_loss: -0.2976\n",
      "Epoch 21/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2708 - val_loss: -0.2831\n",
      "Epoch 22/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2744 - val_loss: -0.2845\n",
      "Epoch 23/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2743 - val_loss: -0.2889\n",
      "Epoch 24/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2785 - val_loss: -0.2902\n",
      "Epoch 25/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2799 - val_loss: -0.2969\n",
      "Epoch 26/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2803 - val_loss: -0.2881\n",
      "Epoch 27/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.2840 - val_loss: -0.2956\n",
      "Epoch 28/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2857 - val_loss: -0.2974\n",
      "Epoch 29/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2869 - val_loss: -0.2958\n",
      "Epoch 30/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2897 - val_loss: -0.2992\n",
      "Epoch 31/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2886 - val_loss: -0.2985\n",
      "Epoch 32/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2912 - val_loss: -0.2969\n",
      "Epoch 33/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2947 - val_loss: -0.3012\n",
      "Epoch 34/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.2928 - val_loss: -0.3052\n",
      "Epoch 35/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.2953 - val_loss: -0.3042\n",
      "Epoch 36/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2956 - val_loss: -0.2988\n",
      "Epoch 37/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.2972 - val_loss: -0.2935\n",
      "Epoch 38/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2978 - val_loss: -0.3046\n",
      "Epoch 39/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.2967 - val_loss: -0.3040\n",
      "Epoch 40/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.3001 - val_loss: -0.3004\n",
      "Epoch 41/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.2997 - val_loss: -0.2992\n",
      "Epoch 42/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3001 - val_loss: -0.3005\n",
      "Epoch 43/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3020 - val_loss: -0.3046\n",
      "Epoch 44/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3015 - val_loss: -0.3043\n",
      "Epoch 45/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3027 - val_loss: -0.2939\n",
      "Epoch 46/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3037 - val_loss: -0.3004\n",
      "Epoch 47/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3035 - val_loss: -0.3048\n",
      "Epoch 48/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.3055 - val_loss: -0.3043\n",
      "Epoch 49/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3043 - val_loss: -0.3032\n",
      "Epoch 50/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3056 - val_loss: -0.3043\n",
      "Epoch 51/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.3076 - val_loss: -0.3045\n",
      "Epoch 52/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3081 - val_loss: -0.3057\n",
      "Epoch 53/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3078 - val_loss: -0.3052\n",
      "Epoch 54/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3090 - val_loss: -0.3020\n",
      "Epoch 55/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3107 - val_loss: -0.3066\n",
      "Epoch 56/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3118 - val_loss: -0.3069\n",
      "Epoch 57/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3119 - val_loss: -0.3084\n",
      "Epoch 58/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3124 - val_loss: -0.3059\n",
      "Epoch 59/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3131 - val_loss: -0.2996\n",
      "Epoch 60/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3135 - val_loss: -0.2994\n",
      "Epoch 61/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.3151 - val_loss: -0.3074\n",
      "Epoch 62/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3179 - val_loss: -0.3005\n",
      "Epoch 63/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3171 - val_loss: -0.3028\n",
      "Epoch 64/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3188 - val_loss: -0.3075\n",
      "Epoch 65/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.3198 - val_loss: -0.3020\n",
      "Epoch 66/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3203 - val_loss: -0.3040\n",
      "Epoch 67/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.3218 - val_loss: -0.3004\n",
      "Epoch 68/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3230 - val_loss: -0.3034\n",
      "Epoch 69/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3244 - val_loss: -0.2950\n",
      "Epoch 70/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3251 - val_loss: -0.2926\n",
      "Epoch 71/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3262 - val_loss: -0.2989\n",
      "Epoch 72/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3283 - val_loss: -0.3016\n",
      "Epoch 73/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3293 - val_loss: -0.3007\n",
      "Epoch 74/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3300 - val_loss: -0.2891\n",
      "Epoch 75/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.3328 - val_loss: -0.2975\n",
      "Epoch 76/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3330 - val_loss: -0.2964\n",
      "Epoch 77/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3338 - val_loss: -0.2969\n",
      "Epoch 78/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3359 - val_loss: -0.2896\n",
      "Epoch 79/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.3379 - val_loss: -0.2926\n",
      "Epoch 80/10000\n",
      "287559/287559 [==============================] - 9s 33us/sample - loss: -0.3396 - val_loss: -0.2947\n",
      "Epoch 81/10000\n",
      "287559/287559 [==============================] - 10s 37us/sample - loss: -0.3419 - val_loss: -0.2869\n",
      "Epoch 82/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3429 - val_loss: -0.2877\n",
      "Epoch 83/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3462 - val_loss: -0.2849\n",
      "Epoch 84/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.3470 - val_loss: -0.2868\n",
      "Epoch 85/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3488 - val_loss: -0.2848\n",
      "Epoch 86/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3511 - val_loss: -0.2844\n",
      "Epoch 87/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3535 - val_loss: -0.2799\n",
      "Epoch 88/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3561 - val_loss: -0.2783\n",
      "Epoch 89/10000\n",
      "287559/287559 [==============================] - 10s 33us/sample - loss: -0.3584 - val_loss: -0.2737\n",
      "Epoch 90/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3595 - val_loss: -0.2690\n",
      "Epoch 91/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3626 - val_loss: -0.2764\n",
      "Epoch 92/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.3645 - val_loss: -0.2583\n",
      "Epoch 93/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.3683 - val_loss: -0.2651\n",
      "Epoch 94/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.3723 - val_loss: -0.2608\n",
      "Epoch 95/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3737 - val_loss: -0.2625\n",
      "Epoch 96/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3761 - val_loss: -0.2577\n",
      "Epoch 97/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.3794 - val_loss: -0.2609\n",
      "Epoch 98/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3820 - val_loss: -0.2508\n",
      "Epoch 99/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3834 - val_loss: -0.2533\n",
      "Epoch 100/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3867 - val_loss: -0.2448\n",
      "Epoch 101/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3907 - val_loss: -0.2402\n",
      "Epoch 102/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.3959 - val_loss: -0.2220\n",
      "Epoch 103/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3979 - val_loss: -0.2256\n",
      "Epoch 104/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.3995 - val_loss: -0.2186\n",
      "Epoch 105/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4046 - val_loss: -0.2179\n",
      "Epoch 106/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4081 - val_loss: -0.2189\n",
      "Epoch 107/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4124 - val_loss: -0.2073\n",
      "Epoch 108/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4163 - val_loss: -0.1984\n",
      "Epoch 109/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4205 - val_loss: -0.1887\n",
      "Epoch 110/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4219 - val_loss: -0.1723\n",
      "Epoch 111/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4269 - val_loss: -0.1719\n",
      "Epoch 112/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4319 - val_loss: -0.1731\n",
      "Epoch 113/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4333 - val_loss: -0.1619\n",
      "Epoch 114/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4396 - val_loss: -0.1485\n",
      "Epoch 115/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4416 - val_loss: -0.1682\n",
      "Epoch 116/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.4462 - val_loss: -0.1587\n",
      "Epoch 117/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4511 - val_loss: -0.1385\n",
      "Epoch 118/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4536 - val_loss: -0.1196\n",
      "Epoch 119/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4563 - val_loss: -0.1141\n",
      "Epoch 120/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4638 - val_loss: -0.1359\n",
      "Epoch 121/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.4672 - val_loss: -0.1125\n",
      "Epoch 122/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4722 - val_loss: -0.1090\n",
      "Epoch 123/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4741 - val_loss: -0.0643\n",
      "Epoch 124/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4799 - val_loss: -0.0797\n",
      "Epoch 125/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4834 - val_loss: -0.0713\n",
      "Epoch 126/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4873 - val_loss: -0.0537\n",
      "Epoch 127/10000\n",
      "287559/287559 [==============================] - 8s 30us/sample - loss: -0.4903 - val_loss: -0.0392\n",
      "Epoch 128/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.4965 - val_loss: -0.0159\n",
      "Epoch 129/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.4981 - val_loss: -0.0477\n",
      "Epoch 130/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5044 - val_loss: -0.0398\n",
      "Epoch 131/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5079 - val_loss: -0.0012\n",
      "Epoch 132/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5125 - val_loss: -0.0011\n",
      "Epoch 133/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5150 - val_loss: 0.0099\n",
      "Epoch 134/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.5188 - val_loss: -0.0070\n",
      "Epoch 135/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.5238 - val_loss: 0.0265\n",
      "Epoch 136/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.5278 - val_loss: 0.0139\n",
      "Epoch 137/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5319 - val_loss: 0.0341\n",
      "Epoch 138/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5356 - val_loss: 0.0192\n",
      "Epoch 139/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5404 - val_loss: 0.0493\n",
      "Epoch 140/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.5433 - val_loss: 0.0468\n",
      "Epoch 141/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5495 - val_loss: 0.0639\n",
      "Epoch 142/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5505 - val_loss: 0.1047\n",
      "Epoch 143/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5559 - val_loss: 0.0870\n",
      "Epoch 144/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.5611 - val_loss: 0.1240\n",
      "Epoch 145/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5635 - val_loss: 0.0979\n",
      "Epoch 146/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5670 - val_loss: 0.1628\n",
      "Epoch 147/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5726 - val_loss: 0.1473\n",
      "Epoch 148/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5766 - val_loss: 0.1463\n",
      "Epoch 149/10000\n",
      "287559/287559 [==============================] - 10s 34us/sample - loss: -0.5805 - val_loss: 0.1397\n",
      "Epoch 150/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5844 - val_loss: 0.1895\n",
      "Epoch 151/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5875 - val_loss: 0.1801\n",
      "Epoch 152/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5930 - val_loss: 0.1669\n",
      "Epoch 153/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.5962 - val_loss: 0.1528\n",
      "Epoch 154/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6016 - val_loss: 0.1874\n",
      "Epoch 155/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6045 - val_loss: 0.2026\n",
      "Epoch 156/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6076 - val_loss: 0.2232\n",
      "Epoch 157/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6121 - val_loss: 0.2561\n",
      "Epoch 158/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.6162 - val_loss: 0.2539\n",
      "Epoch 159/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6222 - val_loss: 0.2683\n",
      "Epoch 160/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6253 - val_loss: 0.2944\n",
      "Epoch 161/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6302 - val_loss: 0.3192\n",
      "Epoch 162/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6318 - val_loss: 0.3283\n",
      "Epoch 163/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6339 - val_loss: 0.2886\n",
      "Epoch 164/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6381 - val_loss: 0.3111\n",
      "Epoch 165/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6407 - val_loss: 0.3293\n",
      "Epoch 166/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6464 - val_loss: 0.3404\n",
      "Epoch 167/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6524 - val_loss: 0.3467\n",
      "Epoch 168/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6554 - val_loss: 0.3569\n",
      "Epoch 169/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6556 - val_loss: 0.3466\n",
      "Epoch 170/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6602 - val_loss: 0.3996\n",
      "Epoch 171/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.6655 - val_loss: 0.3791\n",
      "Epoch 172/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6669 - val_loss: 0.4227\n",
      "Epoch 173/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6704 - val_loss: 0.4156\n",
      "Epoch 174/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6748 - val_loss: 0.4911\n",
      "Epoch 175/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6773 - val_loss: 0.4617\n",
      "Epoch 176/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6798 - val_loss: 0.4391\n",
      "Epoch 177/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6858 - val_loss: 0.4248\n",
      "Epoch 178/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6850 - val_loss: 0.4578\n",
      "Epoch 179/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6913 - val_loss: 0.4873\n",
      "Epoch 180/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6920 - val_loss: 0.5285\n",
      "Epoch 181/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.6974 - val_loss: 0.5189\n",
      "Epoch 182/10000\n",
      "287559/287559 [==============================] - 10s 33us/sample - loss: -0.6997 - val_loss: 0.5290\n",
      "Epoch 183/10000\n",
      "287559/287559 [==============================] - 9s 33us/sample - loss: -0.7074 - val_loss: 0.5638\n",
      "Epoch 184/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.7067 - val_loss: 0.5548\n",
      "Epoch 185/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7105 - val_loss: 0.5473\n",
      "Epoch 186/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7116 - val_loss: 0.5201\n",
      "Epoch 187/10000\n",
      "287559/287559 [==============================] - 8s 29us/sample - loss: -0.7185 - val_loss: 0.5593\n",
      "Epoch 188/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7200 - val_loss: 0.5935\n",
      "Epoch 189/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7242 - val_loss: 0.6066\n",
      "Epoch 190/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7241 - val_loss: 0.6245\n",
      "Epoch 191/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7295 - val_loss: 0.6162\n",
      "Epoch 192/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7285 - val_loss: 0.6445\n",
      "Epoch 193/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7346 - val_loss: 0.5923\n",
      "Epoch 194/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7426 - val_loss: 0.6530\n",
      "Epoch 195/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7393 - val_loss: 0.6391\n",
      "Epoch 196/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7443 - val_loss: 0.6841\n",
      "Epoch 197/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.7471 - val_loss: 0.6885\n",
      "Epoch 198/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7491 - val_loss: 0.7410\n",
      "Epoch 199/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7543 - val_loss: 0.7675\n",
      "Epoch 200/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7530 - val_loss: 0.6886\n",
      "Epoch 201/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7591 - val_loss: 0.8028\n",
      "Epoch 202/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7594 - val_loss: 0.7643\n",
      "Epoch 203/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7614 - val_loss: 0.7647\n",
      "Epoch 204/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7658 - val_loss: 0.7379\n",
      "Epoch 205/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7698 - val_loss: 0.7550\n",
      "Epoch 206/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7730 - val_loss: 0.8178\n",
      "Epoch 207/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7768 - val_loss: 0.8003\n",
      "Epoch 208/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7819 - val_loss: 0.7958\n",
      "Epoch 209/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7812 - val_loss: 0.8478\n",
      "Epoch 210/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7849 - val_loss: 0.8378\n",
      "Epoch 211/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7876 - val_loss: 0.8819\n",
      "Epoch 212/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7910 - val_loss: 0.8920\n",
      "Epoch 213/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7916 - val_loss: 0.8561\n",
      "Epoch 214/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7938 - val_loss: 0.8238\n",
      "Epoch 215/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.7972 - val_loss: 0.8169\n",
      "Epoch 216/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.7970 - val_loss: 0.8877\n",
      "Epoch 217/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8029 - val_loss: 0.8475\n",
      "Epoch 218/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8040 - val_loss: 0.9356\n",
      "Epoch 219/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8079 - val_loss: 0.9075\n",
      "Epoch 220/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8116 - val_loss: 0.8767\n",
      "Epoch 221/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8126 - val_loss: 0.9421\n",
      "Epoch 222/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8166 - val_loss: 0.9588\n",
      "Epoch 223/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8194 - val_loss: 0.9081\n",
      "Epoch 224/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8218 - val_loss: 0.9447\n",
      "Epoch 225/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8236 - val_loss: 0.9963\n",
      "Epoch 226/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8238 - val_loss: 0.9906\n",
      "Epoch 227/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8276 - val_loss: 0.9666\n",
      "Epoch 228/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8309 - val_loss: 0.9922\n",
      "Epoch 229/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8356 - val_loss: 1.0116\n",
      "Epoch 230/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8347 - val_loss: 1.0186\n",
      "Epoch 231/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8389 - val_loss: 1.0062\n",
      "Epoch 232/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8386 - val_loss: 0.9723\n",
      "Epoch 233/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8411 - val_loss: 1.0797\n",
      "Epoch 234/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8437 - val_loss: 1.0792\n",
      "Epoch 235/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8471 - val_loss: 1.1143\n",
      "Epoch 236/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8484 - val_loss: 1.0664\n",
      "Epoch 237/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8547 - val_loss: 1.1451\n",
      "Epoch 238/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.8536 - val_loss: 1.0883\n",
      "Epoch 239/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8563 - val_loss: 1.0528\n",
      "Epoch 240/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8585 - val_loss: 1.1543\n",
      "Epoch 241/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8581 - val_loss: 1.1466\n",
      "Epoch 242/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8636 - val_loss: 1.1778\n",
      "Epoch 243/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8632 - val_loss: 1.1685\n",
      "Epoch 244/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8680 - val_loss: 1.1355\n",
      "Epoch 245/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8673 - val_loss: 1.1173\n",
      "Epoch 246/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8708 - val_loss: 1.1551\n",
      "Epoch 247/10000\n",
      "287559/287559 [==============================] - 9s 32us/sample - loss: -0.8740 - val_loss: 1.1236\n",
      "Epoch 248/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8745 - val_loss: 1.1494\n",
      "Epoch 249/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8776 - val_loss: 1.2221\n",
      "Epoch 250/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8788 - val_loss: 1.1945\n",
      "Epoch 251/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8808 - val_loss: 1.1712\n",
      "Epoch 252/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8848 - val_loss: 1.2419\n",
      "Epoch 253/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8863 - val_loss: 1.2408\n",
      "Epoch 254/10000\n",
      "287559/287559 [==============================] - 9s 31us/sample - loss: -0.8890 - val_loss: 1.2754\n",
      "Epoch 255/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8926 - val_loss: 1.3030\n",
      "Epoch 256/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8926 - val_loss: 1.2742\n",
      "Epoch 257/10000\n",
      "287559/287559 [==============================] - 9s 30us/sample - loss: -0.8945 - val_loss: 1.3294\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1205.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7236, 24, 1)\n",
      "y_test.shape:  (7236, 1)\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:52:41,580 WARNING Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1205.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1211.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7781, 24, 1)\n",
      "y_test.shape:  (7781, 1)\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:52:52,551 WARNING Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1211.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1219.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7823, 24, 1)\n",
      "y_test.shape:  (7823, 1)\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:04,328 WARNING Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1219.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1230.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7397, 24, 1)\n",
      "y_test.shape:  (7397, 1)\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:15,730 WARNING Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1230.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1239.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7554, 24, 1)\n",
      "y_test.shape:  (7554, 1)\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:26,582 WARNING Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1239.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1271.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7906, 24, 1)\n",
      "y_test.shape:  (7906, 1)\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:37,531 WARNING Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1271.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1286.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7927, 24, 1)\n",
      "y_test.shape:  (7927, 1)\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:48,701 WARNING Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1286.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1311.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7148, 24, 1)\n",
      "y_test.shape:  (7148, 1)\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:53:59,750 WARNING Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1311.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1330.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7817, 24, 1)\n",
      "y_test.shape:  (7817, 1)\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:54:11,102 WARNING Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1330.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1336.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7519, 24, 1)\n",
      "y_test.shape:  (7519, 1)\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:54:21,986 WARNING Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1336.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1343.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7156, 24, 1)\n",
      "y_test.shape:  (7156, 1)\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:54:33,145 WARNING Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1343.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1345.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7405, 24, 1)\n",
      "y_test.shape:  (7405, 1)\n",
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:54:43,882 WARNING Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1345.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1348.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7569, 24, 1)\n",
      "y_test.shape:  (7569, 1)\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:54:54,979 WARNING Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1348.csv\n",
      "2025-01-21 21:55:05,338 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold4_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold4_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (290951, 24, 1)\n",
      "y_train.shape:  (290951, 1)\n",
      "x_valid.shape:  (72716, 24, 1)\n",
      "y_valid.shape:  (72716, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 21:55:50,810 WARNING Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 21:55:50,945 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 290951 samples, validate on 72716 samples\n",
      "Epoch 1/10000\n",
      "290951/290951 [==============================] - ETA: 0s - loss: 0.1143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290951/290951 [==============================] - 16s 55us/sample - loss: 0.1143 - val_loss: -0.0923\n",
      "Epoch 2/10000\n",
      "290951/290951 [==============================] - 8s 27us/sample - loss: -0.0838 - val_loss: -0.2211\n",
      "Epoch 3/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.1202 - val_loss: -0.2397\n",
      "Epoch 4/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.1455 - val_loss: -0.2081\n",
      "Epoch 5/10000\n",
      "290951/290951 [==============================] - 12s 43us/sample - loss: -0.1670 - val_loss: -0.2474\n",
      "Epoch 6/10000\n",
      "290951/290951 [==============================] - 8s 29us/sample - loss: -0.1835 - val_loss: -0.2353\n",
      "Epoch 7/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.1956 - val_loss: -0.2553\n",
      "Epoch 8/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.2099 - val_loss: -0.2515\n",
      "Epoch 9/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.2169 - val_loss: -0.2801\n",
      "Epoch 10/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.2246 - val_loss: -0.2693\n",
      "Epoch 11/10000\n",
      "290951/290951 [==============================] - 8s 29us/sample - loss: -0.2314 - val_loss: -0.2677\n",
      "Epoch 12/10000\n",
      "290951/290951 [==============================] - 8s 29us/sample - loss: -0.2361 - val_loss: -0.2799\n",
      "Epoch 13/10000\n",
      "290951/290951 [==============================] - 8s 28us/sample - loss: -0.2439 - val_loss: -0.2921\n",
      "Epoch 14/10000\n",
      "290951/290951 [==============================] - 8s 29us/sample - loss: -0.2464 - val_loss: -0.2841\n",
      "Epoch 15/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2527 - val_loss: -0.2964\n",
      "Epoch 16/10000\n",
      "290951/290951 [==============================] - 8s 29us/sample - loss: -0.2588 - val_loss: -0.2736\n",
      "Epoch 17/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2613 - val_loss: -0.2919\n",
      "Epoch 18/10000\n",
      "290951/290951 [==============================] - 9s 29us/sample - loss: -0.2680 - val_loss: -0.2976\n",
      "Epoch 19/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2688 - val_loss: -0.2977\n",
      "Epoch 20/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2732 - val_loss: -0.3047\n",
      "Epoch 21/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2754 - val_loss: -0.2927\n",
      "Epoch 22/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2790 - val_loss: -0.2921\n",
      "Epoch 23/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2812 - val_loss: -0.3033\n",
      "Epoch 24/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.2839 - val_loss: -0.2969\n",
      "Epoch 25/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.2875 - val_loss: -0.2936\n",
      "Epoch 26/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2881 - val_loss: -0.3044\n",
      "Epoch 27/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.2920 - val_loss: -0.3035\n",
      "Epoch 28/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.2915 - val_loss: -0.2927\n",
      "Epoch 29/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2927 - val_loss: -0.3016\n",
      "Epoch 30/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2937 - val_loss: -0.2997\n",
      "Epoch 31/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2962 - val_loss: -0.3031\n",
      "Epoch 32/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2952 - val_loss: -0.2935\n",
      "Epoch 33/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2992 - val_loss: -0.2993\n",
      "Epoch 34/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3006 - val_loss: -0.3079\n",
      "Epoch 35/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.2999 - val_loss: -0.2791\n",
      "Epoch 36/10000\n",
      "290951/290951 [==============================] - 9s 29us/sample - loss: -0.3014 - val_loss: -0.3049\n",
      "Epoch 37/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3013 - val_loss: -0.3055\n",
      "Epoch 38/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3029 - val_loss: -0.3094\n",
      "Epoch 39/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3048 - val_loss: -0.3058\n",
      "Epoch 40/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3037 - val_loss: -0.3078\n",
      "Epoch 41/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.3058 - val_loss: -0.3057\n",
      "Epoch 42/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3049 - val_loss: -0.3040\n",
      "Epoch 43/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3077 - val_loss: -0.3079\n",
      "Epoch 44/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3086 - val_loss: -0.3070\n",
      "Epoch 45/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3083 - val_loss: -0.3103\n",
      "Epoch 46/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3103 - val_loss: -0.3093\n",
      "Epoch 47/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3109 - val_loss: -0.2850\n",
      "Epoch 48/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3081 - val_loss: -0.3087\n",
      "Epoch 49/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.3127 - val_loss: -0.3034\n",
      "Epoch 50/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3128 - val_loss: -0.3089\n",
      "Epoch 51/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3125 - val_loss: -0.3073\n",
      "Epoch 52/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3145 - val_loss: -0.3076\n",
      "Epoch 53/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3143 - val_loss: -0.2932\n",
      "Epoch 54/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3139 - val_loss: -0.2890\n",
      "Epoch 55/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3160 - val_loss: -0.3016\n",
      "Epoch 56/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3167 - val_loss: -0.3017\n",
      "Epoch 57/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3175 - val_loss: -0.3100\n",
      "Epoch 58/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3196 - val_loss: -0.3083\n",
      "Epoch 59/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3201 - val_loss: -0.3025\n",
      "Epoch 60/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3213 - val_loss: -0.3010\n",
      "Epoch 61/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3223 - val_loss: -0.3094\n",
      "Epoch 62/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3225 - val_loss: -0.3015\n",
      "Epoch 63/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3239 - val_loss: -0.3063\n",
      "Epoch 64/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3247 - val_loss: -0.3067\n",
      "Epoch 65/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3251 - val_loss: -0.3050\n",
      "Epoch 66/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3251 - val_loss: -0.3081\n",
      "Epoch 67/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3262 - val_loss: -0.3040\n",
      "Epoch 68/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3294 - val_loss: -0.2917\n",
      "Epoch 69/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3301 - val_loss: -0.3017\n",
      "Epoch 70/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3314 - val_loss: -0.3077\n",
      "Epoch 71/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3307 - val_loss: -0.2987\n",
      "Epoch 72/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3319 - val_loss: -0.2899\n",
      "Epoch 73/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3347 - val_loss: -0.3024\n",
      "Epoch 74/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3348 - val_loss: -0.3060\n",
      "Epoch 75/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3362 - val_loss: -0.2989\n",
      "Epoch 76/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3361 - val_loss: -0.3012\n",
      "Epoch 77/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3395 - val_loss: -0.3009\n",
      "Epoch 78/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3421 - val_loss: -0.2975\n",
      "Epoch 79/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3431 - val_loss: -0.2889\n",
      "Epoch 80/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3437 - val_loss: -0.2938\n",
      "Epoch 81/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3454 - val_loss: -0.2920\n",
      "Epoch 82/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3465 - val_loss: -0.2931\n",
      "Epoch 83/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3501 - val_loss: -0.2924\n",
      "Epoch 84/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3491 - val_loss: -0.2926\n",
      "Epoch 85/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3512 - val_loss: -0.2856\n",
      "Epoch 86/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3527 - val_loss: -0.2869\n",
      "Epoch 87/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3562 - val_loss: -0.2935\n",
      "Epoch 88/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3562 - val_loss: -0.2806\n",
      "Epoch 89/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3611 - val_loss: -0.2776\n",
      "Epoch 90/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3609 - val_loss: -0.2733\n",
      "Epoch 91/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3640 - val_loss: -0.2701\n",
      "Epoch 92/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3665 - val_loss: -0.2678\n",
      "Epoch 93/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.3689 - val_loss: -0.2670\n",
      "Epoch 94/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3720 - val_loss: -0.2699\n",
      "Epoch 95/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3731 - val_loss: -0.2657\n",
      "Epoch 96/10000\n",
      "290951/290951 [==============================] - 9s 33us/sample - loss: -0.3766 - val_loss: -0.2678\n",
      "Epoch 97/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3773 - val_loss: -0.2576\n",
      "Epoch 98/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3807 - val_loss: -0.2522\n",
      "Epoch 99/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3831 - val_loss: -0.2517\n",
      "Epoch 100/10000\n",
      "290951/290951 [==============================] - 10s 33us/sample - loss: -0.3877 - val_loss: -0.2561\n",
      "Epoch 101/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3907 - val_loss: -0.2473\n",
      "Epoch 102/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3911 - val_loss: -0.2582\n",
      "Epoch 103/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3946 - val_loss: -0.2304\n",
      "Epoch 104/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.3982 - val_loss: -0.2395\n",
      "Epoch 105/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4013 - val_loss: -0.2269\n",
      "Epoch 106/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4049 - val_loss: -0.2171\n",
      "Epoch 107/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4072 - val_loss: -0.2245\n",
      "Epoch 108/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.4105 - val_loss: -0.2201\n",
      "Epoch 109/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4149 - val_loss: -0.2187\n",
      "Epoch 110/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4169 - val_loss: -0.2080\n",
      "Epoch 111/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4219 - val_loss: -0.1997\n",
      "Epoch 112/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4253 - val_loss: -0.1931\n",
      "Epoch 113/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4276 - val_loss: -0.2092\n",
      "Epoch 114/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.4317 - val_loss: -0.1854\n",
      "Epoch 115/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4360 - val_loss: -0.1828\n",
      "Epoch 116/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4390 - val_loss: -0.1724\n",
      "Epoch 117/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4421 - val_loss: -0.1767\n",
      "Epoch 118/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4469 - val_loss: -0.1543\n",
      "Epoch 119/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4490 - val_loss: -0.1472\n",
      "Epoch 120/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4543 - val_loss: -0.1484\n",
      "Epoch 121/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4575 - val_loss: -0.1391\n",
      "Epoch 122/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4601 - val_loss: -0.1500\n",
      "Epoch 123/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4658 - val_loss: -0.1194\n",
      "Epoch 124/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4686 - val_loss: -0.1136\n",
      "Epoch 125/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.4711 - val_loss: -0.0958\n",
      "Epoch 126/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4783 - val_loss: -0.1086\n",
      "Epoch 127/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4801 - val_loss: -0.1161\n",
      "Epoch 128/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4865 - val_loss: -0.0978\n",
      "Epoch 129/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4883 - val_loss: -0.0645\n",
      "Epoch 130/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4931 - val_loss: -0.0693\n",
      "Epoch 131/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.4962 - val_loss: -0.0526\n",
      "Epoch 132/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5000 - val_loss: -0.0688\n",
      "Epoch 133/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5044 - val_loss: -0.0644\n",
      "Epoch 134/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5071 - val_loss: -0.0393\n",
      "Epoch 135/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.5131 - val_loss: -0.0455\n",
      "Epoch 136/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5176 - val_loss: -0.0384\n",
      "Epoch 137/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.5205 - val_loss: -0.0225\n",
      "Epoch 138/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5263 - val_loss: -0.0051\n",
      "Epoch 139/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.5293 - val_loss: 7.4043e-04\n",
      "Epoch 140/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5326 - val_loss: -0.0254\n",
      "Epoch 141/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5363 - val_loss: -0.0022\n",
      "Epoch 142/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5400 - val_loss: 0.0218\n",
      "Epoch 143/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.5457 - val_loss: 0.0346\n",
      "Epoch 144/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5485 - val_loss: 0.0379\n",
      "Epoch 145/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.5542 - val_loss: 0.0319\n",
      "Epoch 146/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5583 - val_loss: 0.0479\n",
      "Epoch 147/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.5618 - val_loss: 0.0709\n",
      "Epoch 148/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.5665 - val_loss: 0.0859\n",
      "Epoch 149/10000\n",
      "290951/290951 [==============================] - 10s 33us/sample - loss: -0.5682 - val_loss: 0.0762\n",
      "Epoch 150/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5744 - val_loss: 0.0805\n",
      "Epoch 151/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5748 - val_loss: 0.1155\n",
      "Epoch 152/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5813 - val_loss: 0.1376\n",
      "Epoch 153/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5846 - val_loss: 0.1128\n",
      "Epoch 154/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5883 - val_loss: 0.1525\n",
      "Epoch 155/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5921 - val_loss: 0.0945\n",
      "Epoch 156/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.5967 - val_loss: 0.1524\n",
      "Epoch 157/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6012 - val_loss: 0.1128\n",
      "Epoch 158/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6030 - val_loss: 0.1636\n",
      "Epoch 159/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6063 - val_loss: 0.1734\n",
      "Epoch 160/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6134 - val_loss: 0.1742\n",
      "Epoch 161/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6155 - val_loss: 0.2035\n",
      "Epoch 162/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.6206 - val_loss: 0.1747\n",
      "Epoch 163/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6246 - val_loss: 0.2081\n",
      "Epoch 164/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6268 - val_loss: 0.2241\n",
      "Epoch 165/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6322 - val_loss: 0.2642\n",
      "Epoch 166/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6353 - val_loss: 0.2430\n",
      "Epoch 167/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6391 - val_loss: 0.2649\n",
      "Epoch 168/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6446 - val_loss: 0.3206\n",
      "Epoch 169/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6464 - val_loss: 0.2733\n",
      "Epoch 170/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6476 - val_loss: 0.2686\n",
      "Epoch 171/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6525 - val_loss: 0.3300\n",
      "Epoch 172/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6578 - val_loss: 0.2906\n",
      "Epoch 173/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6615 - val_loss: 0.3420\n",
      "Epoch 174/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6651 - val_loss: 0.3376\n",
      "Epoch 175/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6697 - val_loss: 0.3069\n",
      "Epoch 176/10000\n",
      "290951/290951 [==============================] - 9s 33us/sample - loss: -0.6710 - val_loss: 0.3274\n",
      "Epoch 177/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6756 - val_loss: 0.3973\n",
      "Epoch 178/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6774 - val_loss: 0.3669\n",
      "Epoch 179/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6825 - val_loss: 0.3700\n",
      "Epoch 180/10000\n",
      "290951/290951 [==============================] - 10s 33us/sample - loss: -0.6826 - val_loss: 0.4085\n",
      "Epoch 181/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.6879 - val_loss: 0.4443\n",
      "Epoch 182/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6922 - val_loss: 0.4040\n",
      "Epoch 183/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6971 - val_loss: 0.3865\n",
      "Epoch 184/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.6991 - val_loss: 0.4522\n",
      "Epoch 185/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7000 - val_loss: 0.4528\n",
      "Epoch 186/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7062 - val_loss: 0.4235\n",
      "Epoch 187/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.6984 - val_loss: 0.4742\n",
      "Epoch 188/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7127 - val_loss: 0.4667\n",
      "Epoch 189/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.7162 - val_loss: 0.4756\n",
      "Epoch 190/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7199 - val_loss: 0.5307\n",
      "Epoch 191/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7244 - val_loss: 0.5201\n",
      "Epoch 192/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7257 - val_loss: 0.5032\n",
      "Epoch 193/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7292 - val_loss: 0.5760\n",
      "Epoch 194/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7326 - val_loss: 0.5247\n",
      "Epoch 195/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7370 - val_loss: 0.5349\n",
      "Epoch 196/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7396 - val_loss: 0.5708\n",
      "Epoch 197/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7426 - val_loss: 0.5814\n",
      "Epoch 198/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7440 - val_loss: 0.5999\n",
      "Epoch 199/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7461 - val_loss: 0.5806\n",
      "Epoch 200/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7502 - val_loss: 0.5945\n",
      "Epoch 201/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.7529 - val_loss: 0.5512\n",
      "Epoch 202/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7581 - val_loss: 0.6385\n",
      "Epoch 203/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7590 - val_loss: 0.6545\n",
      "Epoch 204/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7599 - val_loss: 0.5668\n",
      "Epoch 205/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7663 - val_loss: 0.6888\n",
      "Epoch 206/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7707 - val_loss: 0.6425\n",
      "Epoch 207/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7718 - val_loss: 0.6530\n",
      "Epoch 208/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.7753 - val_loss: 0.6841\n",
      "Epoch 209/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7767 - val_loss: 0.6952\n",
      "Epoch 210/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7825 - val_loss: 0.7050\n",
      "Epoch 211/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7831 - val_loss: 0.6461\n",
      "Epoch 212/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.7866 - val_loss: 0.6910\n",
      "Epoch 213/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7869 - val_loss: 0.6723\n",
      "Epoch 214/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7915 - val_loss: 0.7271\n",
      "Epoch 215/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.7929 - val_loss: 0.6912\n",
      "Epoch 216/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.7963 - val_loss: 0.7368\n",
      "Epoch 217/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8008 - val_loss: 0.7915\n",
      "Epoch 218/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8039 - val_loss: 0.7557\n",
      "Epoch 219/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8103 - val_loss: 0.7956\n",
      "Epoch 220/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8104 - val_loss: 0.7779\n",
      "Epoch 221/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8105 - val_loss: 0.7601\n",
      "Epoch 222/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8147 - val_loss: 0.8251\n",
      "Epoch 223/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8140 - val_loss: 0.7929\n",
      "Epoch 224/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8215 - val_loss: 0.8318\n",
      "Epoch 225/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8226 - val_loss: 0.8246\n",
      "Epoch 226/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8248 - val_loss: 0.7889\n",
      "Epoch 227/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8269 - val_loss: 0.8506\n",
      "Epoch 228/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8321 - val_loss: 0.9051\n",
      "Epoch 229/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.8365 - val_loss: 0.8397\n",
      "Epoch 230/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8372 - val_loss: 0.9163\n",
      "Epoch 231/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8376 - val_loss: 0.9175\n",
      "Epoch 232/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8383 - val_loss: 0.8902\n",
      "Epoch 233/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8422 - val_loss: 0.9113\n",
      "Epoch 234/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.8453 - val_loss: 0.9702\n",
      "Epoch 235/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8500 - val_loss: 0.9440\n",
      "Epoch 236/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.8497 - val_loss: 0.9517\n",
      "Epoch 237/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8531 - val_loss: 0.9078\n",
      "Epoch 238/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8555 - val_loss: 0.9583\n",
      "Epoch 239/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8562 - val_loss: 0.9938\n",
      "Epoch 240/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8637 - val_loss: 0.8961\n",
      "Epoch 241/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8613 - val_loss: 0.9223\n",
      "Epoch 242/10000\n",
      "290951/290951 [==============================] - 9s 32us/sample - loss: -0.8638 - val_loss: 1.0181\n",
      "Epoch 243/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8680 - val_loss: 0.9556\n",
      "Epoch 244/10000\n",
      "290951/290951 [==============================] - 9s 31us/sample - loss: -0.8714 - val_loss: 0.9868\n",
      "Epoch 245/10000\n",
      "290951/290951 [==============================] - 9s 30us/sample - loss: -0.8733 - val_loss: 1.0657\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1361.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7575, 24, 1)\n",
      "y_test.shape:  (7575, 1)\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:32:32,357 WARNING Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1361.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1362.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5935, 24, 1)\n",
      "y_test.shape:  (5935, 1)\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:32:43,800 WARNING Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1362.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1363.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7502, 24, 1)\n",
      "y_test.shape:  (7502, 1)\n",
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:32:54,765 WARNING Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1363.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1377.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7695, 24, 1)\n",
      "y_test.shape:  (7695, 1)\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:33:06,312 WARNING Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1377.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1381.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7665, 24, 1)\n",
      "y_test.shape:  (7665, 1)\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:33:18,051 WARNING Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1381.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1386.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7204, 24, 1)\n",
      "y_test.shape:  (7204, 1)\n",
      "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:33:29,521 WARNING Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1386.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1408.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7161, 24, 1)\n",
      "y_test.shape:  (7161, 1)\n",
      "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:33:40,724 WARNING Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1408.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1422.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6832, 24, 1)\n",
      "y_test.shape:  (6832, 1)\n",
      "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:33:51,950 WARNING Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1422.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1427.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7023, 24, 1)\n",
      "y_test.shape:  (7023, 1)\n",
      "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:34:03,033 WARNING Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1427.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1433.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7520, 24, 1)\n",
      "y_test.shape:  (7520, 1)\n",
      "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:34:14,743 WARNING Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1433.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1435.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6878, 24, 1)\n",
      "y_test.shape:  (6878, 1)\n",
      "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:34:25,987 WARNING Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1435.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1457.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7620, 24, 1)\n",
      "y_test.shape:  (7620, 1)\n",
      "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:34:37,421 WARNING Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1457.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1459.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7391, 24, 1)\n",
      "y_test.shape:  (7391, 1)\n",
      "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:34:49,065 WARNING Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1459.csv\n",
      "2025-01-21 22:34:59,747 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold5_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold5_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (301747, 24, 1)\n",
      "y_train.shape:  (301747, 1)\n",
      "x_valid.shape:  (75415, 24, 1)\n",
      "y_valid.shape:  (75415, 1)\n",
      "x_test.shape:  (0, 24, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 22:35:45,840 WARNING Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 22:35:45,986 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 301747 samples, validate on 75415 samples\n",
      "Epoch 1/10000\n",
      "301056/301747 [============================>.] - ETA: 0s - loss: 0.1504"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301747/301747 [==============================] - 16s 54us/sample - loss: 0.1499 - val_loss: -0.1569\n",
      "Epoch 2/10000\n",
      "301747/301747 [==============================] - 9s 28us/sample - loss: -0.0760 - val_loss: -0.1879\n",
      "Epoch 3/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.1163 - val_loss: -0.2162\n",
      "Epoch 4/10000\n",
      "301747/301747 [==============================] - 9s 28us/sample - loss: -0.1435 - val_loss: -0.2274\n",
      "Epoch 5/10000\n",
      "301747/301747 [==============================] - 13s 43us/sample - loss: -0.1656 - val_loss: -0.2308\n",
      "Epoch 6/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.1789 - val_loss: -0.2268\n",
      "Epoch 7/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.1937 - val_loss: -0.2503\n",
      "Epoch 8/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2052 - val_loss: -0.2595\n",
      "Epoch 9/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2143 - val_loss: -0.2561\n",
      "Epoch 10/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2190 - val_loss: -0.2494\n",
      "Epoch 11/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2255 - val_loss: -0.2734\n",
      "Epoch 12/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2320 - val_loss: -0.2681\n",
      "Epoch 13/10000\n",
      "301747/301747 [==============================] - 9s 28us/sample - loss: -0.2392 - val_loss: -0.2819\n",
      "Epoch 14/10000\n",
      "301747/301747 [==============================] - 8s 28us/sample - loss: -0.2439 - val_loss: -0.2699\n",
      "Epoch 15/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2470 - val_loss: -0.2812\n",
      "Epoch 16/10000\n",
      "301747/301747 [==============================] - 9s 28us/sample - loss: -0.2501 - val_loss: -0.2808\n",
      "Epoch 17/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2540 - val_loss: -0.2601\n",
      "Epoch 18/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2579 - val_loss: -0.2862\n",
      "Epoch 19/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2619 - val_loss: -0.2776\n",
      "Epoch 20/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2630 - val_loss: -0.2736\n",
      "Epoch 21/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2678 - val_loss: -0.2772\n",
      "Epoch 22/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2689 - val_loss: -0.2881\n",
      "Epoch 23/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2731 - val_loss: -0.2866\n",
      "Epoch 24/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.2740 - val_loss: -0.2922\n",
      "Epoch 25/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.2757 - val_loss: -0.2829\n",
      "Epoch 26/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2765 - val_loss: -0.2898\n",
      "Epoch 27/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2798 - val_loss: -0.2885\n",
      "Epoch 28/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2796 - val_loss: -0.2890\n",
      "Epoch 29/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2830 - val_loss: -0.2870\n",
      "Epoch 30/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2831 - val_loss: -0.2921\n",
      "Epoch 31/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2862 - val_loss: -0.2805\n",
      "Epoch 32/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2863 - val_loss: -0.2821\n",
      "Epoch 33/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2873 - val_loss: -0.2914\n",
      "Epoch 34/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2884 - val_loss: -0.2901\n",
      "Epoch 35/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.2898 - val_loss: -0.2934\n",
      "Epoch 36/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2895 - val_loss: -0.2909\n",
      "Epoch 37/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2907 - val_loss: -0.2948\n",
      "Epoch 38/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.2929 - val_loss: -0.2893\n",
      "Epoch 39/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2938 - val_loss: -0.2929\n",
      "Epoch 40/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2940 - val_loss: -0.2889\n",
      "Epoch 41/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2936 - val_loss: -0.2958\n",
      "Epoch 42/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2966 - val_loss: -0.2913\n",
      "Epoch 43/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2969 - val_loss: -0.2969\n",
      "Epoch 44/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2976 - val_loss: -0.2957\n",
      "Epoch 45/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.2981 - val_loss: -0.2955\n",
      "Epoch 46/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.2997 - val_loss: -0.2899\n",
      "Epoch 47/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3002 - val_loss: -0.2909\n",
      "Epoch 48/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3006 - val_loss: -0.2931\n",
      "Epoch 49/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3015 - val_loss: -0.2938\n",
      "Epoch 50/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3031 - val_loss: -0.2966\n",
      "Epoch 51/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3034 - val_loss: -0.2980\n",
      "Epoch 52/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3038 - val_loss: -0.2938\n",
      "Epoch 53/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3052 - val_loss: -0.2654\n",
      "Epoch 54/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3054 - val_loss: -0.2955\n",
      "Epoch 55/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3072 - val_loss: -0.2983\n",
      "Epoch 56/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3070 - val_loss: -0.2906\n",
      "Epoch 57/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3091 - val_loss: -0.2946\n",
      "Epoch 58/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3092 - val_loss: -0.2940\n",
      "Epoch 59/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3093 - val_loss: -0.2942\n",
      "Epoch 60/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3106 - val_loss: -0.2958\n",
      "Epoch 61/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3115 - val_loss: -0.2951\n",
      "Epoch 62/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3128 - val_loss: -0.2984\n",
      "Epoch 63/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3139 - val_loss: -0.2898\n",
      "Epoch 64/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3152 - val_loss: -0.2970\n",
      "Epoch 65/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3162 - val_loss: -0.2917\n",
      "Epoch 66/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3199 - val_loss: -0.2924\n",
      "Epoch 67/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3184 - val_loss: -0.2838\n",
      "Epoch 68/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3204 - val_loss: -0.2924\n",
      "Epoch 69/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3207 - val_loss: -0.2928\n",
      "Epoch 70/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3231 - val_loss: -0.2874\n",
      "Epoch 71/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3234 - val_loss: -0.2921\n",
      "Epoch 72/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3249 - val_loss: -0.2878\n",
      "Epoch 73/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3253 - val_loss: -0.2855\n",
      "Epoch 74/10000\n",
      "301747/301747 [==============================] - 10s 33us/sample - loss: -0.3273 - val_loss: -0.2871\n",
      "Epoch 75/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3290 - val_loss: -0.2864\n",
      "Epoch 76/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3297 - val_loss: -0.2903\n",
      "Epoch 77/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3317 - val_loss: -0.2853\n",
      "Epoch 78/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3334 - val_loss: -0.2862\n",
      "Epoch 79/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3341 - val_loss: -0.2790\n",
      "Epoch 80/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3367 - val_loss: -0.2749\n",
      "Epoch 81/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3391 - val_loss: -0.2829\n",
      "Epoch 82/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3414 - val_loss: -0.2747\n",
      "Epoch 83/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3426 - val_loss: -0.2720\n",
      "Epoch 84/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3428 - val_loss: -0.2772\n",
      "Epoch 85/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3464 - val_loss: -0.2742\n",
      "Epoch 86/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3481 - val_loss: -0.2790\n",
      "Epoch 87/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3498 - val_loss: -0.2644\n",
      "Epoch 88/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3513 - val_loss: -0.2679\n",
      "Epoch 89/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3553 - val_loss: -0.2635\n",
      "Epoch 90/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3575 - val_loss: -0.2702\n",
      "Epoch 91/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.3600 - val_loss: -0.2535\n",
      "Epoch 92/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3628 - val_loss: -0.2543\n",
      "Epoch 93/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3639 - val_loss: -0.2393\n",
      "Epoch 94/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3667 - val_loss: -0.2485\n",
      "Epoch 95/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3707 - val_loss: -0.2455\n",
      "Epoch 96/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3725 - val_loss: -0.2327\n",
      "Epoch 97/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3752 - val_loss: -0.2377\n",
      "Epoch 98/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3790 - val_loss: -0.2416\n",
      "Epoch 99/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.3797 - val_loss: -0.2406\n",
      "Epoch 100/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3837 - val_loss: -0.2346\n",
      "Epoch 101/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3872 - val_loss: -0.2239\n",
      "Epoch 102/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.3905 - val_loss: -0.2179\n",
      "Epoch 103/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3908 - val_loss: -0.2224\n",
      "Epoch 104/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.3966 - val_loss: -0.2023\n",
      "Epoch 105/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4000 - val_loss: -0.2141\n",
      "Epoch 106/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.4030 - val_loss: -0.2034\n",
      "Epoch 107/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4073 - val_loss: -0.1985\n",
      "Epoch 108/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4109 - val_loss: -0.1957\n",
      "Epoch 109/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4135 - val_loss: -0.1955\n",
      "Epoch 110/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4171 - val_loss: -0.1816\n",
      "Epoch 111/10000\n",
      "301747/301747 [==============================] - 10s 33us/sample - loss: -0.4181 - val_loss: -0.1786\n",
      "Epoch 112/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4238 - val_loss: -0.1775\n",
      "Epoch 113/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4284 - val_loss: -0.1635\n",
      "Epoch 114/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4319 - val_loss: -0.1578\n",
      "Epoch 115/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4361 - val_loss: -0.1620\n",
      "Epoch 116/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4398 - val_loss: -0.1399\n",
      "Epoch 117/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4423 - val_loss: -0.1533\n",
      "Epoch 118/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4452 - val_loss: -0.1367\n",
      "Epoch 119/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4479 - val_loss: -0.1302\n",
      "Epoch 120/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4527 - val_loss: -0.1153\n",
      "Epoch 121/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4571 - val_loss: -0.1297\n",
      "Epoch 122/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4595 - val_loss: -0.1030\n",
      "Epoch 123/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4662 - val_loss: -0.0989\n",
      "Epoch 124/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4693 - val_loss: -0.0938\n",
      "Epoch 125/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.4739 - val_loss: -0.0867\n",
      "Epoch 126/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4773 - val_loss: -0.0774\n",
      "Epoch 127/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4818 - val_loss: -0.0738\n",
      "Epoch 128/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.4856 - val_loss: -0.0720\n",
      "Epoch 129/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4887 - val_loss: -0.0633\n",
      "Epoch 130/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4935 - val_loss: -0.0394\n",
      "Epoch 131/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.4982 - val_loss: -0.0127\n",
      "Epoch 132/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5012 - val_loss: -0.0225\n",
      "Epoch 133/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5045 - val_loss: -0.0317\n",
      "Epoch 134/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5089 - val_loss: -0.0084\n",
      "Epoch 135/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5128 - val_loss: -0.0015\n",
      "Epoch 136/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5189 - val_loss: 0.0061\n",
      "Epoch 137/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5204 - val_loss: 1.2894e-04\n",
      "Epoch 138/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5258 - val_loss: 0.0382\n",
      "Epoch 139/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.5290 - val_loss: 0.0120\n",
      "Epoch 140/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5341 - val_loss: 0.0551\n",
      "Epoch 141/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5352 - val_loss: 0.0462\n",
      "Epoch 142/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5409 - val_loss: 0.0645\n",
      "Epoch 143/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5435 - val_loss: 0.0602\n",
      "Epoch 144/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5496 - val_loss: 0.0552\n",
      "Epoch 145/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5525 - val_loss: 0.0997\n",
      "Epoch 146/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5569 - val_loss: 0.0936\n",
      "Epoch 147/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5599 - val_loss: 0.1099\n",
      "Epoch 148/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5626 - val_loss: 0.1394\n",
      "Epoch 149/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5676 - val_loss: 0.1060\n",
      "Epoch 150/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5735 - val_loss: 0.1021\n",
      "Epoch 151/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5753 - val_loss: 0.1551\n",
      "Epoch 152/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5811 - val_loss: 0.1487\n",
      "Epoch 153/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5821 - val_loss: 0.1665\n",
      "Epoch 154/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5879 - val_loss: 0.1770\n",
      "Epoch 155/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5917 - val_loss: 0.2131\n",
      "Epoch 156/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.5943 - val_loss: 0.1948\n",
      "Epoch 157/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.5968 - val_loss: 0.2176\n",
      "Epoch 158/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6003 - val_loss: 0.2264\n",
      "Epoch 159/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.6057 - val_loss: 0.2598\n",
      "Epoch 160/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6081 - val_loss: 0.2308\n",
      "Epoch 161/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6130 - val_loss: 0.2472\n",
      "Epoch 162/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6146 - val_loss: 0.2494\n",
      "Epoch 163/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6196 - val_loss: 0.2647\n",
      "Epoch 164/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6208 - val_loss: 0.2612\n",
      "Epoch 165/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6255 - val_loss: 0.3053\n",
      "Epoch 166/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6311 - val_loss: 0.2775\n",
      "Epoch 167/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.6334 - val_loss: 0.3156\n",
      "Epoch 168/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6378 - val_loss: 0.3033\n",
      "Epoch 169/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6406 - val_loss: 0.3046\n",
      "Epoch 170/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6445 - val_loss: 0.3098\n",
      "Epoch 171/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6476 - val_loss: 0.3505\n",
      "Epoch 172/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6510 - val_loss: 0.3289\n",
      "Epoch 173/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6556 - val_loss: 0.3366\n",
      "Epoch 174/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6570 - val_loss: 0.3722\n",
      "Epoch 175/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6603 - val_loss: 0.3779\n",
      "Epoch 176/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6645 - val_loss: 0.3709\n",
      "Epoch 177/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6668 - val_loss: 0.3820\n",
      "Epoch 178/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6712 - val_loss: 0.4010\n",
      "Epoch 179/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6735 - val_loss: 0.3721\n",
      "Epoch 180/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.6783 - val_loss: 0.4488\n",
      "Epoch 181/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.6804 - val_loss: 0.4080\n",
      "Epoch 182/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6817 - val_loss: 0.4376\n",
      "Epoch 183/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6883 - val_loss: 0.4685\n",
      "Epoch 184/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.6902 - val_loss: 0.4309\n",
      "Epoch 185/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.6924 - val_loss: 0.4090\n",
      "Epoch 186/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6953 - val_loss: 0.4799\n",
      "Epoch 187/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.6995 - val_loss: 0.4952\n",
      "Epoch 188/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7016 - val_loss: 0.5026\n",
      "Epoch 189/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7039 - val_loss: 0.5132\n",
      "Epoch 190/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7087 - val_loss: 0.4986\n",
      "Epoch 191/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7083 - val_loss: 0.5293\n",
      "Epoch 192/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.7137 - val_loss: 0.5503\n",
      "Epoch 193/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7174 - val_loss: 0.5158\n",
      "Epoch 194/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7216 - val_loss: 0.5401\n",
      "Epoch 195/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7229 - val_loss: 0.5471\n",
      "Epoch 196/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7285 - val_loss: 0.5653\n",
      "Epoch 197/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.7301 - val_loss: 0.5666\n",
      "Epoch 198/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7326 - val_loss: 0.6213\n",
      "Epoch 199/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7329 - val_loss: 0.5875\n",
      "Epoch 200/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7370 - val_loss: 0.5745\n",
      "Epoch 201/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7397 - val_loss: 0.6164\n",
      "Epoch 202/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7423 - val_loss: 0.7146\n",
      "Epoch 203/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7478 - val_loss: 0.6743\n",
      "Epoch 204/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7482 - val_loss: 0.6486\n",
      "Epoch 205/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7528 - val_loss: 0.6713\n",
      "Epoch 206/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7557 - val_loss: 0.7054\n",
      "Epoch 207/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7555 - val_loss: 0.6902\n",
      "Epoch 208/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7597 - val_loss: 0.6836\n",
      "Epoch 209/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7624 - val_loss: 0.7432\n",
      "Epoch 210/10000\n",
      "301747/301747 [==============================] - 10s 33us/sample - loss: -0.7655 - val_loss: 0.7146\n",
      "Epoch 211/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7671 - val_loss: 0.7234\n",
      "Epoch 212/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7709 - val_loss: 0.7453\n",
      "Epoch 213/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.7728 - val_loss: 0.7506\n",
      "Epoch 214/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7788 - val_loss: 0.7855\n",
      "Epoch 215/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7797 - val_loss: 0.7766\n",
      "Epoch 216/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7830 - val_loss: 0.7785\n",
      "Epoch 217/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7832 - val_loss: 0.7752\n",
      "Epoch 218/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7874 - val_loss: 0.7710\n",
      "Epoch 219/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7886 - val_loss: 0.8296\n",
      "Epoch 220/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7934 - val_loss: 0.8531\n",
      "Epoch 221/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7946 - val_loss: 0.8150\n",
      "Epoch 222/10000\n",
      "301747/301747 [==============================] - 9s 29us/sample - loss: -0.7961 - val_loss: 0.8003\n",
      "Epoch 223/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7998 - val_loss: 0.8562\n",
      "Epoch 224/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.7980 - val_loss: 0.8430\n",
      "Epoch 225/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8036 - val_loss: 0.8170\n",
      "Epoch 226/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8071 - val_loss: 0.8836\n",
      "Epoch 227/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8067 - val_loss: 0.8147\n",
      "Epoch 228/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8116 - val_loss: 0.8939\n",
      "Epoch 229/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8134 - val_loss: 0.8541\n",
      "Epoch 230/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8144 - val_loss: 0.9195\n",
      "Epoch 231/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8198 - val_loss: 0.9123\n",
      "Epoch 232/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8199 - val_loss: 0.9661\n",
      "Epoch 233/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8227 - val_loss: 1.0185\n",
      "Epoch 234/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.8262 - val_loss: 1.0063\n",
      "Epoch 235/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8280 - val_loss: 0.9514\n",
      "Epoch 236/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8297 - val_loss: 0.9974\n",
      "Epoch 237/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8314 - val_loss: 0.9785\n",
      "Epoch 238/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8343 - val_loss: 0.9700\n",
      "Epoch 239/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8359 - val_loss: 0.9906\n",
      "Epoch 240/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8390 - val_loss: 0.9246\n",
      "Epoch 241/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8408 - val_loss: 1.0046\n",
      "Epoch 242/10000\n",
      "301747/301747 [==============================] - 10s 32us/sample - loss: -0.8442 - val_loss: 1.0556\n",
      "Epoch 243/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8452 - val_loss: 1.0405\n",
      "Epoch 244/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8508 - val_loss: 1.0959\n",
      "Epoch 245/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8473 - val_loss: 1.0242\n",
      "Epoch 246/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8539 - val_loss: 1.0848\n",
      "Epoch 247/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8553 - val_loss: 1.0473\n",
      "Epoch 248/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8577 - val_loss: 1.0659\n",
      "Epoch 249/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8583 - val_loss: 1.0566\n",
      "Epoch 250/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8593 - val_loss: 1.0921\n",
      "Epoch 251/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8631 - val_loss: 1.1159\n",
      "Epoch 252/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8641 - val_loss: 1.0981\n",
      "Epoch 253/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8687 - val_loss: 1.1468\n",
      "Epoch 254/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8705 - val_loss: 1.1402\n",
      "Epoch 255/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8719 - val_loss: 1.0893\n",
      "Epoch 256/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8747 - val_loss: 1.1362\n",
      "Epoch 257/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8761 - val_loss: 1.1119\n",
      "Epoch 258/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8773 - val_loss: 1.1562\n",
      "Epoch 259/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8781 - val_loss: 1.1574\n",
      "Epoch 260/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8823 - val_loss: 1.2101\n",
      "Epoch 261/10000\n",
      "301747/301747 [==============================] - 9s 30us/sample - loss: -0.8831 - val_loss: 1.1688\n",
      "Epoch 262/10000\n",
      "301747/301747 [==============================] - 9s 31us/sample - loss: -0.8871 - val_loss: 1.1654\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1484.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5179, 24, 1)\n",
      "y_test.shape:  (5179, 1)\n",
      "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:16:01,612 WARNING Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1484.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1503.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7735, 24, 1)\n",
      "y_test.shape:  (7735, 1)\n",
      "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:16:12,988 WARNING Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1503.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1554.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7917, 24, 1)\n",
      "y_test.shape:  (7917, 1)\n",
      "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:16:25,825 WARNING Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1554.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1558.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (6336, 24, 1)\n",
      "y_test.shape:  (6336, 1)\n",
      "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:16:37,976 WARNING Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1558.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1636.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7289, 24, 1)\n",
      "y_test.shape:  (7289, 1)\n",
      "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:16:49,124 WARNING Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1636.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1650.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7585, 24, 1)\n",
      "y_test.shape:  (7585, 1)\n",
      "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:17:01,012 WARNING Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1650.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1683.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7545, 24, 1)\n",
      "y_test.shape:  (7545, 1)\n",
      "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:17:13,265 WARNING Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1683.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1689.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7760, 24, 1)\n",
      "y_test.shape:  (7760, 1)\n",
      "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:17:25,109 WARNING Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1689.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1695.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7527, 24, 1)\n",
      "y_test.shape:  (7527, 1)\n",
      "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:17:37,111 WARNING Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1695.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1722.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7837, 24, 1)\n",
      "y_test.shape:  (7837, 1)\n",
      "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:17:48,735 WARNING Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1722.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1726.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 24,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_24sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 24, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 24, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7796, 24, 1)\n",
      "y_test.shape:  (7796, 1)\n",
      "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 23:18:00,965 WARNING Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_24sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1726.csv\n"
     ]
    }
   ],
   "source": [
    "for tuning_sp in [30, 60, 90, 120]:\n",
    "    for fold_number in range(1, 6):\n",
    "        yaml_filepath = f\"./original_t1dexi_experiments_{tuning_sp}min/all_final_experiment_fold{fold_number}.yaml\" \n",
    "        mode = \"train\"\n",
    "        cfgs = load_cfgs(yaml_filepath)\n",
    "        print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "        for cfg in cfgs:\n",
    "            seed = int(cfg['train']['seed'])\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Print the configuration - just to make sure that you loaded what you\n",
    "            # wanted to load\n",
    "\n",
    "            module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "            module_model         = load_module(cfg['model']['script_path'])\n",
    "            module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "            module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "            module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "            pp = pprint.PrettyPrinter(indent=4)\n",
    "            pp.pprint(cfg)\n",
    "\n",
    "            #print(\"loading dataset ...\")\n",
    "            #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "            #nb_past_steps_tmp = 36\n",
    "            #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "            x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "            #x_train = x_train[:,-nb_past_steps:,:]\n",
    "            #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "            #x_test = x_test[:,-nb_past_steps:,:]\n",
    "            print(\"x_train.shape: \", x_train.shape)\n",
    "            print(\"y_train.shape: \", y_train.shape)\n",
    "            print(\"x_valid.shape: \", x_valid.shape)\n",
    "            print(\"y_valid.shape: \", y_valid.shape)\n",
    "            print(\"x_test.shape: \", x_test.shape)\n",
    "            print(\"y_test.shape: \", y_test.shape)\n",
    "            \n",
    "            #print(\"loading optimizer ...\")\n",
    "            optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "            #print(\"loading loss function ...\")\n",
    "            loss_function = module_loss_function.load()\n",
    "            #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "            #print(\"loading model ...\")\n",
    "            if 'tf_nll' in loss_function.__name__:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1]*2,\n",
    "                    cfg['model']\n",
    "                )\n",
    "            else:\n",
    "                model = module_model.load(\n",
    "                    x_train.shape[1:],\n",
    "                    y_train.shape[1],\n",
    "                    cfg['model']\n",
    "                )\n",
    "\n",
    "            if 'initial_weights_path' in cfg['train']:\n",
    "                #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss_function\n",
    "            )\n",
    "\n",
    "            #print(model.summary())\n",
    "\n",
    "            # training mode\n",
    "            if mode == 'train':\n",
    "                #print(\"training model ...\")\n",
    "                train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "            if mode == 'plot_nll':\n",
    "                plot_nll(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_noise_experiment':\n",
    "                plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_seg':\n",
    "                plot_seg(model, x_test, y_test, cfg)\n",
    "            if mode == 'plot_dist':\n",
    "                plot_target_distribution(y_test, cfg)\n",
    "\n",
    "            # evaluation mode\n",
    "            if mode == 'evaluate':\n",
    "                evaluate(model, x_test, y_test, cfg)\n",
    "        \n",
    "        yaml_files = glob.glob(f\"./original_t1dexi_experiments_{tuning_sp}min/fold{fold_number}_eval/*.yaml\")\n",
    "        mode = \"evaluate\"\n",
    "        for yaml_fp in yaml_files:\n",
    "            cfgs = load_cfgs(yaml_fp)\n",
    "            print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "            for cfg in cfgs:\n",
    "                seed = int(cfg['train']['seed'])\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                # Print the configuration - just to make sure that you loaded what you\n",
    "                # wanted to load\n",
    "\n",
    "                module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "                module_model         = load_module(cfg['model']['script_path'])\n",
    "                module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "                module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "                module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "                pp = pprint.PrettyPrinter(indent=4)\n",
    "                pp.pprint(cfg)\n",
    "\n",
    "                #print(\"loading dataset ...\")\n",
    "                #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "                #nb_past_steps_tmp = 36\n",
    "                #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "                x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "                #x_train = x_train[:,-nb_past_steps:,:]\n",
    "                #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "                #x_test = x_test[:,-nb_past_steps:,:]\n",
    "                print(\"x_train.shape: \", x_train.shape)\n",
    "                print(\"y_train.shape: \", y_train.shape)\n",
    "                print(\"x_valid.shape: \", x_valid.shape)\n",
    "                print(\"y_valid.shape: \", y_valid.shape)\n",
    "                print(\"x_test.shape: \", x_test.shape)\n",
    "                print(\"y_test.shape: \", y_test.shape)\n",
    "                #print(\"loading optimizer ...\")\n",
    "                optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "                #print(\"loading loss function ...\")\n",
    "                loss_function = module_loss_function.load()\n",
    "                #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "                #print(\"loading model ...\")\n",
    "                if 'tf_nll' in loss_function.__name__:\n",
    "                    model = module_model.load(\n",
    "                        x_train.shape[1:],\n",
    "                        y_train.shape[1]*2,\n",
    "                        cfg['model']\n",
    "                    )\n",
    "                else:\n",
    "                    model = module_model.load(\n",
    "                        x_train.shape[1:],\n",
    "                        y_train.shape[1],\n",
    "                        cfg['model']\n",
    "                    )\n",
    "\n",
    "                if 'initial_weights_path' in cfg['train']:\n",
    "                    #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "                    model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "                model.compile(\n",
    "                    optimizer=optimizer,\n",
    "                    loss=loss_function\n",
    "                )\n",
    "\n",
    "                #print(model.summary())\n",
    "\n",
    "                # training mode\n",
    "                if mode == 'train':\n",
    "                    #print(\"training model ...\")\n",
    "                    train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "                if mode == 'plot_nll':\n",
    "                    plot_nll(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_noise_experiment':\n",
    "                    plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_seg':\n",
    "                    plot_seg(model, x_test, y_test, cfg)\n",
    "                if mode == 'plot_dist':\n",
    "                    plot_target_distribution(y_test, cfg)\n",
    "\n",
    "                # evaluation mode\n",
    "                if mode == 'evaluate':\n",
    "                    evaluate(model, x_test, y_test, cfg)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./original_t1dexi_experiments_30min/all_final_experiment_fold1.yaml\" \n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 10:54:50,182 ERROR C:\\Users\\baiyi\\OneDrive\\Desktop\\Modify_GenBG\\modified_t1dexi_subset\\T1DEXI_cgm_processed\\fold1_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2025-01-21 10:54:50,186 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\fold1_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (77503, 6, 1)\n",
      "y_train.shape:  (77503, 1)\n",
      "x_valid.shape:  (19369, 6, 1)\n",
      "y_valid.shape:  (19369, 1)\n",
      "x_test.shape:  (0, 6, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 10:55:02,075 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-21 10:55:02,368 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2025-01-21 10:55:02,377 WARNING From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2025-01-21 10:55:02,387 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 77503 samples, validate on 19369 samples\n",
      "Epoch 1/10000\n",
      "77503/77503 [==============================] - 2s 23us/sample - loss: 0.5215 - val_loss: 0.0436\n",
      "Epoch 2/10000\n",
      " 7168/77503 [=>............................] - ETA: 0s - loss: 0.1377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77503/77503 [==============================] - 1s 8us/sample - loss: 0.0737 - val_loss: -0.0639\n",
      "Epoch 3/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: 0.0115 - val_loss: -0.1029\n",
      "Epoch 4/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.0142 - val_loss: -0.1280\n",
      "Epoch 5/10000\n",
      "72704/77503 [===========================>..] - ETA: 0s - loss: -0.03382025-01-21 10:55:07,585 DEBUG Creating converter from 5 to 3\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.0351 - val_loss: -0.1495\n",
      "Epoch 6/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.0488 - val_loss: -0.1683\n",
      "Epoch 7/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.0635 - val_loss: -0.1764\n",
      "Epoch 8/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.0724 - val_loss: -0.1749\n",
      "Epoch 9/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.0818 - val_loss: -0.1871\n",
      "Epoch 10/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.0873 - val_loss: -0.1802\n",
      "Epoch 11/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.0816 - val_loss: -0.1757\n",
      "Epoch 12/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1045 - val_loss: -0.1984\n",
      "Epoch 13/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1069 - val_loss: -0.2076\n",
      "Epoch 14/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1103 - val_loss: -0.1971\n",
      "Epoch 15/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1240 - val_loss: -0.1824\n",
      "Epoch 16/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1273 - val_loss: -0.2115\n",
      "Epoch 17/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1326 - val_loss: -0.2033\n",
      "Epoch 18/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1372 - val_loss: -0.2167\n",
      "Epoch 19/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1446 - val_loss: -0.2103\n",
      "Epoch 20/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1427 - val_loss: -0.2320\n",
      "Epoch 21/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1487 - val_loss: -0.2295\n",
      "Epoch 22/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1522 - val_loss: -0.2212\n",
      "Epoch 23/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1565 - val_loss: -0.2289\n",
      "Epoch 24/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1554 - val_loss: -0.2237\n",
      "Epoch 25/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.1616 - val_loss: -0.2321\n",
      "Epoch 26/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.1632 - val_loss: -0.2259\n",
      "Epoch 27/10000\n",
      "77503/77503 [==============================] - 1s 14us/sample - loss: -0.1666 - val_loss: -0.2366\n",
      "Epoch 28/10000\n",
      "77503/77503 [==============================] - 1s 13us/sample - loss: -0.1738 - val_loss: -0.2408\n",
      "Epoch 29/10000\n",
      "77503/77503 [==============================] - 1s 15us/sample - loss: -0.1735 - val_loss: -0.2327\n",
      "Epoch 30/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.1745 - val_loss: -0.2492\n",
      "Epoch 31/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1829 - val_loss: -0.2254\n",
      "Epoch 32/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1813 - val_loss: -0.2403\n",
      "Epoch 33/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.1819 - val_loss: -0.2400\n",
      "Epoch 34/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1877 - val_loss: -0.2425\n",
      "Epoch 35/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.1931 - val_loss: -0.2536\n",
      "Epoch 36/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.1856 - val_loss: -0.2511\n",
      "Epoch 37/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.1915 - val_loss: -0.2449\n",
      "Epoch 38/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.1926 - val_loss: -0.2554\n",
      "Epoch 39/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.1961 - val_loss: -0.2425\n",
      "Epoch 40/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.1980 - val_loss: -0.2521\n",
      "Epoch 41/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2004 - val_loss: -0.2529\n",
      "Epoch 42/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2025 - val_loss: -0.2476\n",
      "Epoch 43/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.1970 - val_loss: -0.2548\n",
      "Epoch 44/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2063 - val_loss: -0.2523\n",
      "Epoch 45/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2011 - val_loss: -0.2433\n",
      "Epoch 46/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2024 - val_loss: -0.2532\n",
      "Epoch 47/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2099 - val_loss: -0.2581\n",
      "Epoch 48/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2109 - val_loss: -0.2634\n",
      "Epoch 49/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2103 - val_loss: -0.2579\n",
      "Epoch 50/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2113 - val_loss: -0.2617\n",
      "Epoch 51/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2134 - val_loss: -0.2582\n",
      "Epoch 52/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2184 - val_loss: -0.2654\n",
      "Epoch 53/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2167 - val_loss: -0.2573\n",
      "Epoch 54/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2162 - val_loss: -0.2571\n",
      "Epoch 55/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2201 - val_loss: -0.2551\n",
      "Epoch 56/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2244 - val_loss: -0.2652\n",
      "Epoch 57/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2209 - val_loss: -0.2658\n",
      "Epoch 58/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2270 - val_loss: -0.2615\n",
      "Epoch 59/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2273 - val_loss: -0.2532\n",
      "Epoch 60/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2240 - val_loss: -0.2667\n",
      "Epoch 61/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2279 - val_loss: -0.2635\n",
      "Epoch 62/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2303 - val_loss: -0.2631\n",
      "Epoch 63/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2307 - val_loss: -0.2648\n",
      "Epoch 64/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2284 - val_loss: -0.2631\n",
      "Epoch 65/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2326 - val_loss: -0.2685\n",
      "Epoch 66/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2316 - val_loss: -0.2661\n",
      "Epoch 67/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2366 - val_loss: -0.2642\n",
      "Epoch 68/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2370 - val_loss: -0.2690\n",
      "Epoch 69/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2347 - val_loss: -0.2662\n",
      "Epoch 70/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2337 - val_loss: -0.2676\n",
      "Epoch 71/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2317 - val_loss: -0.2607\n",
      "Epoch 72/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2408 - val_loss: -0.2607\n",
      "Epoch 73/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2384 - val_loss: -0.2606\n",
      "Epoch 74/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2407 - val_loss: -0.2712\n",
      "Epoch 75/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2424 - val_loss: -0.2711\n",
      "Epoch 76/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2439 - val_loss: -0.2765\n",
      "Epoch 77/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2407 - val_loss: -0.2725\n",
      "Epoch 78/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2462 - val_loss: -0.2679\n",
      "Epoch 79/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2445 - val_loss: -0.2627\n",
      "Epoch 80/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2422 - val_loss: -0.2479\n",
      "Epoch 81/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2426 - val_loss: -0.2748\n",
      "Epoch 82/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2468 - val_loss: -0.2713\n",
      "Epoch 83/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2466 - val_loss: -0.2694\n",
      "Epoch 84/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2467 - val_loss: -0.2718\n",
      "Epoch 85/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2466 - val_loss: -0.2742\n",
      "Epoch 86/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2472 - val_loss: -0.2770\n",
      "Epoch 87/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2494 - val_loss: -0.2653\n",
      "Epoch 88/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2514 - val_loss: -0.2694\n",
      "Epoch 89/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2510 - val_loss: -0.2693\n",
      "Epoch 90/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2487 - val_loss: -0.2765\n",
      "Epoch 91/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2506 - val_loss: -0.2673\n",
      "Epoch 92/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2525 - val_loss: -0.2744\n",
      "Epoch 93/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2522 - val_loss: -0.2747\n",
      "Epoch 94/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2544 - val_loss: -0.2679\n",
      "Epoch 95/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2518 - val_loss: -0.2696\n",
      "Epoch 96/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2536 - val_loss: -0.2721\n",
      "Epoch 97/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2561 - val_loss: -0.2669\n",
      "Epoch 98/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2570 - val_loss: -0.2802\n",
      "Epoch 99/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2561 - val_loss: -0.2728\n",
      "Epoch 100/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2551 - val_loss: -0.2751\n",
      "Epoch 101/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2550 - val_loss: -0.2786\n",
      "Epoch 102/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2539 - val_loss: -0.2736\n",
      "Epoch 103/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2558 - val_loss: -0.2710\n",
      "Epoch 104/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2609 - val_loss: -0.2675\n",
      "Epoch 105/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2584 - val_loss: -0.2784\n",
      "Epoch 106/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2541 - val_loss: -0.2719\n",
      "Epoch 107/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2545 - val_loss: -0.2729\n",
      "Epoch 108/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2570 - val_loss: -0.2711\n",
      "Epoch 109/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2598 - val_loss: -0.2778\n",
      "Epoch 110/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2592 - val_loss: -0.2763\n",
      "Epoch 111/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2595 - val_loss: -0.2797\n",
      "Epoch 112/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2588 - val_loss: -0.2694\n",
      "Epoch 113/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2594 - val_loss: -0.2755\n",
      "Epoch 114/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2606 - val_loss: -0.2723\n",
      "Epoch 115/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2601 - val_loss: -0.2734\n",
      "Epoch 116/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2600 - val_loss: -0.2786\n",
      "Epoch 117/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2624 - val_loss: -0.2753\n",
      "Epoch 118/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2624 - val_loss: -0.2799\n",
      "Epoch 119/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2618 - val_loss: -0.2789\n",
      "Epoch 120/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2619 - val_loss: -0.2780\n",
      "Epoch 121/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2617 - val_loss: -0.2739\n",
      "Epoch 122/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2611 - val_loss: -0.2666\n",
      "Epoch 123/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2617 - val_loss: -0.2745\n",
      "Epoch 124/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2617 - val_loss: -0.2746\n",
      "Epoch 125/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2570 - val_loss: -0.2732\n",
      "Epoch 126/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2602 - val_loss: -0.2770\n",
      "Epoch 127/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2638 - val_loss: -0.2752\n",
      "Epoch 128/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2637 - val_loss: -0.2743\n",
      "Epoch 129/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2632 - val_loss: -0.2733\n",
      "Epoch 130/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2621 - val_loss: -0.2748\n",
      "Epoch 131/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2621 - val_loss: -0.2729\n",
      "Epoch 132/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2615 - val_loss: -0.2762\n",
      "Epoch 133/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2629 - val_loss: -0.2737\n",
      "Epoch 134/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2629 - val_loss: -0.2770\n",
      "Epoch 135/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2631 - val_loss: -0.2716\n",
      "Epoch 136/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2628 - val_loss: -0.2756\n",
      "Epoch 137/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2621 - val_loss: -0.2753\n",
      "Epoch 138/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2657 - val_loss: -0.2704\n",
      "Epoch 139/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2631 - val_loss: -0.2716\n",
      "Epoch 140/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2631 - val_loss: -0.2808\n",
      "Epoch 141/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2657 - val_loss: -0.2728\n",
      "Epoch 142/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2664 - val_loss: -0.2805\n",
      "Epoch 143/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2647 - val_loss: -0.2807\n",
      "Epoch 144/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2642 - val_loss: -0.2642\n",
      "Epoch 145/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2664 - val_loss: -0.2788\n",
      "Epoch 146/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2659 - val_loss: -0.2743\n",
      "Epoch 147/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2644 - val_loss: -0.2763\n",
      "Epoch 148/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2673 - val_loss: -0.2772\n",
      "Epoch 149/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2662 - val_loss: -0.2776\n",
      "Epoch 150/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2674 - val_loss: -0.2794\n",
      "Epoch 151/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2644 - val_loss: -0.2826\n",
      "Epoch 152/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2677 - val_loss: -0.2780\n",
      "Epoch 153/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2661 - val_loss: -0.2752\n",
      "Epoch 154/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2653 - val_loss: -0.2760\n",
      "Epoch 155/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2657 - val_loss: -0.2813\n",
      "Epoch 156/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2682 - val_loss: -0.2822\n",
      "Epoch 157/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2687 - val_loss: -0.2738\n",
      "Epoch 158/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2677 - val_loss: -0.2792\n",
      "Epoch 159/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2653 - val_loss: -0.2793\n",
      "Epoch 160/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2674 - val_loss: -0.2757\n",
      "Epoch 161/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2704 - val_loss: -0.2841\n",
      "Epoch 162/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2675 - val_loss: -0.2779\n",
      "Epoch 163/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2655 - val_loss: -0.2771\n",
      "Epoch 164/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2678 - val_loss: -0.2842\n",
      "Epoch 165/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2696 - val_loss: -0.2764\n",
      "Epoch 166/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2670 - val_loss: -0.2795\n",
      "Epoch 167/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2684 - val_loss: -0.2794\n",
      "Epoch 168/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2696 - val_loss: -0.2758\n",
      "Epoch 169/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2694 - val_loss: -0.2839\n",
      "Epoch 170/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2711 - val_loss: -0.2744\n",
      "Epoch 171/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2701 - val_loss: -0.2833\n",
      "Epoch 172/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2685 - val_loss: -0.2797\n",
      "Epoch 173/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2679 - val_loss: -0.2800\n",
      "Epoch 174/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2703 - val_loss: -0.2757\n",
      "Epoch 175/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2678 - val_loss: -0.2819\n",
      "Epoch 176/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2717 - val_loss: -0.2794\n",
      "Epoch 177/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2684 - val_loss: -0.2803\n",
      "Epoch 178/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2718 - val_loss: -0.2778\n",
      "Epoch 179/10000\n",
      "77503/77503 [==============================] - 1s 13us/sample - loss: -0.2736 - val_loss: -0.2817\n",
      "Epoch 180/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.2725 - val_loss: -0.2804\n",
      "Epoch 181/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.2705 - val_loss: -0.2818\n",
      "Epoch 182/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2697 - val_loss: -0.2759\n",
      "Epoch 183/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.2716 - val_loss: -0.2831\n",
      "Epoch 184/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2709 - val_loss: -0.2698\n",
      "Epoch 185/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2709 - val_loss: -0.2838\n",
      "Epoch 186/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2728 - val_loss: -0.2799\n",
      "Epoch 187/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2715 - val_loss: -0.2811\n",
      "Epoch 188/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2705 - val_loss: -0.2783\n",
      "Epoch 189/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2712 - val_loss: -0.2830\n",
      "Epoch 190/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2725 - val_loss: -0.2729\n",
      "Epoch 191/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2712 - val_loss: -0.2830\n",
      "Epoch 192/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2727 - val_loss: -0.2829\n",
      "Epoch 193/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2730 - val_loss: -0.2824\n",
      "Epoch 194/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2732 - val_loss: -0.2781\n",
      "Epoch 195/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2689 - val_loss: -0.2772\n",
      "Epoch 196/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2735 - val_loss: -0.2867\n",
      "Epoch 197/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2742 - val_loss: -0.2834\n",
      "Epoch 198/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2692 - val_loss: -0.2747\n",
      "Epoch 199/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2739 - val_loss: -0.2848\n",
      "Epoch 200/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2753 - val_loss: -0.2781\n",
      "Epoch 201/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2749 - val_loss: -0.2814\n",
      "Epoch 202/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2734 - val_loss: -0.2820\n",
      "Epoch 203/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2749 - val_loss: -0.2824\n",
      "Epoch 204/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2753 - val_loss: -0.2810\n",
      "Epoch 205/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2747 - val_loss: -0.2809\n",
      "Epoch 206/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2732 - val_loss: -0.2801\n",
      "Epoch 207/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2718 - val_loss: -0.2793\n",
      "Epoch 208/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2752 - val_loss: -0.2825\n",
      "Epoch 209/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2750 - val_loss: -0.2830\n",
      "Epoch 210/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2739 - val_loss: -0.2823\n",
      "Epoch 211/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2742 - val_loss: -0.2782\n",
      "Epoch 212/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2758 - val_loss: -0.2705\n",
      "Epoch 213/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2739 - val_loss: -0.2808\n",
      "Epoch 214/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2728 - val_loss: -0.2839\n",
      "Epoch 215/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2762 - val_loss: -0.2833\n",
      "Epoch 216/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2750 - val_loss: -0.2827\n",
      "Epoch 217/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2763 - val_loss: -0.2855\n",
      "Epoch 218/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2782 - val_loss: -0.2754\n",
      "Epoch 219/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2756 - val_loss: -0.2783\n",
      "Epoch 220/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2766 - val_loss: -0.2784\n",
      "Epoch 221/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2752 - val_loss: -0.2849\n",
      "Epoch 222/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2764 - val_loss: -0.2839\n",
      "Epoch 223/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2781 - val_loss: -0.2826\n",
      "Epoch 224/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2785 - val_loss: -0.2854\n",
      "Epoch 225/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2766 - val_loss: -0.2825\n",
      "Epoch 226/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2764 - val_loss: -0.2790\n",
      "Epoch 227/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2779 - val_loss: -0.2866\n",
      "Epoch 228/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2749 - val_loss: -0.2777\n",
      "Epoch 229/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2747 - val_loss: -0.2852\n",
      "Epoch 230/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2778 - val_loss: -0.2850\n",
      "Epoch 231/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2772 - val_loss: -0.2818\n",
      "Epoch 232/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2801 - val_loss: -0.2803\n",
      "Epoch 233/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2752 - val_loss: -0.2806\n",
      "Epoch 234/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2782 - val_loss: -0.2820\n",
      "Epoch 235/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2797 - val_loss: -0.2749\n",
      "Epoch 236/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2772 - val_loss: -0.2813\n",
      "Epoch 237/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2764 - val_loss: -0.2826\n",
      "Epoch 238/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2771 - val_loss: -0.2839\n",
      "Epoch 239/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2770 - val_loss: -0.2785\n",
      "Epoch 240/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2781 - val_loss: -0.2846\n",
      "Epoch 241/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2787 - val_loss: -0.2781\n",
      "Epoch 242/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2750 - val_loss: -0.2752\n",
      "Epoch 243/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2773 - val_loss: -0.2816\n",
      "Epoch 244/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2755 - val_loss: -0.2794\n",
      "Epoch 245/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2773 - val_loss: -0.2766\n",
      "Epoch 246/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2765 - val_loss: -0.2849\n",
      "Epoch 247/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2749 - val_loss: -0.2837\n",
      "Epoch 248/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2790 - val_loss: -0.2790\n",
      "Epoch 249/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2782 - val_loss: -0.2778\n",
      "Epoch 250/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2795 - val_loss: -0.2800\n",
      "Epoch 251/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2797 - val_loss: -0.2860\n",
      "Epoch 252/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2781 - val_loss: -0.2640\n",
      "Epoch 253/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2781 - val_loss: -0.2811\n",
      "Epoch 254/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2781 - val_loss: -0.2823\n",
      "Epoch 255/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2764 - val_loss: -0.2783\n",
      "Epoch 256/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2812 - val_loss: -0.2800\n",
      "Epoch 257/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2805 - val_loss: -0.2855\n",
      "Epoch 258/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2795 - val_loss: -0.2855\n",
      "Epoch 259/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2789 - val_loss: -0.2785\n",
      "Epoch 260/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2800 - val_loss: -0.2825\n",
      "Epoch 261/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2769 - val_loss: -0.2851\n",
      "Epoch 262/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2792 - val_loss: -0.2832\n",
      "Epoch 263/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2812 - val_loss: -0.2847\n",
      "Epoch 264/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2794 - val_loss: -0.2789\n",
      "Epoch 265/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2791 - val_loss: -0.2774\n",
      "Epoch 266/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2775 - val_loss: -0.2781\n",
      "Epoch 267/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2779 - val_loss: -0.2851\n",
      "Epoch 268/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2812 - val_loss: -0.2837\n",
      "Epoch 269/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2799 - val_loss: -0.2839\n",
      "Epoch 270/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2808 - val_loss: -0.2868\n",
      "Epoch 271/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2781 - val_loss: -0.2761\n",
      "Epoch 272/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2796 - val_loss: -0.2828\n",
      "Epoch 273/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2850 - val_loss: -0.2799\n",
      "Epoch 274/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2810 - val_loss: -0.2853\n",
      "Epoch 275/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2800 - val_loss: -0.2803\n",
      "Epoch 276/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2835 - val_loss: -0.2861\n",
      "Epoch 277/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2826 - val_loss: -0.2843\n",
      "Epoch 278/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2818 - val_loss: -0.2761\n",
      "Epoch 279/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2815 - val_loss: -0.2865\n",
      "Epoch 280/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2795 - val_loss: -0.2820\n",
      "Epoch 281/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2805 - val_loss: -0.2816\n",
      "Epoch 282/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2793 - val_loss: -0.2810\n",
      "Epoch 283/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2823 - val_loss: -0.2832\n",
      "Epoch 284/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2821 - val_loss: -0.2826\n",
      "Epoch 285/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2818 - val_loss: -0.2863\n",
      "Epoch 286/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2815 - val_loss: -0.2843\n",
      "Epoch 287/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2810 - val_loss: -0.2775\n",
      "Epoch 288/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2799 - val_loss: -0.2826\n",
      "Epoch 289/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2823 - val_loss: -0.2849\n",
      "Epoch 290/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2799 - val_loss: -0.2838\n",
      "Epoch 291/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2816 - val_loss: -0.2847\n",
      "Epoch 292/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2813 - val_loss: -0.2848\n",
      "Epoch 293/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2823 - val_loss: -0.2732\n",
      "Epoch 294/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2800 - val_loss: -0.2755\n",
      "Epoch 295/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2827 - val_loss: -0.2812\n",
      "Epoch 296/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2838 - val_loss: -0.2816\n",
      "Epoch 297/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2816 - val_loss: -0.2765\n",
      "Epoch 298/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2815 - val_loss: -0.2785\n",
      "Epoch 299/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2839 - val_loss: -0.2845\n",
      "Epoch 300/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2823 - val_loss: -0.2796\n",
      "Epoch 301/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2833\n",
      "Epoch 302/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2819 - val_loss: -0.2827\n",
      "Epoch 303/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2831 - val_loss: -0.2843\n",
      "Epoch 304/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2816 - val_loss: -0.2835\n",
      "Epoch 305/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2831 - val_loss: -0.2840\n",
      "Epoch 306/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2855 - val_loss: -0.2803\n",
      "Epoch 307/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2848 - val_loss: -0.2603\n",
      "Epoch 308/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2813 - val_loss: -0.2870\n",
      "Epoch 309/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2827 - val_loss: -0.2851\n",
      "Epoch 310/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2811 - val_loss: -0.2839\n",
      "Epoch 311/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2838 - val_loss: -0.2813\n",
      "Epoch 312/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2841 - val_loss: -0.2818\n",
      "Epoch 313/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2832 - val_loss: -0.2785\n",
      "Epoch 314/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2826 - val_loss: -0.2853\n",
      "Epoch 315/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2837 - val_loss: -0.2856\n",
      "Epoch 316/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2805 - val_loss: -0.2816\n",
      "Epoch 317/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2875\n",
      "Epoch 318/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2827 - val_loss: -0.2871\n",
      "Epoch 319/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2855 - val_loss: -0.2699\n",
      "Epoch 320/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2829 - val_loss: -0.2792\n",
      "Epoch 321/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2824 - val_loss: -0.2837\n",
      "Epoch 322/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2860 - val_loss: -0.2838\n",
      "Epoch 323/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2844 - val_loss: -0.2851\n",
      "Epoch 324/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2822 - val_loss: -0.2824\n",
      "Epoch 325/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2837 - val_loss: -0.2824\n",
      "Epoch 326/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2837 - val_loss: -0.2844\n",
      "Epoch 327/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2838 - val_loss: -0.2809\n",
      "Epoch 328/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2850 - val_loss: -0.2872\n",
      "Epoch 329/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2855 - val_loss: -0.2831\n",
      "Epoch 330/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2857 - val_loss: -0.2842\n",
      "Epoch 331/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2838 - val_loss: -0.2865\n",
      "Epoch 332/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2863 - val_loss: -0.2839\n",
      "Epoch 333/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2837 - val_loss: -0.2691\n",
      "Epoch 334/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2841 - val_loss: -0.2846\n",
      "Epoch 335/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2845\n",
      "Epoch 336/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2858 - val_loss: -0.2844\n",
      "Epoch 337/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2829 - val_loss: -0.2843\n",
      "Epoch 338/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2857 - val_loss: -0.2821\n",
      "Epoch 339/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2871 - val_loss: -0.2831\n",
      "Epoch 340/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2869 - val_loss: -0.2856\n",
      "Epoch 341/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2876 - val_loss: -0.2832\n",
      "Epoch 342/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2830 - val_loss: -0.2851\n",
      "Epoch 343/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2859 - val_loss: -0.2789\n",
      "Epoch 344/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2844 - val_loss: -0.2830\n",
      "Epoch 345/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2867 - val_loss: -0.2836\n",
      "Epoch 346/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2851 - val_loss: -0.2794\n",
      "Epoch 347/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2865 - val_loss: -0.2825\n",
      "Epoch 348/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2874 - val_loss: -0.2735\n",
      "Epoch 349/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2855 - val_loss: -0.2851\n",
      "Epoch 350/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2861\n",
      "Epoch 351/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2846 - val_loss: -0.2816\n",
      "Epoch 352/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2856 - val_loss: -0.2808\n",
      "Epoch 353/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2866 - val_loss: -0.2865\n",
      "Epoch 354/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2862 - val_loss: -0.2830\n",
      "Epoch 355/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2882 - val_loss: -0.2835\n",
      "Epoch 356/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2848 - val_loss: -0.2826\n",
      "Epoch 357/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2878 - val_loss: -0.2842\n",
      "Epoch 358/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2839 - val_loss: -0.2799\n",
      "Epoch 359/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2864 - val_loss: -0.2804\n",
      "Epoch 360/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2867 - val_loss: -0.2839\n",
      "Epoch 361/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2823\n",
      "Epoch 362/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2872 - val_loss: -0.2823\n",
      "Epoch 363/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2863 - val_loss: -0.2832\n",
      "Epoch 364/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2877 - val_loss: -0.2830\n",
      "Epoch 365/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2890 - val_loss: -0.2847\n",
      "Epoch 366/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2861 - val_loss: -0.2836\n",
      "Epoch 367/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2868 - val_loss: -0.2873\n",
      "Epoch 368/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2874 - val_loss: -0.2853\n",
      "Epoch 369/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2897 - val_loss: -0.2850\n",
      "Epoch 370/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2872 - val_loss: -0.2841\n",
      "Epoch 371/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2903 - val_loss: -0.2835\n",
      "Epoch 372/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2868 - val_loss: -0.2801\n",
      "Epoch 373/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.2878 - val_loss: -0.2812\n",
      "Epoch 374/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2847 - val_loss: -0.2851\n",
      "Epoch 375/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2876 - val_loss: -0.2845\n",
      "Epoch 376/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2885 - val_loss: -0.2854\n",
      "Epoch 377/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2867 - val_loss: -0.2858\n",
      "Epoch 378/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2894 - val_loss: -0.2853\n",
      "Epoch 379/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2845 - val_loss: -0.2829\n",
      "Epoch 380/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2883 - val_loss: -0.2843\n",
      "Epoch 381/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.2873 - val_loss: -0.2845\n",
      "Epoch 382/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2876 - val_loss: -0.2712\n",
      "Epoch 383/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2901 - val_loss: -0.2829\n",
      "Epoch 384/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2895 - val_loss: -0.2836\n",
      "Epoch 385/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2871 - val_loss: -0.2797\n",
      "Epoch 386/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2873 - val_loss: -0.2828\n",
      "Epoch 387/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2884 - val_loss: -0.2851\n",
      "Epoch 388/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2864 - val_loss: -0.2772\n",
      "Epoch 389/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2868 - val_loss: -0.2839\n",
      "Epoch 390/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2901 - val_loss: -0.2816\n",
      "Epoch 391/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2882 - val_loss: -0.2812\n",
      "Epoch 392/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2892 - val_loss: -0.2836\n",
      "Epoch 393/10000\n",
      "77503/77503 [==============================] - 1s 15us/sample - loss: -0.2918 - val_loss: -0.2829\n",
      "Epoch 394/10000\n",
      "77503/77503 [==============================] - 1s 13us/sample - loss: -0.2908 - val_loss: -0.2807\n",
      "Epoch 395/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2891 - val_loss: -0.2836\n",
      "Epoch 396/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2879 - val_loss: -0.2826\n",
      "Epoch 397/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2875 - val_loss: -0.2823\n",
      "Epoch 398/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2895 - val_loss: -0.2826\n",
      "Epoch 399/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2902 - val_loss: -0.2810\n",
      "Epoch 400/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2891 - val_loss: -0.2769\n",
      "Epoch 401/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2900 - val_loss: -0.2819\n",
      "Epoch 402/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2892 - val_loss: -0.2843\n",
      "Epoch 403/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2895 - val_loss: -0.2838\n",
      "Epoch 404/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2888 - val_loss: -0.2768\n",
      "Epoch 405/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2898 - val_loss: -0.2660\n",
      "Epoch 406/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2883 - val_loss: -0.2776\n",
      "Epoch 407/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2914 - val_loss: -0.2847\n",
      "Epoch 408/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2881 - val_loss: -0.2857\n",
      "Epoch 409/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2899 - val_loss: -0.2818\n",
      "Epoch 410/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2890 - val_loss: -0.2849\n",
      "Epoch 411/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2933 - val_loss: -0.2847\n",
      "Epoch 412/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2892 - val_loss: -0.2761\n",
      "Epoch 413/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2880 - val_loss: -0.2786\n",
      "Epoch 414/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2924 - val_loss: -0.2818\n",
      "Epoch 415/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2890 - val_loss: -0.2832\n",
      "Epoch 416/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2923 - val_loss: -0.2841\n",
      "Epoch 417/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2929 - val_loss: -0.2859\n",
      "Epoch 418/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2905 - val_loss: -0.2864\n",
      "Epoch 419/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2920 - val_loss: -0.2812\n",
      "Epoch 420/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2926 - val_loss: -0.2833\n",
      "Epoch 421/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2918 - val_loss: -0.2852\n",
      "Epoch 422/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2939 - val_loss: -0.2827\n",
      "Epoch 423/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2924 - val_loss: -0.2853\n",
      "Epoch 424/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2939 - val_loss: -0.2839\n",
      "Epoch 425/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2897 - val_loss: -0.2806\n",
      "Epoch 426/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2899 - val_loss: -0.2822\n",
      "Epoch 427/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2912 - val_loss: -0.2859\n",
      "Epoch 428/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2913 - val_loss: -0.2754\n",
      "Epoch 429/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2922 - val_loss: -0.2725\n",
      "Epoch 430/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2915 - val_loss: -0.2849\n",
      "Epoch 431/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2918 - val_loss: -0.2822\n",
      "Epoch 432/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2905 - val_loss: -0.2814\n",
      "Epoch 433/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2909 - val_loss: -0.2787\n",
      "Epoch 434/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2910 - val_loss: -0.2765\n",
      "Epoch 435/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2900 - val_loss: -0.2836\n",
      "Epoch 436/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2914 - val_loss: -0.2820\n",
      "Epoch 437/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2937 - val_loss: -0.2847\n",
      "Epoch 438/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2912 - val_loss: -0.2835\n",
      "Epoch 439/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2904 - val_loss: -0.2810\n",
      "Epoch 440/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2928 - val_loss: -0.2798\n",
      "Epoch 441/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2925 - val_loss: -0.2818\n",
      "Epoch 442/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2927 - val_loss: -0.2794\n",
      "Epoch 443/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2940 - val_loss: -0.2757\n",
      "Epoch 444/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2911 - val_loss: -0.2809\n",
      "Epoch 445/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2892 - val_loss: -0.2814\n",
      "Epoch 446/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2929 - val_loss: -0.2776\n",
      "Epoch 447/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2942 - val_loss: -0.2775\n",
      "Epoch 448/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2914 - val_loss: -0.2751\n",
      "Epoch 449/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2931 - val_loss: -0.2805\n",
      "Epoch 450/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2907 - val_loss: -0.2828\n",
      "Epoch 451/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2953 - val_loss: -0.2802\n",
      "Epoch 452/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2931 - val_loss: -0.2813\n",
      "Epoch 453/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2917 - val_loss: -0.2759\n",
      "Epoch 454/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2889 - val_loss: -0.2827\n",
      "Epoch 455/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2925 - val_loss: -0.2784\n",
      "Epoch 456/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2942 - val_loss: -0.2840\n",
      "Epoch 457/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2932 - val_loss: -0.2783\n",
      "Epoch 458/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2939 - val_loss: -0.2832\n",
      "Epoch 459/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2930 - val_loss: -0.2803\n",
      "Epoch 460/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2930 - val_loss: -0.2740\n",
      "Epoch 461/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2910 - val_loss: -0.2799\n",
      "Epoch 462/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2928 - val_loss: -0.2829\n",
      "Epoch 463/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2955 - val_loss: -0.2854\n",
      "Epoch 464/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2935 - val_loss: -0.2817\n",
      "Epoch 465/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2898 - val_loss: -0.2833\n",
      "Epoch 466/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2923 - val_loss: -0.2784\n",
      "Epoch 467/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2951 - val_loss: -0.2795\n",
      "Epoch 468/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2954 - val_loss: -0.2853\n",
      "Epoch 469/10000\n",
      "77503/77503 [==============================] - 1s 12us/sample - loss: -0.2937 - val_loss: -0.2838\n",
      "Epoch 470/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2923 - val_loss: -0.2806\n",
      "Epoch 471/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2925 - val_loss: -0.2761\n",
      "Epoch 472/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2964 - val_loss: -0.2791\n",
      "Epoch 473/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2939 - val_loss: -0.2799\n",
      "Epoch 474/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2942 - val_loss: -0.2822\n",
      "Epoch 475/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2951 - val_loss: -0.2843\n",
      "Epoch 476/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2933 - val_loss: -0.2807\n",
      "Epoch 477/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2968 - val_loss: -0.2788\n",
      "Epoch 478/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2951 - val_loss: -0.2744\n",
      "Epoch 479/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2943 - val_loss: -0.2802\n",
      "Epoch 480/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2933 - val_loss: -0.2767\n",
      "Epoch 481/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2924 - val_loss: -0.2826\n",
      "Epoch 482/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2945 - val_loss: -0.2804\n",
      "Epoch 483/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2958 - val_loss: -0.2802\n",
      "Epoch 484/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2953 - val_loss: -0.2801\n",
      "Epoch 485/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2957 - val_loss: -0.2752\n",
      "Epoch 486/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2956 - val_loss: -0.2809\n",
      "Epoch 487/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2963 - val_loss: -0.2773\n",
      "Epoch 488/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2960 - val_loss: -0.2737\n",
      "Epoch 489/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2953 - val_loss: -0.2767\n",
      "Epoch 490/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2957 - val_loss: -0.2808\n",
      "Epoch 491/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2963 - val_loss: -0.2809\n",
      "Epoch 492/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2921 - val_loss: -0.2800\n",
      "Epoch 493/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2963 - val_loss: -0.2832\n",
      "Epoch 494/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2965 - val_loss: -0.2833\n",
      "Epoch 495/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2957 - val_loss: -0.2774\n",
      "Epoch 496/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2942 - val_loss: -0.2823\n",
      "Epoch 497/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2962 - val_loss: -0.2806\n",
      "Epoch 498/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2935 - val_loss: -0.2683\n",
      "Epoch 499/10000\n",
      "77503/77503 [==============================] - 1s 11us/sample - loss: -0.2924 - val_loss: -0.2784\n",
      "Epoch 500/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2978 - val_loss: -0.2806\n",
      "Epoch 501/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2951 - val_loss: -0.2820\n",
      "Epoch 502/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2953 - val_loss: -0.2752\n",
      "Epoch 503/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2973 - val_loss: -0.2817\n",
      "Epoch 504/10000\n",
      "77503/77503 [==============================] - 1s 10us/sample - loss: -0.2965 - val_loss: -0.2799\n",
      "Epoch 505/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2990 - val_loss: -0.2772\n",
      "Epoch 506/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2978 - val_loss: -0.2796\n",
      "Epoch 507/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2975 - val_loss: -0.2748\n",
      "Epoch 508/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2970 - val_loss: -0.2745\n",
      "Epoch 509/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2981 - val_loss: -0.2804\n",
      "Epoch 510/10000\n",
      "77503/77503 [==============================] - 1s 7us/sample - loss: -0.2938 - val_loss: -0.2784\n",
      "Epoch 511/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2981 - val_loss: -0.2800\n",
      "Epoch 512/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2954 - val_loss: -0.2816\n",
      "Epoch 513/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2959 - val_loss: -0.2805\n",
      "Epoch 514/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2958 - val_loss: -0.2806\n",
      "Epoch 515/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2975 - val_loss: -0.2815\n",
      "Epoch 516/10000\n",
      "77503/77503 [==============================] - 1s 9us/sample - loss: -0.2958 - val_loss: -0.2806\n",
      "Epoch 517/10000\n",
      "77503/77503 [==============================] - 1s 8us/sample - loss: -0.2954 - val_loss: -0.2787\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yaml_files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./original_t1dexi_experiments_30min/fold1_eval/*.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "yaml_files = glob.glob(f\"./original_t1dexi_experiments_30min/fold1_eval/*.yaml\")\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./original_t1dexi_experiments_30min/fold1_eval\\\\103_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\114_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\115_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\11_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\144_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\152_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\173_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\187_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\18_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\1_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\248_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\24_evaluation.yaml',\n",
       " './original_t1dexi_experiments_30min/fold1_eval\\\\25_evaluation.yaml']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\103.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5701, 6, 1)\n",
      "y_test.shape:  (5701, 1)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:35,282 WARNING Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n",
      "2025-01-21 11:06:35,382 DEBUG Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  103.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\114.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7700, 6, 1)\n",
      "y_test.shape:  (7700, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:37,218 WARNING Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  114.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\115.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7931, 6, 1)\n",
      "y_test.shape:  (7931, 1)\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:39,282 WARNING Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  115.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\11.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7970, 6, 1)\n",
      "y_test.shape:  (7970, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:41,382 WARNING Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  11.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\144.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7456, 6, 1)\n",
      "y_test.shape:  (7456, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:43,564 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  144.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\152.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7869, 6, 1)\n",
      "y_test.shape:  (7869, 1)\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:45,682 WARNING Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  152.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\173.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7806, 6, 1)\n",
      "y_test.shape:  (7806, 1)\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:47,813 WARNING Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  173.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\187.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7871, 6, 1)\n",
      "y_test.shape:  (7871, 1)\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:50,004 WARNING Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  187.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\18.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7944, 6, 1)\n",
      "y_test.shape:  (7944, 1)\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:52,184 WARNING Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  18.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\1.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7799, 6, 1)\n",
      "y_test.shape:  (7799, 1)\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:54,427 WARNING Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\248.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (5698, 6, 1)\n",
      "y_test.shape:  (5698, 1)\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:56,628 WARNING Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  248.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\24.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7684, 6, 1)\n",
      "y_test.shape:  (7684, 1)\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:06:58,626 WARNING Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  24.csv\n",
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\Modify_GenBG\\\\modified_t1dexi_subset\\\\T1DEXI_cgm_processed\\\\25.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1.0,\n",
      "                   'train_fraction': 0.0,\n",
      "                   'valid_fraction': 0.0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': 0.001,\n",
      "                     'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\artifacts\\\\martinsson_t1dexi_experiment_6sh\\\\nb_future_steps_6_seed_20_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 10000,\n",
      "                 'param_seed': [20],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\baiyi\\\\OneDrive\\\\Desktop\\\\ReproGenBG_ML4H\\\\MartinssonAndvanDoorn\\\\train\\\\train_keras.py',\n",
      "                 'seed': 20,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (7443, 6, 1)\n",
      "y_test.shape:  (7443, 1)\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2025-01-21 11:07:00,882 WARNING Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\baiyi\\OneDrive\\Desktop\\ReproGenBG_ML4H\\MartinssonAndvanDoorn\\artifacts\\martinsson_t1dexi_experiment_6sh\\nb_future_steps_6_seed_20_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  25.csv\n"
     ]
    }
   ],
   "source": [
    "for yaml_fp in yaml_files:\n",
    "    cfgs = load_cfgs(yaml_fp)\n",
    "    print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "    for cfg in cfgs:\n",
    "        seed = int(cfg['train']['seed'])\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Print the configuration - just to make sure that you loaded what you\n",
    "        # wanted to load\n",
    "\n",
    "        module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "        module_model         = load_module(cfg['model']['script_path'])\n",
    "        module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "        module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "        module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(cfg)\n",
    "\n",
    "        #print(\"loading dataset ...\")\n",
    "        #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "        #nb_past_steps_tmp = 36\n",
    "        #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "        x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "        #x_train = x_train[:,-nb_past_steps:,:]\n",
    "        #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "        #x_test = x_test[:,-nb_past_steps:,:]\n",
    "        print(\"x_train.shape: \", x_train.shape)\n",
    "        print(\"y_train.shape: \", y_train.shape)\n",
    "        print(\"x_valid.shape: \", x_valid.shape)\n",
    "        print(\"y_valid.shape: \", y_valid.shape)\n",
    "        print(\"x_test.shape: \", x_test.shape)\n",
    "        print(\"y_test.shape: \", y_test.shape)\n",
    "        #print(\"loading optimizer ...\")\n",
    "        optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "        #print(\"loading loss function ...\")\n",
    "        loss_function = module_loss_function.load()\n",
    "        #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "        #print(\"loading model ...\")\n",
    "        if 'tf_nll' in loss_function.__name__:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1]*2,\n",
    "                cfg['model']\n",
    "            )\n",
    "        else:\n",
    "            model = module_model.load(\n",
    "                x_train.shape[1:],\n",
    "                y_train.shape[1],\n",
    "                cfg['model']\n",
    "            )\n",
    "\n",
    "        if 'initial_weights_path' in cfg['train']:\n",
    "            #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "            model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_function\n",
    "        )\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        # training mode\n",
    "        if mode == 'train':\n",
    "            #print(\"training model ...\")\n",
    "            train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "        if mode == 'plot_nll':\n",
    "            plot_nll(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_noise_experiment':\n",
    "            plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_seg':\n",
    "            plot_seg(model, x_test, y_test, cfg)\n",
    "        if mode == 'plot_dist':\n",
    "            plot_target_distribution(y_test, cfg)\n",
    "\n",
    "        # evaluation mode\n",
    "        if mode == 'evaluate':\n",
    "            evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
