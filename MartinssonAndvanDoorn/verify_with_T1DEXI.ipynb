{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-26 20:55:20,595 DEBUG matplotlib data path: c:\\Users\\username\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2024-08-26 20:55:20,604 DEBUG CONFIGDIR=C:\\Users\\username\\.matplotlib\n",
      "2024-08-26 20:55:20,606 DEBUG interactive is False\n",
      "2024-08-26 20:55:20,606 DEBUG platform is win32\n",
      "2024-08-26 20:55:20,656 DEBUG CACHEDIR=C:\\Users\\username\\.matplotlib\n",
      "2024-08-26 20:55:20,659 DEBUG Using fontManager instance from C:\\Users\\username\\.matplotlib\\fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import importlib.util\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import copy\n",
    "import datetime\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import metrics\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the output of ohio data loader\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_module(script_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"module.name\", script_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def load_cfg(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load a YAML configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream)\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "    return cfg\n",
    "\n",
    "def load_cfgs(yaml_filepath):\n",
    "    \"\"\"\n",
    "    Load YAML configuration files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_filepath : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfgs : [dict]\n",
    "    \"\"\"\n",
    "    # Read YAML experiment definition file\n",
    "    with open(yaml_filepath, 'r') as stream:\n",
    "        cfg = yaml.load(stream, Loader=yaml.SafeLoader)\n",
    "\n",
    "    cfg = make_paths_absolute(os.path.dirname(yaml_filepath), cfg)\n",
    "\n",
    "    hyperparameters = []\n",
    "    hyperparameter_names = []\n",
    "    hyperparameter_values = []\n",
    "    # TODO: ugly, should handle arbitrary depth\n",
    "    for k1 in cfg.keys():\n",
    "        for k2 in cfg[k1].keys():\n",
    "            if k2.startswith(\"param_\"):\n",
    "                hyperparameters.append((k1, k2))\n",
    "                hyperparameter_names.append((k1, k2[6:]))\n",
    "                hyperparameter_values.append(cfg[k1][k2])\n",
    "\n",
    "    hyperparameter_valuess = itertools.product(*hyperparameter_values)\n",
    "\n",
    "\n",
    "    artifacts_path = cfg['train']['artifacts_path']\n",
    "\n",
    "    cfgs = []\n",
    "    for hyperparameter_values in hyperparameter_valuess:\n",
    "        configuration_name = \"\"\n",
    "        for ((k1, k2), value) in zip(hyperparameter_names, hyperparameter_values):\n",
    "            #print(k1, k2, value)\n",
    "            cfg[k1][k2] = value\n",
    "            configuration_name += \"{}_{}_\".format(k2, str(value))\n",
    "\n",
    "        cfg['train']['artifacts_path'] = os.path.join(artifacts_path, configuration_name)\n",
    "\n",
    "        cfgs.append(copy.deepcopy(cfg))\n",
    "\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "\n",
    "def make_paths_absolute(dir_, cfg):\n",
    "    \"\"\"\n",
    "    Make all values for keys ending with `_path` absolute to dir_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_ : str\n",
    "    cfg : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cfg : dict\n",
    "    \"\"\"\n",
    "    for key in cfg.keys():\n",
    "        if key.endswith(\"_path\"):\n",
    "            cfg[key] = os.path.join(dir_, cfg[key])\n",
    "            cfg[key] = os.path.abspath(cfg[key])\n",
    "            if not os.path.exists(cfg[key]):\n",
    "                logging.error(\"%s does not exist.\", cfg[key])\n",
    "        if type(cfg[key]) is dict:\n",
    "            cfg[key] = make_paths_absolute(dir_, cfg[key])\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    weights_path = os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\")\n",
    "    print(\"loading weights: {}\".format(weights_path))\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)[:,1].flatten()/scale\n",
    "    y_std  = model.predict(x_test)[:,0].flatten()/scale\n",
    "    y_test = y_test.flatten()/scale\n",
    "    t0 = x_test[:,-1,0]/scale\n",
    "\n",
    "    rmse = metrics.root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"patient id: \", patient_id)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(rmse))\n",
    "\n",
    "    seg = metrics.surveillance_error(y_test, y_pred)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(seg))\n",
    "\n",
    "    t0_rmse = metrics.root_mean_squared_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_rmse.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_rmse))\n",
    "\n",
    "    t0_seg = metrics.surveillance_error(y_test, t0)\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_t0_seg.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(t0_seg))\n",
    "\n",
    "    with open(os.path.join(cfg['train']['artifacts_path'], \"{}_mean_std.txt\".format(patient_id)), \"w\") as outfile:\n",
    "        outfile.write(\"{}\\n\".format(np.mean(y_std)))\n",
    "\n",
    "    print(\"RMSE: \", rmse)\n",
    "    print(\"t0 RMSE: \", t0_rmse)\n",
    "    print(\"SEG: \", seg)\n",
    "    print(\"t0 SEG: \", t0_seg)\n",
    "\n",
    "def train(model, module_train, x_train, y_train, x_valid, y_valid, cfg):\n",
    "    model = module_train.train(\n",
    "        model          = model,\n",
    "        x_train        = x_train,\n",
    "        y_train        = y_train,\n",
    "        x_valid        = x_valid,\n",
    "        y_valid        = y_valid,\n",
    "        batch_size     = int(cfg['train']['batch_size']),\n",
    "        epochs         = int(cfg['train']['epochs']),\n",
    "        patience       = int(cfg['train']['patience']),\n",
    "        shuffle        = cfg['train']['shuffle'],\n",
    "        artifacts_path = cfg['train']['artifacts_path']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_target_distribution(y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    plt.figure()\n",
    "    sns.distplot(y_test.flatten()/scale, kde=False, norm_hist=True)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_dist_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_nll(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    for i in range(5):\n",
    "        start_index = i*to_plot\n",
    "        y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]/scale\n",
    "        y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]/scale\n",
    "        y_true      = y_test[:,0][start_index:start_index+to_plot]/scale\n",
    "\n",
    "        xs = np.arange(len(y_true))\n",
    "        plt.clf()\n",
    "        plt.ylim([0, 400])\n",
    "        #plt.ylim([-2, 2])\n",
    "        plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "        plt.plot(xs, y_pred_mean, label='prediction')\n",
    "        plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "                alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "        plt.xlabel(\"Time [h]\")\n",
    "        plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "        plt.legend(loc='upper right')\n",
    "        #plt.xlabel(\"y\")\n",
    "        #plt.ylabel(\"x\")\n",
    "        plt.xticks(ticks, ticks_labels)\n",
    "        save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_nll_plot_{}.pdf\".format(patient_id, i))\n",
    "        print(\"saving plot to: \", save_path)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "def plot_noise_experiment(model, x_test, y_test, cfg):\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    #day = (24*60//5)\n",
    "    start_index = 0\n",
    "    hours = 8\n",
    "    to_plot=hours*12\n",
    "    ticks_per_hour = 12\n",
    "    ticks = [i*ticks_per_hour for i in range(hours+1)]\n",
    "    ticks_labels = [str(i) for i in range(hours+1)]\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "\n",
    "    start_index = 0\n",
    "    y_pred_std  = y_pred[:,0][start_index:start_index+to_plot]\n",
    "    y_pred_mean = y_pred[:,1][start_index:start_index+to_plot]\n",
    "    y_true      = y_test[:,0][start_index:start_index+to_plot]\n",
    "\n",
    "    xs = np.arange(len(y_true))\n",
    "    plt.clf()\n",
    "    #plt.ylim([0, 400])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.plot(xs, y_true, label='ground truth', linestyle='--')\n",
    "    plt.plot(xs, y_pred_mean, label='prediction')\n",
    "    plt.fill_between(xs, y_pred_mean-y_pred_std, y_pred_mean+y_pred_std,\n",
    "            alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "    #plt.xlabel(\"Time [h]\")\n",
    "    #plt.ylabel(\"Glucose Concentration [mg/dl]\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xticks(ticks, ticks_labels)\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"noise_experiment_plot.pdf\")\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "def plot_seg(model, x_test, y_test, cfg):\n",
    "    if 'csv_path' in cfg['dataset']:\n",
    "        basename = os.path.basename(cfg['dataset']['csv_path'])\n",
    "        patient_id = basename.split('-')[0]\n",
    "    else:\n",
    "        patient_id = \"\"\n",
    "    if 'scale' in cfg['dataset']:\n",
    "        scale = float(cfg['dataset']['scale'])\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    # load the trained weights\n",
    "    model.load_weights(os.path.join(cfg['train']['artifacts_path'], \"model.hdf5\"))\n",
    "\n",
    "    y_pred      = model.predict(x_test)\n",
    "    y_pred_std  = y_pred[:,0][:]/scale\n",
    "    y_pred_mean = y_pred[:,1][:]/scale\n",
    "    y_true      = y_test[:,0][:]/scale\n",
    "\n",
    "    data = np.loadtxt('seg.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Patient {} SEG'.format(patient_id))\n",
    "    ax.set_xlabel('Reference Concentration [mg/dl]')\n",
    "    ax.set_ylabel('Predicted Concentration [mg/dl]')\n",
    "    cax = ax.imshow(np.transpose(data), origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cax, ticks=[0.25, 1.0, 2.0, 3.0, 3.75], orientation='vertical')\n",
    "    cbar.ax.set_yticklabels(['None', 'Mild', 'Moderate', 'High', 'Extreme'],\n",
    "            rotation=90, va='center')\n",
    "\n",
    "    plt.scatter(y_true, y_pred_mean, s=25, facecolors='white', edgecolors='black')\n",
    "\n",
    "    save_path = os.path.join(cfg['train']['artifacts_path'], \"{}_seg_plot.pdf\".format(patient_id))\n",
    "    print(\"saving plot to: \", save_path)\n",
    "    plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./t1dexi_cfg/all_final_experiment.yaml\"\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-26 20:55:49,315 ERROR C:\\Users\\username\\OneDrive\\Desktop\\BGprediction\\LB_split\\population_training\\all does not exist.\n",
      "Running 1 experiments.\n",
      "WARNING:tensorflow:From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2024-08-26 20:55:49,323 WARNING From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\LB_split\\\\population_training\\\\all',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 12,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 0,\n",
      "                   'train_fraction': 0.8,\n",
      "                   'valid_fraction': 0.2},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\artifacts\\\\t1dexi\\\\nb_future_steps_6_seed_9_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 1000,\n",
      "                 'param_seed': [9],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\train\\\\train_keras.py',\n",
      "                 'seed': 9,\n",
      "                 'shuffle': True}}\n",
      "loading training data for all patients ...\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (8875, 12, 1)\n",
      "y_train.shape:  (8875, 1)\n",
      "x_valid.shape:  (2212, 12, 1)\n",
      "y_valid.shape:  (2212, 1)\n",
      "x_test.shape:  (0, 12, 1)\n",
      "y_test.shape:  (0, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-08-26 20:56:01,357 WARNING Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-08-26 20:56:01,669 WARNING From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "2024-08-26 20:56:01,673 WARNING From C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2024-08-26 20:56:01,684 WARNING `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Train on 8875 samples, validate on 2212 samples\n",
      "Epoch 1/1000\n",
      "8875/8875 [==============================] - 6s 623us/sample - loss: 1.0950 - val_loss: 0.3497\n",
      "Epoch 2/1000\n",
      "5120/8875 [================>.............] - ETA: 0s - loss: 0.4378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8875/8875 [==============================] - 0s 19us/sample - loss: 0.4083 - val_loss: 0.3876\n",
      "Epoch 3/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: 0.3199 - val_loss: 0.2150\n",
      "Epoch 4/1000\n",
      "8875/8875 [==============================] - 2s 225us/sample - loss: 0.2534 - val_loss: 0.0944\n",
      "Epoch 5/1000\n",
      "8192/8875 [==========================>...] - ETA: 0s - loss: 0.18432024-08-26 20:56:10,076 DEBUG Creating converter from 5 to 3\n",
      "8875/8875 [==============================] - 0s 33us/sample - loss: 0.1842 - val_loss: -0.0593\n",
      "Epoch 6/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: 0.0862 - val_loss: -0.0383\n",
      "Epoch 7/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.0041 - val_loss: -0.2468\n",
      "Epoch 8/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.0646 - val_loss: -0.2982\n",
      "Epoch 9/1000\n",
      "8875/8875 [==============================] - 1s 159us/sample - loss: -0.1091 - val_loss: -0.3369\n",
      "Epoch 10/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.1205 - val_loss: -0.3047\n",
      "Epoch 11/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.1501 - val_loss: -0.2865\n",
      "Epoch 12/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.1611 - val_loss: -0.3763\n",
      "Epoch 13/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.1674 - val_loss: -0.3112\n",
      "Epoch 14/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.1717 - val_loss: -0.3242\n",
      "Epoch 15/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.1943 - val_loss: -0.3239\n",
      "Epoch 16/1000\n",
      "8875/8875 [==============================] - 1s 159us/sample - loss: -0.1886 - val_loss: -0.3674\n",
      "Epoch 17/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.1872 - val_loss: -0.3462\n",
      "Epoch 18/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.1981 - val_loss: -0.3697\n",
      "Epoch 19/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.2074 - val_loss: -0.4199\n",
      "Epoch 20/1000\n",
      "8875/8875 [==============================] - 0s 24us/sample - loss: -0.2277 - val_loss: -0.3729\n",
      "Epoch 21/1000\n",
      "8875/8875 [==============================] - 1s 167us/sample - loss: -0.2314 - val_loss: -0.3481\n",
      "Epoch 22/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.2217 - val_loss: -0.4014\n",
      "Epoch 23/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.2325 - val_loss: -0.4387\n",
      "Epoch 24/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.2253 - val_loss: -0.4293\n",
      "Epoch 25/1000\n",
      "8875/8875 [==============================] - 0s 23us/sample - loss: -0.2069 - val_loss: -0.4344\n",
      "Epoch 26/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.2343 - val_loss: -0.4114\n",
      "Epoch 27/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.2495 - val_loss: -0.4271\n",
      "Epoch 28/1000\n",
      "8875/8875 [==============================] - 2s 179us/sample - loss: -0.2578 - val_loss: -0.4031\n",
      "Epoch 29/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.2667 - val_loss: -0.4126\n",
      "Epoch 30/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.2657 - val_loss: -0.4317\n",
      "Epoch 31/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.2542 - val_loss: -0.4383\n",
      "Epoch 32/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.2500 - val_loss: -0.4522\n",
      "Epoch 33/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.2528 - val_loss: -0.4713\n",
      "Epoch 34/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.2566 - val_loss: -0.4515\n",
      "Epoch 35/1000\n",
      "8875/8875 [==============================] - 0s 27us/sample - loss: -0.2850 - val_loss: -0.4351\n",
      "Epoch 36/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.2944 - val_loss: -0.4398\n",
      "Epoch 37/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.2924 - val_loss: -0.4518\n",
      "Epoch 38/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.2995 - val_loss: -0.4232\n",
      "Epoch 39/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.3052 - val_loss: -0.4367\n",
      "Epoch 40/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.3039 - val_loss: -0.4516\n",
      "Epoch 41/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.3048 - val_loss: -0.4498\n",
      "Epoch 42/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.2880 - val_loss: -0.4496\n",
      "Epoch 43/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3045 - val_loss: -0.4520\n",
      "Epoch 44/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.3063 - val_loss: -0.4120\n",
      "Epoch 45/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3138 - val_loss: -0.4647\n",
      "Epoch 46/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.3075 - val_loss: -0.4535\n",
      "Epoch 47/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3177 - val_loss: -0.4358\n",
      "Epoch 48/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3151 - val_loss: -0.4983\n",
      "Epoch 49/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3178 - val_loss: -0.4676\n",
      "Epoch 50/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3149 - val_loss: -0.4357\n",
      "Epoch 51/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3189 - val_loss: -0.4752\n",
      "Epoch 52/1000\n",
      "8875/8875 [==============================] - 1s 159us/sample - loss: -0.3180 - val_loss: -0.4741\n",
      "Epoch 53/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3296 - val_loss: -0.4569\n",
      "Epoch 54/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3237 - val_loss: -0.4871\n",
      "Epoch 55/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.3263 - val_loss: -0.4931\n",
      "Epoch 56/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.3420 - val_loss: -0.4894\n",
      "Epoch 57/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3437 - val_loss: -0.4565\n",
      "Epoch 58/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.3301 - val_loss: -0.4887\n",
      "Epoch 59/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3340 - val_loss: -0.4803\n",
      "Epoch 60/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3352 - val_loss: -0.4573\n",
      "Epoch 61/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.3331 - val_loss: -0.4875\n",
      "Epoch 62/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3458 - val_loss: -0.4996\n",
      "Epoch 63/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.3491 - val_loss: -0.5009\n",
      "Epoch 64/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.3413 - val_loss: -0.4274\n",
      "Epoch 65/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3302 - val_loss: -0.4965\n",
      "Epoch 66/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3481 - val_loss: -0.5395\n",
      "Epoch 67/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3470 - val_loss: -0.5397\n",
      "Epoch 68/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3597 - val_loss: -0.5024\n",
      "Epoch 69/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3514 - val_loss: -0.5103\n",
      "Epoch 70/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.3522 - val_loss: -0.5276\n",
      "Epoch 71/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3570 - val_loss: -0.4982\n",
      "Epoch 72/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3631 - val_loss: -0.5207\n",
      "Epoch 73/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3617 - val_loss: -0.5066\n",
      "Epoch 74/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.3636 - val_loss: -0.5251\n",
      "Epoch 75/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3591 - val_loss: -0.4893\n",
      "Epoch 76/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.3619 - val_loss: -0.4951\n",
      "Epoch 77/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.3591 - val_loss: -0.5507\n",
      "Epoch 78/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.3591 - val_loss: -0.5443\n",
      "Epoch 79/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3623 - val_loss: -0.5425\n",
      "Epoch 80/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3717 - val_loss: -0.5364\n",
      "Epoch 81/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3667 - val_loss: -0.5447\n",
      "Epoch 82/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3680 - val_loss: -0.5461\n",
      "Epoch 83/1000\n",
      "8875/8875 [==============================] - 1s 160us/sample - loss: -0.3739 - val_loss: -0.5419\n",
      "Epoch 84/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3701 - val_loss: -0.5069\n",
      "Epoch 85/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3705 - val_loss: -0.5318\n",
      "Epoch 86/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3709 - val_loss: -0.5199\n",
      "Epoch 87/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.3659 - val_loss: -0.5324\n",
      "Epoch 88/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.3622 - val_loss: -0.5719\n",
      "Epoch 89/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.3675 - val_loss: -0.5463\n",
      "Epoch 90/1000\n",
      "8875/8875 [==============================] - 0s 25us/sample - loss: -0.3744 - val_loss: -0.5746\n",
      "Epoch 91/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.3813 - val_loss: -0.4818\n",
      "Epoch 92/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3794 - val_loss: -0.5531\n",
      "Epoch 93/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3895 - val_loss: -0.5629\n",
      "Epoch 94/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3507 - val_loss: -0.5658\n",
      "Epoch 95/1000\n",
      "8875/8875 [==============================] - 1s 158us/sample - loss: -0.3630 - val_loss: -0.5710\n",
      "Epoch 96/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3752 - val_loss: -0.5323\n",
      "Epoch 97/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3849 - val_loss: -0.5227\n",
      "Epoch 98/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.3801 - val_loss: -0.5193\n",
      "Epoch 99/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3651 - val_loss: -0.5199\n",
      "Epoch 100/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3829 - val_loss: -0.5202\n",
      "Epoch 101/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.3814 - val_loss: -0.5249\n",
      "Epoch 102/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.3718 - val_loss: -0.5610\n",
      "Epoch 103/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.3886 - val_loss: -0.5386\n",
      "Epoch 104/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.3782 - val_loss: -0.5476\n",
      "Epoch 105/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.3903 - val_loss: -0.4952\n",
      "Epoch 106/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.3870 - val_loss: -0.5425\n",
      "Epoch 107/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4035 - val_loss: -0.5499\n",
      "Epoch 108/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.3992 - val_loss: -0.5611\n",
      "Epoch 109/1000\n",
      "8875/8875 [==============================] - 0s 11us/sample - loss: -0.4014 - val_loss: -0.5324\n",
      "Epoch 110/1000\n",
      "8875/8875 [==============================] - 1s 159us/sample - loss: -0.3907 - val_loss: -0.5724\n",
      "Epoch 111/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3954 - val_loss: -0.5766\n",
      "Epoch 112/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4021 - val_loss: -0.5700\n",
      "Epoch 113/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4036 - val_loss: -0.5317\n",
      "Epoch 114/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3809 - val_loss: -0.5377\n",
      "Epoch 115/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.3964 - val_loss: -0.5725\n",
      "Epoch 116/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.3965 - val_loss: -0.5504\n",
      "Epoch 117/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4107 - val_loss: -0.5448\n",
      "Epoch 118/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4013 - val_loss: -0.5806\n",
      "Epoch 119/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4123 - val_loss: -0.5196\n",
      "Epoch 120/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4055 - val_loss: -0.5658\n",
      "Epoch 121/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4227 - val_loss: -0.5496\n",
      "Epoch 122/1000\n",
      "8875/8875 [==============================] - 1s 150us/sample - loss: -0.4130 - val_loss: -0.5589\n",
      "Epoch 123/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4154 - val_loss: -0.5815\n",
      "Epoch 124/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4108 - val_loss: -0.5451\n",
      "Epoch 125/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4132 - val_loss: -0.5545\n",
      "Epoch 126/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4095 - val_loss: -0.5784\n",
      "Epoch 127/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.3977 - val_loss: -0.5608\n",
      "Epoch 128/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.3946 - val_loss: -0.5209\n",
      "Epoch 129/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4091 - val_loss: -0.5177\n",
      "Epoch 130/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4129 - val_loss: -0.5386\n",
      "Epoch 131/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4019 - val_loss: -0.5611\n",
      "Epoch 132/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4191 - val_loss: -0.5806\n",
      "Epoch 133/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4015 - val_loss: -0.5616\n",
      "Epoch 134/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.4199 - val_loss: -0.5344\n",
      "Epoch 135/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4222 - val_loss: -0.5488\n",
      "Epoch 136/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4204 - val_loss: -0.5796\n",
      "Epoch 137/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4309 - val_loss: -0.5588\n",
      "Epoch 138/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.4362 - val_loss: -0.5970\n",
      "Epoch 139/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.4259 - val_loss: -0.5621\n",
      "Epoch 140/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4179 - val_loss: -0.5724\n",
      "Epoch 141/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4291 - val_loss: -0.5735\n",
      "Epoch 142/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4324 - val_loss: -0.5940\n",
      "Epoch 143/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4329 - val_loss: -0.5880\n",
      "Epoch 144/1000\n",
      "8875/8875 [==============================] - 1s 158us/sample - loss: -0.4164 - val_loss: -0.5704\n",
      "Epoch 145/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4206 - val_loss: -0.5440\n",
      "Epoch 146/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4306 - val_loss: -0.5593\n",
      "Epoch 147/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4306 - val_loss: -0.5556\n",
      "Epoch 148/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4228 - val_loss: -0.5688\n",
      "Epoch 149/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.4315 - val_loss: -0.5795\n",
      "Epoch 150/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.4375 - val_loss: -0.5937\n",
      "Epoch 151/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4430 - val_loss: -0.5887\n",
      "Epoch 152/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4293 - val_loss: -0.5576\n",
      "Epoch 153/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4401 - val_loss: -0.5643\n",
      "Epoch 154/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4380 - val_loss: -0.5717\n",
      "Epoch 155/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.4360 - val_loss: -0.5811\n",
      "Epoch 156/1000\n",
      "8875/8875 [==============================] - 1s 150us/sample - loss: -0.4459 - val_loss: -0.6073\n",
      "Epoch 157/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4201 - val_loss: -0.5828\n",
      "Epoch 158/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4308 - val_loss: -0.5922\n",
      "Epoch 159/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4248 - val_loss: -0.5761\n",
      "Epoch 160/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4393 - val_loss: -0.5846\n",
      "Epoch 161/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4416 - val_loss: -0.5723\n",
      "Epoch 162/1000\n",
      "8875/8875 [==============================] - 1s 152us/sample - loss: -0.4322 - val_loss: -0.5261\n",
      "Epoch 163/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.4275 - val_loss: -0.5722\n",
      "Epoch 164/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4408 - val_loss: -0.5949\n",
      "Epoch 165/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4308 - val_loss: -0.5689\n",
      "Epoch 166/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4387 - val_loss: -0.6010\n",
      "Epoch 167/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.4356 - val_loss: -0.5872\n",
      "Epoch 168/1000\n",
      "8875/8875 [==============================] - 1s 152us/sample - loss: -0.4432 - val_loss: -0.5801\n",
      "Epoch 169/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4399 - val_loss: -0.5956\n",
      "Epoch 170/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4399 - val_loss: -0.6050\n",
      "Epoch 171/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4489 - val_loss: -0.5928\n",
      "Epoch 172/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.4505 - val_loss: -0.5689\n",
      "Epoch 173/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4489 - val_loss: -0.5729\n",
      "Epoch 174/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4517 - val_loss: -0.5866\n",
      "Epoch 175/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4501 - val_loss: -0.5777\n",
      "Epoch 176/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.4538 - val_loss: -0.5765\n",
      "Epoch 177/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4497 - val_loss: -0.5984\n",
      "Epoch 178/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4536 - val_loss: -0.5915\n",
      "Epoch 179/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4528 - val_loss: -0.5596\n",
      "Epoch 180/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4475 - val_loss: -0.5789\n",
      "Epoch 181/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4566 - val_loss: -0.5252\n",
      "Epoch 182/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.4471 - val_loss: -0.5715\n",
      "Epoch 183/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4453 - val_loss: -0.5620\n",
      "Epoch 184/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4494 - val_loss: -0.5756\n",
      "Epoch 185/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4483 - val_loss: -0.5824\n",
      "Epoch 186/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4551 - val_loss: -0.5922\n",
      "Epoch 187/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4476 - val_loss: -0.5464\n",
      "Epoch 188/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4477 - val_loss: -0.5546\n",
      "Epoch 189/1000\n",
      "8875/8875 [==============================] - 1s 158us/sample - loss: -0.4552 - val_loss: -0.6009\n",
      "Epoch 190/1000\n",
      "8875/8875 [==============================] - 0s 24us/sample - loss: -0.4446 - val_loss: -0.6057\n",
      "Epoch 191/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4559 - val_loss: -0.5968\n",
      "Epoch 192/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4425 - val_loss: -0.5751\n",
      "Epoch 193/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.4574 - val_loss: -0.5686\n",
      "Epoch 194/1000\n",
      "8875/8875 [==============================] - 1s 158us/sample - loss: -0.4565 - val_loss: -0.5782\n",
      "Epoch 195/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4605 - val_loss: -0.5828\n",
      "Epoch 196/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4574 - val_loss: -0.5798\n",
      "Epoch 197/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4563 - val_loss: -0.5844\n",
      "Epoch 198/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4644 - val_loss: -0.6013\n",
      "Epoch 199/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.4646 - val_loss: -0.5573\n",
      "Epoch 200/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4439 - val_loss: -0.5261\n",
      "Epoch 201/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4610 - val_loss: -0.5920\n",
      "Epoch 202/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4618 - val_loss: -0.6050\n",
      "Epoch 203/1000\n",
      "8875/8875 [==============================] - 0s 12us/sample - loss: -0.4707 - val_loss: -0.5783\n",
      "Epoch 204/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4591 - val_loss: -0.5612\n",
      "Epoch 205/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4679 - val_loss: -0.5894\n",
      "Epoch 206/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4707 - val_loss: -0.5694\n",
      "Epoch 207/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4728 - val_loss: -0.6037\n",
      "Epoch 208/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.4731 - val_loss: -0.5996\n",
      "Epoch 209/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4688 - val_loss: -0.5893\n",
      "Epoch 210/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4674 - val_loss: -0.5862\n",
      "Epoch 211/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4632 - val_loss: -0.6195\n",
      "Epoch 212/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4813 - val_loss: -0.5846\n",
      "Epoch 213/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4733 - val_loss: -0.5909\n",
      "Epoch 214/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.4754 - val_loss: -0.5887\n",
      "Epoch 215/1000\n",
      "8875/8875 [==============================] - 0s 23us/sample - loss: -0.4672 - val_loss: -0.6104\n",
      "Epoch 216/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4867 - val_loss: -0.5985\n",
      "Epoch 217/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4684 - val_loss: -0.5867\n",
      "Epoch 218/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4749 - val_loss: -0.6141\n",
      "Epoch 219/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4709 - val_loss: -0.5794\n",
      "Epoch 220/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.4683 - val_loss: -0.6133\n",
      "Epoch 221/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4746 - val_loss: -0.6112\n",
      "Epoch 222/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4740 - val_loss: -0.5996\n",
      "Epoch 223/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4728 - val_loss: -0.6013\n",
      "Epoch 224/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4638 - val_loss: -0.5910\n",
      "Epoch 225/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4715 - val_loss: -0.5900\n",
      "Epoch 226/1000\n",
      "8875/8875 [==============================] - 1s 151us/sample - loss: -0.4827 - val_loss: -0.5771\n",
      "Epoch 227/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4722 - val_loss: -0.5799\n",
      "Epoch 228/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4802 - val_loss: -0.5970\n",
      "Epoch 229/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4778 - val_loss: -0.5743\n",
      "Epoch 230/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4794 - val_loss: -0.6084\n",
      "Epoch 231/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4808 - val_loss: -0.6165\n",
      "Epoch 232/1000\n",
      "8875/8875 [==============================] - 1s 151us/sample - loss: -0.4864 - val_loss: -0.6140\n",
      "Epoch 233/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4755 - val_loss: -0.5867\n",
      "Epoch 234/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4742 - val_loss: -0.5898\n",
      "Epoch 235/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.4775 - val_loss: -0.6152\n",
      "Epoch 236/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4711 - val_loss: -0.5935\n",
      "Epoch 237/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4829 - val_loss: -0.5888\n",
      "Epoch 238/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4887 - val_loss: -0.5912\n",
      "Epoch 239/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4773 - val_loss: -0.6153\n",
      "Epoch 240/1000\n",
      "8875/8875 [==============================] - 1s 151us/sample - loss: -0.4834 - val_loss: -0.5815\n",
      "Epoch 241/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4827 - val_loss: -0.5945\n",
      "Epoch 242/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4770 - val_loss: -0.6069\n",
      "Epoch 243/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4942 - val_loss: -0.5670\n",
      "Epoch 244/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4946 - val_loss: -0.6147\n",
      "Epoch 245/1000\n",
      "8875/8875 [==============================] - 0s 23us/sample - loss: -0.4903 - val_loss: -0.6214\n",
      "Epoch 246/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.4914 - val_loss: -0.6131\n",
      "Epoch 247/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.4840 - val_loss: -0.6021\n",
      "Epoch 248/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4801 - val_loss: -0.5703\n",
      "Epoch 249/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4793 - val_loss: -0.5658\n",
      "Epoch 250/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4917 - val_loss: -0.5870\n",
      "Epoch 251/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4939 - val_loss: -0.6325\n",
      "Epoch 252/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.4833 - val_loss: -0.5950\n",
      "Epoch 253/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4828 - val_loss: -0.6024\n",
      "Epoch 254/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.4898 - val_loss: -0.5997\n",
      "Epoch 255/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4907 - val_loss: -0.5854\n",
      "Epoch 256/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.4965 - val_loss: -0.6091\n",
      "Epoch 257/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4897 - val_loss: -0.5667\n",
      "Epoch 258/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.4879 - val_loss: -0.5907\n",
      "Epoch 259/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4989 - val_loss: -0.6112\n",
      "Epoch 260/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5026 - val_loss: -0.5963\n",
      "Epoch 261/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4987 - val_loss: -0.6090\n",
      "Epoch 262/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4833 - val_loss: -0.5855\n",
      "Epoch 263/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.4886 - val_loss: -0.5552\n",
      "Epoch 264/1000\n",
      "8875/8875 [==============================] - 1s 151us/sample - loss: -0.4946 - val_loss: -0.6248\n",
      "Epoch 265/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4857 - val_loss: -0.6009\n",
      "Epoch 266/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5004 - val_loss: -0.5564\n",
      "Epoch 267/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4871 - val_loss: -0.5795\n",
      "Epoch 268/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4859 - val_loss: -0.5933\n",
      "Epoch 269/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4925 - val_loss: -0.5475\n",
      "Epoch 270/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.4913 - val_loss: -0.6090\n",
      "Epoch 271/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5066 - val_loss: -0.6068\n",
      "Epoch 272/1000\n",
      "8875/8875 [==============================] - 1s 149us/sample - loss: -0.5095 - val_loss: -0.5862\n",
      "Epoch 273/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4930 - val_loss: -0.5594\n",
      "Epoch 274/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5112 - val_loss: -0.6136\n",
      "Epoch 275/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.4953 - val_loss: -0.5805\n",
      "Epoch 276/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5034 - val_loss: -0.6141\n",
      "Epoch 277/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5050 - val_loss: -0.5960\n",
      "Epoch 278/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5021 - val_loss: -0.6071\n",
      "Epoch 279/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5043 - val_loss: -0.6140\n",
      "Epoch 280/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5075 - val_loss: -0.5731\n",
      "Epoch 281/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5145 - val_loss: -0.6179\n",
      "Epoch 282/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5071 - val_loss: -0.5864\n",
      "Epoch 283/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5072 - val_loss: -0.6244\n",
      "Epoch 284/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.4972 - val_loss: -0.5920\n",
      "Epoch 285/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.5071 - val_loss: -0.6138\n",
      "Epoch 286/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5148 - val_loss: -0.5993\n",
      "Epoch 287/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5003 - val_loss: -0.6147\n",
      "Epoch 288/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5077 - val_loss: -0.6036\n",
      "Epoch 289/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5049 - val_loss: -0.5542\n",
      "Epoch 290/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5109 - val_loss: -0.6237\n",
      "Epoch 291/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.5033 - val_loss: -0.6086\n",
      "Epoch 292/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5084 - val_loss: -0.5868\n",
      "Epoch 293/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5049 - val_loss: -0.6129\n",
      "Epoch 294/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5187 - val_loss: -0.5913\n",
      "Epoch 295/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5120 - val_loss: -0.6038\n",
      "Epoch 296/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.5137 - val_loss: -0.5850\n",
      "Epoch 297/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5048 - val_loss: -0.6322\n",
      "Epoch 298/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5106 - val_loss: -0.6050\n",
      "Epoch 299/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5235 - val_loss: -0.5975\n",
      "Epoch 300/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5067 - val_loss: -0.6312\n",
      "Epoch 301/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5198 - val_loss: -0.5958\n",
      "Epoch 302/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5081 - val_loss: -0.6130\n",
      "Epoch 303/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5119 - val_loss: -0.6125\n",
      "Epoch 304/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.5120 - val_loss: -0.5780\n",
      "Epoch 305/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5181 - val_loss: -0.6005\n",
      "Epoch 306/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5222 - val_loss: -0.6155\n",
      "Epoch 307/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5220 - val_loss: -0.6135\n",
      "Epoch 308/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5191 - val_loss: -0.6011\n",
      "Epoch 309/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5124 - val_loss: -0.6053\n",
      "Epoch 310/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.5196 - val_loss: -0.6045\n",
      "Epoch 311/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5226 - val_loss: -0.6049\n",
      "Epoch 312/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5242 - val_loss: -0.5895\n",
      "Epoch 313/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5206 - val_loss: -0.6174\n",
      "Epoch 314/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5209 - val_loss: -0.6186\n",
      "Epoch 315/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5145 - val_loss: -0.6068\n",
      "Epoch 316/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.5250 - val_loss: -0.6025\n",
      "Epoch 317/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5130 - val_loss: -0.6275\n",
      "Epoch 318/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5213 - val_loss: -0.5983\n",
      "Epoch 319/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5292 - val_loss: -0.5973\n",
      "Epoch 320/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5172 - val_loss: -0.5956\n",
      "Epoch 321/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5097 - val_loss: -0.5204\n",
      "Epoch 322/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5088 - val_loss: -0.6023\n",
      "Epoch 323/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5061 - val_loss: -0.6101\n",
      "Epoch 324/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5242 - val_loss: -0.6149\n",
      "Epoch 325/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5220 - val_loss: -0.5963\n",
      "Epoch 326/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5227 - val_loss: -0.6145\n",
      "Epoch 327/1000\n",
      "8875/8875 [==============================] - 1s 152us/sample - loss: -0.5172 - val_loss: -0.6181\n",
      "Epoch 328/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5321 - val_loss: -0.5996\n",
      "Epoch 329/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5341 - val_loss: -0.6139\n",
      "Epoch 330/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5232 - val_loss: -0.6086\n",
      "Epoch 331/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5175 - val_loss: -0.6231\n",
      "Epoch 332/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5348 - val_loss: -0.5999\n",
      "Epoch 333/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5258 - val_loss: -0.6067\n",
      "Epoch 334/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5320 - val_loss: -0.6205\n",
      "Epoch 335/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5300 - val_loss: -0.6239\n",
      "Epoch 336/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5213 - val_loss: -0.6203\n",
      "Epoch 337/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5265 - val_loss: -0.6159\n",
      "Epoch 338/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5295 - val_loss: -0.6114\n",
      "Epoch 339/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5370 - val_loss: -0.6163\n",
      "Epoch 340/1000\n",
      "8875/8875 [==============================] - 1s 158us/sample - loss: -0.5360 - val_loss: -0.6157\n",
      "Epoch 341/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5376 - val_loss: -0.5961\n",
      "Epoch 342/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5249 - val_loss: -0.6060\n",
      "Epoch 343/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5257 - val_loss: -0.6049\n",
      "Epoch 344/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5157 - val_loss: -0.5478\n",
      "Epoch 345/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5291 - val_loss: -0.6209\n",
      "Epoch 346/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5315 - val_loss: -0.5981\n",
      "Epoch 347/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.5206 - val_loss: -0.6009\n",
      "Epoch 348/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5223 - val_loss: -0.6106\n",
      "Epoch 349/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5444 - val_loss: -0.6267\n",
      "Epoch 350/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5442 - val_loss: -0.6105\n",
      "Epoch 351/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5294 - val_loss: -0.6062\n",
      "Epoch 352/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5421 - val_loss: -0.6110\n",
      "Epoch 353/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.5402 - val_loss: -0.5901\n",
      "Epoch 354/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5336 - val_loss: -0.6023\n",
      "Epoch 355/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5400 - val_loss: -0.6304\n",
      "Epoch 356/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5318 - val_loss: -0.6122\n",
      "Epoch 357/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5386 - val_loss: -0.5653\n",
      "Epoch 358/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5338 - val_loss: -0.6263\n",
      "Epoch 359/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.5455 - val_loss: -0.5929\n",
      "Epoch 360/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5418 - val_loss: -0.6102\n",
      "Epoch 361/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5358 - val_loss: -0.6132\n",
      "Epoch 362/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5326 - val_loss: -0.6260\n",
      "Epoch 363/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5426 - val_loss: -0.5790\n",
      "Epoch 364/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5441 - val_loss: -0.6187\n",
      "Epoch 365/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5417 - val_loss: -0.6138\n",
      "Epoch 366/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5431 - val_loss: -0.5913\n",
      "Epoch 367/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5405 - val_loss: -0.6061\n",
      "Epoch 368/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5499 - val_loss: -0.6150\n",
      "Epoch 369/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5378 - val_loss: -0.5981\n",
      "Epoch 370/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5434 - val_loss: -0.6051\n",
      "Epoch 371/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5427 - val_loss: -0.6106\n",
      "Epoch 372/1000\n",
      "8875/8875 [==============================] - 1s 157us/sample - loss: -0.5473 - val_loss: -0.5964\n",
      "Epoch 373/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5420 - val_loss: -0.5933\n",
      "Epoch 374/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5385 - val_loss: -0.5993\n",
      "Epoch 375/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5394 - val_loss: -0.6129\n",
      "Epoch 376/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5507 - val_loss: -0.5888\n",
      "Epoch 377/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5526 - val_loss: -0.6120\n",
      "Epoch 378/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5497 - val_loss: -0.6056\n",
      "Epoch 379/1000\n",
      "8875/8875 [==============================] - 1s 159us/sample - loss: -0.5508 - val_loss: -0.5942\n",
      "Epoch 380/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5487 - val_loss: -0.6120\n",
      "Epoch 381/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5611 - val_loss: -0.5897\n",
      "Epoch 382/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5543 - val_loss: -0.6170\n",
      "Epoch 383/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5457 - val_loss: -0.5885\n",
      "Epoch 384/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5296 - val_loss: -0.5045\n",
      "Epoch 385/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.5228 - val_loss: -0.6051\n",
      "Epoch 386/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5318 - val_loss: -0.5689\n",
      "Epoch 387/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5503 - val_loss: -0.6056\n",
      "Epoch 388/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5407 - val_loss: -0.6050\n",
      "Epoch 389/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5481 - val_loss: -0.6127\n",
      "Epoch 390/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5530 - val_loss: -0.6087\n",
      "Epoch 391/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5586 - val_loss: -0.5875\n",
      "Epoch 392/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5546 - val_loss: -0.6077\n",
      "Epoch 393/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5527 - val_loss: -0.6004\n",
      "Epoch 394/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5557 - val_loss: -0.6203\n",
      "Epoch 395/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5614 - val_loss: -0.5852\n",
      "Epoch 396/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.5544 - val_loss: -0.6183\n",
      "Epoch 397/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5442 - val_loss: -0.6197\n",
      "Epoch 398/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.5621 - val_loss: -0.6173\n",
      "Epoch 399/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5724 - val_loss: -0.6049\n",
      "Epoch 400/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5597 - val_loss: -0.6174\n",
      "Epoch 401/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5554 - val_loss: -0.6063\n",
      "Epoch 402/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5606 - val_loss: -0.6139\n",
      "Epoch 403/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5581 - val_loss: -0.6045\n",
      "Epoch 404/1000\n",
      "8875/8875 [==============================] - 1s 153us/sample - loss: -0.5632 - val_loss: -0.6107\n",
      "Epoch 405/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5625 - val_loss: -0.6050\n",
      "Epoch 406/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5642 - val_loss: -0.6118\n",
      "Epoch 407/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5774 - val_loss: -0.6188\n",
      "Epoch 408/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5657 - val_loss: -0.6210\n",
      "Epoch 409/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5671 - val_loss: -0.6138\n",
      "Epoch 410/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5620 - val_loss: -0.6157\n",
      "Epoch 411/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5648 - val_loss: -0.6106\n",
      "Epoch 412/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5594 - val_loss: -0.5871\n",
      "Epoch 413/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5432 - val_loss: -0.6010\n",
      "Epoch 414/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5591 - val_loss: -0.5894\n",
      "Epoch 415/1000\n",
      "8875/8875 [==============================] - 0s 17us/sample - loss: -0.5537 - val_loss: -0.5894\n",
      "Epoch 416/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.5489 - val_loss: -0.5937\n",
      "Epoch 417/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5664 - val_loss: -0.6098\n",
      "Epoch 418/1000\n",
      "8875/8875 [==============================] - 0s 22us/sample - loss: -0.5720 - val_loss: -0.5866\n",
      "Epoch 419/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5659 - val_loss: -0.6050\n",
      "Epoch 420/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5597 - val_loss: -0.5990\n",
      "Epoch 421/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5590 - val_loss: -0.6111\n",
      "Epoch 422/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5609 - val_loss: -0.6201\n",
      "Epoch 423/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5690 - val_loss: -0.5567\n",
      "Epoch 424/1000\n",
      "8875/8875 [==============================] - 0s 13us/sample - loss: -0.5617 - val_loss: -0.6084\n",
      "Epoch 425/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5708 - val_loss: -0.6189\n",
      "Epoch 426/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5704 - val_loss: -0.5892\n",
      "Epoch 427/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5632 - val_loss: -0.5865\n",
      "Epoch 428/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5794 - val_loss: -0.5551\n",
      "Epoch 429/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5737 - val_loss: -0.6158\n",
      "Epoch 430/1000\n",
      "8875/8875 [==============================] - 1s 156us/sample - loss: -0.5748 - val_loss: -0.6060\n",
      "Epoch 431/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5737 - val_loss: -0.6244\n",
      "Epoch 432/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5690 - val_loss: -0.6236\n",
      "Epoch 433/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5833 - val_loss: -0.6009\n",
      "Epoch 434/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5659 - val_loss: -0.6008\n",
      "Epoch 435/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5763 - val_loss: -0.6013\n",
      "Epoch 436/1000\n",
      "8875/8875 [==============================] - 1s 154us/sample - loss: -0.5750 - val_loss: -0.6112\n",
      "Epoch 437/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5612 - val_loss: -0.6152\n",
      "Epoch 438/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5720 - val_loss: -0.6027\n",
      "Epoch 439/1000\n",
      "8875/8875 [==============================] - 0s 16us/sample - loss: -0.5614 - val_loss: -0.6188\n",
      "Epoch 440/1000\n",
      "8875/8875 [==============================] - 0s 15us/sample - loss: -0.5651 - val_loss: -0.6179\n",
      "Epoch 441/1000\n",
      "8875/8875 [==============================] - 0s 14us/sample - loss: -0.5703 - val_loss: -0.6063\n",
      "Epoch 442/1000\n",
      "8875/8875 [==============================] - 1s 152us/sample - loss: -0.5768 - val_loss: -0.6060\n",
      "Epoch 443/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5849 - val_loss: -0.6158\n",
      "Epoch 444/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5827 - val_loss: -0.6021\n",
      "Epoch 445/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5737 - val_loss: -0.6003\n",
      "Epoch 446/1000\n",
      "8875/8875 [==============================] - 0s 18us/sample - loss: -0.5752 - val_loss: -0.5866\n",
      "Epoch 447/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5766 - val_loss: -0.6072\n",
      "Epoch 448/1000\n",
      "8875/8875 [==============================] - 1s 155us/sample - loss: -0.5759 - val_loss: -0.5983\n",
      "Epoch 449/1000\n",
      "8875/8875 [==============================] - 0s 20us/sample - loss: -0.5779 - val_loss: -0.5841\n",
      "Epoch 450/1000\n",
      "8875/8875 [==============================] - 0s 21us/sample - loss: -0.5711 - val_loss: -0.6181\n",
      "Epoch 451/1000\n",
      "8875/8875 [==============================] - 0s 19us/sample - loss: -0.5719 - val_loss: -0.5755\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    \n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_filepath = f\"./t1dexi_test_cfg/subject1617_evaluate.yaml\" # Replace the yaml\n",
    "mode = \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 experiments.\n",
      "{   'dataset': {   'csv_path': 'C:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\LB_split\\\\population_testing\\\\1617_training_data.csv',\n",
      "                   'nb_future_steps': 6,\n",
      "                   'nb_past_steps': 6,\n",
      "                   'param_nb_future_steps': [6],\n",
      "                   'scale': 0.01,\n",
      "                   'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\datasets\\\\t1dexi.py',\n",
      "                   'test_fraction': 1,\n",
      "                   'train_fraction': 0,\n",
      "                   'valid_fraction': 0},\n",
      "    'loss_function': {   'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\loss_functions\\\\nll_keras.py'},\n",
      "    'model': {   'activation_function': 'exp',\n",
      "                 'nb_lstm_states': 256,\n",
      "                 'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\models\\\\lstm_experiment_keras.py'},\n",
      "    'optimizer': {   'learning_rate': '1e-3',\n",
      "                     'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\optimizers\\\\adam_keras.py'},\n",
      "    'train': {   'artifacts_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\artifacts\\\\t1dexi\\\\nb_future_steps_6_seed_1_',\n",
      "                 'batch_size': 1024,\n",
      "                 'epochs': 1000,\n",
      "                 'param_seed': [1],\n",
      "                 'patience': 200,\n",
      "                 'script_path': 'c:\\\\Users\\\\username\\\\OneDrive\\\\Desktop\\\\BGprediction\\\\Method1\\\\blood-glucose-prediction\\\\train\\\\train_keras.py',\n",
      "                 'seed': 1,\n",
      "                 'shuffle': True}}\n",
      "nb_future_steps  6\n",
      "x_train.shape:  (0, 6, 1)\n",
      "y_train.shape:  (0, 1)\n",
      "x_valid.shape:  (0, 6, 1)\n",
      "y_valid.shape:  (0, 1)\n",
      "x_test.shape:  (75, 6, 1)\n",
      "y_test.shape:  (75, 1)\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "2024-08-26 21:04:33,772 WARNING Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "loading weights: c:\\Users\\username\\OneDrive\\Desktop\\BGprediction\\Method1\\blood-glucose-prediction\\artifacts\\t1dexi\\nb_future_steps_6_seed_1_\\model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient id:  1617_training_data.csv\n",
      "RMSE:  21.871165936041482\n",
      "t0 RMSE:  27.860366113890173\n",
      "SEG:  (0.17878328736241264, 0.20014844878841118)\n",
      "t0 SEG:  (0.23692731242481505, 0.2539320654735406)\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_cfgs(yaml_filepath)\n",
    "print(\"Running {} experiments.\".format(len(cfgs)))\n",
    "for cfg in cfgs:\n",
    "    seed = int(cfg['train']['seed'])\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Print the configuration - just to make sure that you loaded what you\n",
    "    # wanted to load\n",
    "\n",
    "    module_dataset       = load_module(cfg['dataset']['script_path'])\n",
    "    module_model         = load_module(cfg['model']['script_path'])\n",
    "    module_optimizer     = load_module(cfg['optimizer']['script_path'])\n",
    "    module_loss_function = load_module(cfg['loss_function']['script_path'])\n",
    "    module_train         = load_module(cfg['train']['script_path'])\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(cfg)\n",
    "\n",
    "    #print(\"loading dataset ...\")\n",
    "    #nb_past_steps = cfg['dataset']['nb_past_steps']\n",
    "    #nb_past_steps_tmp = 36\n",
    "    #cfg['dataset']['nb_past_steps'] = nb_past_steps_tmp\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = module_dataset.load_dataset(cfg['dataset'])\n",
    "    #x_train = x_train[:,-nb_past_steps:,:]\n",
    "    #x_valid = x_valid[:,-nb_past_steps:,:]\n",
    "    #x_test = x_test[:,-nb_past_steps:,:]\n",
    "    print(\"x_train.shape: \", x_train.shape)\n",
    "    print(\"y_train.shape: \", y_train.shape)\n",
    "    print(\"x_valid.shape: \", x_valid.shape)\n",
    "    print(\"y_valid.shape: \", y_valid.shape)\n",
    "    print(\"x_test.shape: \", x_test.shape)\n",
    "    print(\"y_test.shape: \", y_test.shape)\n",
    "    #print(\"loading optimizer ...\")\n",
    "    optimizer = module_optimizer.load(cfg['optimizer'])\n",
    "\n",
    "    #print(\"loading loss function ...\")\n",
    "    loss_function = module_loss_function.load()\n",
    "    #print(\"loaded function {} ...\".format(loss_function.__name__))\n",
    "\n",
    "    #print(\"loading model ...\")\n",
    "    if 'tf_nll' in loss_function.__name__:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1]*2,\n",
    "            cfg['model']\n",
    "        )\n",
    "    else:\n",
    "        model = module_model.load(\n",
    "            x_train.shape[1:],\n",
    "            y_train.shape[1],\n",
    "            cfg['model']\n",
    "        )\n",
    "\n",
    "    if 'initial_weights_path' in cfg['train']:\n",
    "        #print(\"Loading initial weights: \", cfg['train']['initial_weights_path'])\n",
    "        model.load_weights(cfg['train']['initial_weights_path'])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function\n",
    "    )\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    # training mode\n",
    "    if mode == 'train':\n",
    "        #print(\"training model ...\")\n",
    "        train(model, module_train, x_train, y_train, x_valid, y_valid, cfg)\n",
    "    if mode == 'plot_nll':\n",
    "        plot_nll(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_noise_experiment':\n",
    "        plot_noise_experiment(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_seg':\n",
    "        plot_seg(model, x_test, y_test, cfg)\n",
    "    if mode == 'plot_dist':\n",
    "        plot_target_distribution(y_test, cfg)\n",
    "\n",
    "    # evaluation mode\n",
    "    if mode == 'evaluate':\n",
    "        evaluate(model, x_test, y_test, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
