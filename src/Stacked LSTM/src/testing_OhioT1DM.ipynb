{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import root_mean_squared_error\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 0\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set the \n",
    "def read_ohio(filepath, category, round):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # interval_timedelta = datetime.timedelta(minutes=interval_timedelta)\n",
    "\n",
    "    res = []\n",
    "    for item in root.findall(category):\n",
    "        entry0 = item[0].attrib\n",
    "        if round == True:\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts'])\n",
    "            entry0['ts'] = adjusted_ts\n",
    "        ts = entry0['ts']\n",
    "        entry0['ts'] = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        res.append([entry0])\n",
    "        for i in range(1, len(item)):\n",
    "            # last_entry = item[i - 1].attrib\n",
    "            entry = item[i].attrib\n",
    "            # t1 = datetime.datetime.strptime(entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # t0 = datetime.datetime.strptime(last_entry[\"ts\"], \"%d-%m-%Y %H:%M:%S\")\n",
    "            # delt = t1 - t0\n",
    "            # if category == \"glucose_level\":\n",
    "            #     if delt <= interval_timedelta:\n",
    "            #         res[-1].append([entry])\n",
    "            #     else:\n",
    "            #         res.append([entry])\n",
    "            # else:\n",
    "            ts = entry['ts']\n",
    "            if round == True:\n",
    "                adjusted_ts = round_up_to_nearest_five_minutes(ts)\n",
    "                entry['ts'] = adjusted_ts\n",
    "            ts = entry['ts']\n",
    "            entry['ts'] = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "            res.append([entry])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_into_table(glucose):\n",
    "    glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "    # Create the multi-channel database\n",
    "    g_data = []\n",
    "    for timestamp in glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': glucose_dict[timestamp],\n",
    "            # 'meal_type': None,\n",
    "            # 'meal_carbs': 0\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "    # Convert glucose values to numeric type for analysis\n",
    "    glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "    glucose_df['glucose_value'] = glucose_df['glucose_value'] # Shrink to its 1/100 for scaling\n",
    "\n",
    "    return glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_15min(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(minutes=15)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_glucose_index(glucose_df, meal_time, threshold_seconds=300):\n",
    "    time_diffs = (glucose_df['timestamp'] - meal_time).abs()\n",
    "    within_threshold = time_diffs < pd.Timedelta(seconds=threshold_seconds)\n",
    "    if within_threshold.any():\n",
    "        closest_index = time_diffs[within_threshold].idxmin()\n",
    "        return closest_index\n",
    "    return None\n",
    "\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carb_effect'] = 0\n",
    "\n",
    "        for index, meal_row in meal_df.iterrows():\n",
    "            meal_time = meal_row['ts']\n",
    "            closest_glucose_idx = find_closest_glucose_index(segment_df, meal_time)\n",
    "            \n",
    "            if closest_glucose_idx is not None:\n",
    "                segment_df.loc[closest_glucose_idx, 'carb_effect'] = int(meal_row['carb_effect'])\n",
    "                meal_df.loc[index, 'assigned'] = True\n",
    "\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_meal_entry(meal_row):\n",
    "    meal_time = meal_row['ts']\n",
    "    end_effect_time = meal_time + timedelta(hours=3)\n",
    "    carb = float(meal_row['carbs'])\n",
    "\n",
    "    c_eff_list = [0, 0, 0, ]\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        c_eff = (i * 0.111) * carb\n",
    "        if c_eff > carb:\n",
    "            print(\"C_eff > carb\")\n",
    "            c_eff = carb\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    for j in range(1, 25):\n",
    "        c_eff = (1 - (j * 0.028)) * carb\n",
    "        if c_eff < 0:\n",
    "            print(\"C_eff < 0\")\n",
    "            c_eff = 0\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    timestamp_list = pd.date_range(start=meal_time, end=end_effect_time, freq='5min')\n",
    "    d = {\"ts\": timestamp_list[:-1], \"carb_effect\": c_eff_list}\n",
    "    meal_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return meal_effect_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ohio_bolus_tempbasal(filepath, category, round):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # interval_timedelta = datetime.timedelta(minutes=interval_timedelta)\n",
    "\n",
    "    res = []\n",
    "    for item in root.findall(category):\n",
    "        if len(item) == 0:\n",
    "            continue  # Skip if the item has no children\n",
    "            \n",
    "        entry0 = item[0].attrib\n",
    "        if round == True:\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts_begin'])\n",
    "            entry0['ts_begin'] = adjusted_ts\n",
    "            adjusted_ts = round_up_to_nearest_five_minutes(entry0['ts_end'])\n",
    "            entry0['ts_end'] = adjusted_ts\n",
    "        \n",
    "        entry0['ts_begin'] = datetime.strptime(entry0['ts_begin'], \"%d-%m-%Y %H:%M:%S\")\n",
    "        entry0['ts_end'] = datetime.strptime(entry0['ts_end'], \"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "        res.append([entry0])\n",
    "        for i in range(1, len(item)):\n",
    "            # last_entry = item[i - 1].attrib\n",
    "            entry = item[i].attrib\n",
    "            ts_begin = entry['ts_begin']\n",
    "            ts_end = entry['ts_end']\n",
    "            if round == True:\n",
    "                adjusted_ts_begin = round_up_to_nearest_five_minutes(ts_begin)\n",
    "                entry['ts_end'] = adjusted_ts_begin\n",
    "                adjusted_ts_end = round_up_to_nearest_five_minutes(ts_end)\n",
    "                entry['ts_end'] = adjusted_ts_end\n",
    "            entry['ts_begin'] = datetime.strptime(entry['ts_begin'], \"%d-%m-%Y %H:%M:%S\")\n",
    "            entry['ts_end'] = datetime.strptime(entry['ts_end'], \"%d-%m-%Y %H:%M:%S\")\n",
    "            if category == \"bolus\":\n",
    "                if entry['ts_begin'] != entry['ts_end']:\n",
    "                    print(\"Unequal: begin: \" + str(entry['ts_begin']) + \"end: \" + str(entry['ts_end']))\n",
    "            res.append([entry])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bolus_entry(bolus_row):\n",
    "    bolus_time = bolus_row['ts_begin']\n",
    "    timestamp_list = [bolus_time, ]\n",
    "    # end_effect_time = bolus_time + timedelta(hours=3)\n",
    "    dose = float(bolus_row['dose'])\n",
    "\n",
    "    b_eff_list = [dose, ]\n",
    "    b_eff = dose\n",
    "\n",
    "    i = 1\n",
    "    while b_eff > 0:\n",
    "        b_eff = dose - (i * 0.07)\n",
    "        b_eff_list.append(b_eff)\n",
    "        timestamp_list.append(bolus_time + timedelta(minutes=5 * i))\n",
    "        i += 1\n",
    "    # print(len(timestamp_list[:-1]))\n",
    "    # print(len(b_eff_list[:-1]))\n",
    "\n",
    "\n",
    "    d = {\"ts\": timestamp_list[:-1], \"bolus_effect\": b_eff_list[:-1]}\n",
    "    bolus_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return bolus_effect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['bolus_effect'] = 0\n",
    "\n",
    "        for index, bolus_row in bolus_df.iterrows():\n",
    "            bolus_time = bolus_row['ts']\n",
    "            closest_glucose_idx = find_closest_glucose_index(segment_df, bolus_time)\n",
    "            \n",
    "            if closest_glucose_idx is not None:\n",
    "                segment_df.loc[closest_glucose_idx, 'bolus_effect'] = float(bolus_row['bolus_effect'])\n",
    "                bolus_df.loc[index, 'assigned'] = True\n",
    "\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_step(window_list, step_df):\n",
    "    start_time = window_list[0]\n",
    "    end_time = window_list[-1]\n",
    "\n",
    "    step_list = []\n",
    "    counter = 1\n",
    "    for idx, step_row in step_df.iterrows():\n",
    "        \n",
    "        if step_row['ts'] >= start_time and step_row['ts'] < end_time:\n",
    "            step_list.append(counter * float(step_row['value']))\n",
    "            counter += 1\n",
    "\n",
    "        if step_row['ts'] >= end_time:\n",
    "            break\n",
    "    # print(\"length of step_list \", len(step_list))\n",
    "    if len(step_list) == 0:\n",
    "        return None\n",
    "    accumulate_step = sum(step_list)/len(step_list)\n",
    "    return accumulate_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(segments, ph, history):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carb_effect'] = pd.to_numeric(segment_df['carb_effect'], errors='coerce')\n",
    "        segment_df['steps'] = pd.to_numeric(segment_df['steps'], errors='coerce')\n",
    "        segment_df['steps'] = segment_df['steps'] \n",
    "        segment_df['bolus_effect'] = pd.to_numeric(segment_df['bolus_effect'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        max_index = len(segment_df) - (history-1+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + history-1 to exist\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index + 1):\n",
    "            # Extracting features from index i to i+history-1\n",
    "            features = segment_df.loc[i:i+history-1, ['glucose_value', 'carb_effect', 'bolus_effect', 'steps']] # .values.flatten() # 'carb_effect', 'bolus_effect', 'steps'\n",
    "            # Extracting label for index i+21\n",
    "            # Do the label transform\n",
    "            label = segment_df.loc[i+history-1+ph, 'glucose_value'] - segment_df.loc[i+history-1, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history-1+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "    return features_list, labels_list, raw_glu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)\n",
    "        self.fc2 = nn.Linear(512, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_size).to(device)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from the input tensor\n",
    "\n",
    "        # Initialize hidden and cell state for the first LSTM layer\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, (hn, cn) = self.lstm1(x, (h0, c0))\n",
    "        \n",
    "        # Dropout layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Initialize hidden and cell state for the second LSTM layer\n",
    "        h1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        out, (hn, cn) = self.lstm2(out, (h1, c1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = out[:, -1, :]  # Get the last time step output\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../OhioT1DM/2018/test/559-ws-testing.xml\"\n",
    "glucose = read_ohio(filepath, \"glucose_level\", True)\n",
    "glucose_df = transfer_into_table(glucose)\n",
    "segments = segement_data_as_15min(glucose_df)\n",
    "\n",
    "#Include meal:\n",
    "meal = read_ohio(filepath, \"meal\", True)\n",
    "flattened_meal_data = [item[0] for item in meal]  # Take the first (and only) item from each sublist\n",
    "# Convert to DataFrame\n",
    "meal_df = pd.DataFrame(flattened_meal_data)\n",
    "meal_df['assigned'] = False\n",
    "empty_d = {\"ts\": [], \"carb_effect\": []}\n",
    "whole_meal_effect_df = pd.DataFrame(data = empty_d)\n",
    "for index, meal_row in meal_df.iterrows():\n",
    "    meal_effect_df = expand_meal_entry(meal_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_meal_effect_df, meal_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['carb_effect_df1'] = merged_df['carb_effect_df1'].fillna(0)\n",
    "    merged_df['carb_effect_df2'] = merged_df['carb_effect_df2'].fillna(0)\n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['carb_effect'] = merged_df['carb_effect_df1'] + merged_df['carb_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_meal_effect_df = merged_df[['ts', 'carb_effect']]\n",
    "\n",
    "whole_meal_effect_df['assigned'] = False\n",
    "meal_updated_segments = update_segments_with_meals(segments, whole_meal_effect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include bolus:\n",
    "bolus = read_ohio_bolus_tempbasal(filepath, \"bolus\", True)\n",
    "flattened_bolus_data = [item[0] for item in bolus]  # Take the first (and only) item from each sublist\n",
    "# Convert to DataFrame\n",
    "bolus_df = pd.DataFrame(flattened_bolus_data)\n",
    "\n",
    "empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "whole_bolus_effect_df = pd.DataFrame(data = empty_b)\n",
    "\n",
    "for index, bolus_row in bolus_df.iterrows():\n",
    "    bolus_effect_df = expand_bolus_entry(bolus_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_bolus_effect_df, bolus_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['bolus_effect_df1'] = merged_df['bolus_effect_df1'].fillna(0)\n",
    "    merged_df['bolus_effect_df2'] = merged_df['bolus_effect_df2'].fillna(0)\n",
    "    \n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['bolus_effect'] = merged_df['bolus_effect_df1'] + merged_df['bolus_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_bolus_effect_df = merged_df[['ts', 'bolus_effect']]\n",
    "\n",
    "whole_bolus_effect_df[\"assigned\"] = False\n",
    "bolus_updated_segments = update_segments_with_bolus(meal_updated_segments, whole_bolus_effect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process step information\n",
    "# OLD CODE, VERY TIME CONSUMING\n",
    "# steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "# flattened_steps_data = [item[0] for item in steps]  # Take the first (and only) item from each sublist\n",
    "# # Convert to DataFrame\n",
    "# step_df = pd.DataFrame(flattened_steps_data)\n",
    "\n",
    "# accumulate_step_list = []\n",
    "# # test_segment = segments[\"segment_1\"]\n",
    "# for segment_name, segment_df in bolus_updated_segments.items():\n",
    "#     accumulate_step_list = []\n",
    "#     for index, cgm_row in segment_df.iterrows():\n",
    "#         current = cgm_row['timestamp']\n",
    "#         first_timestamp = current - timedelta(minutes=50)\n",
    "#         window_list = pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "#         accumulated_step = compute_accumulated_step(window_list, step_df)\n",
    "#         accumulate_step_list.append(accumulated_step)\n",
    "#     segment_df['steps'] = accumulate_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize step information\n",
    "# Process step information\n",
    "def optimize_step_processing(bolus_updated_segments, step_df):\n",
    "    # Convert step_df timestamps to datetime if they aren't already\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    step_df['value'] = pd.to_numeric(step_df['value'])\n",
    "    \n",
    "    # Pre-calculate weights for step accumulation (1 to 10 for 50 minutes window)\n",
    "    weights = np.arange(1, 11)\n",
    "    \n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        # Convert timestamps if needed\n",
    "        segment_df['timestamp'] = pd.to_datetime(segment_df['timestamp'])\n",
    "        \n",
    "        # Create array to store accumulated steps\n",
    "        accumulate_step_list = []\n",
    "        \n",
    "        # Get all unique window starts for this segment\n",
    "        window_starts = segment_df['timestamp'].apply(lambda x: x - timedelta(minutes=50))\n",
    "        window_ends = segment_df['timestamp']\n",
    "        \n",
    "        # Process each window\n",
    "        for start, end in zip(window_starts, window_ends):\n",
    "            # Filter steps within the window\n",
    "            mask = (step_df['ts'] >= start) & (step_df['ts'] < end)\n",
    "            window_steps = step_df.loc[mask, 'value']\n",
    "            \n",
    "            if len(window_steps) == 0:\n",
    "                accumulate_step_list.append(None)\n",
    "            else:\n",
    "                # Take last 10 steps (or pad with zeros if less than 10)\n",
    "                last_steps = window_steps.iloc[-10:] if len(window_steps) > 10 else window_steps\n",
    "                weighted_sum = (last_steps.values * weights[:len(last_steps)]).sum()\n",
    "                accumulate_step_list.append(weighted_sum / len(last_steps))\n",
    "        \n",
    "        # Assign accumulated steps to segment\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "    return bolus_updated_segments\n",
    "\n",
    "\n",
    "# Main execution\n",
    "steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "flattened_steps_data = [item[0] for item in steps]\n",
    "step_df = pd.DataFrame(flattened_steps_data)\n",
    "step_updated_segments = optimize_step_processing(bolus_updated_segments, step_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data CAREFULL!!!\n",
    "# Specify the file name\n",
    "filename = './processed_data/559_combined_segments_noshrink.pkl'\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(step_updated_segments, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "import pickle\n",
    "\n",
    "filename = './processed_data/559_combined_segments_noshrink.pkl'\n",
    "# Load the dictionary from the file\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_df_dict = pickle.load(f)\n",
    "\n",
    "# Verify the content\n",
    "\n",
    "print(loaded_df_dict['segment_1'])\n",
    "print(loaded_df_dict['segment_2'])\n",
    "step_updated_segments = loaded_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "features_list, labels_list, raw_glu_list = prepare_dataset(step_updated_segments, 6, 6)\n",
    "\n",
    "# Build training and validation loader\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list) # Maybe need to replace this\n",
    "\n",
    "# For one channel\n",
    "# data_sequences = np.reshape(features_array, (features_array.shape[0], features_array.shape[1], 1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_array, labels_array, test_size=0.2, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # The original batch size = 128, however, training on 128 cannot get the model fully trained, so change to 32.\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4 # Number of input features\n",
    "hidden_size = 128  # Hidden vector size\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single output\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "\n",
    "\n",
    "\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[:700])\n",
    "plt.plot(actuals[:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class StackedLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "#         super(StackedLSTM, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "#         # LSTM layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob).to(device)\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(hidden_size, 512).to(device)\n",
    "#         self.fc2 = nn.Linear(512, 128).to(device)\n",
    "#         self.fc3 = nn.Linear(128, output_size).to(device)\n",
    "        \n",
    "#         # Activation functions\n",
    "#         self.relu = nn.ReLU()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         # Initialize hidden and cell state for LSTM layers\n",
    "#         h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "#         # LSTM layers\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         out = out[:, -1, :]  # Get the last time step output\n",
    "#         out = self.relu(self.fc1(out))\n",
    "#         out = self.relu(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "# input_size = 4\n",
    "# hidden_size = 128\n",
    "# num_layers = 3  # Increased number of layers\n",
    "# output_size = 1\n",
    "# dropout_prob = 0.3  # Adjusted dropout probability\n",
    "\n",
    "# model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjusted learning rate\n",
    "\n",
    "# num_epochs = 200\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "    \n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "        \n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         total_loss = 0\n",
    "#         for inputs, targets in val_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets.float())\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         avg_loss = total_loss / len(val_loader)\n",
    "#         print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "# model.eval()\n",
    "# predictions = []\n",
    "# actuals = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in val_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         predictions.append(outputs)\n",
    "#         actuals.append(targets)\n",
    "\n",
    "# predictions = torch.cat(predictions).cpu().numpy()\n",
    "# actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "# rmse = np.sqrt(np.mean((actuals - predictions) ** 2))\n",
    "# print(f'RMSE on validation set: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "# filename = '588_combined_segments_test.pkl'\n",
    "# # Load the dictionary from the file\n",
    "# with open(filename, 'rb') as f:\n",
    "#     loaded_df_dict = pickle.load(f)\n",
    "\n",
    "# # Verify the content\n",
    "\n",
    "# print(loaded_df_dict['segment_1'])\n",
    "# print(loaded_df_dict['segment_2'])\n",
    "# bolus_updated_segments_test = loaded_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_test = f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/OhioT1DM/2018/test/559-ws-testing.xml\"\n",
    "glucose_test = read_ohio(filepath_test, \"glucose_level\", True)\n",
    "glucose_df_test = transfer_into_table(glucose_test)\n",
    "segments_test = segement_data_as_15min(glucose_df_test) # segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include meal:\n",
    "meal_test = read_ohio(filepath_test, \"meal\", True)\n",
    "flattened_meal_data_test = [item[0] for item in meal_test]  # Take the first (and only) item from each sublist\n",
    "# Convert to DataFrame\n",
    "meal_df_test = pd.DataFrame(flattened_meal_data_test)\n",
    "meal_df_test['assigned'] = False\n",
    "empty_d = {\"ts\": [], \"carb_effect\": []}\n",
    "whole_meal_effect_df_test = pd.DataFrame(data = empty_d)\n",
    "for index, meal_row in meal_df_test.iterrows():\n",
    "    meal_effect_df_test = expand_meal_entry(meal_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df_test = pd.merge(whole_meal_effect_df_test, meal_effect_df_test, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df_test['carb_effect_df1'] = merged_df_test['carb_effect_df1'].fillna(0)\n",
    "    merged_df_test['carb_effect_df2'] = merged_df_test['carb_effect_df2'].fillna(0)\n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df_test['carb_effect'] = merged_df_test['carb_effect_df1'] + merged_df_test['carb_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_meal_effect_df_test = merged_df_test[['ts', 'carb_effect']]\n",
    "\n",
    "whole_meal_effect_df_test['assigned'] = False\n",
    "meal_updated_segments_test = update_segments_with_meals(segments_test, whole_meal_effect_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include bolus:\n",
    "bolus_test = read_ohio_bolus_tempbasal(filepath_test, \"bolus\", True)\n",
    "flattened_bolus_data_test = [item[0] for item in bolus_test]  # Take the first (and only) item from each sublist\n",
    "# Convert to DataFrame\n",
    "bolus_df_test = pd.DataFrame(flattened_bolus_data_test)\n",
    "\n",
    "empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "whole_bolus_effect_df_test = pd.DataFrame(data = empty_b)\n",
    "\n",
    "for index, bolus_row in bolus_df_test.iterrows():\n",
    "    bolus_effect_df_test = expand_bolus_entry(bolus_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df_test = pd.merge(whole_bolus_effect_df_test, bolus_effect_df_test, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df_test['bolus_effect_df1'] = merged_df_test['bolus_effect_df1'].fillna(0)\n",
    "    merged_df_test['bolus_effect_df2'] = merged_df_test['bolus_effect_df2'].fillna(0)\n",
    "    \n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df_test['bolus_effect'] = merged_df_test['bolus_effect_df1'] + merged_df_test['bolus_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_bolus_effect_df_test = merged_df_test[['ts', 'bolus_effect']]\n",
    "\n",
    "whole_bolus_effect_df_test[\"assigned\"] = False\n",
    "bolus_updated_segments_test = update_segments_with_bolus(meal_updated_segments_test, whole_bolus_effect_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process step information\n",
    "# steps_test = read_ohio(filepath_test, \"basis_steps\", True)\n",
    "# flattened_steps_data_test = [item[0] for item in steps_test]  # Take the first (and only) item from each sublist\n",
    "# # Convert to DataFrame\n",
    "# step_df_test = pd.DataFrame(flattened_steps_data_test)\n",
    "\n",
    "# accumulate_step_list_test = []\n",
    "# # test_segment = segments[\"segment_1\"]\n",
    "# for segment_name, segment_df in bolus_updated_segments_test.items():\n",
    "#     accumulate_step_list_test = []\n",
    "#     for index, cgm_row in segment_df.iterrows():\n",
    "#         current = cgm_row['timestamp']\n",
    "#         first_timestamp = current - timedelta(minutes=50)\n",
    "#         window_list_test= pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "#         accumulated_step_test = compute_accumulated_step(window_list_test, step_df_test)\n",
    "#         accumulate_step_list_test.append(accumulated_step_test)\n",
    "#     segment_df['steps'] = accumulate_step_list_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process step information\n",
    "def optimize_step_processing(bolus_updated_segments, step_df):\n",
    "    # Convert step_df timestamps to datetime if they aren't already\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    step_df['value'] = pd.to_numeric(step_df['value'])\n",
    "    \n",
    "    # Pre-calculate weights for step accumulation (1 to 10 for 50 minutes window)\n",
    "    weights = np.arange(1, 11)\n",
    "    \n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        # Convert timestamps if needed\n",
    "        segment_df['timestamp'] = pd.to_datetime(segment_df['timestamp'])\n",
    "        \n",
    "        # Create array to store accumulated steps\n",
    "        accumulate_step_list = []\n",
    "        \n",
    "        # Get all unique window starts for this segment\n",
    "        window_starts = segment_df['timestamp'].apply(lambda x: x - timedelta(minutes=50))\n",
    "        window_ends = segment_df['timestamp']\n",
    "        \n",
    "        # Process each window\n",
    "        for start, end in zip(window_starts, window_ends):\n",
    "            # Filter steps within the window\n",
    "            mask = (step_df['ts'] >= start) & (step_df['ts'] < end)\n",
    "            window_steps = step_df.loc[mask, 'value']\n",
    "            \n",
    "            if len(window_steps) == 0:\n",
    "                accumulate_step_list.append(None)\n",
    "            else:\n",
    "                # Take last 10 steps (or pad with zeros if less than 10)\n",
    "                last_steps = window_steps.iloc[-10:] if len(window_steps) > 10 else window_steps\n",
    "                weighted_sum = (last_steps.values * weights[:len(last_steps)]).sum()\n",
    "                accumulate_step_list.append(weighted_sum / len(last_steps))\n",
    "        \n",
    "        # Assign accumulated steps to segment\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "    return bolus_updated_segments\n",
    "\n",
    "# Main execution\n",
    "steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "flattened_steps_data = [item[0] for item in steps]\n",
    "step_df = pd.DataFrame(flattened_steps_data)\n",
    "step_updated_segments_test = optimize_step_processing(bolus_updated_segments_test, step_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "features_list_test, labels_list_test, raw_glu_list_test = prepare_dataset(step_updated_segments_test, 6, 6)\n",
    "\n",
    "# Build training and validation loader\n",
    "features_array_test = np.array(features_list_test)\n",
    "labels_array_test = np.array(raw_glu_list_test) # Maybe need to replace this\n",
    "\n",
    "X_test, y_test = features_array_test, labels_array_test\n",
    "\n",
    "# Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[:700])\n",
    "plt.plot(actuals[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[:700])\n",
    "plt.plot(actuals[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
