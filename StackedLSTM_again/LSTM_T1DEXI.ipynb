{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 7\n",
    "ph = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_to_nearest_five_minutes(ts):\n",
    "    # Parse the timestamp\n",
    "    dt = datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "    \n",
    "    # Calculate minutes to add to round up to the nearest 5 minutes\n",
    "    minutes_to_add = (5 - dt.minute % 5) % 5\n",
    "    if minutes_to_add == 0 and dt.second == 0:\n",
    "        # If exactly on a 5 minute mark and second is 0, no need to add time\n",
    "        minutes_to_add = 0\n",
    "    \n",
    "    # Add the necessary minutes\n",
    "    new_dt = dt + timedelta(minutes=minutes_to_add)\n",
    "    \n",
    "    # Return the new timestamp in the same format\n",
    "    return new_dt.strftime( \"%d-%m-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t1dexi_cgm(subject, round):\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject.groupby('LBCAT')\n",
    "    # Create a dictionary to store the split DataFrames\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "    new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "\n",
    "    new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    if round == True:\n",
    "        rounded_timestamp = []\n",
    "        for ts in new_df_cgm[\"LBDTC\"]:\n",
    "            rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "        new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "        formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "    else:\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segement_data_as_15min(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df['timestamp'].diff()\n",
    "\n",
    "    # Identify large gaps\n",
    "    df['new_segment'] = df['time_diff'] > pd.Timedelta(hours=0.25)\n",
    "\n",
    "    # Find indices where new segments start\n",
    "    segment_starts = df[df['new_segment']].index\n",
    "\n",
    "    # Initialize an empty dictionary to store segments\n",
    "    segments = {}\n",
    "    prev_index = 0\n",
    "\n",
    "    # Loop through each segment start and slice the DataFrame accordingly\n",
    "    for i, start in enumerate(segment_starts, 1):\n",
    "        segments[f'segment_{i}'] = df.iloc[prev_index:start].reset_index(drop=True)\n",
    "        prev_index = start\n",
    "\n",
    "    # Add the last segment from the last gap to the end of the DataFrame\n",
    "    segments[f'segment_{len(segment_starts) + 1}'] = df.iloc[prev_index:].reset_index(drop=True)\n",
    "\n",
    "    # Optionally remove helper columns from each segment\n",
    "    for segment in segments.values():\n",
    "        segment.drop(columns=['time_diff', 'new_segment'], inplace=True)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_meals(segments, meal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['carb_effect'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            meal_df['time_difference'] = abs(meal_df['ts'] - row['timestamp'])\n",
    "            closest_meal = meal_df.loc[meal_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest meal is within 5 minutes\n",
    "            if closest_meal['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the meal is assigned to only one segment and is the closest\n",
    "                if not meal_df.at[closest_meal.name, 'assigned']:\n",
    "                    segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']\n",
    "                    meal_df.at[closest_meal.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['carb_effect'] == closest_meal['carb_effect']].index[0]\n",
    "                    if row['timestamp'] - closest_meal['ts'] < segment_df.at[assigned_index, 'timestamp'] - closest_meal['ts']:\n",
    "                        # Reassign the meal to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'carb_effect'] = 0  # Remove carbs from previously assigned timestamp\n",
    "                        segment_df.at[i, 'carb_effect'] = closest_meal['carb_effect']  # Assign carbs to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"Meal type {meal['type']} on {meal['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align and update segments with meal data\n",
    "def update_segments_with_basal(segments, basal_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'carbs' column to zeros\n",
    "        segment_df['basal_rate'] = None\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest meal timestamp and its carb information\n",
    "            for _, basal_row in basal_df.iterrows():\n",
    "                if basal_row['ts'] <= row['timestamp'] < (basal_row['end_ts'] if pd.notna(basal_row['end_ts']) else pd.Timestamp('2099-12-31')):\n",
    "                    segment_df.at[i, 'basal_rate'] = basal_row['value']\n",
    "                    break\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_meal_entry(meal_row):\n",
    "    meal_time = meal_row['ts']\n",
    "    end_effect_time = meal_time + timedelta(hours=3)\n",
    "    carb = float(meal_row['carbs'])\n",
    "\n",
    "    c_eff_list = [0, 0, 0, ]\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        c_eff = (i * 0.111) * carb\n",
    "        if c_eff > carb:\n",
    "            print(\"C_eff > carb\")\n",
    "            c_eff = carb\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    for j in range(1, 25):\n",
    "        c_eff = (1 - (j * 0.028)) * carb\n",
    "        if c_eff < 0:\n",
    "            print(\"C_eff < 0\")\n",
    "            c_eff = 0\n",
    "        c_eff_list.append(c_eff)\n",
    "\n",
    "    timestamp_list = pd.date_range(start=meal_time, end=end_effect_time, freq='5min')\n",
    "    d = {\"ts\": timestamp_list[:-1], \"carb_effect\": c_eff_list}\n",
    "    meal_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return meal_effect_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bolus and temp basal information\n",
    "# Need to set the \n",
    "def preprocess_t1dexi_bolus_tempbasal(filepath, round):\n",
    "    subject_facm = pd.read_csv(filepath)\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_bolus = split_dfs[\"BOLUS\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_bolus['FADTC'] = pd.to_datetime(new_df_bolus['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_bolus.reset_index(drop=True, inplace=True)\n",
    "    new_df_bolus = new_df_bolus.rename(columns={'FAORRES': 'dose', 'FADTC': 'ts_begin'})\n",
    "    new_df_bolus['assigned'] = False\n",
    "    # new_df_bolus['end_ts'] = new_df_bolus['ts_begin'].shift(-1)\n",
    "    return new_df_bolus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segments_with_bolus(segments, bolus_df):\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Initialize the 'dose' column to zeros\n",
    "        segment_df['bolus_dose'] = 0\n",
    "\n",
    "        # Iterate through each timestamp in the segment\n",
    "        for i, row in segment_df.iterrows():\n",
    "            # Find the closest bolus timestamp and its carb information\n",
    "            bolus_df['time_difference'] = abs(bolus_df['ts'] - row['timestamp'])\n",
    "            closest_bolus = bolus_df.loc[bolus_df['time_difference'].idxmin()]\n",
    "            \n",
    "            # Check if the closest bolus is within 5 minutes\n",
    "            if closest_bolus['time_difference'] <= pd.Timedelta(minutes=5):\n",
    "                # Ensure that the bolus is assigned to only one segment and is the closest\n",
    "                if not bolus_df.at[closest_bolus.name, 'assigned']:\n",
    "                    segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']\n",
    "                    bolus_df.at[closest_bolus.name, 'assigned'] = True  # Mark as assigned\n",
    "                else:\n",
    "                    # Check if the current timestamp is closer than the one it was assigned to\n",
    "                    assigned_index = segment_df[segment_df['bolus_dose'] == closest_bolus['bolus_effect']].index[0]\n",
    "                    if row['timestamp'] - closest_bolus['ts'] < closest_bolus['ts'] - segment_df.at[assigned_index, 'timestamp']:\n",
    "                        # Reassign the bolus to the new closer timestamp\n",
    "                        segment_df.at[assigned_index, 'bolus_dose'] = 0  # Remove dose from previously assigned timestamp\n",
    "                        segment_df.at[i, 'bolus_dose'] = closest_bolus['bolus_effect']  # Assign dose to the new closer timestamp\n",
    "            # else:\n",
    "            #     print(f\"bolus type {bolus['type']} on {bolus['ts']} is too far from closest timestamp in {closest_segment} with a difference of {closest_diff}.\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bolus_entry(bolus_row):\n",
    "    bolus_time = bolus_row['ts_begin']\n",
    "    timestamp_list = [bolus_time, ]\n",
    "    # end_effect_time = bolus_time + timedelta(hours=3)\n",
    "    dose = float(bolus_row['dose'])\n",
    "\n",
    "    b_eff_list = [dose, ]\n",
    "    b_eff = dose\n",
    "\n",
    "    i = 1\n",
    "    while b_eff > 0:\n",
    "        b_eff = dose - (i * 0.07)\n",
    "        b_eff_list.append(b_eff)\n",
    "        timestamp_list.append(bolus_time + timedelta(minutes=5 * i))\n",
    "        i += 1\n",
    "    # print(len(timestamp_list[:-1]))\n",
    "    # print(len(b_eff_list[:-1]))\n",
    "\n",
    "\n",
    "    d = {\"ts\": timestamp_list[:-1], \"bolus_effect\": b_eff_list[:-1]}\n",
    "    bolus_effect_df = pd.DataFrame(data = d)\n",
    "\n",
    "    return bolus_effect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_step(window_list, step_df):\n",
    "    start_time = window_list[0]\n",
    "    end_time = window_list[-1]\n",
    "\n",
    "    step_list = []\n",
    "    counter = 1\n",
    "    for idx, step_row in step_df.iterrows():\n",
    "        \n",
    "        if step_row['ts'] >= start_time and step_row['ts'] < end_time:\n",
    "            step_list.append(counter * float(step_row['value']))\n",
    "            counter += 1\n",
    "\n",
    "        if step_row['ts'] >= end_time:\n",
    "            break\n",
    "    # print(\"length of step_list \", len(step_list))\n",
    "    if len(step_list) == 0:\n",
    "        return None\n",
    "    accumulate_step = sum(step_list)/len(step_list)\n",
    "    return accumulate_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "def prepare_dataset(segments, ph):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carb_effect'] = pd.to_numeric(segment_df['carb_effect'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "        segment_df['steps'] = pd.to_numeric(segment_df['steps'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value', 'carb_effect', 'bolus_dose', 'steps']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)\n",
    "        self.fc2 = nn.Linear(512, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_size).to(device)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from the input tensor\n",
    "\n",
    "        # Initialize hidden and cell state for the first LSTM layer\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, (hn, cn) = self.lstm1(x, (h0, c0))\n",
    "        \n",
    "        # Dropout layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Initialize hidden and cell state for the second LSTM layer\n",
    "        h1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        out, (hn, cn) = self.lstm2(out, (h1, c1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = out[:, -1, :]  # Get the last time step output\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4# Number of input features\n",
    "hidden_size = 128  # Hidden vector size\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single output\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "\n",
    "\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_channels = 4  # Number of features\n",
    "# output_channels = 1  # Predicting a single value (glucose level)\n",
    "# num_blocks = 4  # Number of WaveNet blocks\n",
    "# dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "# model = StackedLSTM(input_channels, output_channels, num_blocks, dilations)\n",
    "# print(model)\n",
    "\n",
    "# # Example of how to define the loss and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv'] \n",
    "# '85.csv',\n",
    "# '911.csv',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = pd.read_csv(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose = preprocess_t1dexi_cgm(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/LB_split/854.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: print each segment\n",
    "segments = segement_data_as_15min(glucose_df)\n",
    "# interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = pd.read_csv(\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/ML_split/854.csv\")\n",
    "selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "# Fix: Use format='mixed' to handle different date formats\n",
    "meal_df['ts'] = pd.to_datetime(meal_df['ts'], format='mixed')\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "# Extract unique dates\n",
    "unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "# Convert to list\n",
    "meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "cleaned_segments = {}\n",
    "\n",
    "# Iterate through each segment and filter by unique dates\n",
    "for segment_name, df in segments.items():\n",
    "    # Convert timestamp column to datetime and then extract the date part\n",
    "    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    \n",
    "    # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "    filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "    \n",
    "    # Drop the 'date' column as it's no longer needed\n",
    "    filtered_df = filtered_df.drop(columns=['date'])\n",
    "    \n",
    "    # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "    cleaned_segments[segment_name] = filtered_df\n",
    "empty_d = {\"ts\": [], \"carb_effect\": []}\n",
    "whole_meal_effect_df = pd.DataFrame(data = empty_d)\n",
    "# Expand meal entries\n",
    "for index, meal_row in meal_df.iterrows():\n",
    "    meal_effect_df = expand_meal_entry(meal_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_meal_effect_df, meal_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['carb_effect_df1'] = merged_df['carb_effect_df1'].fillna(0)\n",
    "    merged_df['carb_effect_df2'] = merged_df['carb_effect_df2'].fillna(0)\n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['carb_effect'] = merged_df['carb_effect_df1'] + merged_df['carb_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_meal_effect_df = merged_df[['ts', 'carb_effect']]\n",
    "\n",
    "whole_meal_effect_df['assigned'] = False\n",
    "\n",
    "    # Update the segments with meal data\n",
    "meal_updated_segments = update_segments_with_meals(cleaned_segments, whole_meal_effect_df)\n",
    "# Update the segments with meal data\n",
    "# meal_updated_segments = update_segments_with_meals(cleaned_segments, meal_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_df['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_facm = pd.read_csv(f\"../FACM_split/854.csv\")\n",
    "# # Group by 'Category' column\n",
    "# grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "# split_dfs = {category: group for category, group in grouped}\n",
    "# # Step 1: Extract the desired columns\n",
    "# new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "# new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# new_df_basal.reset_index(drop=True, inplace=True)\n",
    "# new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "# new_df_basal['assigned'] = False\n",
    "# new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "# new_df_basal[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/FACM_split/854.csv\", False)\n",
    "\n",
    "empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "whole_bolus_effect_df = pd.DataFrame(data = empty_b)\n",
    "\n",
    "for index, bolus_row in new_df_bolus.iterrows():\n",
    "    bolus_effect_df = expand_bolus_entry(bolus_row)\n",
    "\n",
    "    # Merge the DataFrames on the 'ts' column with an outer join\n",
    "    merged_df = pd.merge(whole_bolus_effect_df, bolus_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # Fill NaN values with 0 for the carb_effect columns\n",
    "    merged_df['bolus_effect_df1'] = merged_df['bolus_effect_df1'].fillna(0)\n",
    "    merged_df['bolus_effect_df2'] = merged_df['bolus_effect_df2'].fillna(0)\n",
    "    \n",
    "\n",
    "    # Sum the carb_effect values\n",
    "    merged_df['bolus_effect'] = merged_df['bolus_effect_df1'] + merged_df['bolus_effect_df2']\n",
    "\n",
    "    # Keep only the required columns\n",
    "    whole_bolus_effect_df = merged_df[['ts', 'bolus_effect']]\n",
    "\n",
    "whole_bolus_effect_df[\"assigned\"] = False\n",
    "\n",
    "bolus_updated_segments = update_segments_with_bolus(meal_updated_segments, whole_bolus_effect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolus_updated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_step_processing(bolus_updated_segments, step_df):\n",
    "    # Convert step_df timestamps to datetime if they aren't already\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    step_df['stepvalue'] = pd.to_numeric(step_df['stepvalue'])\n",
    "    \n",
    "    # Pre-calculate weights for step accumulation (1 to 10 for 50 minutes window)\n",
    "    weights = np.arange(1, 11)\n",
    "    \n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        print(segment_name)  # Keep the progress print if needed\n",
    "        # Convert timestamps if needed\n",
    "        segment_df['timestamp'] = pd.to_datetime(segment_df['timestamp'])\n",
    "        \n",
    "        # Create array to store accumulated steps\n",
    "        accumulate_step_list = []\n",
    "        \n",
    "        # Get all unique window starts for this segment\n",
    "        window_starts = segment_df['timestamp'].apply(lambda x: x - timedelta(minutes=50))\n",
    "        window_ends = segment_df['timestamp']\n",
    "        \n",
    "        # Process each window\n",
    "        for start, end in zip(window_starts, window_ends):\n",
    "            # Filter steps within the window\n",
    "            mask = (step_df['ts'] >= start) & (step_df['ts'] < end)\n",
    "            window_steps = step_df.loc[mask, 'stepvalue']\n",
    "            \n",
    "            if len(window_steps) == 0:\n",
    "                accumulate_step_list.append(None)\n",
    "            else:\n",
    "                # Take last 10 steps (or pad with zeros if less than 10)\n",
    "                last_steps = window_steps.iloc[-10:] if len(window_steps) > 10 else window_steps\n",
    "                weighted_sum = (last_steps.values * weights[:len(last_steps)]).sum()\n",
    "                accumulate_step_list.append(weighted_sum / len(last_steps))\n",
    "        \n",
    "        # Assign accumulated steps to segment\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "    return bolus_updated_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Step count data\n",
    "subject_fa = pd.read_csv(f\"/Users/baiyinglu/Desktop/AugmentedHealthLab/T1DEXI_Apr52024/try/FA_split/854.csv\")\n",
    "grouped = subject_fa.groupby('FAOBJ')\n",
    "split_dfs = {category: group for category, group in grouped}\n",
    "new_df_step = split_dfs[\"10-SECOND INTERVAL STEP COUNT\"][[\"FAORRES\", \"FADTC\"]]\n",
    "step_df = new_df_step.rename(columns={'FAORRES': 'stepvalue', 'FADTC': 'ts'})\n",
    "step_df.reset_index(inplace=True)\n",
    "step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "\n",
    "# Process steps with optimized function\n",
    "step_updated_segments = optimize_step_processing(bolus_updated_segments, step_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulate_step_list = []\n",
    "# # test_segment = segments[\"segment_1\"]\n",
    "# for segment_name, segment_df in bolus_updated_segments.items():\n",
    "#     print(segment_name)\n",
    "#     accumulate_step_list = []\n",
    "#     for index, cgm_row in segment_df.iterrows():\n",
    "#         current = cgm_row['timestamp']\n",
    "#         first_timestamp = current - timedelta(minutes=50)\n",
    "#         window_list = pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "#         accumulated_step = compute_accumulated_step(window_list, step_df)\n",
    "#         accumulate_step_list.append(accumulated_step)\n",
    "#     segment_df['steps'] = accumulate_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "# Assuming features_list and raw_glu_list are already defined\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement on the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854.csv',\n",
    " '979.csv',\n",
    " '816.csv',\n",
    " '953.csv',\n",
    " '981.csv',\n",
    " '1617.csv',\n",
    " '1343.csv',\n",
    " '987.csv',\n",
    " '255.csv',\n",
    " '907.csv',\n",
    " '856.csv',\n",
    " '354.csv',\n",
    " '894.csv',\n",
    " '862.csv',\n",
    " '900.csv',\n",
    " '695.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm = pd.read_csv('../T1DEXI/cgm_valid.csv')\n",
    "grouped = cgm.groupby('LBCAT')\n",
    "split_dfs = {category: group for category, group in grouped}\n",
    "selected_cgm = split_dfs[\"CGM\"][[\"LBORRES\", \"LBDTC\"]]\n",
    "new_df_cgm = pd.DataFrame(selected_cgm)\n",
    "\n",
    "\n",
    "new_df_cgm['LBDTC'] = pd.to_datetime(new_df_cgm['LBDTC'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "new_df_cgm.sort_values('LBDTC', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "if round == True:\n",
    "    rounded_timestamp = []\n",
    "    for ts in new_df_cgm[\"LBDTC\"]:\n",
    "        rounded_timestamp.append(round_up_to_nearest_five_minutes(ts))\n",
    "    new_df_cgm[\"rounded_LBDTC\"] = rounded_timestamp\n",
    "    formatted_data = [[{'ts': row['rounded_LBDTC'], 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n",
    "\n",
    "else:\n",
    "    # Convert each row to the desired format\n",
    "    formatted_data = [[{'ts': row['LBDTC'].to_pydatetime(), 'value': row['LBORRES']}] for _, row in new_df_cgm.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_dict = formatted_data\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")\n",
    "\n",
    "glucose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ffile in overlap[:-1]:\n",
    "    print(ffile)\n",
    "    subject = pd.read_csv(f\"../LB_split/{ffile}\")\n",
    "    glucose = preprocess_t1dexi_cgm(f\"../LB_split/{ffile}\", False)\n",
    "    glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "    # Create the multi-channel database\n",
    "    g_data = []\n",
    "    for timestamp in glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': glucose_dict[timestamp],\n",
    "            # 'meal_type': None,\n",
    "            # 'meal_carbs': 0\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "    segments = segement_data_as_15min(glucose_df)\n",
    "\n",
    "    meal = pd.read_csv(f\"../ML_split/{ffile}\")\n",
    "    selected_meal_column = meal[[\"MLDOSE\", \"MLDTC\"]]\n",
    "\n",
    "    meal_df = selected_meal_column.rename(columns={'MLDOSE': 'carbs', 'MLDTC': 'ts'})\n",
    "    meal_df['ts'] = pd.to_datetime(meal_df['ts'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    meal_df['assigned'] = False\n",
    "\n",
    "    # Extract unique dates\n",
    "    unique_dates = meal_df['ts'].dt.date.unique()\n",
    "\n",
    "    # Convert to list\n",
    "    meal_avaiable_dates_list = unique_dates.tolist()\n",
    "\n",
    "    cleaned_segments = {}\n",
    "\n",
    "    # Iterate through each segment and filter by unique dates\n",
    "    for segment_name, df in segments.items():\n",
    "        # Convert timestamp column to datetime and then extract the date part\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        \n",
    "        # Filter the DataFrame to only include rows where the date is in unique_dates_list\n",
    "        filtered_df = df[df['date'].isin(meal_avaiable_dates_list)]\n",
    "        \n",
    "        # Drop the 'date' column as it's no longer needed\n",
    "        filtered_df = filtered_df.drop(columns=['date'])\n",
    "        \n",
    "        # Store the filtered DataFrame in the cleaned_segments dictionary\n",
    "        cleaned_segments[segment_name] = filtered_df\n",
    "\n",
    "    # Expand meal entries\n",
    "    for index, meal_row in meal_df.iterrows():\n",
    "        meal_effect_df = expand_meal_entry(meal_row)\n",
    "\n",
    "        # Merge the DataFrames on the 'ts' column with an outer join\n",
    "        merged_df = pd.merge(whole_meal_effect_df, meal_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "        # Fill NaN values with 0 for the carb_effect columns\n",
    "        merged_df['carb_effect_df1'] = merged_df['carb_effect_df1'].fillna(0)\n",
    "        merged_df['carb_effect_df2'] = merged_df['carb_effect_df2'].fillna(0)\n",
    "\n",
    "        # Sum the carb_effect values\n",
    "        merged_df['carb_effect'] = merged_df['carb_effect_df1'] + merged_df['carb_effect_df2']\n",
    "\n",
    "        # Keep only the required columns\n",
    "        whole_meal_effect_df = merged_df[['ts', 'carb_effect']]\n",
    "\n",
    "    whole_meal_effect_df['assigned'] = False\n",
    "\n",
    "    # Update the segments with meal data\n",
    "    meal_updated_segments = update_segments_with_meals(cleaned_segments, whole_meal_effect_df)\n",
    "\n",
    "    subject_facm = pd.read_csv(f\"../FACM_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_facm.groupby('FACAT')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_basal = split_dfs[\"BASAL\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    new_df_basal['FADTC'] = pd.to_datetime(new_df_basal['FADTC'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_df_basal.reset_index(drop=True, inplace=True)\n",
    "    new_df_basal = new_df_basal.rename(columns={'FAORRES': 'value', 'FADTC': 'ts'})\n",
    "    new_df_basal['assigned'] = False\n",
    "    new_df_basal['end_ts'] = new_df_basal['ts'].shift(-1)\n",
    "    \n",
    "    basal_updated_segments = update_segments_with_basal(meal_updated_segments, new_df_basal)\n",
    "\n",
    "    new_df_bolus = preprocess_t1dexi_bolus_tempbasal(f\"../FACM_split/{ffile}\", False)\n",
    "\n",
    "\n",
    "    empty_b = {\"ts\": [], \"bolus_effect\": []}\n",
    "    whole_bolus_effect_df = pd.DataFrame(data = empty_b)\n",
    "\n",
    "    for index, bolus_row in new_df_bolus.iterrows():\n",
    "        bolus_effect_df = expand_bolus_entry(bolus_row)\n",
    "\n",
    "        # Merge the DataFrames on the 'ts' column with an outer join\n",
    "        merged_df = pd.merge(whole_bolus_effect_df, bolus_effect_df, on='ts', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "        # Fill NaN values with 0 for the carb_effect columns\n",
    "        merged_df['bolus_effect_df1'] = merged_df['bolus_effect_df1'].fillna(0)\n",
    "        merged_df['bolus_effect_df2'] = merged_df['bolus_effect_df2'].fillna(0)\n",
    "        \n",
    "\n",
    "        # Sum the carb_effect values\n",
    "        merged_df['bolus_effect'] = merged_df['bolus_effect_df1'] + merged_df['bolus_effect_df2']\n",
    "\n",
    "        # Keep only the required columns\n",
    "        whole_bolus_effect_df = merged_df[['ts', 'bolus_effect']]\n",
    "\n",
    "    whole_bolus_effect_df[\"assigned\"] = False\n",
    "\n",
    "    bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, whole_bolus_effect_df)\n",
    "\n",
    "    # bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, new_df_bolus)\n",
    "    # Steps\n",
    "    # Read in the Step count data\n",
    "    subject_fa = pd.read_csv(f\"../FA_split/{ffile}\")\n",
    "    # Group by 'Category' column\n",
    "    grouped = subject_fa.groupby('FAOBJ')\n",
    "\n",
    "    split_dfs = {category: group for category, group in grouped}\n",
    "    # Step 1: Extract the desired columns\n",
    "    new_df_step = split_dfs[\"10-SECOND INTERVAL STEP COUNT\"][[\"FAORRES\", \"FADTC\"]]\n",
    "    step_df = new_df_step.rename(columns={'FAORRES': 'stepvalue', 'FADTC': 'ts'})\n",
    "    step_df.reset_index(inplace=True)\n",
    "    # for i in range(len(step_df)):\n",
    "    #     step_ts = datetime.strptime(step_df['ts'][i], \"%Y-%m-%d %H:%M:%S\")\n",
    "    #     step_df['ts'][i] = step_ts\n",
    "    step_df['ts'] = pd.to_datetime(step_df['ts'])\n",
    "    accumulate_step_list = []\n",
    "    # test_segment = segments[\"segment_1\"]\n",
    "    for segment_name, segment_df in bolus_updated_segments.items():\n",
    "        print(segment_name)\n",
    "        accumulate_step_list = []\n",
    "        for index, cgm_row in segment_df.iterrows():\n",
    "            current = cgm_row['timestamp']\n",
    "            first_timestamp = current - timedelta(minutes=50)\n",
    "            window_list = pd.date_range(start=first_timestamp, end=current, freq='5min')\n",
    "\n",
    "            accumulated_step = compute_accumulated_step(window_list, step_df)\n",
    "            accumulate_step_list.append(accumulated_step)\n",
    "        segment_df['steps'] = accumulate_step_list\n",
    "    \n",
    "\n",
    "    features_list, raw_glu_list = prepare_dataset(bolus_updated_segments, ph)\n",
    "    # Assuming features_list and raw_glu_list are already defined\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(raw_glu_list)\n",
    "\n",
    "    # Step 1: Split into 80% train+val and 20% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Convert the splits to torch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "    num_epochs =500\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(val_loader)\n",
    "            print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on validation set: {rmse}')\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on validation set: {rmse}')\n",
    "    new_test_rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
