{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(segments, ph, history):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        # segment_df['carb_effect'] = pd.to_numeric(segment_df['carb_effect'], errors='coerce')\n",
    "        # segment_df['steps'] = pd.to_numeric(segment_df['steps'], errors='coerce')\n",
    "        # segment_df['steps'] = segment_df['steps'] \n",
    "        segment_df['bolus_effect'] = pd.to_numeric(segment_df['bolus_effect'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        max_index = len(segment_df) - (history-1+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + history-1 to exist\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index + 1):\n",
    "            # Extracting features from index i to i+history-1\n",
    "            features = segment_df.loc[i:i+history-1, ['glucose_value',  'bolus_effect']] # .values.flatten() # 'carb_effect', 'bolus_effect', 'steps'\n",
    "            # Extracting label for index i+21\n",
    "            # Do the label transform\n",
    "            label = segment_df.loc[i+history-1+ph, 'glucose_value'] - segment_df.loc[i+history-1, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history-1+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "    return features_list, labels_list, raw_glu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob).to(device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True).to(device)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)\n",
    "        self.fc2 = nn.Linear(512, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_size).to(device)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from the input tensor\n",
    "\n",
    "        # Initialize hidden and cell state for the first LSTM layer\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        out, (hn, cn) = self.lstm1(x, (h0, c0))\n",
    "        \n",
    "        # Dropout layer\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Initialize hidden and cell state for the second LSTM layer\n",
    "        h1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        c1 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        out, (hn, cn) = self.lstm2(out, (h1, c1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = out[:, -1, :]  # Get the last time step output\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "filename = \"./processed_data/BIG_training_data_onlyCGM.pkl\"\n",
    "# Load the dictionary from the file\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_df_dict = pickle.load(f)\n",
    "\n",
    "# Verify the content\n",
    "\n",
    "\n",
    "print(loaded_df_dict['1segment_1'])\n",
    "print(loaded_df_dict['1segment_2'])\n",
    "step_updated_segments = loaded_df_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "features_list, labels_list, raw_glu_list = prepare_dataset(step_updated_segments, 6, 6)\n",
    "\n",
    "# Build training and validation loader\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(raw_glu_list) # Maybe need to replace this\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_array, labels_array, test_size=0.2, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # The original batch size = 128, however, training on 128 cannot get the model fully trained, so change to 32.\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # The original batch size = 128, however, training on 128 cannot get the model fully trained, so change to 32.\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2 # Number of input features\n",
    "hidden_size = 128  # Hidden vector size\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single output\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model \n",
    "torch.save(model.state_dict(), './processed_data/stacked_lstm_model_cgm_only_6_6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[:700], label = 'predictions')\n",
    "plt.plot(actuals[:700], label = 'actuals')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_step_updated_segments):\n",
    "\n",
    "    # Prepare for training\n",
    "    features_list_test, labels_list_test, raw_glu_list_test = prepare_dataset(test_step_updated_segments, 6, 6)\n",
    "    \n",
    "    # Build training and validation loader\n",
    "    features_array_test = np.array(features_list_test)\n",
    "    labels_array_test = np.array(raw_glu_list_test) # Maybe need to replace this\n",
    "\n",
    "    X_test, y_test = features_array_test, labels_array_test\n",
    "\n",
    "    # Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "    rmse = root_mean_squared_error(actuals,predictions)\n",
    "    print(f'RMSE on validation set: {rmse}')\n",
    "    \n",
    "    return predictions, actuals, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from .pth\n",
    "model_dir = \"./processed_data/stacked_lstm_model_cgm_only_6_6.pth\"\n",
    "input_size = 2 # Number of input features\n",
    "hidden_size = 128  # Hidden vector size\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single output\n",
    "dropout_prob = 0.2  # Dropout probability\n",
    "\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob) # input_size, hidden_size, num_layers, output_size, dropout_prob\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "model.load_state_dict(torch.load(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "errors = []\n",
    "fname = []\n",
    "\n",
    "for file in glob.glob(\"./processed_data/*test*ONLY*\"):\n",
    "    test_filename = file\n",
    "    with open(test_filename, 'rb') as f:\n",
    "        test_loaded_df_dict = pickle.load(f)\n",
    "\n",
    "    # Verify the content\n",
    "    test_step_updated_segments = test_loaded_df_dict\n",
    "    pred, true, rmse = test_model(model, test_step_updated_segments)\n",
    "    preds.append(pred)\n",
    "    trues.append(true)\n",
    "    errors.append(rmse)\n",
    "    fname.append(file.split('/')[-1].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 3x 2 plot and plot the preds and actuals of each one in each and have the title be the filename and the rmse\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        idx = i*2 + j\n",
    "        axs[i, j].plot(preds[idx], label = 'predictions')\n",
    "        axs[i, j].plot(trues[idx], label = 'actuals')\n",
    "        axs[i, j].legend()\n",
    "        axs[i, j].set_title(f'{fname[idx]} RMSE: {errors[idx]:.4f}')\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this all to a table \n",
    "# save the table as a csv\n",
    "# convert the fname and the rmse to a dataframe\n",
    "pd.DataFrame({'fname': fname, 'rmse': errors}).to_csv('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old results\n",
    "data = [\n",
    "    {'fname': 559, 'rmse': 28.529722},\n",
    "    {'fname': 591, 'rmse': 27.535217},\n",
    "    {'fname': 570, 'rmse': 25.205898},\n",
    "    {'fname': 563, 'rmse': 23.448462},\n",
    "    {'fname': 588, 'rmse': 22.671024},\n",
    "    {'fname': 575, 'rmse': 30.21184}\n",
    "]\n",
    "\n",
    "# plot new data and old data\n",
    "old_data = pd.DataFrame(data)\n",
    "new_data = pd.DataFrame({'fname': fname, 'rmse': errors})\n",
    "\n",
    "old_data.fname = old_data.fname.astype(str)\n",
    "new_data.fname = new_data.fname.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dat = old_data.merge(new_data, on='fname', suffixes=('_all', '_only_cgm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these are bar plots \n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "n_dat.plot.bar(x='fname', y=['rmse_all', 'rmse_only_cgm'], ax=ax)\n",
    "plt.title('Comparison of RMSE between all features and only CGM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this all to a table \n",
    "# save the table as a csv\n",
    "# convert the fname and the rmse to a dataframe\n",
    "pd.DataFrame({'fname': fname, 'rmse': errors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "features_list_test, labels_list_test, raw_glu_list_test = prepare_dataset(test_step_updated_segments, 6, 6)\n",
    "\n",
    "# Build training and validation loader\n",
    "features_array_test = np.array(features_list_test)\n",
    "labels_array_test = np.array(raw_glu_list_test) # Maybe need to replace this\n",
    "\n",
    "X_test, y_test = features_array_test, labels_array_test\n",
    "\n",
    "# Data Preparation (assuming X_train, y_train, X_val, y_val are numpy arrays)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(targets)\n",
    "\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(actuals,predictions)\n",
    "print(f'RMSE on validation set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[:700])\n",
    "plt.plot(actuals[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
