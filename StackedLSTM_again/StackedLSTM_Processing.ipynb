{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "filepath = f\"../OhioT1DM/2018/train/559-ws-training.xml\"\n",
    "\n",
    "# repeat the same process for the test dataset\n",
    "glucose = read_ohio(filepath, \"glucose_level\", True)\n",
    "glucose_df = transfer_into_table(glucose)\n",
    "segments = segement_data_as_15min(glucose_df)\n",
    "meal = add_meal_segments(filepath)\n",
    "bolus = add_bolus_segments(filepath, meal)\n",
    "\n",
    "steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "flattened_steps_data = [item[0] for item in steps]\n",
    "step_df = pd.DataFrame(flattened_steps_data)\n",
    "step_updated_segments = optimize_step_processing(bolus, step_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def process_all_training_files(directory_path):\n",
    "    # Get all XML files in the directory\n",
    "    xml_files = glob.glob(os.path.join(directory_path, \"*-ws-training.xml\"))\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for filepath in xml_files:\n",
    "        try:\n",
    "            # Process each file\n",
    "            glucose = read_ohio(filepath, \"glucose_level\", True)\n",
    "            glucose_df = transfer_into_table(glucose)\n",
    "            segments = segement_data_as_15min(glucose_df)\n",
    "            # meal = add_meal_segments(filepath)\n",
    "            bolus = add_bolus_segments(filepath, segments)\n",
    "\n",
    "            # steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "            # flattened_steps_data = [item[0] for item in steps]\n",
    "            # step_df = pd.DataFrame(flattened_steps_data)\n",
    "            # step_updated_segments = optimize_step_processing(bolus, step_df)\n",
    "            \n",
    "            # Add to list of processed data\n",
    "            all_processed_data.append({\n",
    "                'filepath': filepath,\n",
    "                'segments': bolus\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully processed {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {str(e)}\")\n",
    "    \n",
    "    return all_processed_data\n",
    "\n",
    "# Usage\n",
    "directory_path = \"../OhioT1DM/2018/train/\"\n",
    "training_data = process_all_training_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary such that the key is the segment number + i and the value is the segment data\n",
    "segment_dict = {}\n",
    "count = 0\n",
    "\n",
    "segment_name_list = []\n",
    "segment_data_list= []\n",
    "for i in training_data: \n",
    "    count += 1\n",
    "    for j in i['segments']:\n",
    "        segment_dict[str(count)+j] = i['segments'][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the processed data CAREFULL!!!\n",
    "# # Specify the file name\n",
    "# filename = './processed_data/BIG_training_data_onlyCGM.pkl'\n",
    "# # Save the dictionary to a file\n",
    "# if not os.path.exists(filename):\n",
    "#     open(filename, 'wb').close()\n",
    "# # Save the dictionary to a file\n",
    "# with open(filename, 'wb') as f:\n",
    "#     pickle.dump(segment_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "dirpath = f\"../OhioT1DM/2018/test/\"\n",
    "\n",
    "for filename in os.listdir(dirpath):\n",
    "    filepath = os.path.join(dirpath,filename)\n",
    "\n",
    "    # repeat the same process for the test dataset\n",
    "    glucose = read_ohio(filepath, \"glucose_level\", True)\n",
    "    glucose_df = transfer_into_table(glucose)\n",
    "\n",
    "\n",
    "    segments = segement_data_as_15min(glucose_df)\n",
    "    # meal = add_meal_segments(filepath)\n",
    "    bolus = add_bolus_segments(filepath, segments)\n",
    "\n",
    "    # steps = read_ohio(filepath, \"basis_steps\", True)\n",
    "    # flattened_steps_data = [item[0] for item in steps]\n",
    "    # step_df = pd.DataFrame(flattened_steps_data)\n",
    "    # # step_updated_segments = optimize_step_processing(bolus, step_df)\n",
    "    # filename = './processed_data/{}_test_combined_segments_noshrink_ONLYCGM.pkl'.format(filename.split('-')[0])\n",
    "\n",
    "    # # Save the dictionary to a file\n",
    "    # if not os.path.exists(filename):\n",
    "    #     open(filename, 'wb').close()\n",
    "\n",
    "    # # Save the dictionary to the file\n",
    "    # with open(filename, 'wb') as f:\n",
    "    #     pickle.dump(bolus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data CAREFULL!!!\n",
    "# Specify the file name\n",
    "import os\n",
    "\n",
    "filename = './processed_data/559_test_combined_segments_noshrink.pkl'\n",
    "\n",
    "# Save the dictionary to a file\n",
    "if not os.path.exists(filename):\n",
    "    open(filename, 'wb').close()\n",
    "\n",
    "# Save the dictionary to the file\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(step_updated_segments, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
