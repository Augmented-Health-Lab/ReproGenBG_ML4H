{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "from GlucNet_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../OhioT1DM/2018/train/559-ws-training.xml\"\n",
    "tree = ET.parse(filepath)\n",
    "root = tree.getroot()\n",
    "glucose = read_ohio(filepath, \"glucose_level\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in glucose}\n",
    "\n",
    "# Create the multi-channel database\n",
    "g_data = []\n",
    "for timestamp in glucose_dict:\n",
    "    record = {\n",
    "        'timestamp': timestamp,\n",
    "        'glucose_value': glucose_dict[timestamp],\n",
    "        # 'meal_type': None,\n",
    "        # 'meal_carbs': 0\n",
    "    }\n",
    "    \n",
    "    g_data.append(record)\n",
    "glucose_df = pd.DataFrame(g_data)\n",
    "\n",
    "# Convert glucose values to numeric type for analysis\n",
    "glucose_df['glucose_value'] = pd.to_numeric(glucose_df['glucose_value'])\n",
    "\n",
    "# Calculate percentiles\n",
    "lower_percentile = np.percentile(glucose_df['glucose_value'], 2)\n",
    "upper_percentile = np.percentile(glucose_df['glucose_value'], 98)\n",
    "\n",
    "# Print thresholds\n",
    "print(f\"2% lower threshold: {lower_percentile}\")\n",
    "print(f\"98% upper threshold: {upper_percentile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2, P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segement_data_as_1hour(glucose_df)\n",
    "interpolated_segements = detect_missing_and_spline_interpolate(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal = read_ohio(filepath, \"meal\", False)\n",
    "\n",
    "flattened_meal_data = [item[0] for item in meal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "meal_df = pd.DataFrame(flattened_meal_data)\n",
    "\n",
    "meal_df['assigned'] = False\n",
    "\n",
    "meal_updated_segments = update_segments_with_meals(interpolated_segements, meal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basal = read_ohio(filepath, \"basal\", False)\n",
    "\n",
    "flattened_basal_data = [item[0] for item in basal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# Convert to DataFrame\n",
    "basal_df = pd.DataFrame(flattened_basal_data)\n",
    "\n",
    "basal_df['assigned'] = False\n",
    "basal_df['end_ts'] = basal_df['ts'].shift(-1)\n",
    "basal_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basal_updated_segments = update_segments_with_basal(meal_updated_segments, basal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge Bolus into the dataframe\n",
    "# bolus = read_ohio_bolus_tempbasal(filepath, \"bolus\", False)\n",
    "\n",
    "# flattened_bolus_data = [item[0] for item in bolus]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# bolus_df = pd.DataFrame(flattened_bolus_data)\n",
    "\n",
    "# bolus_df['assigned'] = False\n",
    "# bolus_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bolus_updated_segments = update_segments_with_bolus(basal_updated_segments, bolus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempbasal = read_ohio_bolus_tempbasal(filepath, \"temp_basal\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_tempbasal_data = [item[0] for item in tempbasal]  # Take the first (and only) item from each sublist\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# tempbasal_df = pd.DataFrame(flattened_tempbasal_data)\n",
    "\n",
    "# tempbasal_df['assigned'] = False\n",
    "# tempbasal_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Update the segments with meal data\n",
    "# final_updated_segments = update_segments_with_tempbasal(bolus_updated_segments, tempbasal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_updated_segments['segment_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_delta_transform(labels_list):\n",
    "    # label_lower_percentile = -12.75\n",
    "    # label_upper_percentile = 12.85\n",
    "    label_lower_percentile = np.percentile(labels_list, 10)\n",
    "    label_upper_percentile = np.percentile(labels_list, 90)\n",
    "    transformed_labels = []\n",
    "    for label in labels_list:\n",
    "        if label <= label_lower_percentile:\n",
    "            transformed_labels.append(1)\n",
    "        elif label_lower_percentile < label < label_upper_percentile:\n",
    "            trans_label = round((256/(label_upper_percentile - label_lower_percentile))*(label + abs(label_lower_percentile) + 0.05))\n",
    "            transformed_labels.append(trans_label)\n",
    "        elif label >= label_upper_percentile:\n",
    "            transformed_labels.append(256)\n",
    "    return transformed_labels\n",
    "\n",
    "\n",
    "# def prepare_dataset(segments, ph):\n",
    "#     '''\n",
    "#     ph = 6, 30 minutes ahead\n",
    "#     ph = 12, 60 minutes ahead\n",
    "#     '''\n",
    "#     features_list = []\n",
    "#     labels_list = []\n",
    "#     raw_glu_list = []\n",
    "    \n",
    "#     # Iterate over each segment\n",
    "#     for segment_name, segment_df in segments.items():\n",
    "#         # Ensure all columns are of numeric type\n",
    "#         segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "#         segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "#         segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "#         # Fill NaNs that might have been introduced by conversion errors\n",
    "#         segment_df.fillna(0, inplace=True)\n",
    "\n",
    "#         # Maximum index for creating a complete feature set\n",
    "#         max_index = len(segment_df) - (15+ph+1)  # Subtracting 22 because we need to predict index + 21 and need index + 15 to exist\n",
    "        \n",
    "#         # Iterate through the data to create feature-label pairs\n",
    "#         for i in range(max_index + 1):\n",
    "#             # Extracting features from index i to i+15\n",
    "#             features = segment_df.loc[i:i+15, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values#.flatten()\n",
    "#             # Extracting label for index i+21\n",
    "#             # Do the label transform\n",
    "#             label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "#             raw_glu_list.append(segment_df.loc[i+15+ph, 'glucose_value'])\n",
    "#             features_list.append(features)\n",
    "#             labels_list.append(label)\n",
    "            \n",
    "#     print(\"len of features_list \" + str(len(features_list)))\n",
    "#     print(\"len of labels_list \" + str(len(labels_list)))\n",
    "#     new_labels_list = label_delta_transform(labels_list)    \n",
    "#     print(\"after label transform. the len of label list \"+str(len(new_labels_list)))    \n",
    "#     return features_list, labels_list, new_labels_list, raw_glu_list\n",
    "\n",
    "def prepare_dataset(segments, ph, history_len = 15):\n",
    "    '''\n",
    "    ph = 6, 30 minutes ahead\n",
    "    ph = 12, 60 minutes ahead\n",
    "    '''\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    raw_glu_list = []\n",
    "    \n",
    "    \n",
    "    # Iterate over each segment\n",
    "    for segment_name, segment_df in segments.items():\n",
    "        # Ensure all columns are of numeric type\n",
    "        segment_df['carbs'] = pd.to_numeric(segment_df['carbs'], errors='coerce')\n",
    "        segment_df['basal_rate'] = pd.to_numeric(segment_df['basal_rate'], errors='coerce')\n",
    "        segment_df['bolus_dose'] = pd.to_numeric(segment_df['bolus_dose'], errors='coerce')\n",
    "\n",
    "        # Fill NaNs that might have been introduced by conversion errors\n",
    "        segment_df.fillna(0, inplace=True)\n",
    "\n",
    "        # Maximum index for creating a complete feature set\n",
    "        print(\"len of segment_df is \", len(segment_df))\n",
    "        max_index = len(segment_df) - (history_len + ph)  # Subtracting only 15+ph to ensure i + 15 + ph is within bounds\n",
    "        \n",
    "        # Iterate through the data to create feature-label pairs\n",
    "        for i in range(max_index):\n",
    "            # Extracting features from index i to i+15\n",
    "            segment_df = segment_df.reset_index(drop = True)\n",
    "            features = segment_df.loc[i:i+history_len, ['glucose_value', 'carbs', 'basal_rate', 'bolus_dose']].values\n",
    "            # Extracting label for index i+15+ph\n",
    "            # label = segment_df.loc[i+15+ph, 'glucose_value'] - segment_df.loc[i+15, 'glucose_value']\n",
    "            \n",
    "            raw_glu_list.append(segment_df.loc[i+history_len+ph, 'glucose_value'])\n",
    "            features_list.append(features)\n",
    "            # labels_list.append(label)\n",
    "            \n",
    "    print(\"len of features_list \" + str(len(features_list)))\n",
    "    # print(\"len of labels_list \" + str(len(labels_list)))\n",
    "    \n",
    "    # new_labels_list = label_delta_transform(labels_list)    \n",
    "    # print(\"after label transform, the len of label list \"+str(len(new_labels_list)))    \n",
    "    \n",
    "    return features_list, raw_glu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data from the .pkl file\n",
    "import pickle\n",
    "file_num = 591\n",
    "PH = 6\n",
    "HISTORY = 15\n",
    "with open(f'./{file_num}_train_combined_segments_wavenet.pkl', 'rb') as f:\n",
    "    final_updated_segments= pickle.load(f)\n",
    "\n",
    "\n",
    "features_list, labels_list = prepare_dataset(final_updated_segments, PH, HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# Step 1: Split into 80% train+val and 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features_array, labels_array, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 2: Split the 80% into 70% train and 10% val (0.7/0.8 = 0.875)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Convert the splits to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to PyTorch tensors\n",
    "features_tensor = torch.tensor(features_list, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_list, dtype=torch.float32).unsqueeze(1)  # Making labels tensor 2D\n",
    "\n",
    "feature_label_tensor = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(feature_label_tensor, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example of using DataLoader in a training loop\n",
    "for features, labels in train_loader:\n",
    "    print(\"Features batch shape:\", features.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    # Example: print(features, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dilate CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation):\n",
    "        super(WaveNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=1+dilation - 2^(dilation-1))\n",
    "        self.conv2 = nn.Conv1d(in_channels, in_channels, kernel_size=2, dilation=dilation, padding=dilation)\n",
    "        self.res_conv = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape of x: \", x.shape)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        # print(\"shape of first out: \", out.shape)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        # print(\"shape of second out: \", out.shape)\n",
    "        res = self.res_conv(x)\n",
    "        # print(\"shape of res: \", res.shape)\n",
    "        return out + res\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks, dilations):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv1d(in_channels, 32, kernel_size=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([WaveNetBlock(32, dilation) for dilation in dilations])\n",
    "        self.final_conv1 = nn.Conv1d(32, 128, kernel_size=2, padding=0)\n",
    "        self.final_conv2 = nn.Conv1d(128, 256, kernel_size=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        for block in self.blocks:\n",
    "            # print(\"enter the block loop\")\n",
    "            x = block(x)\n",
    "        x = F.relu(self.final_conv1(x))\n",
    "        x = F.relu(self.final_conv2(x))\n",
    "        x = x[:, :, -1]  # Get the last time step\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_channels = 4  # Number of features\n",
    "output_channels = 1  # Predicting a single value (glucose level)\n",
    "num_blocks = 4  # Number of WaveNet blocks\n",
    "dilations = [2**i for i in range(num_blocks)]  # Dilation rates: 1, 2, 4, 8\n",
    "\n",
    "model = WaveNet(input_channels, output_channels, num_blocks, dilations)\n",
    "\n",
    "# Example of how to define the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(\"Input tensor shape:\", inputs.shape)\n",
    "    print(\"Input tensor total elements:\", inputs.numel())\n",
    "    print(\"Target tensor shape:\", targets.shape)\n",
    "    print(\"Sequence length:\", inputs.shape[1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "        outputs = outputs.squeeze()  # Remove extra dimensions if present\n",
    "        targets = targets.squeeze()  # Remove extra dimensions if present\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))  # Permute to match (batch, channels, seq_len)\n",
    "            outputs = outputs.squeeze()  # Remove extra dimensions if present\n",
    "            targets = targets.squeeze()  # Remove extra dimensions if present\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model, f'./glucnet_model_{file_num}_{PH}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_test_dataset( file_dir = f'../OhioT1DM/2018/test/559-ws-testing.xml', ph = 6):\n",
    "        # test data\n",
    "    g_data = []\n",
    "\n",
    "    print(\"file_dir is \", file_dir)\n",
    "    test_file_path = file_dir \n",
    "    test_glucose = read_ohio(test_file_path, \"glucose_level\", False)\n",
    "    test_glucose_dict = {entry[0]['ts']: entry[0]['value'] for entry in test_glucose}\n",
    "\n",
    "    for timestamp in test_glucose_dict:\n",
    "        record = {\n",
    "            'timestamp': timestamp,\n",
    "            'glucose_value': test_glucose_dict[timestamp],\n",
    "        }\n",
    "        \n",
    "        g_data.append(record)\n",
    "    test_glucose_df = pd.DataFrame(g_data)\n",
    "    test_glucose_df['glucose_value'] = pd.to_numeric(test_glucose_df['glucose_value'])\n",
    "\n",
    "    test_segmebts = segement_data_as_1hour(test_glucose_df)\n",
    "    test_interpolated_segments = detect_missing_and_spline_interpolate(test_segmebts)\n",
    "\n",
    "    test_meal = read_ohio(test_file_path, \"meal\", False)\n",
    "    flattened_test_meal_data = [item[0] for item in test_meal]  # Take the first (and only) item from each sublist\n",
    "    test_meal_df = pd.DataFrame(flattened_test_meal_data)\n",
    "    test_meal_df['assigned'] = False\n",
    "    test_meal_updated_segments = update_segments_with_meals(test_interpolated_segments, test_meal_df)\n",
    "\n",
    "    test_basal = read_ohio(test_file_path, \"basal\", False)\n",
    "    flattened_test_basal_data = [item[0] for item in test_basal]  # Take the first (and only) item from each sublist\n",
    "    test_basal_df = pd.DataFrame(flattened_test_basal_data)\n",
    "    test_basal_df['assigned'] = False\n",
    "    test_basal_df['end_ts'] = test_basal_df['ts'].shift(-1)\n",
    "    test_basal_updated_segments = update_segments_with_basal(test_meal_updated_segments, test_basal_df)\n",
    "\n",
    "    test_bolus = read_ohio_bolus_tempbasal(test_file_path, \"bolus\", False)\n",
    "    flattened_test_bolus_data = [item[0] for item in test_bolus]  # Take the first (and only) item from each sublist\n",
    "    test_bolus_df = pd.DataFrame(flattened_test_bolus_data)\n",
    "    test_bolus_df['assigned'] = False\n",
    "    test_bolus_updated_segments = update_segments_with_bolus(test_basal_updated_segments, test_bolus_df)\n",
    "\n",
    "    test_tempbasal = read_ohio_bolus_tempbasal(test_file_path, \"temp_basal\", False)\n",
    "    flattened_test_tempbasal_data = [item[0] for item in test_tempbasal]  # Take the first (and only) item from each sublist\n",
    "    test_tempbasal_df = pd.DataFrame(flattened_test_tempbasal_data)\n",
    "    test_tempbasal_df['assigned'] = False\n",
    "\n",
    "    test_final_updated_segments = update_segments_with_tempbasal(test_bolus_updated_segments, test_tempbasal_df)\n",
    "    test_features_list, test_labels_list = prepare_dataset(test_final_updated_segments, ph)\n",
    "\n",
    "    test_features_array = np.array(test_features_list)\n",
    "    test_labels_array = np.array(test_labels_list)\n",
    "\n",
    "    X_test, y_test = torch.tensor(test_features_array, dtype=torch.float32), torch.tensor(test_labels_array, dtype=torch.float32)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "errors = []\n",
    "fname = []\n",
    "\n",
    "for file in glob.glob(\"../OhioT1DM/2018/test/*.xml\"):\n",
    "    test_filename = file\n",
    "    test_loader = prepare_dataset_test_dataset(test_filename, PH)\n",
    "    # Verify the content\n",
    "    # Calculate RMSE on test set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))\n",
    "            predictions.append(outputs)\n",
    "            actuals.append(targets)\n",
    "            \n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    actuals = torch.cat(actuals).cpu().numpy()\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actuals,predictions))\n",
    "    print(f'RMSE on test set: {rmse}')\n",
    "\n",
    "\n",
    "\n",
    "    preds.append(predictions)\n",
    "    trues.append(actuals)\n",
    "    errors.append(rmse)\n",
    "    fname.append(test_filename.split('-ws')[0][-3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dat = pd.DataFrame({'fname': fname, 'rmse': errors})\n",
    "curr_dat.to_csv(f'wavenet_ph_{PH}_{file_num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trues[0][:100], label='Actual')\n",
    "plt.plot(preds[0][:100], label='Predicted')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the data from the csv files\n",
    "df_list = []\n",
    "for file in glob.glob('./wavenet_ph_12*.csv'):\n",
    "    temp = pd.read_csv(file)\n",
    "    temp['trained on'] = file.split('_')[-1].split('.')[0]\n",
    "    df_list.append(temp)\n",
    "# combine all the data into one dataframe\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "# pivot on trained on \n",
    "df = df.pivot(index='fname', columns='trained on', values='rmse') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average of the columns\n",
    "df['mean'] = df.mean(axis=0)\n",
    "\n",
    "df['559'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
